# Structure Group: src/310_report.rs

## File: src/310_report.rs

- Layer(s): 310_report.rs
- Language coverage: Rust (18)
- Element types: Enum (4), Impl (4), Struct (10)
- Total elements: 18

### Elements

- [Rust | Enum] `ActionKind` (line 0, pub)
- [Rust | Struct] `ClusterMember` (line 0, pub)
  - Signature: `# [derive (Clone)] pub struct ClusterMember { pub file : PathBuf , pub name : String , }`
- [Rust | Struct] `ClusterPlan` (line 0, pub)
  - Signature: `# [derive (Clone)] pub struct ClusterPlan { pub target : PathBuf , pub cohesion : f64 , pub members : Vec < ClusterMe...`
- [Rust | Struct] `ModuleAggregate` (line 0, priv)
  - Signature: `struct ModuleAggregate { name : String , layer : String , imports : BTreeSet < String > , exports : BTreeSet < String...`
- [Rust | Struct] `ModuleIssue` (line 0, priv)
  - Signature: `# [derive (Clone)] struct ModuleIssue { parent_file : PathBuf , module : String , line : usize , details : Vec < Path...`
- [Rust | Enum] `ModuleIssueKind` (line 0, priv)
- [Rust | Struct] `PlanItem` (line 0, pub)
  - Signature: `# [derive (Clone)] pub struct PlanItem { pub kind : ActionKind , pub priority : Priority , pub description : String ,...`
- [Rust | Enum] `Priority` (line 0, pub)
- [Rust | Struct] `ReportConfig` (line 0, pub)
  - Signature: `# [derive (Clone , Debug)] pub struct ReportConfig { pub file_line_warning : usize , pub dir_file_warning : usize , p...`
- [Rust | Struct] `ReportGenerator` (line 0, pub)
  - Signature: `pub struct ReportGenerator { output_dir : String , }`
- [Rust | Enum] `TestIssueKind` (line 0, priv)
- [Rust | Struct] `TestTopologyIssue` (line 0, priv)
  - Signature: `struct TestTopologyIssue { file : PathBuf , line : usize , kind : TestIssueKind , note : String , }`
- [Rust | Struct] `WarningEntry` (line 0, priv)
  - Signature: `struct WarningEntry { message : String , location : Option < WarningLocation > , }`
- [Rust | Struct] `WarningLocation` (line 0, priv)
  - Signature: `struct WarningLocation { file : PathBuf , line : usize , }`
- [Rust | Impl] `impl ModuleAggregate { fn new (name : String , layer : String) -> Self { Self { name : if name . is_empty () { "unknown" . to_string () } else { name } , layer , imports : BTreeSet :: new () , exports : BTreeSet :: new () , submodules : BTreeSet :: new () , } } } . self_ty` (line 0, priv)
- [Rust | Impl] `impl ReportConfig { fn defaults () -> Self { Self { file_line_warning : 800 , dir_file_warning : 30 , naming_score_warning : 70.0 , baseline_path : "metrics_baseline.txt" . to_string () , } } } . self_ty` (line 0, priv)
- [Rust | Impl] `impl ReportGenerator { pub fn new (output_dir : String) -> Self { Self { output_dir } } pub fn generate_all (& self , result : & AnalysisResult , cf_analyzer : & ControlFlowAnalyzer , rust_layers : & LayerGraph , julia_layers : & LayerGraph , rust_ordering : & FileOrderingResult , julia_ordering : & FileOrderingResult , function_placements : & [FunctionPlacement] , function_clusters : & [FunctionCluster] , directory_structure : & DirectoryAnalysis , root_path : & Path , correction_intelligence : bool , correction_json : Option < PathBuf > , verification_policy_json : Option < PathBuf > , correction_path_slice : bool , correction_path_slice_dir : Option < PathBuf > , correction_visibility_slice : bool , correction_visibility_slice_dir : Option < PathBuf > ,) -> Result < () > { fs :: create_dir_all (& self . output_dir) ? ; self . cleanup_legacy_reports () ? ; println ! ("  Report: structure") ; self . generate_structure_report (result) ? ; println ! ("  Report: call_graph") ; self . generate_call_graph_report (cf_analyzer) ? ; println ! ("  Report: cfg") ; self . generate_cfg_report (cf_analyzer) ? ; println ! ("  Report: module_dependencies") ; self . generate_module_dependencies (result) ? ; println ! ("  Report: module_map_mismatch") ; self . generate_module_map_mismatch (root_path) ? ; println ! ("  Report: test_topology") ; self . generate_test_topology (root_path) ? ; println ! ("  Report: warning_hygiene") ; self . generate_warning_hygiene (root_path) ? ; println ! ("  Report: function_analysis") ; self . generate_function_analysis (result) ? ; println ! ("  Report: layer_dependencies") ; self . generate_layer_dependency_report (rust_layers , julia_layers , root_path) ? ; println ! ("  Report: file_ordering") ; self . generate_file_ordering_report (rust_ordering , julia_ordering , root_path) ? ; println ! ("  Report: cohesion_analysis") ; self . generate_cohesion_report (function_placements , function_clusters , root_path) ? ; println ! ("  Report: refactoring_plan") ; let correction_report = if correction_intelligence { let actions = collect_refactor_actions (result , rust_ordering , julia_ordering , function_placements , function_clusters , directory_structure , root_path ,) ; let metrics = build_correction_metrics (function_placements , directory_structure) ; let state = crate :: correction_intelligence_report :: build_state (root_path , result , metrics) ; let report = crate :: correction_intelligence_report :: generate_intelligence_report (& actions , & state) ; let output_dir = Path :: new (& self . output_dir) . join ("97_correction_intelligence") ; crate :: correction_intelligence_report :: write_intelligence_outputs_at (& report , & output_dir , correction_json . as_deref () , verification_policy_json . as_deref () ,) ? ; if correction_path_slice { println ! ("  Report: correction_intelligence_path_coherence") ; let path_report = crate :: correction_intelligence_report :: filter_path_coherence_report (& report) ; let slice_dir = correction_path_slice_dir . unwrap_or_else (| | { output_dir . join ("slice_path_coherence") }) ; crate :: correction_intelligence_report :: write_intelligence_outputs_at (& path_report , & slice_dir , None , None ,) ? ; } if correction_visibility_slice { println ! ("  Report: correction_intelligence_visibility") ; let visibility_report = crate :: correction_intelligence_report :: filter_visibility_report (& report) ; let slice_dir = correction_visibility_slice_dir . unwrap_or_else (| | { output_dir . join ("slice_visibility") }) ; crate :: correction_intelligence_report :: write_intelligence_outputs_at (& visibility_report , & slice_dir , None , None ,) ? ; } Some (report) } else { None } ; self . generate_refactoring_plan (rust_ordering , julia_ordering , function_placements , function_clusters , directory_structure , root_path , correction_report . as_ref () ,) ? ; println ! ("  Report: file_organization") ; self . generate_file_organization_report (directory_structure , rust_ordering , julia_ordering , root_path ,) ? ; Ok (()) } fn cleanup_legacy_reports (& self) -> Result < () > { let legacy_files = ["structure.md" , "call_graph.md" , "cfg.md" , "module_dependencies.md" , "function_analysis.md" , "layer_dependencies.md" , "file_ordering.md" , "cohesion_analysis.md" , "refactoring_plan.md" , "file_organization.md" ,] ; for file in legacy_files { let path = Path :: new (& self . output_dir) . join (file) ; if path . exists () { fs :: remove_file (path) ? ; } } let report_dirs = ["structure" , "call_graph" , "cfg" , "module_dependencies" , "function_analysis" , "layer_dependencies" , "file_ordering" , "cohesion_analysis" , "refactoring_plan" , "file_organization" , "00_refactoring_plan" , "10_structure" , "20_call_graph" , "30_cfg" , "40_module_dependencies" , "41_module_map_mismatch" , "42_test_topology" , "43_warning_hygiene" , "50_function_analysis" , "60_layer_dependencies" , "70_file_ordering" , "80_cohesion_analysis" , "90_file_organization" ,] ; for dir in report_dirs { let path = Path :: new (& self . output_dir) . join (dir) ; if ! path . exists () { continue ; } for entry in fs :: read_dir (& path) ? { let entry = entry ? ; let entry_path = entry . path () ; if entry_path . is_dir () { if dir == "30_cfg" && entry_path . file_name () . map_or (false , | n | n == "dots") { continue ; } fs :: remove_dir_all (entry_path) ? ; } else { fs :: remove_file (entry_path) ? ; } } } Ok (()) } fn prepare_report_dir (& self , name : & str) -> Result < PathBuf > { let dir = Path :: new (& self . output_dir) . join (name) ; fs :: create_dir_all (& dir) ? ; Ok (dir) } fn generate_structure_report (& self , result : & AnalysisResult) -> Result < () > { let dir = self . prepare_report_dir ("10_structure") ? ; let generated_at = chrono :: Local :: now () . format ("%Y-%m-%d %H:%M:%S") ; let mut files : BTreeMap < String , Vec < & CodeElement > > = BTreeMap :: new () ; for elem in & result . elements { files . entry (elem . file_path . clone ()) . or_insert_with (Vec :: new) . push (elem) ; } let mut grouped : BTreeMap < String , Vec < (String , Vec < & CodeElement >) > > = BTreeMap :: new () ; for (file_path , elements) in files { let compressed = compress_path (& file_path) ; let key = prefix_key_from_path (& compressed) ; grouped . entry (key) . or_insert_with (Vec :: new) . push ((compressed , elements)) ; } let mut grouped : Vec < _ > = grouped . into_iter () . collect () ; grouped . sort_by (| a , b | group_key_cmp (& a . 0 , & b . 0)) ; let mut index = String :: from ("# MMSB Code Structure Overview\n\n") ; index . push_str (& format ! ("Generated: {}\n\n" , generated_at)) ; index . push_str ("Each numbered file groups source files by MMSB prefix so a simple `ls 10_structure/` \
shows the traversal order.\n\n" ,) ; if grouped . is_empty () { index . push_str ("No code elements were recorded.\n") ; } else { index . push_str ("## Group Files\n\n") ; for (idx , (group_key , _)) in grouped . iter () . enumerate () { let slug = slugify_key (group_key) ; let file_name = format ! ("{:03}-{}.md" , idx * 10 , slug) ; index . push_str (& format ! ("- `{}` → `{}`\n" , group_key , file_name)) ; } } for (idx , (group_key , mut entries)) in grouped . into_iter () . enumerate () { entries . sort_by (| a , b | a . 0 . cmp (& b . 0)) ; let slug = slugify_key (& group_key) ; let file_name = format ! ("{:03}-{}.md" , idx * 10 , slug) ; let mut content = format ! ("# Structure Group: {}\n\n" , group_key) ; for (file_path , mut elements) in entries { content . push_str (& format ! ("## File: {}\n\n" , file_path)) ; let layers : BTreeSet < String > = elements . iter () . map (| e | e . layer . clone ()) . collect () ; let layer_summary = if layers . is_empty () { "root" . to_string () } else { layers . iter () . cloned () . collect :: < Vec < _ > > () . join (", ") } ; let mut language_counts : BTreeMap < String , usize > = BTreeMap :: new () ; let mut type_counts : BTreeMap < String , usize > = BTreeMap :: new () ; for elem in & elements { * language_counts . entry (language_label (& elem . language) . to_string ()) . or_insert (0) += 1 ; * type_counts . entry (format ! ("{:?}" , elem . element_type)) . or_insert (0) += 1 ; } let lang_summary = if language_counts . is_empty () { "n/a" . to_string () } else { language_counts . iter () . map (| (lang , count) | format ! ("{} ({})" , lang , count)) . collect :: < Vec < _ > > () . join (", ") } ; let type_summary = if type_counts . is_empty () { "n/a" . to_string () } else { type_counts . iter () . map (| (ty , count) | format ! ("{} ({})" , ty , count)) . collect :: < Vec < _ > > () . join (", ") } ; content . push_str (& format ! ("- Layer(s): {}\n" , layer_summary)) ; content . push_str (& format ! ("- Language coverage: {}\n" , lang_summary)) ; content . push_str (& format ! ("- Element types: {}\n" , type_summary)) ; content . push_str (& format ! ("- Total elements: {}\n\n" , elements . len ())) ; content . push_str ("### Elements\n\n") ; elements . sort_by (| a , b | { a . line_number . cmp (& b . line_number) . then_with (| | a . name . cmp (& b . name)) }) ; for elem in elements { content . push_str (& self . format_element_entry (elem)) ; } content . push ('\n') ; } fs :: write (dir . join (file_name) , content) ? ; } index . push_str ("\n## Summary Statistics\n\n") ; index . push_str (& format ! ("- Total elements: {}\n" , result . elements . len ())) ; index . push_str (& format ! ("- Rust elements: {}\n" , result . elements . iter () . filter (| e | matches ! (e . language , Language :: Rust)) . count ())) ; index . push_str (& format ! ("- Julia elements: {}\n" , result . elements . iter () . filter (| e | matches ! (e . language , Language :: Julia)) . count ())) ; let mut type_counts : HashMap < String , usize > = HashMap :: new () ; for elem in & result . elements { let key = format ! ("{:?}_{:?}" , elem . language , elem . element_type) ; * type_counts . entry (key) . or_insert (0) += 1 ; } index . push_str ("\n### Elements by Type\n\n") ; let mut sorted_types : Vec < _ > = type_counts . iter () . collect () ; sorted_types . sort_by_key (| (k , _) | k . as_str ()) ; for (type_name , count) in sorted_types { index . push_str (& format ! ("- {}: {}\n" , type_name , count)) ; } fs :: write (dir . join ("index.md") , index) ? ; Ok (()) } fn format_element_entry (& self , elem : & CodeElement) -> String { let mut entry = format ! ("- [{} | {:?}] `{}` (line {}, {})\n" , language_label (& elem . language) , elem . element_type , elem . name , elem . line_number , visibility_label (& elem . visibility) ,) ; if ! elem . signature . is_empty () && matches ! (elem . element_type , ElementType :: Function | ElementType :: Struct) { entry . push_str (& format ! ("  - Signature: `{}`\n" , short_signature (& elem . signature))) ; } if ! elem . generic_params . is_empty () { entry . push_str (& format ! ("  - Generics: {}\n" , elem . generic_params . join (", "))) ; } if matches ! (elem . element_type , ElementType :: Function) && ! elem . calls . is_empty () { entry . push_str (& format ! ("  - Calls: {}\n" , elem . calls . join (", "))) ; } entry } fn generate_call_graph_report (& self , cf_analyzer : & ControlFlowAnalyzer) -> Result < () > { let dir = self . prepare_report_dir ("20_call_graph") ? ; let path = dir . join ("index.md") ; let mut content = String :: from ("# Call Graph Analysis\n\n") ; content . push_str ("This document shows the **interprocedural call graph** - which functions call which other functions.\n\n") ; content . push_str ("> **Note:** This is NOT a control flow graph (CFG). CFG shows intraprocedural control flow (branches, loops) within individual functions.\n\n") ; let stats = cf_analyzer . get_statistics () ; content . push_str ("## Call Graph Statistics\n\n") ; content . push_str (& format ! ("- Total functions: {}\n" , stats . total_functions)) ; content . push_str (& format ! ("- Total function calls: {}\n" , stats . total_calls)) ; content . push_str (& format ! ("- Maximum call depth: {}\n" , stats . max_depth)) ; content . push_str (& format ! ("- Leaf functions (no outgoing calls): {}\n\n" , stats . leaf_functions)) ; content . push_str ("## Call Graph Visualization\n\n") ; content . push_str (& cf_analyzer . generate_mermaid ()) ; fs :: write (path , content) ? ; Ok (()) } fn generate_cfg_report (& self , cf_analyzer : & ControlFlowAnalyzer) -> Result < () > { let dir = self . prepare_report_dir ("30_cfg") ? ; let mut index = String :: from ("# Control Flow Graphs (CFG)\n\n") ; index . push_str (& format ! ("Generated: {}\n\n" , chrono :: Local :: now () . format ("%Y-%m-%d %H:%M:%S"))) ; if cf_analyzer . cfgs () . is_empty () { index . push_str ("No control flow graphs were captured.\n") ; fs :: write (dir . join ("index.md") , index) ? ; return Ok (()) ; } let mut grouped : BTreeMap < String , Vec < (String , & FunctionCfg) > > = BTreeMap :: new () ; for cfg in cf_analyzer . cfgs () { let compressed = compress_path (& cfg . file_path) ; let key = prefix_key_from_path (& compressed) ; grouped . entry (key) . or_insert_with (Vec :: new) . push ((compressed , cfg)) ; } let mut grouped : Vec < _ > = grouped . into_iter () . collect () ; grouped . sort_by (| a , b | group_key_cmp (& a . 0 , & b . 0)) ; index . push_str (& format ! ("- Total CFGs: {}\n" , cf_analyzer . cfgs () . len ())) ; index . push_str ("- Files are grouped by MMSB directory prefix; numeric prefixes match lexical ordering.\n\n" ,) ; index . push_str ("## Group Files\n\n") ; for (idx , (group_key , _)) in grouped . iter () . enumerate () { let file_name = format ! ("{:03}-{}.md" , idx * 10 , slugify_key (group_key)) ; index . push_str (& format ! ("- `{}` → `{}`\n" , group_key , file_name)) ; } for (idx , (group_key , mut entries)) in grouped . into_iter () . enumerate () { entries . sort_by (| a , b | a . 1 . function . cmp (& b . 1 . function)) ; let slug = slugify_key (& group_key) ; let file_name = format ! ("{:03}-{}.md" , idx * 10 , slug) ; let mut content = format ! ("# CFG Group: {}\n\n" , group_key) ; for (compressed , cfg) in entries { content . push_str (& format ! ("## Function: `{}`\n\n" , cfg . function)) ; content . push_str (& format ! ("- File: {}\n- Branches: {}\n- Loops: {}\n- Nodes: {}\n- Edges: {}\n\n" , compressed , cfg . branch_count , cfg . loop_count , cfg . nodes . len () , cfg . edges . len () ,)) ; if let Some (dot_rel) = self . dot_path_for (& compressed) { content . push_str (& format ! ("- DOT call graph: `{}`\n\n" , dot_rel)) ; } content . push_str ("```mermaid\nflowchart TD\n") ; let mut id_map = HashMap :: new () ; let prefix = sanitize_mermaid_id (& cfg . function) ; for node in & cfg . nodes { let raw_id = format ! ("{}_{}" , prefix , node . id) ; let safe_id = sanitize_mermaid_id (& raw_id) ; id_map . insert (node . id , safe_id . clone ()) ; content . push_str (& format ! ("    {}[\"{}\"]\n" , safe_id , sanitize_mermaid_label (& node . label))) ; } for edge in & cfg . edges { if let (Some (src) , Some (dst)) = (id_map . get (& edge . from) , id_map . get (& edge . to)) { content . push_str (& format ! ("    {} --> {}\n" , src , dst)) ; } } content . push_str ("```\n\n") ; } fs :: write (dir . join (file_name) , content) ? ; } fs :: write (dir . join ("index.md") , index) ? ; Ok (()) } fn generate_layer_dependency_report (& self , rust_layers : & LayerGraph , julia_layers : & LayerGraph , root_path : & Path ,) -> Result < () > { let dir = self . prepare_report_dir ("60_layer_dependencies") ? ; let path = dir . join ("index.md") ; let mut content = String :: from ("# Layer Dependency Report\n\n") ; content . push_str (& format ! ("Generated: {}\n\n" , chrono :: Local :: now () . format ("%Y-%m-%d %H:%M:%S"))) ; self . write_layer_section (& mut content , "Rust" , rust_layers , root_path) ; self . write_layer_section (& mut content , "Julia" , julia_layers , root_path) ; fs :: write (path , content) ? ; Ok (()) } fn generate_file_ordering_report (& self , rust_ordering : & FileOrderingResult , julia_ordering : & FileOrderingResult , root_path : & Path ,) -> Result < () > { let dir = self . prepare_report_dir ("70_file_ordering") ? ; let path = dir . join ("index.md") ; let mut content = String :: from ("# File Ordering Report\n\n") ; content . push_str (& format ! ("Generated: {}\n\n" , chrono :: Local :: now () . format ("%Y-%m-%d %H:%M:%S"))) ; self . write_ordering_section (& mut content , "Rust" , rust_ordering , root_path) ; self . write_ordering_section (& mut content , "Julia" , julia_ordering , root_path) ; fs :: write (path , content) ? ; Ok (()) } fn write_ordering_section (& self , content : & mut String , label : & str , ordering : & FileOrderingResult , root_path : & Path ,) { content . push_str (& format ! ("## {} File Ordering\n\n" , label)) ; if ordering . ordered_files . is_empty () { content . push_str ("No files analyzed.\n\n") ; return ; } let rename_count = ordering . ordered_files . iter () . filter (| entry | entry . needs_rename) . count () ; content . push_str ("### Metrics\n\n") ; content . push_str (& format ! ("- Total files: {}\n- Rename suggestions: {}\n- Ordering violations: {}\n- Layer violations: {}\n- Directories: {}\n\n" , ordering . ordered_files . len () , rename_count , ordering . violations . len () , ordering . layer_violations . len () , ordering . ordered_directories . len ())) ; if ! ordering . cycles . is_empty () { content . push_str ("### Cycles Detected\n") ; for cycle in & ordering . cycles { let listing = cycle . iter () . map (| p | compress_path (p . to_string_lossy () . as_ref ())) . collect :: < Vec < _ > > () . join (", ") ; content . push_str (& format ! ("- {}\n" , listing)) ; } content . push ('\n') ; } content . push_str ("### Canonical Order\n\n") ; content . push_str ("| Order | Current | Suggested | Rename |\n") ; content . push_str ("| --- | --- | --- | --- |\n") ; for entry in & ordering . ordered_files { let current = display_path (& entry . current_path , root_path) ; let rename = if entry . needs_rename { "yes" } else { "no" } ; content . push_str (& format ! ("| {} | `{}` | `{}` | {} |\n" , entry . canonical_order , current , entry . suggested_name , rename)) ; } content . push ('\n') ; content . push_str ("### Ordering Violations\n") ; if ordering . violations . is_empty () { content . push_str ("- None detected.\n\n") ; } else { for violation in & ordering . violations { let file = display_path (& violation . file , root_path) ; content . push_str (& format ! ("- `{}`: alphabetical position {}, required position {}\n" , file , violation . current_position , violation . required_position)) ; if ! violation . blocking_dependencies . is_empty () { for dep in & violation . blocking_dependencies { let dep_path = display_path (dep , root_path) ; content . push_str (& format ! ("  - depends on `{}`\n" , dep_path)) ; } } } content . push ('\n') ; } content . push_str ("### Layer Violations\n") ; if ordering . layer_violations . is_empty () { content . push_str ("- None detected.\n\n") ; } else { for violation in & ordering . layer_violations { let from = display_path (& violation . from , root_path) ; let to = display_path (& violation . to , root_path) ; content . push_str (& format ! ("- `{}` ({}) depends on `{}` ({})\n" , to , violation . to_layer , from , violation . from_layer)) ; } content . push ('\n') ; } content . push_str ("### Directory Order\n") ; if ordering . ordered_directories . is_empty () { content . push_str ("- None detected.\n\n") ; } else { for dir in & ordering . ordered_directories { let path = display_path (dir , root_path) ; content . push_str (& format ! ("- `{}`\n" , path)) ; } content . push ('\n') ; } } fn generate_cohesion_report (& self , placements : & [FunctionPlacement] , clusters : & [FunctionCluster] , root_path : & Path ,) -> Result < () > { let dir = self . prepare_report_dir ("80_cohesion_analysis") ? ; let path = dir . join ("index.md") ; let mut content = String :: from ("# Function Cohesion Analysis\n\n") ; content . push_str (& format ! ("Generated: {}\n\n" , chrono :: Local :: now () . format ("%Y-%m-%d %H:%M:%S"))) ; if placements . is_empty () { content . push_str ("No function placement data recorded.\n") ; fs :: write (path , content) ? ; return Ok (()) ; } let mut by_file : BTreeMap < String , Vec < & FunctionPlacement > > = BTreeMap :: new () ; let avg_cohesion = if placements . is_empty () { 0.0 } else { placements . iter () . map (| p | p . cohesion_score) . sum :: < f64 > () / placements . len () as f64 } ; let move_count = placements . iter () . filter (| p | matches ! (p . placement_status , PlacementStatus :: ShouldMove { .. })) . count () ; let (orphaned , delete_candidates) = filter_orphaned (placements , root_path , & self . output_dir) ; let orphaned_count = orphaned . len () ; let layer_violation_count = placements . iter () . filter (| p | matches ! (p . placement_status , PlacementStatus :: LayerViolation { .. })) . count () ; content . push_str ("## Metrics\n\n") ; content . push_str (& format ! ("- Avg cohesion: {:.2}\n- Move suggestions: {}\n- Orphaned functions: {}\n- Layer violations: {}\n\n" , avg_cohesion , move_count , orphaned_count , layer_violation_count)) ; for placement in placements { by_file . entry (placement . current_file . to_string_lossy () . to_string ()) . or_default () . push (placement) ; } for (file , mut entries) in by_file { entries . sort_by (| a , b | { a . cohesion_score . partial_cmp (& b . cohesion_score) . unwrap_or (Ordering :: Equal) }) ; let compressed = compress_path (& file) ; content . push_str (& format ! ("## File: {}\n\n" , compressed)) ; content . push_str ("| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |\n") ; content . push_str ("| --- | --- | --- | --- | --- | --- | --- |\n") ; for entry in entries { let status = placement_status_label (& entry . placement_status) ; let mut suggestion = entry . suggested_file . as_ref () . map (| p | compress_path (p . to_string_lossy () . as_ref ())) . unwrap_or_else (| | "-" . to_string ()) ; let notes = placement_status_notes (& entry . placement_status) ; if ! notes . is_empty () { suggestion = format ! ("{} ({})" , suggestion , notes) ; } let call_summary = format ! ("intra {}, inter {}" , entry . call_analysis . intra_file_calls , entry . call_analysis . inter_file_calls . len ()) ; let type_summary = format ! ("same {}, other {}" , entry . call_analysis . same_file_type_refs , entry . call_analysis . other_file_type_refs) ; content . push_str (& format ! ("| `{}` | `{}` | {:.2} | {} | {} | {} | {} |\n" , entry . name , entry . signature . replace ('|' , "\\|") , entry . cohesion_score , call_summary , type_summary , status , suggestion)) ; } content . push ('\n') ; } content . push_str ("## Orphaned Functions (Review Only)\n\n") ; content . push_str ("Action: review each item for expected usage. Delete only if it also appears under \"Delete Candidates (Orphaned + Dead Code)\".\n") ; content . push_str ("Note: excludes public symbols referenced by other modules and entry points. Delete candidates require dead_code warnings.\n\n") ; if orphaned . is_empty () { content . push_str ("- None detected.\n") ; } else { for entry in orphaned { let file = compress_path (entry . current_file . to_string_lossy () . as_ref ()) ; content . push_str (& format ! ("- `{}` in `{}`\n" , entry . name , file)) ; } } content . push ('\n') ; content . push_str ("## Delete Candidates (Orphaned + Dead Code)\n\n") ; if delete_candidates . is_empty () { content . push_str ("- None detected.\n") ; } else { for entry in delete_candidates { let file = compress_path (entry . current_file . to_string_lossy () . as_ref ()) ; content . push_str (& format ! ("- `{}` in `{}`\n" , entry . name , file)) ; } } content . push ('\n') ; let utility_candidates = collect_utility_candidates (placements) ; content . push_str ("## Utility Module Candidates\n\n") ; if utility_candidates . is_empty () { content . push_str ("- None detected.\n\n") ; } else { for candidate in utility_candidates { content . push_str (& format ! ("- {}\n" , candidate)) ; } content . push ('\n') ; } content . push_str ("## Function Clusters\n\n") ; if clusters . is_empty () { content . push_str ("- None detected.\n") ; } else { for cluster in clusters { let suggested = cluster . suggested_file . as_ref () . map (| p | compress_path (p . to_string_lossy () . as_ref ())) . unwrap_or_else (| | "-" . to_string ()) ; let members = cluster . members . iter () . map (| m | compress_path (m)) . collect :: < Vec < _ > > () . join (", ") ; content . push_str (& format ! ("- cohesion {:.2}, suggested `{}`\n  - {}\n" , cluster . cohesion , suggested , members)) ; } } content . push ('\n') ; fs :: write (path , content) ? ; Ok (()) } fn generate_refactoring_plan (& self , rust_ordering : & FileOrderingResult , julia_ordering : & FileOrderingResult , placements : & [FunctionPlacement] , clusters : & [FunctionCluster] , directory : & DirectoryAnalysis , root_path : & Path , correction_report : Option < & crate :: correction_intelligence_report :: CorrectionIntelligenceReport > ,) -> Result < () > { let dir = self . prepare_report_dir ("00_refactoring_plan") ? ; let generated_at = chrono :: Local :: now () . format ("%Y-%m-%d %H:%M:%S") . to_string () ; let mut renames = collect_rename_items (rust_ordering , "Rust") . into_iter () . chain (collect_rename_items (julia_ordering , "Julia")) . collect :: < Vec < _ > > () ; renames . extend (directory_moves_to_plan ("Rust" , collect_directory_moves (rust_ordering , root_path) ,)) ; renames . extend (directory_moves_to_plan ("Julia" , collect_directory_moves (julia_ordering , root_path) ,)) ; let cluster_plans = collect_cluster_plans (clusters , root_path) ; let cluster_items = collect_cluster_items (& cluster_plans) ; let mut utility_names = BTreeSet :: new () ; for placement in placements { if placement . call_analysis . calls_from_other_files . len () >= 3 { utility_names . insert (placement . name . clone ()) ; } } let mut moves = collect_move_items (placements , & utility_names , directory , root_path) ; if let Ok (constraints_json) = std :: fs :: read_to_string (std :: path :: Path :: new (& self . output_dir) . join ("96_constraints/refactor_constraints.json")) { use crate :: refactor_constraints :: RefactorConstraint ; if let Ok (constraints) = serde_json :: from_str :: < Vec < RefactorConstraint > > (& constraints_json) { let mut blocked_count = 0 ; moves . retain (| m | { use crate :: action_validator :: check_move_allowed ; let name = match & m . name { Some (n) => n . clone () , None => return true , } ; let from = match & m . current_file { Some (f) => f . clone () , None => return true , } ; let to = match & m . target_file { Some (t) => t . clone () , None => return true , } ; match check_move_allowed (& name , & from , & to , & constraints) { Ok (_) => true , Err (reason) => { if std :: env :: var ("VERBOSE") . is_ok () { eprintln ! ("⚠️  BLOCKED: {} - {}" , name , reason) ; } blocked_count += 1 ; false } } }) ; if blocked_count > 0 { println ! ("✅ Constraint enforcement: {} moves allowed, {} blocked by invariants" , moves . len () , blocked_count) ; } } } let (orphaned , delete_candidates) = filter_orphaned (placements , root_path , & self . output_dir) ; let mut all_items = Vec :: new () ; all_items . extend (cluster_items . iter () . cloned ()) ; all_items . extend (renames . iter () . cloned ()) ; all_items . extend (moves . iter () . cloned ()) ; let mut correctness : Vec < PlanItem > = Vec :: new () ; let mut clusters_phase = Vec :: new () ; let mut structural = Vec :: new () ; let mut cohesion = Vec :: new () ; let mut ordering = Vec :: new () ; for item in all_items { match item . kind { ActionKind :: Cluster => clusters_phase . push (item) , ActionKind :: Structural => structural . push (item) , ActionKind :: Cohesion => cohesion . push (item) , ActionKind :: Ordering => ordering . push (item) , } } sort_plan_items (& mut correctness) ; sort_cluster_items (& mut clusters_phase) ; sort_structural_items (& mut structural) ; sort_plan_items (& mut cohesion) ; sort_plan_items (& mut ordering) ; let config = load_report_config (& self . output_dir) ; let dir_cohesion = compute_directory_cohesion (placements) ; let avg_cohesion = if placements . is_empty () { 0.0 } else { placements . iter () . map (| p | p . cohesion_score) . sum :: < f64 > () / placements . len () as f64 } ; let ordering_correctness = compute_ordering_correctness (rust_ordering , julia_ordering) ; let relocations = placements . iter () . filter (| p | matches ! (p . placement_status , PlacementStatus :: ShouldMove { .. } | PlacementStatus :: LayerViolation { .. })) . count () ; write_baseline_metrics (& config , & self . output_dir , dir_cohesion , ordering_correctness , avg_cohesion , renames . len () , relocations ,) ; let mut summary = String :: from ("# Refactoring Plan\n\n") ; summary . push_str (& format ! ("Generated: {}\n\n" , generated_at)) ; summary . push_str ("## Summary\n\n") ; summary . push_str ("Action: use this as the quick status snapshot for planning work.\n") ; summary . push_str ("Note: counts are derived from current analysis output.\n\n") ; summary . push_str (& format ! ("- File/dir renames: {}\n- Function moves: {}\n- Orphaned functions: {}\n- Clusters: {}\n\n" , renames . len () , moves . len () , orphaned . len () , clusters . len ())) ; let mut metrics = String :: new () ; metrics . push_str ("## Metrics\n\n") ; metrics . push_str ("Action: monitor trends and regressions across runs.\n") ; metrics . push_str ("Note: compare against baseline metrics when available.\n\n") ; metrics . push_str (& format ! ("- Directory cohesion: {:.2}\n- Ordering correctness: {:.1}%\n- Avg function cohesion: {:.2}\n- Rename ops needed: {}\n- Function relocations suggested: {}\n\n" , dir_cohesion , ordering_correctness * 100.0 , avg_cohesion , renames . len () , relocations)) ; let mut baseline_section = String :: new () ; if let Some (baseline) = load_baseline_metrics (& config , & self . output_dir) { let mut regression_warnings = Vec :: new () ; let epsilon = 0.005 ; if let Some (prev) = baseline . get ("directory_cohesion") { if dir_cohesion + epsilon < * prev { regression_warnings . push (format ! ("Directory cohesion dropped from {:.2} to {:.2}." , prev , dir_cohesion)) ; } } if let Some (prev) = baseline . get ("ordering_correctness") { let current = ordering_correctness * 100.0 ; if current + epsilon < * prev { regression_warnings . push (format ! ("Ordering correctness dropped from {:.1}% to {:.1}%." , prev , current)) ; } } if let Some (prev) = baseline . get ("avg_function_cohesion") { if avg_cohesion + epsilon < * prev { regression_warnings . push (format ! ("Avg function cohesion dropped from {:.2} to {:.2}." , prev , avg_cohesion)) ; } } if let Some (prev) = baseline . get ("rename_ops_needed") { if (renames . len () as f64) > * prev + epsilon { regression_warnings . push (format ! ("Rename ops needed increased from {:.0} to {}." , prev , renames . len ())) ; } } if let Some (prev) = baseline . get ("function_relocations") { if (relocations as f64) > * prev + epsilon { regression_warnings . push (format ! ("Function relocations suggested increased from {:.0} to {}." , prev , relocations)) ; } } let deltas = baseline_deltas (& baseline , dir_cohesion , ordering_correctness , avg_cohesion , renames . len () , relocations ,) ; if ! deltas . is_empty () { println ! ("Baseline deltas:") ; for line in & deltas { println ! ("  {}" , line) ; } } baseline_section . push_str ("## Baseline Regression Warnings\n\n") ; baseline_section . push_str ("Action: investigate any regressions before proceeding with refactors.\n") ; baseline_section . push_str ("Note: derived from the last saved baseline metrics.\n\n") ; if regression_warnings . is_empty () { baseline_section . push_str ("- None.\n\n") ; } else { for warning in regression_warnings { baseline_section . push_str (& format ! ("- {}\n" , warning)) ; } baseline_section . push ('\n') ; } } else { baseline_section . push_str ("## Baseline Regression Warnings\n\n") ; baseline_section . push_str ("Action: save a baseline to enable regression tracking.\n") ; baseline_section . push_str ("Note: baseline metrics file not found for this output directory.\n\n") ; baseline_section . push_str ("- None.\n\n") ; } let mut phase1 = String :: new () ; write_priority_section (& mut phase1 , "Phase 1: Correctness Blockers" , & correctness) ; let mut phase2 = String :: new () ; write_priority_section (& mut phase2 , "Phase 2: Cluster Extraction" , & clusters_phase) ; write_cluster_tips (& mut phase2 , & cluster_plans) ; write_cluster_batches (& mut phase2 , & cluster_plans , root_path) ; let mut phase3 = String :: new () ; write_priority_section (& mut phase3 , "Phase 3: Structural Constraints" , & structural) ; write_structural_tips (& mut phase3 , & structural) ; write_structural_batches (& mut phase3 , & structural) ; let mut phase4 = String :: new () ; write_priority_section (& mut phase4 , "Phase 4: Cohesion Improvements" , & cohesion) ; let mut phase5 = String :: new () ; write_priority_section (& mut phase5 , "Phase 5: Ordering & Renames" , & ordering) ; let mut orphaned_section = String :: new () ; orphaned_section . push_str ("## Orphaned Functions (Review Only)\n\n") ; orphaned_section . push_str ("Action: review each item for expected usage. Delete only if it also appears under \"Delete Candidates (Orphaned + Dead Code)\".\n") ; orphaned_section . push_str ("Note: excludes public symbols referenced by other modules and entry points. Delete candidates require dead_code warnings.\n\n") ; if orphaned . is_empty () { orphaned_section . push_str ("- None detected.\n\n") ; } else { for entry in & orphaned { let file = compress_path (entry . current_file . to_string_lossy () . as_ref ()) ; orphaned_section . push_str (& format ! ("- `{}` in `{}`\n" , entry . name , file)) ; } orphaned_section . push ('\n') ; } let mut delete_section = String :: new () ; delete_section . push_str ("## Delete Candidates (Orphaned + Dead Code)\n\n") ; delete_section . push_str ("Action: consider removal after confirming behavior and running tests.\n") ; delete_section . push_str ("Note: derived from orphaned list plus compiler dead_code warnings.\n\n") ; if delete_candidates . is_empty () { delete_section . push_str ("- None detected.\n\n") ; } else { for entry in & delete_candidates { let file = compress_path (entry . current_file . to_string_lossy () . as_ref ()) ; delete_section . push_str (& format ! ("- `{}` in `{}`\n" , entry . name , file)) ; } delete_section . push ('\n') ; } let mut cluster_suggestions = String :: new () ; cluster_suggestions . push_str ("## Suggested New Files (Clusters)\n\n") ; cluster_suggestions . push_str ("Action: consider creating these files to improve cohesion.\n") ; cluster_suggestions . push_str ("Note: suggestions are heuristic and should be validated.\n\n") ; if clusters . is_empty () { cluster_suggestions . push_str ("- None detected.\n\n") ; } else { for cluster in clusters { let suggested = cluster . suggested_file . as_ref () . map (| p | compress_path (p . to_string_lossy () . as_ref ())) . unwrap_or_else (| | "new module" . to_string ()) ; let members = cluster . members . iter () . map (| m | compress_path (m)) . collect :: < Vec < _ > > () . join (", ") ; cluster_suggestions . push_str (& format ! ("- cohesion {:.2}, suggested `{}`\n  - {}\n" , cluster . cohesion , suggested , members)) ; } cluster_suggestions . push ('\n') ; } let utility_candidates = collect_utility_candidates (placements) ; let mut utility_section = String :: new () ; utility_section . push_str ("## Utility Module Candidates\n\n") ; utility_section . push_str ("Action: consider consolidating these into a shared utilities module.\n") ; utility_section . push_str ("Note: candidates are based on cross-file usage frequency.\n\n") ; if utility_candidates . is_empty () { utility_section . push_str ("- None detected.\n\n") ; } else { for candidate in utility_candidates { utility_section . push_str (& format ! ("- {}\n" , candidate)) ; } utility_section . push ('\n') ; } let mut naming_warnings = Vec :: new () ; let _ = collect_naming_warnings (directory , & config , & mut naming_warnings) ; let mut naming_section = String :: new () ; naming_section . push_str ("## Naming Warnings\n\n") ; naming_section . push_str ("Action: rename files if the suggested name improves ordering clarity.\n") ; naming_section . push_str ("Note: `_old` paths are excluded from naming warnings.\n\n") ; if naming_warnings . is_empty () { naming_section . push_str ("- None.\n\n") ; } else { for warning in naming_warnings { naming_section . push_str (& format ! ("- {}\n" , warning)) ; } naming_section . push ('\n') ; } let mut size_warnings = Vec :: new () ; collect_size_warnings (directory , & config , & mut size_warnings) ; let mut size_section = String :: new () ; size_section . push_str ("## Size Warnings\n\n") ; size_section . push_str ("Action: consider extracting helpers to reduce file size.\n") ; size_section . push_str ("Note: thresholds come from report configuration.\n\n") ; if size_warnings . is_empty () { size_section . push_str ("- None.\n\n") ; } else { for warning in size_warnings { size_section . push_str (& format ! ("- {}\n" , warning)) ; } size_section . push ('\n') ; } let mut cargo_section = String :: new () ; if let Some (warnings) = load_cargo_warnings (& self . output_dir) { cargo_section . push_str ("## Cargo Warnings\n\n") ; cargo_section . push_str ("Action: address compiler warnings before major refactors.\n") ; cargo_section . push_str ("Note: captured from cargo check/test outputs.\n\n") ; if warnings . trim () . is_empty () { cargo_section . push_str ("- None.\n\n") ; } else { cargo_section . push_str ("```text\n") ; cargo_section . push_str (warnings . trim ()) ; cargo_section . push_str ("\n```\n\n") ; } } else { cargo_section . push_str ("## Cargo Warnings\n\n") ; cargo_section . push_str ("Action: address compiler warnings before major refactors.\n") ; cargo_section . push_str ("Note: no cargo warnings captured in this run.\n\n") ; cargo_section . push_str ("- None.\n\n") ; } let mut correction_section = String :: new () ; if let Some (report) = correction_report { correction_section . push_str ("## Correction Intelligence\n\n") ; correction_section . push_str ("Action: review correction plans and verification policies before execution.\n") ; correction_section . push_str ("Note: generated for mmsb-executor ingestion; analyzer does not mutate code.\n\n") ; correction_section . push_str (& format ! ("- Actions analyzed: {}\n- Trivial: {}\n- Moderate: {}\n- Complex: {}\n- Avg confidence: {:.1}%\n\n" , report . actions_analyzed , report . summary . trivial_count , report . summary . moderate_count , report . summary . complex_count , report . summary . average_confidence * 100.0)) ; correction_section . push_str ("- JSON: `97_correction_intelligence/correction_intelligence.json`\n" ,) ; correction_section . push_str ("- Verification policy: `97_correction_intelligence/verification_policy.json`\n\n" ,) ; } let mut files = Vec :: new () ; files . push (("00_summary.md" , summary)) ; files . push (("01_metrics.md" , metrics)) ; files . push (("02_baseline_regressions.md" , baseline_section)) ; files . push (("03_phase1_correctness.md" , phase1)) ; files . push (("04_phase2_clusters.md" , phase2)) ; files . push (("05_phase3_structural.md" , phase3)) ; files . push (("06_phase4_cohesion.md" , phase4)) ; files . push (("07_phase5_ordering_renames.md" , phase5)) ; files . push (("08_orphaned_functions.md" , orphaned_section)) ; files . push (("09_delete_candidates.md" , delete_section)) ; files . push (("10_suggested_new_files.md" , cluster_suggestions)) ; files . push (("11_utility_module_candidates.md" , utility_section)) ; files . push (("12_naming_warnings.md" , naming_section)) ; files . push (("13_size_warnings.md" , size_section)) ; files . push (("14_cargo_warnings.md" , cargo_section)) ; if correction_report . is_some () { files . push (("15_correction_intelligence.md" , correction_section)) ; } let mut index = String :: from ("# Refactoring Plan Index\n\n") ; index . push_str (& format ! ("Generated: {}\n\n" , generated_at)) ; for (name , _) in & files { index . push_str (& format ! ("- `{}`\n" , name)) ; } fs :: write (dir . join ("index.md") , index) ? ; for (name , content) in files { fs :: write (dir . join (name) , content) ? ; } Ok (()) } fn generate_file_organization_report (& self , directory : & DirectoryAnalysis , _rust_ordering : & FileOrderingResult , _julia_ordering : & FileOrderingResult , root_path : & Path ,) -> Result < () > { let dir = self . prepare_report_dir ("90_file_organization") ? ; let mut index = String :: from ("# File Organization Report\n\n") ; index . push_str (& format ! ("Generated: {}\n\n" , chrono :: Local :: now () . format ("%Y-%m-%d %H:%M:%S"))) ; let mut entries = Vec :: new () ; collect_directories (directory , & mut entries) ; for entry in & entries { if entry . files . is_empty () { continue ; } let file_map = build_directory_entry_map (& entry . files) ? ; let relative = entry . path . strip_prefix (root_path) . unwrap_or (& entry . path) . to_path_buf () ; let slug = slugify_path (& relative) ; let file_name = format ! ("{}.md" , slug) ; index . push_str (& format ! ("- `{}` → `{}`\n" , compress_path (entry . path . to_string_lossy () . as_ref ()) , file_name)) ; let mut content = format ! ("# Directory: {}\n\n" , compress_path (entry . path . to_string_lossy () . as_ref ())) ; content . push_str (& format ! ("- Layer: `{}`\n\n" , entry . layer)) ; content . push_str ("## Files\n\n") ; content . push_str ("| File | Suggested | Rename |\n") ; content . push_str ("| --- | --- | --- |\n") ; let mut files = entry . files . clone () ; files . sort () ; for file in files { let entry_info = file_map . get (& file) ; let suggested = entry_info . map (| info | info . suggested_name . as_str ()) . unwrap_or ("-") ; let rename = entry_info . map (| info | if info . needs_rename { "yes" } else { "no" }) . unwrap_or ("no") ; content . push_str (& format ! ("| `{}` | `{}` | {} |\n" , compress_path (file . to_string_lossy () . as_ref ()) , suggested , rename)) ; } content . push ('\n') ; content . push_str ("## Dependency Graph\n\n") ; if entry . files . is_empty () { content . push_str ("No source files.\n\n") ; } else { let graph = build_file_dependency_graph (& entry . files) ? ; content . push_str (& render_mermaid_graph (& graph)) ; content . push ('\n') ; } fs :: write (dir . join (file_name) , content) ? ; } fs :: write (dir . join ("index.md") , index) ? ; Ok (()) } fn write_layer_section (& self , content : & mut String , label : & str , graph : & LayerGraph , root_path : & Path ,) { content . push_str (& format ! ("## {} Layer Graph\n\n" , label)) ; if graph . ordered_layers . is_empty () { content . push_str ("No layers discovered.\n\n") ; return ; } content . push_str ("### Layer Order\n") ; for (idx , layer) in graph . ordered_layers . iter () . enumerate () { let cycle_tag = if graph . cycles . contains (layer) { " (cycle)" } else { "" } ; content . push_str (& format ! ("{}. `{}`{}\n" , idx + 1 , layer , cycle_tag)) ; } content . push ('\n') ; if ! graph . cycles . is_empty () { content . push_str ("### Cycles Detected\n") ; for cycle in & graph . cycles { content . push_str (& format ! ("- `{}`\n" , cycle)) ; } content . push ('\n') ; } let violations : Vec < _ > = graph . edges . iter () . filter (| e | e . violation) . collect () ; content . push_str ("### Layer Violations\n") ; if violations . is_empty () { content . push_str ("- None detected.\n\n") ; } else { for edge in violations { content . push_str (& format ! ("- `{}` depends on `{}` ({} references)\n" , edge . to , edge . from , edge . references . len ())) ; for reference in & edge . references { let compressed = display_path (& reference . file , root_path) ; content . push_str (& format ! ("  - {} :: {}\n" , compressed , reference . reference)) ; } } content . push ('\n') ; } content . push_str ("### Dependency Edges\n") ; if graph . edges . is_empty () { content . push_str ("- No cross-layer dependencies recorded.\n\n") ; } else { for edge in & graph . edges { content . push_str (& format ! ("- `{}` → `{}` ({} references{})\n" , edge . from , edge . to , edge . references . len () , if edge . violation { ", VIOLATION" } else { "" })) ; for reference in & edge . references { let compressed = display_path (& reference . file , root_path) ; content . push_str (& format ! ("  - {} :: {}\n" , compressed , reference . reference)) ; } } content . push ('\n') ; } content . push_str ("### Unresolved References\n") ; if graph . unresolved . is_empty () { content . push_str ("- None.\n\n") ; } else { for unresolved in & graph . unresolved { let compressed = display_path (& unresolved . file , root_path) ; content . push_str (& format ! ("- {} → `{}`\n" , compressed , unresolved . reference)) ; } content . push ('\n') ; } } fn generate_module_dependencies (& self , result : & AnalysisResult) -> Result < () > { let dir = self . prepare_report_dir ("40_module_dependencies") ? ; let index_path = dir . join ("index.md") ; let mut index = String :: from ("# Module Dependencies\n\n") ; if result . modules . is_empty () { index . push_str ("No module metadata captured yet.\n") ; fs :: write (index_path , index) ? ; return Ok (()) ; } let mut modules_by_file : BTreeMap < String , ModuleAggregate > = BTreeMap :: new () ; for module in & result . modules { let layer = self . extract_layer_from_path (& module . file_path) ; let entry = modules_by_file . entry (module . file_path . clone ()) . or_insert_with (| | ModuleAggregate :: new (module . name . clone () , layer . clone ())) ; if entry . name == "unknown" && ! module . name . is_empty () { entry . name = module . name . clone () ; } entry . layer = layer ; for import in & module . imports { entry . imports . insert (normalize_use_stmt (import)) ; } for export in & module . exports { entry . exports . insert (normalize_use_stmt (export)) ; } for sub in & module . submodules { entry . submodules . insert (sub . clone ()) ; } } let total_imports : usize = modules_by_file . values () . map (| m | m . imports . len ()) . sum () ; let total_exports : usize = modules_by_file . values () . map (| m | m . exports . len ()) . sum () ; let total_submodules : usize = modules_by_file . values () . map (| m | m . submodules . len ()) . sum () ; let mut modules : Vec < _ > = modules_by_file . into_iter () . collect () ; modules . sort_by (| a , b | a . 0 . cmp (& b . 0)) ; index . push_str (& format ! ("- Module files analyzed: {}\n" , modules . len ())) ; index . push_str (& format ! ("- Unique imports captured: {}\n" , total_imports)) ; index . push_str (& format ! ("- Unique exports captured: {}\n" , total_exports)) ; index . push_str (& format ! ("- Submodule declarations captured: {}\n\n" , total_submodules)) ; index . push_str ("## Per-file Summary\n\n") ; for (file_path , module) in & modules { let compressed = compress_path (file_path) ; index . push_str (& format ! ("- `{}` → module `{}` (layer {}, {} imports / {} exports / {} submodules)\n" , compressed , module . name , module . layer , module . imports . len () , module . exports . len () , module . submodules . len ())) ; } index . push_str ("\n## Detailed Files\n\n- `010-imports.md` – expanded import lists\n- `020-exports.md` – export statements\n- `030-submodules.md` – nested module declarations\n- `040-violations.md` – placeholder for future per-module violations\n" ,) ; fs :: write (& index_path , index) ? ; let mut imports_doc = String :: from ("# Module Imports\n\n") ; let mut has_imports = false ; for (file_path , module) in & modules { if module . imports . is_empty () { continue ; } has_imports = true ; let compressed = compress_path (file_path) ; imports_doc . push_str (& format ! ("## {} ({})\n\n" , compressed , module . layer)) ; imports_doc . push_str (& format ! ("Module `{}`\n\n" , module . name)) ; for import in & module . imports { imports_doc . push_str (& format ! ("- `{}`\n" , import)) ; } imports_doc . push ('\n') ; } if ! has_imports { imports_doc . push_str ("No imports captured across modules.\n") ; } fs :: write (dir . join ("010-imports.md") , imports_doc) ? ; let mut exports_doc = String :: from ("# Module Exports\n\n") ; let mut has_exports = false ; for (file_path , module) in & modules { if module . exports . is_empty () { continue ; } has_exports = true ; let compressed = compress_path (file_path) ; exports_doc . push_str (& format ! ("## {} ({})\n\n" , compressed , module . layer)) ; exports_doc . push_str (& format ! ("Module `{}`\n\n" , module . name)) ; for export in & module . exports { exports_doc . push_str (& format ! ("- `{}`\n" , export)) ; } exports_doc . push ('\n') ; } if ! has_exports { exports_doc . push_str ("No exports captured across modules.\n") ; } fs :: write (dir . join ("020-exports.md") , exports_doc) ? ; let mut subs_doc = String :: from ("# Submodules\n\n") ; let mut has_submodules = false ; for (file_path , module) in & modules { if module . submodules . is_empty () { continue ; } has_submodules = true ; let compressed = compress_path (file_path) ; subs_doc . push_str (& format ! ("## {} ({})\n\n" , compressed , module . layer)) ; subs_doc . push_str (& format ! ("Module `{}`\n\n" , module . name)) ; for sub in & module . submodules { subs_doc . push_str (& format ! ("- `{}`\n" , sub)) ; } subs_doc . push ('\n') ; } if ! has_submodules { subs_doc . push_str ("No nested modules recorded.\n") ; } fs :: write (dir . join ("030-submodules.md") , subs_doc) ? ; let mut violations_doc = String :: from ("# Module Violations\n\n") ; violations_doc . push_str ("Per-module import/export violations are not computed yet.\n\
Refer to `60_layer_dependencies/index.md` for cross-layer problems.\n" ,) ; fs :: write (dir . join ("040-violations.md") , violations_doc) ? ; Ok (()) } fn generate_module_map_mismatch (& self , root_path : & Path) -> Result < () > { let dir = self . prepare_report_dir ("41_module_map_mismatch") ? ; let index_path = dir . join ("index.md") ; let src_root = crate :: layer_utilities :: resolve_source_root (root_path) ; let rust_files = crate :: cluster_010 :: gather_rust_files (root_path) . into_iter () . filter (| path | path . starts_with (& src_root)) . collect :: < Vec < _ > > () ; let mut missing_modules = Vec :: new () ; let mut ambiguous_modules = Vec :: new () ; let mut outside_root = Vec :: new () ; let mut mod_decl_count = 0usize ; for file in & rust_files { let Ok (content) = fs :: read_to_string (file) else { continue ; } ; let parent_dir = file . parent () . unwrap_or (& src_root) ; let mut pending_path : Option < String > = None ; for (idx , line) in content . lines () . enumerate () { let trimmed = line . trim () ; if let Some (path_attr) = extract_path_attr (trimmed) { pending_path = Some (path_attr) ; continue ; } if trimmed . starts_with ("#[") || trimmed . is_empty () || trimmed . starts_with ("//") { continue ; } if let Some (module) = parse_mod_decl (trimmed) { mod_decl_count += 1 ; let line_no = idx + 1 ; let path_attr = pending_path . take () ; if let Some (attr) = path_attr { let resolved = resolve_path_attr (parent_dir , & attr) ; if ! resolved . starts_with (& src_root) { outside_root . push (ModuleIssue { parent_file : file . clone () , module : module . clone () , line : line_no , details : vec ! [resolved] , note : format ! ("path attribute `{}` resolves outside src root" , attr) , kind : ModuleIssueKind :: PathOutsideRoot , }) ; continue ; } if ! resolved . is_file () { missing_modules . push (ModuleIssue { parent_file : file . clone () , module , line : line_no , details : vec ! [resolved] , note : format ! ("path attribute `{}` missing" , attr) , kind : ModuleIssueKind :: MissingPathAttr , }) ; } continue ; } let direct = parent_dir . join (format ! ("{}.rs" , module)) ; let nested = parent_dir . join (& module) . join ("mod.rs") ; let direct_exists = direct . is_file () ; let nested_exists = nested . is_file () ; if direct_exists && nested_exists { ambiguous_modules . push (ModuleIssue { parent_file : file . clone () , module , line : line_no , details : vec ! [direct , nested] , note : "both module paths exist" . to_string () , kind : ModuleIssueKind :: AmbiguousModulePaths , }) ; } else if ! direct_exists && ! nested_exists { missing_modules . push (ModuleIssue { parent_file : file . clone () , module , line : line_no , details : vec ! [direct , nested] , note : "no module file found" . to_string () , kind : ModuleIssueKind :: MissingModuleFile , }) ; } } else { pending_path = None ; } } } missing_modules . sort_by (| a , b | (a . parent_file . cmp (& b . parent_file)) . then (a . module . cmp (& b . module))) ; ambiguous_modules . sort_by (| a , b | (a . parent_file . cmp (& b . parent_file)) . then (a . module . cmp (& b . module))) ; outside_root . sort_by (| a , b | (a . parent_file . cmp (& b . parent_file)) . then (a . module . cmp (& b . module))) ; let mut index = String :: from ("# Module Map Mismatch\n\n") ; index . push_str ("Action: Review only. Detection-only report.\n\n") ; index . push_str (& format ! ("- Rust files scanned: {}\n" , rust_files . len ())) ; index . push_str (& format ! ("- mod declarations scanned: {}\n" , mod_decl_count)) ; index . push_str (& format ! ("- Missing module files: {}\n" , missing_modules . len ())) ; index . push_str (& format ! ("- Ambiguous module paths: {}\n" , ambiguous_modules . len ())) ; index . push_str (& format ! ("- #[path] outside src root: {}\n\n" , outside_root . len ())) ; if missing_modules . is_empty () && ambiguous_modules . is_empty () && outside_root . is_empty () { index . push_str ("No module map mismatches detected.\n") ; fs :: write (index_path , index) ? ; return Ok (()) ; } if ! missing_modules . is_empty () { index . push_str ("## Missing Module Files\n\n") ; for issue in & missing_modules { let parent = compress_path (issue . parent_file . to_string_lossy () . as_ref ()) ; index . push_str (& format ! ("- `{}`:{} `mod {}` → {} ({})\n" , parent , issue . line , issue . module , format_paths (& issue . details) , issue . note)) ; index . push_str (& format ! ("  Plan: {}\n" , issue_plan_summary (issue))) ; } index . push ('\n') ; } if ! ambiguous_modules . is_empty () { index . push_str ("## Ambiguous Module Paths\n\n") ; for issue in & ambiguous_modules { let parent = compress_path (issue . parent_file . to_string_lossy () . as_ref ()) ; index . push_str (& format ! ("- `{}`:{} `mod {}` → {} ({})\n" , parent , issue . line , issue . module , format_paths (& issue . details) , issue . note)) ; index . push_str (& format ! ("  Plan: {}\n" , issue_plan_summary (issue))) ; } index . push ('\n') ; } if ! outside_root . is_empty () { index . push_str ("## #[path] Outside Source Root\n\n") ; for issue in & outside_root { let parent = compress_path (issue . parent_file . to_string_lossy () . as_ref ()) ; index . push_str (& format ! ("- `{}`:{} `mod {}` → {} ({})\n" , parent , issue . line , issue . module , format_paths (& issue . details) , issue . note)) ; index . push_str (& format ! ("  Plan: {}\n" , issue_plan_summary (issue))) ; } index . push ('\n') ; } fs :: write (index_path , index) ? ; Ok (()) } fn generate_test_topology (& self , root_path : & Path) -> Result < () > { let dir = self . prepare_report_dir ("42_test_topology") ? ; let index_path = dir . join ("index.md") ; let src_root = crate :: layer_utilities :: resolve_source_root (root_path) ; let rust_files = crate :: cluster_010 :: gather_rust_files (root_path) . into_iter () . filter (| path | path . starts_with (& src_root)) . collect :: < Vec < _ > > () ; let test_files = gather_test_files (root_path) ; let mut issues = Vec :: new () ; for file in & rust_files { let Ok (content) = fs :: read_to_string (file) else { continue ; } ; let mut pending_attrs : Vec < String > = Vec :: new () ; let mut file_has_submodules = false ; let mut file_has_cfg_tests = false ; for (idx , line) in content . lines () . enumerate () { let trimmed = line . trim () ; if trimmed . is_empty () || trimmed . starts_with ("//") { continue ; } if trimmed . starts_with ("#[") { pending_attrs . push (trimmed . to_string ()) ; continue ; } if let Some (mod_name) = parse_mod_decl (trimmed) { if mod_name != "tests" { file_has_submodules = true ; } } if is_mod_tests_decl (trimmed) { let line_no = idx + 1 ; if ! has_cfg_test_attr (& pending_attrs) { issues . push (TestTopologyIssue { file : file . clone () , line : line_no , kind : TestIssueKind :: TestsWithoutCfg , note : "mod tests without #[cfg(test)]" . to_string () , }) ; } else { file_has_cfg_tests = true ; } } pending_attrs . clear () ; } if file_has_cfg_tests && file_has_submodules { issues . push (TestTopologyIssue { file : file . clone () , line : 0 , kind : TestIssueKind :: CfgTestsInNonLeaf , note : "cfg(test) block in module with submodules" . to_string () , }) ; } } for file in & test_files { let Ok (content) = fs :: read_to_string (file) else { continue ; } ; for (idx , line) in content . lines () . enumerate () { let trimmed = line . trim () ; if trimmed . starts_with ("use crate::") { issues . push (TestTopologyIssue { file : file . clone () , line : idx + 1 , kind : TestIssueKind :: IntegrationUsesCratePath , note : "integration test uses crate:: path (check crate name)" . to_string () , }) ; } if trimmed . starts_with ("use super::") { issues . push (TestTopologyIssue { file : file . clone () , line : idx + 1 , kind : TestIssueKind :: IntegrationUsesSuper , note : "integration test uses super:: (invalid module path)" . to_string () , }) ; } } } issues . sort_by (| a , b | { a . file . cmp (& b . file) . then (a . line . cmp (& b . line)) . then (a . kind . cmp (& b . kind)) }) ; let mut index = String :: from ("# Test Topology\n\n") ; index . push_str ("Action: Review only. Detection-only report.\n\n") ; index . push_str (& format ! ("- Rust source files scanned: {}\n" , rust_files . len ())) ; index . push_str (& format ! ("- Integration test files scanned: {}\n" , test_files . len ())) ; index . push_str (& format ! ("- Issues detected: {}\n\n" , issues . len ())) ; if issues . is_empty () { index . push_str ("No test topology issues detected.\n") ; fs :: write (index_path , index) ? ; return Ok (()) ; } let mut grouped : BTreeMap < TestIssueKind , Vec < & TestTopologyIssue > > = BTreeMap :: new () ; for issue in & issues { grouped . entry (issue . kind) . or_default () . push (issue) ; } for (kind , entries) in grouped { index . push_str (& format ! ("## {}\n\n" , kind . label ())) ; for issue in entries { let file = compress_path (issue . file . to_string_lossy () . as_ref ()) ; if issue . line == 0 { index . push_str (& format ! ("- `{}` → {} (review_only)\n" , file , issue . note)) ; } else { index . push_str (& format ! ("- `{}`:{} → {} (review_only)\n" , file , issue . line , issue . note)) ; } } index . push ('\n') ; } fs :: write (index_path , index) ? ; Ok (()) } fn generate_warning_hygiene (& self , root_path : & Path) -> Result < () > { let dir = self . prepare_report_dir ("43_warning_hygiene") ? ; let index_path = dir . join ("index.md") ; let output_dir = Path :: new (& self . output_dir) ; let candidates = [output_dir . join ("cargo_warnings.txt") , root_path . join ("cargo_warnings.txt") ,] ; let warnings_path = candidates . iter () . find (| p | p . exists ()) ; let mut index = String :: from ("# Warning Hygiene\n\n") ; index . push_str ("Action: Review only. Detection-only report.\n\n") ; let Some (path) = warnings_path else { index . push_str ("No cargo_warnings.txt found.\n") ; fs :: write (index_path , index) ? ; return Ok (()) ; } ; let content = fs :: read_to_string (path) . unwrap_or_default () ; let warnings = parse_cargo_warnings (& content) ; index . push_str (& format ! ("- Source: `{}`\n" , compress_path (path . to_string_lossy () . as_ref ()))) ; index . push_str (& format ! ("- Warnings parsed: {}\n\n" , warnings . len ())) ; if warnings . is_empty () { index . push_str ("No warnings detected.\n") ; fs :: write (index_path , index) ? ; return Ok (()) ; } let mut by_message : BTreeMap < String , usize > = BTreeMap :: new () ; for warning in & warnings { * by_message . entry (warning . message . clone ()) . or_insert (0) += 1 ; } index . push_str ("## Warning Types\n\n") ; for (message , count) in & by_message { index . push_str (& format ! ("- {} ({} occurrences)\n" , message , count)) ; } index . push ('\n') ; index . push_str ("## Warning Details\n\n") ; for warning in & warnings { let location = if let Some (loc) = warning . location . as_ref () { format ! ("{}:{}" , compress_path (loc . file . to_string_lossy () . as_ref ()) , loc . line) } else { "unknown" . to_string () } ; index . push_str (& format ! ("- `{}` → {}\n" , location , warning . message)) ; } index . push ('\n') ; fs :: write (index_path , index) ? ; Ok (()) } fn generate_function_analysis (& self , result : & AnalysisResult) -> Result < () > { let dir = self . prepare_report_dir ("50_function_analysis") ? ; let mut index = String :: from ("# Function Analysis\n\n") ; let functions : Vec < _ > = result . elements . iter () . filter (| e | matches ! (e . element_type , ElementType :: Function)) . collect () ; index . push_str (& format ! ("## Total Functions: {}\n\n" , functions . len ())) ; index . push_str ("Functions are bucketed alphabetically so `ls 50_function_analysis/` advertises the range.\n\n" ,) ; if functions . is_empty () { fs :: write (dir . join ("index.md") , index) ? ; return Ok (()) ; } let bucket_labels = ["A-F" , "G-M" , "N-S" , "T-Z" , "Other"] ; let mut buckets : HashMap < & 'static str , Vec < & CodeElement > > = HashMap :: new () ; for label in bucket_labels { buckets . insert (label , Vec :: new ()) ; } for func in & functions { let label = function_bucket_label (& func . name) ; buckets . entry (label) . or_insert_with (Vec :: new) . push (func) ; } index . push_str ("## Bucket Files\n\n") ; for (idx , label) in bucket_labels . iter () . enumerate () { let file_name = format ! ("{:03}-functions_{}.md" , (idx + 1) * 10 , label) ; let count = buckets . get (label) . map (| v | v . len ()) . unwrap_or (0) ; index . push_str (& format ! ("- `{}` → `{}` ({} functions)\n" , label , file_name , count)) ; } fs :: write (dir . join ("index.md") , index) ? ; for (idx , label) in bucket_labels . iter () . enumerate () { let mut funcs = buckets . remove (label) . unwrap_or_default () ; funcs . sort_by_key (| f | (& f . layer , & f . name)) ; let file_name = format ! ("{:03}-functions_{}.md" , (idx + 1) * 10 , label) ; let mut content = format ! ("# Functions {}\n\n" , label) ; if funcs . is_empty () { content . push_str ("No functions fell into this range.\n") ; fs :: write (dir . join (file_name) , content) ? ; continue ; } let mut layer_map : BTreeMap < String , Vec < & CodeElement > > = BTreeMap :: new () ; for func in funcs { layer_map . entry (func . layer . clone ()) . or_insert_with (Vec :: new) . push (func) ; } for (layer , entries) in layer_map { content . push_str (& format ! ("## Layer: {}\n\n" , layer)) ; let mut rust_funcs : Vec < _ > = entries . iter () . filter (| f | matches ! (f . language , Language :: Rust)) . collect () ; let mut julia_funcs : Vec < _ > = entries . iter () . filter (| f | matches ! (f . language , Language :: Julia)) . collect () ; rust_funcs . sort_by_key (| f | & f . name) ; julia_funcs . sort_by_key (| f | & f . name) ; if ! rust_funcs . is_empty () { content . push_str ("### Rust Functions\n\n") ; for func in rust_funcs { content . push_str (& format ! ("#### `{}`\n\n" , func . name)) ; let compressed = compress_path (& func . file_path) ; content . push_str (& format ! ("- **File:** {}:{}\n" , compressed , func . line_number)) ; content . push_str (& format ! ("- **Visibility:** {:?}\n" , func . visibility)) ; if ! func . generic_params . is_empty () { content . push_str (& format ! ("- **Generics:** {}\n" , func . generic_params . join (", "))) ; } if ! func . calls . is_empty () { content . push_str ("- **Calls:**\n") ; for call in & func . calls { content . push_str (& format ! ("  - `{}`\n" , call)) ; } } content . push_str ("\n") ; } } if ! julia_funcs . is_empty () { content . push_str ("### Julia Functions\n\n") ; for func in julia_funcs { content . push_str (& format ! ("#### `{}`\n\n" , func . name)) ; let compressed = compress_path (& func . file_path) ; content . push_str (& format ! ("- **File:** {}:{}\n" , compressed , func . line_number)) ; content . push_str (& format ! ("- **Signature:** `{}`\n" , func . signature)) ; if ! func . calls . is_empty () { content . push_str ("- **Calls:**\n") ; for call in & func . calls { content . push_str (& format ! ("  - `{}`\n" , call)) ; } } content . push_str ("\n") ; } } } fs :: write (dir . join (file_name) , content) ? ; } Ok (()) } fn extract_layer_from_path (& self , path : & str) -> String { for component in path . split ('/') { if component . chars () . next () . map_or (false , | c | c . is_ascii_digit ()) { if let Some (pos) = component . find ('_') { if component [.. pos] . chars () . all (| c | c . is_ascii_digit ()) { return component . to_string () ; } } } } "root" . to_string () } fn dot_path_for (& self , compressed_path : & str) -> Option < String > { let slug = slugify_file_path (compressed_path) ; let rel = format ! ("30_cfg/dots/{}/call_graph.dot" , slug) ; let absolute = Path :: new (& self . output_dir) . join (& rel) ; if absolute . exists () { Some (rel) } else { None } } } . self_ty` (line 0, priv)
- [Rust | Impl] `impl TestIssueKind { fn label (self) -> & 'static str { match self { TestIssueKind :: TestsWithoutCfg => "Inline tests missing #[cfg(test)]" , TestIssueKind :: CfgTestsInNonLeaf => "cfg(test) in non-leaf module" , TestIssueKind :: IntegrationUsesCratePath => "Integration tests using crate::" , TestIssueKind :: IntegrationUsesSuper => "Integration tests using super::" , } } } . self_ty` (line 0, priv)

