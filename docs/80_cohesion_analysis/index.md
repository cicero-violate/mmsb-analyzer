# Function Cohesion Analysis

Generated: 2025-12-31 06:34:48

## Metrics

- Avg cohesion: 0.68
- Move suggestions: 17
- Orphaned functions: 9
- Layer violations: 0

## File: src/000_cluster_001.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `run_analysis` | `pub fn run_analysis (root_path : & Path , output_path : & Path , verbose : bool , skip_julia : bool , dead_code : bool , dead_code_filter : bool , dead_code_json : Option < PathBuf > , dead_code_summary : Option < PathBuf > , dead_code_summary_limit : usize , dead_code_policy : Option < PathBuf > , correction_intelligence : bool , correction_json : Option < PathBuf > , verification_policy_json : Option < PathBuf > , correction_path_slice : bool , correction_path_slice_dir : Option < PathBuf > , correction_visibility_slice : bool , correction_visibility_slice_dir : Option < PathBuf > ,) -> Result < () > { use crate :: control_flow :: ControlFlowAnalyzer ; use crate :: cohesion_analyzer :: FunctionCohesionAnalyzer ; use crate :: dependency :: LayerGraph ; use crate :: directory_analyzer :: DirectoryAnalyzer ; use crate :: dot_exporter :: export_program_cfg_to_path ; use crate :: julia_parser :: JuliaAnalyzer ; use crate :: report :: ReportGenerator ; use crate :: rust_parser :: RustAnalyzer ; use crate :: types :: { AnalysisResult , FileOrderingResult } ; let julia_script_path = root_path . join ("src/000_main.jl") ; println ! ("MMSB Intelligence Substrate Analyzer") ; println ! ("=====================================\n") ; println ! ("Root directory: {:?}" , root_path) ; println ! ("Output directory: {:?}" , output_path) ; println ! ("Julia script: {:?}\n" , julia_script_path) ; let rust_analyzer = RustAnalyzer :: new (root_path . to_string_lossy () . to_string ()) ; let mut combined_result = AnalysisResult :: new () ; println ! ("Scanning Rust files (dependency-ordered)...") ; let mut rust_count = 0 ; let rust_files = gather_rust_files (root_path) ; let (ordered_rust_files , rust_layer_graph) = crate :: dependency :: order_rust_files_by_dependency (& rust_files , root_path) . context ("Failed to resolve Rust dependency order") ? ; let rust_file_ordering = crate :: dependency :: analyze_file_ordering (& rust_files , None) . context ("Failed to analyze Rust file ordering") ? ; let julia_file_ordering = FileOrderingResult { ordered_files : Vec :: new () , violations : Vec :: new () , layer_violations : Vec :: new () , ordered_directories : Vec :: new () , cycles : Vec :: new () , } ; for path in ordered_rust_files { if verbose { println ! ("  Analyzing: {:?}" , path) ; } match rust_analyzer . analyze_file (& path) { Ok (result) => { rust_count += 1 ; combined_result . merge (result) ; } Err (e) => { eprintln ! ("Warning: Failed to analyze {:?}: {}" , path , e) ; } } } println ! ("  Analyzed {} Rust files\n" , rust_count) ; let mut julia_count = 0 ; let mut julia_layer_graph = LayerGraph { ordered_layers : Vec :: new () , edges : Vec :: new () , cycles : Vec :: new () , unresolved : Vec :: new () , } ; if ! skip_julia { println ! ("Scanning Julia files (dependency-ordered)...") ; let julia_files = gather_julia_files (root_path) ; let (ordered_julia_files , jlg) = crate :: dependency :: order_julia_files_by_dependency (& julia_files , root_path) . context ("Failed to resolve Julia dependency order") ? ; julia_layer_graph = jlg ; if julia_script_path . exists () { let julia_analyzer = JuliaAnalyzer :: new (root_path . to_path_buf () , julia_script_path . clone () , output_path . join ("30_cfg/dots") ,) ; for path in ordered_julia_files { if verbose { println ! ("  Analyzing: {:?}" , path) ; } match julia_analyzer . analyze_file (& path) { Ok (result) => { julia_count += 1 ; combined_result . merge (result) ; } Err (e) => { eprintln ! ("Warning: Failed to analyze {:?}: {}" , path , e) ; } } } } else { println ! ("  Skipping Julia analysis (script not found)") ; } println ! ("  Analyzed {} Julia files\n" , julia_count) ; } if dead_code \|\| dead_code_filter \|\| dead_code_json . is_some () \|\| dead_code_summary . is_some () { let policy = if let Some (policy_path) = dead_code_policy { Some (crate :: dead_code_policy :: load_policy (& policy_path) . context ("Failed to load dead code policy") ? ,) } else { None } ; let config = crate :: dead_code_cli :: DeadCodeRunConfig { root : root_path . to_path_buf () , output_dir : output_path . to_path_buf () , policy , write_json : dead_code_json , write_summary : dead_code_summary , summary_limit : dead_code_summary_limit , } ; let report = crate :: dead_code_cli :: run_dead_code_pipeline (& combined_result . elements , & config) . context ("Dead code analysis failed") ? ; if dead_code_filter { combined_result . elements = crate :: dead_code_filter :: filter_dead_code_elements (& combined_result . elements , & report) ; } } println ! ("Building call graph...") ; let mut cf_analyzer = ControlFlowAnalyzer :: new () ; cf_analyzer . build_call_graph (& combined_result) ; use crate :: invariant_integrator :: InvariantDetector ; println ! ("Detecting invariants...") ; let invariants_result = { let invariant_detector = InvariantDetector :: new (& combined_result , & combined_result . call_graph ,) ; invariant_detector . detect_all () } ; let constraints = { let invariant_detector = InvariantDetector :: new (& combined_result , & combined_result . call_graph ,) ; invariant_detector . generate_constraints (& invariants_result) } ; combined_result . invariants = invariants_result ; combined_result . constraints = constraints ; println ! ("Analyzing function cohesion...") ; let cohesion_analyzer = FunctionCohesionAnalyzer :: new () ; let placements = cohesion_analyzer . analyze (& combined_result) ? ; let clusters = cohesion_analyzer . detect_clusters (& combined_result) ? ; println ! ("Analyzing directory structure...") ; let dir_analyzer = DirectoryAnalyzer :: new (root_path . to_path_buf ()) ; let dir_analysis = dir_analyzer . analyze () ? ; println ! ("\nGenerating reports...") ; let report_gen = ReportGenerator :: new (output_path . to_string_lossy () . to_string ()) ; report_gen . generate_all (& combined_result , & cf_analyzer , & rust_layer_graph , & julia_layer_graph , & rust_file_ordering , & julia_file_ordering , & placements , & clusters , & dir_analysis , root_path , correction_intelligence , correction_json , verification_policy_json , correction_path_slice , correction_path_slice_dir , correction_visibility_slice , correction_visibility_slice_dir ,) . context ("Failed to generate reports") ? ; println ! ("\nExporting program CFG...") ; export_program_cfg_to_path (& combined_result , & cf_analyzer . call_edges () , output_path) ? ; println ! ("\nGenerating invariant report...") ; use crate :: invariant_reporter ; invariant_reporter :: generate_invariant_report (& combined_result . invariants , output_path) . context ("Failed to generate invariant report") ? ; invariant_reporter :: export_constraints_json (& combined_result . constraints , output_path) . context ("Failed to export constraints") ? ; println ! ("\nâœ“ Analysis complete!") ; println ! ("  Total elements: {}" , combined_result . elements . len ()) ; println ! ("  Rust files: {}" , rust_count) ; println ! ("  Julia files: {}" , julia_count) ; println ! ("  Output: {}\n" , output_path . display ()) ; Ok (()) } . sig` | 0.20 | intra 1, inter 4 | same 0, other 22 | move | - (cohesion 0.20 below threshold 0.60 (impact 0.40)) |
| `order_julia_files_by_dependency` | `pub fn order_julia_files_by_dependency (files : & [PathBuf] , root : & Path ,) -> Result < (Vec < PathBuf > , crate :: dependency :: LayerGraph) > { use crate :: cluster_001 :: { collect_julia_dependencies , JuliaTarget } ; use crate :: dependency :: ReferenceDetail ; let mut file_layers : HashMap < PathBuf , String > = HashMap :: new () ; let mut nodes : BTreeSet < String > = BTreeSet :: new () ; let mut edges_map : BTreeMap < (String , String) , BTreeSet < ReferenceDetail > > = BTreeMap :: new () ; let mut unresolved = Vec :: new () ; let resolver = LayerResolver :: build (root) ? ; let entry_files = crate :: cluster_001 :: julia_entry_paths (root) ; for file in files { let layer = crate :: cluster_001 :: detect_layer (file) ; nodes . insert (layer . clone ()) ; file_layers . insert (file . clone () , layer . clone ()) ; let references = collect_julia_dependencies (file) . with_context (\| \| format ! ("Failed to analyze Julia dependencies for {:?}" , file)) ? ; for dep in references { match dep . target { JuliaTarget :: Include (include_path) => { let resolved = if include_path . is_absolute () { include_path . clone () } else { file . parent () . map (\| p \| p . join (& include_path)) . unwrap_or (include_path . clone ()) } ; if resolved . exists () { let target_layer = crate :: cluster_001 :: detect_layer (& resolved) ; nodes . insert (target_layer . clone ()) ; if target_layer != layer { edges_map . entry ((target_layer . clone () , layer . clone ())) . or_default () . insert (ReferenceDetail { file : file . clone () , reference : dep . detail . clone () , }) ; } } else { unresolved . push (crate :: dependency :: UnresolvedDependency { file : file . clone () , reference : dep . detail . clone () , }) ; } } JuliaTarget :: Module (module) => { if let Some (target_layer) = resolver . resolve_module (& module) { nodes . insert (target_layer . clone ()) ; if target_layer != layer { edges_map . entry ((target_layer . clone () , layer . clone ())) . or_default () . insert (ReferenceDetail { file : file . clone () , reference : dep . detail . clone () , }) ; } } else { unresolved . push (crate :: dependency :: UnresolvedDependency { file : file . clone () , reference : dep . detail . clone () , }) ; } } } } } crate :: cluster_008 :: build_result (files , file_layers , nodes , edges_map , unresolved , & entry_files ,) } . sig` | 0.43 | intra 1, inter 1 | same 3, other 8 | move | - (cohesion 0.43 below threshold 0.60 (impact 0.17)) |
| `build_directory_entry_map` | `pub fn build_directory_entry_map (files : & [PathBuf] ,) -> Result < HashMap < PathBuf , crate :: types :: FileOrderEntry > > { use crate :: file_ordering :: { build_dependency_map , build_entries , build_file_dag , detect_cycles , ordered_by_name , topological_sort , } ; use crate :: layer_core :: layer_constrained_sort ; use crate :: layer_utilities :: build_file_layers ; use crate :: types :: FileOrderingResult ; use std :: collections :: HashSet ; const DEFAULT_STEP : usize = 10 ; if files . is_empty () { return Ok (HashMap :: new ()) ; } let file_set : HashSet < PathBuf > = files . iter () . cloned () . collect () ; let module_map = crate :: cluster_011 :: build_module_map (files) ; let dep_map = build_dependency_map (files , & file_set , & module_map) ? ; let file_layers = build_file_layers (files) ; let (graph , node_map) = build_file_dag (files , & dep_map) ; let cycles = detect_cycles (& graph , files) ; let ordered_nodes = if cycles . is_empty () { layer_constrained_sort (& graph , & file_layers) . unwrap_or_else (\| _ \| { topological_sort (& graph) . unwrap_or_else (\| _ \| ordered_by_name (files , & node_map)) }) } else { ordered_by_name (files , & node_map) } ; let ordered_files = ordered_nodes . into_iter () . map (\| idx \| graph [idx] . clone ()) . collect :: < Vec < _ > > () ; let ordering = FileOrderingResult { ordered_files : build_entries (& ordered_files , DEFAULT_STEP) , violations : Vec :: new () , layer_violations : Vec :: new () , ordered_directories : Vec :: new () , cycles , } ; let mut map = HashMap :: new () ; for entry in ordering . ordered_files { map . insert (entry . current_path . clone () , entry) ; } Ok (map) } . sig` | 0.49 | intra 7, inter 2 | same 0, other 3 | move | - (cohesion 0.49 below threshold 0.60 (impact 0.11)) |
| `layer_constrained_sort` | `pub fn layer_constrained_sort (graph : & DiGraph < PathBuf , () > , file_layers : & HashMap < PathBuf , String > ,) -> Result < Vec < NodeIndex > > { use crate :: cluster_006 :: layer_prefix_value ; let mut layer_nodes : BTreeMap < i32 , Vec < NodeIndex > > = BTreeMap :: new () ; for node in graph . node_indices () { let file = & graph [node] ; let layer_name = file_layers . get (file) . cloned () . unwrap_or_else (\| \| "root" . to_string ()) ; let layer_value = layer_prefix_value (& layer_name) . unwrap_or (0) ; layer_nodes . entry (layer_value) . or_default () . push (node) ; } let mut ordered = Vec :: new () ; for (_layer , nodes) in layer_nodes { let sorted = topo_sort_within (graph , & nodes) ? ; ordered . extend (sorted) ; } Ok (ordered) } . sig` | 0.57 | intra 1, inter 2 | same 0, other 0 | move | - (cohesion 0.57 below threshold 0.60 (impact 0.03)) |
| `collect_naming_warnings` | `pub fn collect_naming_warnings (directory : & crate :: types :: DirectoryAnalysis , config : & crate :: report :: ReportConfig , warnings : & mut Vec < String > ,) -> Result < () > { use crate :: utilities :: compress_path ; use crate :: dependency :: naming_score_for_file ; if directory . path . components () . any (\| comp \| comp . as_os_str () == "_old") { return Ok (()) ; } let file_map = build_directory_entry_map (& directory . files) ? ; for file in & directory . files { if file . components () . any (\| comp \| comp . as_os_str () == "_old") { continue ; } let entry = file_map . get (file) ; if let Some (score) = naming_score_for_file (file , entry) { if score < config . naming_score_warning { let suggested = entry . map (\| e \| e . suggested_name . as_str ()) . unwrap_or ("suggested name unavailable") ; warnings . push (format ! ("File `{}` has naming score {:.0}; consider renaming to `{}`." , compress_path (file . to_string_lossy () . as_ref ()) , score , suggested ,)) ; } } } for child in & directory . subdirectories { collect_naming_warnings (child , config , warnings) ? ; } Ok (()) } . sig` | 0.60 | intra 3, inter 0 | same 0, other 2 | ok | - |
| `order_rust_files_by_dependency` | `# [doc = " Order Rust files by dependency and capture layer graph details."] pub fn order_rust_files_by_dependency (files : & [PathBuf] , root : & Path ,) -> Result < (Vec < PathBuf > , LayerGraph) > { let module_map = crate :: cluster_010 :: build_module_root_map (root) ? ; let entry_files = rust_entry_paths (root) ; let mut file_layers : HashMap < PathBuf , String > = HashMap :: new () ; let mut nodes : BTreeSet < String > = BTreeSet :: new () ; let mut edges_map : BTreeMap < (String , String) , BTreeSet < ReferenceDetail > > = BTreeMap :: new () ; let mut unresolved = Vec :: new () ; for file in files { let layer = detect_layer (file) ; nodes . insert (layer . clone ()) ; file_layers . insert (file . clone () , layer . clone ()) ; let deps = collect_rust_dependencies (file) . with_context (\| \| format ! ("Failed to collect dependencies for {:?}" , file)) ? ; for dep in deps { if let Some (info) = module_map . get (& dep . root) { nodes . insert (info . layer . clone ()) ; if info . layer != layer { edges_map . entry ((info . layer . clone () , layer . clone ())) . or_default () . insert (ReferenceDetail { file : file . clone () , reference : dep . detail . clone () , }) ; } } else { unresolved . push (UnresolvedDependency { file : file . clone () , reference : dep . detail . clone () , }) ; } } } crate :: cluster_008 :: build_result (files , file_layers , nodes , edges_map , unresolved , & entry_files ,) } . sig` | 0.60 | intra 3, inter 0 | same 0, other 4 | ok | - |
| `build_entries` | `# [doc = " Builds file ordering entries with canonical names and rename flags"] pub fn build_entries (ordered : & [PathBuf] , step : usize) -> Vec < crate :: types :: FileOrderEntry > { ordered . iter () . enumerate () . map (\| (idx , path) \| { let canonical_order = idx * step ; let suggested_name = crate :: cluster_006 :: generate_canonical_name (path , canonical_order) ; let needs_rename = path . file_name () . and_then (\| n \| n . to_str ()) . map (\| name \| name != suggested_name) . unwrap_or (false) ; crate :: types :: FileOrderEntry { current_path : path . clone () , canonical_order , suggested_name , needs_rename , } }) . collect () } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | ok | - |
| `analyze_file_ordering` | `pub fn analyze_file_ordering (files : & [PathBuf] , step : Option < usize > ,) -> Result < crate :: types :: FileOrderingResult > { let step = step . unwrap_or (10) ; let file_set : HashSet < PathBuf > = files . iter () . cloned () . collect () ; let module_map = crate :: cluster_011 :: build_module_map (files) ; let dep_map = crate :: cluster_010 :: build_dependency_map (files , & file_set , & module_map) ? ; let file_layers = build_file_layers (files) ; let ordered_directories = crate :: layer_core :: order_directories (files , & dep_map) ; let (graph , node_map) = crate :: cluster_011 :: build_file_dag (files , & dep_map) ; let layer_violations = crate :: cluster_008 :: detect_layer_violations (& graph , & file_layers) ; let cycles = detect_cycles (& graph , files) ; let ordered_nodes = if cycles . is_empty () { crate :: layer_core :: layer_constrained_sort (& graph , & file_layers) . unwrap_or_else (\| _ \| { topological_sort (& graph) . unwrap_or_else (\| _ \| ordered_by_name (files , & node_map)) }) } else { ordered_by_name (files , & node_map) } ; let ordered_files = ordered_nodes . into_iter () . map (\| idx \| graph [idx] . clone ()) . collect :: < Vec < _ > > () ; let file_entries = build_entries (& ordered_files , step) ; let violations = detect_violations (& file_entries , & dep_map) ; Ok (crate :: types :: FileOrderingResult { ordered_files : file_entries , violations , layer_violations , ordered_directories , cycles , }) } . sig` | 0.60 | intra 7, inter 0 | same 0, other 2 | ok | - |
| `naming_score_for_file` | `pub fn naming_score_for_file (file : & Path , order_entry : Option < & crate :: types :: FileOrderEntry > ,) -> Option < f64 > { let name = file . file_name () ? . to_string_lossy () ; let stem = file . file_stem () ? . to_string_lossy () ; let mut score = 1.0f64 ; if stem . len () < 3 { score -= 0.2 ; } if stem . len () > 40 { score -= 0.1 ; } if stem . chars () . any (\| c \| c . is_uppercase ()) { score -= 0.1 ; } if ! stem . chars () . all (\| c \| c . is_ascii_lowercase () \|\| c . is_ascii_digit () \|\| c == '_') { score -= 0.1 ; } if name . contains ("__") { score -= 0.1 ; } if let Some (entry) = order_entry { let expected = entry . suggested_name . as_str () ; let actual = name . as_ref () ; if expected != actual { score -= 0.3 ; } else { score += 0.1 ; } } if let Ok (contents) = fs :: read_to_string (file) { let mut ident_counts : HashMap < String , usize > = HashMap :: new () ; let ident_re = match Regex :: new (r"[A-Za-z_][A-Za-z0-9_]*") { Ok (regex) => regex , Err (_) => return None , } ; for cap in ident_re . captures_iter (& contents) { let Some (m) = cap . get (0) else { continue ; } ; let ident = m . as_str () . to_lowercase () ; if matches ! (ident . as_str () , "fn" \| "pub" \| "use" \| "struct" \| "enum" \| "impl" \| "mod" \| "let" \| "mut" \| "ref" \| "self" \| "crate" \| "super" \| "where" \| "trait" \| "type" \| "const" \| "static" \| "match" \| "if" \| "else" \| "for" \| "while" \| "loop" \| "return" \| "async" \| "await" \| "move" \| "dyn" \| "as") { continue ; } * ident_counts . entry (ident) . or_insert (0) += 1 ; } let mut idents = ident_counts . into_iter () . collect :: < Vec < _ > > () ; idents . sort_by (\| a , b \| b . 1 . cmp (& a . 1)) ; let top_idents = idents . into_iter () . take (8) . map (\| (k , _) \| k) . collect :: < Vec < _ > > () ; let name_tokens = stem . split ('_') . map (\| s \| s . to_lowercase ()) . filter (\| s \| ! s . is_empty () && ! s . chars () . all (\| c \| c . is_ascii_digit ())) . collect :: < Vec < _ > > () ; let overlap = top_idents . iter () . filter (\| ident \| name_tokens . iter () . any (\| t \| t == * ident)) . count () ; if overlap == 0 { score -= 0.1 ; } else if overlap >= 2 { score += 0.1 ; } } if score < 0.0 { score = 0.0 ; } if score > 1.0 { score = 1.0 ; } Some (score * 100.0) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `detect_violations` | `pub (crate) fn detect_violations (ordered_files : & [crate :: types :: FileOrderEntry] , dep_map : & HashMap < PathBuf , Vec < PathBuf > > ,) -> Vec < crate :: types :: OrderViolation > { let mut alpha = ordered_files . to_vec () ; alpha . sort_by (\| a , b \| a . current_path . cmp (& b . current_path)) ; let alpha_positions : HashMap < PathBuf , usize > = alpha . iter () . enumerate () . map (\| (idx , entry) \| (entry . current_path . clone () , idx)) . collect () ; let canonical_positions : HashMap < PathBuf , usize > = ordered_files . iter () . enumerate () . map (\| (idx , entry) \| (entry . current_path . clone () , idx)) . collect () ; let mut violations = Vec :: new () ; for entry in ordered_files { let Some (& alpha_pos) = alpha_positions . get (& entry . current_path) else { continue ; } ; let Some (& required_pos) = canonical_positions . get (& entry . current_path) else { continue ; } ; if alpha_pos != required_pos { let blocking_dependencies = dep_map . get (& entry . current_path) . map (\| deps \| { deps . iter () . filter (\| dep \| { let dep_alpha = alpha_positions . get (* dep) . copied () . unwrap_or (0) ; dep_alpha > alpha_pos }) . cloned () . collect :: < Vec < _ > > () }) . unwrap_or_default () ; violations . push (crate :: types :: OrderViolation { file : entry . current_path . clone () , current_position : alpha_pos , required_position : required_pos , blocking_dependencies , }) ; } } violations } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `export_complete_program_dot` | `# [doc = " Exports a complete program CFG to DOT format"] pub fn export_complete_program_dot (program : & crate :: types :: ProgramCFG , path : & str ,) -> std :: io :: Result < () > { use std :: collections :: HashMap ; use std :: fmt :: Write ; fn escape_dot (s : & str) -> String { s . replace ('\\' , "\\\\") . replace ('"' , "\\\"") . replace ('\n' , "\\n") } let mut dot = String :: new () ; writeln ! (dot , "digraph ProgramCFG {{") . unwrap () ; writeln ! (dot , "  rankdir=TB;") . unwrap () ; writeln ! (dot , "  compound=true;") . unwrap () ; writeln ! (dot , "  newrank=true;") . unwrap () ; writeln ! (dot , "  label=\"Complete Program CFG - {} functions\";" , program . functions . len ()) . unwrap () ; writeln ! (dot , "  labelloc=t;") . unwrap () ; writeln ! (dot , "  fontsize=16;") . unwrap () ; writeln ! (dot , "") . unwrap () ; let mut funcs : Vec < _ > = program . functions . iter () . collect () ; funcs . sort_by_key (\| (fid , _) \| fid . as_str ()) ; let mut func_to_cluster : HashMap < & String , usize > = HashMap :: new () ; for (cluster_idx , (func_id , cfg)) in funcs . iter () . enumerate () { let safe_name = func_id . replace (['!' , '?' , '*'] , "_") ; let cc = crate :: cluster_008 :: cyclomatic_complexity (cfg) ; func_to_cluster . insert (func_id , cluster_idx) ; writeln ! (dot , "  subgraph cluster_{} {{" , cluster_idx) . unwrap () ; writeln ! (dot , "    label=\"{} (CC={})\";" , safe_name , cc) . unwrap () ; writeln ! (dot , "    style=filled;") . unwrap () ; writeln ! (dot , "    fillcolor=lightgray;") . unwrap () ; writeln ! (dot , "    color=black;") . unwrap () ; writeln ! (dot , "") . unwrap () ; for node in & cfg . nodes { let (shape , color , style) = crate :: cluster_008 :: node_style (& node . node_type) ; let mut label = node . label . clone () ; if ! node . lines . is_empty () { let lines_str : String = node . lines . iter () . map (\| l \| l . to_string ()) . collect :: < Vec < _ > > () . join (",") ; label = format ! ("{} L{}" , label , lines_str) ; } let url = format ! ("http://127.0.0.1:8081/run?f={}" , func_id) ; writeln ! (dot , "    f{}_n{} [label=\"{}\", shape={}, fillcolor={}, style={}, URL=\"{}\"];" , cluster_idx , node . id , escape_dot (& label) , shape , color , style , url) . unwrap () ; } writeln ! (dot , "") . unwrap () ; for edge in & cfg . edges { let mut attrs = Vec :: new () ; if let Some (cond) = edge . condition { let label = if cond { "T" } else { "F" } ; let color = if cond { "darkgreen" } else { "red" } ; attrs . push (format ! ("label=\"{}\"" , label)) ; attrs . push (format ! ("color=\"{}\"" , color)) ; } let attr_str = if attrs . is_empty () { "" . to_string () } else { format ! (" [{}]" , attrs . join (", ")) } ; writeln ! (dot , "    f{}_n{} -> f{}_n{}{};" , cluster_idx , edge . from , cluster_idx , edge . to , attr_str) . unwrap () ; } writeln ! (dot , "  }}") . unwrap () ; writeln ! (dot , "") . unwrap () ; } writeln ! (dot , "  // Inter-function calls") . unwrap () ; writeln ! (dot , "  edge [style=dashed, color=blue, penwidth=2];") . unwrap () ; writeln ! (dot , "") . unwrap () ; for (caller , callee) in & program . call_edges { if let (Some (& caller_idx) , Some (& callee_idx)) = (func_to_cluster . get (caller) , func_to_cluster . get (callee)) { if let (Some (caller_cfg) , Some (callee_cfg)) = (program . functions . get (caller) , program . functions . get (callee)) { writeln ! (dot , "  f{}_n{} -> f{}_n{} [ltail=cluster_{}, lhead=cluster_{}, label=\"call\"];" , caller_idx , caller_cfg . exit_id , callee_idx , callee_cfg . entry_id , caller_idx , callee_idx) . unwrap () ; } } } writeln ! (dot , "}}") . unwrap () ; std :: fs :: write (path , dot) ? ; println ! ("Complete program CFG exported to {}" , path) ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | orphaned | - (suggest module utilities) |
| `topo_sort_within` | `pub fn topo_sort_within (graph : & DiGraph < PathBuf , () > , nodes : & [NodeIndex] ,) -> Result < Vec < NodeIndex > > { let node_set : HashSet < NodeIndex > = nodes . iter () . copied () . collect () ; let mut indegree : HashMap < NodeIndex , usize > = HashMap :: new () ; for & node in nodes { indegree . insert (node , 0) ; } for & node in nodes { let incoming = graph . neighbors_directed (node , petgraph :: Direction :: Incoming) . filter (\| n \| node_set . contains (n)) . count () ; indegree . insert (node , incoming) ; } let mut queue = std :: collections :: VecDeque :: new () ; for & node in nodes { if indegree . get (& node) . copied () . unwrap_or (0) == 0 { queue . push_back (node) ; } } let mut ordered = Vec :: new () ; while let Some (node) = queue . pop_front () { ordered . push (node) ; for neighbor in graph . neighbors_directed (node , petgraph :: Direction :: Outgoing) { if ! node_set . contains (& neighbor) { continue ; } if let Some (entry) = indegree . get_mut (& neighbor) { * entry = entry . saturating_sub (1) ; if * entry == 0 { queue . push_back (neighbor) ; } } } } if ordered . len () != nodes . len () { return Err (anyhow :: anyhow ! ("Cycle detected within layer group")) ; } Ok (ordered) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `detect_layer` | `# [doc = " Detects the layer identifier from a path by finding first digit-prefixed component"] pub fn detect_layer (path : & Path) -> String { for component in path . components () { if let Some (name) = component . as_os_str () . to_str () { if let Some (first) = name . chars () . next () { if first . is_ascii_digit () { if let Some (pos) = name . find ('_') { if name [.. pos] . chars () . all (\| c \| c . is_ascii_digit ()) { return name . to_string () ; } } } } } } "root" . to_string () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `rust_entry_paths` | `pub fn rust_entry_paths (root : & Path) -> BTreeSet < PathBuf > { let src_dir = crate :: layer_utilities :: resolve_source_root (root) ; ["lib.rs" , "main.rs"] . iter () . map (\| rel \| src_dir . join (rel)) . filter (\| p \| p . exists ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `collect_rust_dependencies` | `fn collect_rust_dependencies (path : & Path) -> Result < Vec < RustDependency > > { let content = fs :: read_to_string (path) . with_context (\| \| format ! ("Unable to read Rust file {:?}" , path)) ? ; let syntax = syn :: parse_file (& content) . with_context (\| \| format ! ("Unable to parse Rust file {:?}" , path)) ? ; let mut collector = UseCollector :: default () ; collector . visit_file (& syntax) ; Ok (collector . deps) } . sig` | 0.90 | intra 0, inter 0 | same 2, other 0 | ok | - |
| `collect_julia_dependencies` | `pub (crate) fn collect_julia_dependencies (path : & Path) -> Result < Vec < JuliaDependency > > { let content = fs :: read_to_string (path) . with_context (\| \| format ! ("Unable to read Julia file {:?}" , path)) ? ; let mut deps = Vec :: new () ; for cap in INCLUDE_REGEX . captures_iter (& content) { if let Some (path_match) = cap . get (1) { let relative = PathBuf :: from (path_match . as_str ()) ; let detail = cap . get (0) . map (\| m \| m . as_str () . trim () . to_string ()) . unwrap_or_default () ; deps . push (JuliaDependency { target : JuliaTarget :: Include (relative) , detail , }) ; } } for cap in USING_REGEX . captures_iter (& content) { if let Some (module_match) = cap . get (1) { let module = module_match . as_str () ; let primary = module . split ('.') . next () . unwrap_or (module) . to_string () ; let detail = cap . get (0) . map (\| m \| m . as_str () . trim () . to_string ()) . unwrap_or_default () ; deps . push (JuliaDependency { target : JuliaTarget :: Module (primary) , detail , }) ; } } for cap in ROOT_USING_REGEX . captures_iter (& content) { if let Some (symbols) = cap . get (1) { let detail = cap . get (0) . map (\| m \| m . as_str () . trim () . to_string ()) . unwrap_or_default () ; for symbol in symbols . as_str () . split (',') . map (\| s \| s . trim ()) . filter (\| s \| ! s . is_empty ()) { let primary = symbol . split ('.') . next () . unwrap_or (symbol) . to_string () ; deps . push (JuliaDependency { target : JuliaTarget :: Module (primary) , detail : detail . clone () , }) ; } } } for cap in LOCAL_USING_REGEX . captures_iter (& content) { if let Some (module_match) = cap . get (1) { let module = module_match . as_str () ; let detail = cap . get (0) . map (\| m \| m . as_str () . trim () . to_string ()) . unwrap_or_default () ; deps . push (JuliaDependency { target : JuliaTarget :: Module (module . to_string ()) , detail , }) ; } } Ok (deps) } . sig` | 0.90 | intra 0, inter 0 | same 9, other 0 | ok | - |
| `julia_entry_paths` | `pub fn julia_entry_paths (root : & Path) -> BTreeSet < PathBuf > { let src_dir = crate :: layer_utilities :: resolve_source_root (root) ; ["MMSB.jl" , "API.jl" , "MMSB/API.jl"] . iter () . map (\| rel \| src_dir . join (rel)) . filter (\| p \| p . exists ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `build_file_layers` | `pub fn build_file_layers (files : & [PathBuf]) -> HashMap < PathBuf , String > { let mut layers = HashMap :: new () ; for file in files { layers . insert (file . clone () , detect_layer (file)) ; } layers } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `gather_julia_files` | `pub fn gather_julia_files (root : & Path) -> Vec < PathBuf > { use walkdir :: WalkDir ; let src_root = crate :: layer_utilities :: resolve_source_root (root) ; WalkDir :: new (& src_root) . into_iter () . filter_entry (\| entry \| { if entry . depth () == 0 { return true ; } if ! entry . file_type () . is_dir () { return true ; } crate :: layer_utilities :: allow_analysis_dir (& src_root , entry . path ()) }) . filter_map (\| e \| e . ok ()) . filter (\| e \| e . path () . extension () . map_or (false , \| ext \| ext == "jl")) . filter (\| e \| { let rel = e . path () . strip_prefix (& src_root) . unwrap_or (e . path ()) ; rel . components () . count () == 1 \|\| e . path () . starts_with (src_root . join ("src")) }) . map (\| entry \| entry . into_path ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `topological_sort` | `pub fn topological_sort (graph : & DiGraph < PathBuf , () >) -> Result < Vec < NodeIndex > > { use petgraph :: Direction ; use std :: collections :: VecDeque ; let mut indegree = vec ! [0usize ; graph . node_count ()] ; for node in graph . node_indices () { indegree [node . index ()] = graph . neighbors_directed (node , Direction :: Incoming) . count () ; } let mut queue = VecDeque :: new () ; for node in graph . node_indices () { if indegree [node . index ()] == 0 { queue . push_back (node) ; } } let mut ordered = Vec :: new () ; while let Some (node) = queue . pop_front () { ordered . push (node) ; for neighbor in graph . neighbors_directed (node , Direction :: Outgoing) { let entry = & mut indegree [neighbor . index ()] ; * entry = entry . saturating_sub (1) ; if * entry == 0 { queue . push_back (neighbor) ; } } } if ordered . len () != graph . node_count () { return Err (anyhow :: anyhow ! ("Cycle detected in dependency graph")) ; } Ok (ordered) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `ordered_by_name` | `pub fn ordered_by_name (files : & [PathBuf] , node_map : & HashMap < PathBuf , NodeIndex > ,) -> Vec < NodeIndex > { let mut sorted = files . to_vec () ; sorted . sort () ; sorted . into_iter () . filter_map (\| path \| node_map . get (& path) . copied ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `detect_cycles` | `pub (crate) fn detect_cycles (graph : & DiGraph < PathBuf , () > , files : & [PathBuf] ,) -> Vec < Vec < PathBuf > > { let sccs = tarjan_scc (graph) ; let mut cycles = Vec :: new () ; for scc in sccs { if scc . len () > 1 { cycles . push (scc . into_iter () . map (\| idx \| graph [idx] . clone ()) . collect ()) ; } } if cycles . is_empty () { return cycles ; } if cycles . iter () . all (\| cycle \| cycle . is_empty ()) { let mut fallback = files . to_vec () ; fallback . sort () ; cycles . push (fallback) ; } cycles } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/010_cluster_008.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `cluster_target_path` | `pub fn cluster_target_path (target : PathBuf , members : & [crate :: report :: ClusterMember] , root_path : & Path , idx : usize ,) -> PathBuf { if ! is_core_module_path (& target) { return target ; } let prefix = target . file_stem () . and_then (\| name \| name . to_str ()) . and_then (\| stem \| layer_prefix_value (stem)) . unwrap_or (900) ; let file_name = format ! ("{:03}_cluster_{:03}.rs" , prefix , idx + 1) ; let dir = members . first () . and_then (\| member \| member . file . parent ()) . unwrap_or (root_path) ; dir . join (file_name) } . sig` | 0.43 | intra 2, inter 1 | same 0, other 1 | move | - (cohesion 0.43 below threshold 0.60 (impact 0.17)) |
| `build_result` | `pub fn build_result (files : & [PathBuf] , file_layers : HashMap < PathBuf , String > , nodes : BTreeSet < String > , edges_map : BTreeMap < (String , String) , BTreeSet < ReferenceDetail > > , unresolved : Vec < UnresolvedDependency > , entry_files : & BTreeSet < PathBuf > ,) -> Result < (Vec < PathBuf > , LayerGraph) > { let adjacency = adjacency_from_edges (& edges_map) ; let (mut ordered_layers , cycles) = topo_sort (& nodes , & adjacency) ; if let Some (pos) = ordered_layers . iter () . position (\| layer \| layer == "root") { let root_layer = ordered_layers . remove (pos) ; ordered_layers . insert (0 , root_layer) ; } let rank = layer_rank_map (& ordered_layers) ; let mut ordered_files = files . to_vec () ; ordered_files . sort_by (\| a , b \| { let mmsb_a = is_mmsb_main (a) ; let mmsb_b = is_mmsb_main (b) ; if mmsb_a && ! mmsb_b { return Ordering :: Less ; } else if mmsb_b && ! mmsb_a { return Ordering :: Greater ; } let entry_a = entry_files . contains (a) ; let entry_b = entry_files . contains (b) ; if entry_a && ! entry_b { return Ordering :: Less ; } else if entry_b && ! entry_a { return Ordering :: Greater ; } let layer_a = file_layers . get (a) . cloned () . unwrap_or_else (\| \| "root" . to_string ()) ; let layer_b = file_layers . get (b) . cloned () . unwrap_or_else (\| \| "root" . to_string ()) ; let rank_a = rank . get (& layer_a) . cloned () . unwrap_or (ordered_layers . len ()) ; let rank_b = rank . get (& layer_b) . cloned () . unwrap_or (ordered_layers . len ()) ; rank_a . cmp (& rank_b) . then_with (\| \| layer_a . cmp (& layer_b)) . then_with (\| \| a . cmp (b)) }) ; let edges = edges_map . into_iter () . map (\| ((from , to) , references) \| LayerEdge { violation : is_layer_violation (& from , & to) , from , to , references : references . into_iter () . collect () , }) . collect () ; let graph = LayerGraph { ordered_layers , edges , cycles , unresolved , } ; Ok ((ordered_files , graph)) } . sig` | 0.60 | intra 6, inter 0 | same 0, other 5 | ok | - |
| `adjacency_from_edges` | `fn adjacency_from_edges (edges_map : & BTreeMap < (String , String) , BTreeSet < ReferenceDetail > > ,) -> HashMap < String , BTreeSet < String > > { let mut adjacency : HashMap < String , BTreeSet < String > > = HashMap :: new () ; for ((from , to) , _) in edges_map { adjacency . entry (from . clone ()) . or_default () . insert (to . clone ()) ; } adjacency } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `parse_cluster_members` | `pub fn parse_cluster_members (cluster : & crate :: types :: FunctionCluster ,) -> Vec < crate :: report :: ClusterMember > { cluster . members . iter () . filter_map (\| member \| { let (file , name) = member . rsplit_once ("::") ? ; Some (crate :: report :: ClusterMember { file : PathBuf :: from (file) , name : name . to_string () , }) }) . collect () } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `collect_cluster_plans` | `pub fn collect_cluster_plans (clusters : & [crate :: types :: FunctionCluster] , root_path : & Path ,) -> Vec < crate :: report :: ClusterPlan > { let mut plans = Vec :: new () ; for (idx , cluster) in clusters . iter () . enumerate () { let all_members = parse_cluster_members (cluster) ; let target = if let Some (suggested) = & cluster . suggested_file { suggested . clone () } else if let Some (first) = all_members . first () { let file_name = format ! ("900_cluster_{:03}.rs" , idx + 1) ; first . file . parent () . unwrap_or (root_path) . join (file_name) } else { let file_name = format ! ("900_cluster_{:03}.rs" , idx + 1) ; root_path . join (file_name) } ; let target = cluster_target_path (target , & all_members , root_path , idx) ; let members = all_members . into_iter () . filter (\| member \| member . file != target) . collect :: < Vec < _ > > () ; if members . len () < 2 { continue ; } plans . push (crate :: report :: ClusterPlan { target , cohesion : cluster . cohesion , members , }) ; } plans . sort_by (\| a , b \| { use std :: cmp :: Ordering ; b . cohesion . partial_cmp (& a . cohesion) . unwrap_or (Ordering :: Equal) . then_with (\| \| b . members . len () . cmp (& a . members . len ())) . then_with (\| \| a . target . cmp (& b . target)) }) ; plans } . sig` | 0.60 | intra 2, inter 0 | same 0, other 3 | ok | - |
| `node_style` | `pub fn node_style (node_type : & NodeType) -> (& str , & str , & str) { match node_type { NodeType :: Entry => ("ellipse" , "lightgreen" , "\"filled,bold\"") , NodeType :: Exit => ("doubleoctagon" , "lightcoral" , "\"filled,bold\"") , NodeType :: BasicBlock => ("box" , "lightblue" , "filled") , NodeType :: Branch => ("diamond" , "yellow" , "filled") , NodeType :: LoopHeader => ("box" , "orange" , "\"filled,rounded\"") , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 6 | orphaned | - (suggest module utilities) |
| `cyclomatic_complexity` | `pub fn cyclomatic_complexity (cfg : & crate :: types :: FunctionCfg) -> usize { let edges = cfg . edges . len () as isize ; let nodes = cfg . nodes . len () as isize ; let exits = 1isize ; let cc = edges - nodes + 2 * exits ; if cc <= 0 { 1 } else { cc as usize } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `structural_cmp` | `pub fn structural_cmp (a : & crate :: report :: PlanItem , b : & crate :: report :: PlanItem) -> std :: cmp :: Ordering { let a_required = structural_layer_value (& a . required_layer , i32 :: MAX) ; let b_required = structural_layer_value (& b . required_layer , i32 :: MAX) ; let a_current = structural_layer_value (& a . current_layer , i32 :: MIN) ; let b_current = structural_layer_value (& b . current_layer , i32 :: MIN) ; let a_benefit = if a . cost == 0 { 0 } else { (a . benefit . saturating_mul (1000)) / a . cost } ; let b_benefit = if b . cost == 0 { 0 } else { (b . benefit . saturating_mul (1000)) / b . cost } ; a_required . cmp (& b_required) . then_with (\| \| b . is_utility . cmp (& a . is_utility)) . then_with (\| \| b_benefit . cmp (& a_benefit)) . then_with (\| \| b . impact_weight . cmp (& a . impact_weight)) . then_with (\| \| b_current . cmp (& a_current)) . then_with (\| \| a . description . cmp (& b . description)) } . sig` | 0.60 | intra 4, inter 0 | same 0, other 2 | ok | - |
| `sort_structural_items` | `pub fn sort_structural_items (items : & mut Vec < crate :: report :: PlanItem >) { use std :: collections :: HashMap ; use std :: path :: PathBuf ; if items . len () <= 1 { return ; } let count = items . len () ; let mut edges : Vec < Vec < usize > > = vec ! [Vec :: new () ; count] ; let mut indegree = vec ! [0usize ; count] ; let mut file_to_items : HashMap < PathBuf , Vec < usize > > = HashMap :: new () ; for (idx , item) in items . iter () . enumerate () { if let Some (path) = & item . current_file { file_to_items . entry (path . clone ()) . or_default () . push (idx) ; } } for i in 0 .. count { for j in (i + 1) .. count { let req_i = structural_layer_value (& items [i] . required_layer , i32 :: MAX) ; let req_j = structural_layer_value (& items [j] . required_layer , i32 :: MAX) ; let mut edge = None ; if req_i != req_j { edge = if req_i < req_j { Some ((i , j)) } else { Some ((j , i)) } ; } else if items [i] . is_utility != items [j] . is_utility { edge = if items [i] . is_utility { Some ((i , j)) } else { Some ((j , i)) } ; } if let Some ((from , to)) = edge { edges [from] . push (to) ; indegree [to] += 1 ; } } } for (idx , item) in items . iter () . enumerate () { for file in & item . outgoing_files { if let Some (dependents) = file_to_items . get (file) { for & dependent_idx in dependents { if dependent_idx == idx { continue ; } edges [dependent_idx] . push (idx) ; indegree [idx] += 1 ; } } } } let mut ordered_indices = Vec :: with_capacity (count) ; let mut available : Vec < usize > = (0 .. count) . filter (\| & i \| indegree [i] == 0) . collect () ; while ! available . is_empty () { available . sort_by (\| & a , & b \| structural_cmp (& items [a] , & items [b])) ; let next = available . remove (0) ; ordered_indices . push (next) ; for & neighbor in & edges [next] { indegree [neighbor] = indegree [neighbor] . saturating_sub (1) ; if indegree [neighbor] == 0 { available . push (neighbor) ; } } } if ordered_indices . len () != count { items . sort_by (structural_cmp) ; return ; } let mut reordered = Vec :: with_capacity (count) ; for idx in ordered_indices { reordered . push (items [idx] . clone ()) ; } * items = reordered ; } . sig` | 0.60 | intra 3, inter 0 | same 0, other 1 | ok | - |
| `is_layer_violation` | `# [doc = " Checks if a dependency from one layer to another violates layer ordering"] # [doc = " Returns true if from_layer > to_layer (violation: higher depends on lower)"] pub fn is_layer_violation (from : & str , to : & str) -> bool { match (layer_prefix_value (from) , layer_prefix_value (to)) { (Some (a) , Some (b)) => a > b , _ => false , } } . sig` | 0.65 | intra 2, inter 1 | same 0, other 0 | ok | - |
| `compare_dir_layers` | `pub fn compare_dir_layers (a : & Path , b : & Path) -> Ordering { let a_name = a . file_name () . and_then (\| n \| n . to_str ()) . unwrap_or ("") ; let b_name = b . file_name () . and_then (\| n \| n . to_str ()) . unwrap_or ("") ; let a_layer = layer_prefix_value (a_name) . unwrap_or (i32 :: MAX) ; let b_layer = layer_prefix_value (b_name) . unwrap_or (i32 :: MAX) ; a_layer . cmp (& b_layer) . then_with (\| \| a_name . cmp (b_name)) } . sig` | 0.65 | intra 2, inter 1 | same 0, other 0 | ok | - |
| `compare_path_components` | `pub fn compare_path_components (a : & Path , b : & Path) -> Ordering { let a_components : Vec < _ > = a . components () . collect () ; let b_components : Vec < _ > = b . components () . collect () ; let min_len = a_components . len () . min (b_components . len ()) ; for idx in 0 .. min_len { let a_name = a_components [idx] . as_os_str () . to_string_lossy () ; let b_name = b_components [idx] . as_os_str () . to_string_lossy () ; let a_prefix = layer_prefix_value (& a_name) ; let b_prefix = layer_prefix_value (& b_name) ; let cmp = match (a_prefix , b_prefix) { (Some (a_val) , Some (b_val)) => a_val . cmp (& b_val) , _ => a_name . cmp (& b_name) , } ; if cmp != Ordering :: Equal { return cmp ; } } a_components . len () . cmp (& b_components . len ()) } . sig` | 0.65 | intra 2, inter 1 | same 0, other 0 | ok | - |
| `layer_adheres` | `pub fn layer_adheres (current_layer : & str , target_layer : & str) -> bool { match (layer_prefix_value (current_layer) , layer_prefix_value (target_layer)) { (Some (curr) , Some (target)) => curr <= target , _ => true , } } . sig` | 0.65 | intra 2, inter 1 | same 0, other 0 | ok | - |
| `structural_layer_value` | `pub (crate) fn structural_layer_value (layer : & Option < String > , default : i32) -> i32 { layer . as_ref () . and_then (\| value \| layer_prefix_value (value)) . unwrap_or (default) } . sig` | 0.65 | intra 1, inter 1 | same 0, other 0 | ok | - |
| `detect_layer_violation` | `pub fn detect_layer_violation (func : & FunctionInfo , functions : & [FunctionInfo] , outgoing : & HashMap < usize , usize > , file_layers : & HashMap < String , String > ,) -> Option < (String , String) > { let current_layer = file_layers . get (& func . file_path) . cloned () . unwrap_or_else (\| \| func . layer . clone ()) ; let current_value = layer_prefix_value (& current_layer) ? ; let mut violation : Option < (i32 , String) > = None ; for (callee_idx , _) in outgoing { let callee = & functions [* callee_idx] ; let target_layer = file_layers . get (& callee . file_path) . cloned () . unwrap_or_else (\| \| callee . layer . clone ()) ; if let Some (target_value) = layer_prefix_value (& target_layer) { if target_value < current_value { match violation { Some ((best_value , _)) if target_value >= best_value => { } _ => { violation = Some ((target_value , target_layer)) ; } } } } } violation . map (\| (_ , target_layer) \| (current_layer , target_layer)) } . sig` | 0.65 | intra 2, inter 1 | same 2, other 0 | ok | - |
| `topo_sort` | `fn topo_sort (nodes : & BTreeSet < String > , adjacency : & HashMap < String , BTreeSet < String > > ,) -> (Vec < String > , Vec < String >) { let mut indegree : HashMap < String , usize > = HashMap :: new () ; for node in nodes { indegree . entry (node . clone ()) . or_insert (0) ; } for targets in adjacency . values () { for target in targets { * indegree . entry (target . clone ()) . or_insert (0) += 1 ; } } let mut queue : VecDeque < String > = indegree . iter () . filter_map (\| (node , & deg) \| if deg == 0 { Some (node . clone ()) } else { None }) . collect () ; queue . make_contiguous () . sort () ; let mut order = Vec :: new () ; while let Some (node) = queue . pop_front () { order . push (node . clone ()) ; if let Some (targets) = adjacency . get (& node) { for target in targets { if let Some (entry) = indegree . get_mut (target) { * entry -= 1 ; if * entry == 0 { insert_sorted (& mut queue , target . clone ()) ; } } } } } if order . len () != nodes . len () { let mut remaining : Vec < _ > = nodes . iter () . filter (\| layer \| ! order . contains (layer)) . cloned () . collect () ; remaining . sort () ; let cycles = remaining . clone () ; order . extend (remaining) ; return (order , cycles) ; } (order , Vec :: new ()) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `layer_rank_map` | `fn layer_rank_map (order : & [String]) -> HashMap < String , usize > { let mut rank = HashMap :: new () ; for (idx , layer) in order . iter () . enumerate () { rank . insert (layer . clone () , idx) ; } rank } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `insert_sorted` | `fn insert_sorted (queue : & mut VecDeque < String > , value : String) { let mut inserted = false ; for idx in 0 .. queue . len () { if value < queue [idx] { queue . insert (idx , value . clone ()) ; inserted = true ; break ; } } if ! inserted { queue . push_back (value) ; } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `is_mmsb_main` | `fn is_mmsb_main (path : & Path) -> bool { path . file_name () . and_then (\| n \| n . to_str ()) . map (\| n \| n == "MMSB.jl") . unwrap_or (false) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `layer_prefix_value` | `# [doc = " Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)"] fn layer_prefix_value (layer : & str) -> Option < i32 > { let mut chars = layer . chars () ; let mut digits = String :: new () ; while let Some (ch) = chars . next () { if ch . is_ascii_digit () { digits . push (ch) ; } else { break ; } } if digits . is_empty () { None } else { digits . parse :: < i32 > () . ok () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `is_core_module_path` | `pub fn is_core_module_path (path : & Path) -> bool { let Some (stem) = path . file_stem () . and_then (\| name \| name . to_str ()) else { return false ; } ; stem . starts_with ("040_dependency") \|\| stem . starts_with ("060_layer_core") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/020_cluster_010.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `gather_rust_files` | `pub fn gather_rust_files (root : & Path) -> Vec < PathBuf > { use walkdir :: WalkDir ; let src_root = resolve_source_root (root) ; WalkDir :: new (& src_root) . into_iter () . filter_entry (\| entry \| { if entry . depth () == 0 { return true ; } if ! entry . file_type () . is_dir () { return true ; } allow_analysis_dir (& src_root , entry . path ()) }) . filter_map (\| e \| e . ok ()) . filter (\| e \| e . path () . extension () . map_or (false , \| ext \| ext == "rs")) . filter (\| e \| { let rel = e . path () . strip_prefix (& src_root) . unwrap_or (e . path ()) ; rel . components () . count () == 1 \|\| e . path () . starts_with (src_root . join ("src")) }) . map (\| entry \| entry . into_path ()) . collect () } . sig` | 0.40 | intra 0, inter 1 | same 0, other 0 | move | src/000_cluster_001.rs (cohesion 0.40 below threshold 0.60 (impact 0.20)) |

## File: src/030_refactor_constraints.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `generate_constraints` | `# [doc = " Generate all constraints from an invariant analysis result"] pub fn generate_constraints (analysis : & InvariantAnalysisResult) -> Vec < RefactorConstraint > { analysis . invariants . iter () . filter_map (from_invariant) . collect () } . sig` | 0.75 | intra 0, inter 0 | same 1, other 1 | ok | - |

## File: src/050_cluster_010.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `extract_rust_dependencies` | `pub fn extract_rust_dependencies (file : & Path , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Result < Vec < PathBuf > > { # [derive (Default)] struct UseCollector { roots : BTreeSet < String > , mods : BTreeSet < String > , } impl < 'ast > Visit < 'ast > for UseCollector { fn visit_item_use (& mut self , node : & 'ast ItemUse) { crate :: dependency :: collect_roots (& node . tree , RootState :: Start , & mut self . roots) ; } fn visit_item_mod (& mut self , node : & 'ast syn :: ItemMod) { if node . content . is_none () { self . mods . insert (node . ident . to_string ()) ; } } } let content = fs :: read_to_string (file) . with_context (\| \| format ! ("Unable to read {:?}" , file)) ? ; let syntax = syn :: parse_file (& content) . with_context (\| \| format ! ("Unable to parse Rust file {:?}" , file)) ? ; let mut collector = UseCollector :: default () ; collector . visit_file (& syntax) ; let mut deps = Vec :: new () ; for root in collector . roots { if let Some (path) = resolve_module (& root , file_set , module_map) { deps . push (path) ; } } for module in collector . mods { if let Some (path) = resolve_module (& module , file_set , module_map) { deps . push (path) ; } } Ok (deps) } . sig` | 0.82 | intra 2, inter 0 | same 3, other 1 | ok | - |
| `normalize_module_name` | `pub fn normalize_module_name (name : & str) -> String { if let Some (pos) = name . find ('_') { if name [.. pos] . chars () . all (\| c \| c . is_ascii_digit ()) { return name [pos + 1 ..] . to_string () ; } } name . to_string () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `resolve_module` | `pub fn resolve_module (root : & str , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Option < PathBuf > { let key = normalize_module_name (root) ; if let Some (path) = module_map . get (& key) { return Some (path . clone ()) ; } module_map . iter () . find (\| (name , _) \| name == & & key) . map (\| (_ , path) \| path . clone ()) . or_else (\| \| { module_map . iter () . find (\| (name , _) \| key . starts_with (name . as_str ())) . map (\| (_ , path) \| path . clone ()) }) . or_else (\| \| crate :: cluster_011 :: resolve_path (& PathBuf :: from (root) , file_set , module_map)) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `contains_tools` | `pub fn contains_tools (path : & Path) -> bool { path . components () . any (\| c \| c . as_os_str () == "tools") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `build_module_root_map` | `pub fn build_module_root_map (root : & Path) -> Result < HashMap < String , ModuleRoot > , std :: io :: Error > { let src_dir = root . join ("src") ; let mut map = HashMap :: new () ; if src_dir . is_dir () { for entry in fs :: read_dir (& src_dir) ? { let entry = entry ? ; let path = entry . path () ; if contains_tools (& path) { continue ; } let name = entry . file_name () . to_string_lossy () . to_string () . trim_end_matches (".rs") . to_string () ; if path . is_dir () { let normalized = normalize_module_name (& name) ; map . insert (normalized , ModuleRoot { layer : name . clone () , } ,) ; } else if path . extension () . map (\| ext \| ext == "rs") . unwrap_or (false) { map . insert (name . clone () , ModuleRoot { layer : crate :: cluster_001 :: detect_layer (& path) , } ,) ; } } } Ok (map) } . sig` | 0.90 | intra 2, inter 0 | same 3, other 0 | ok | - |
| `extract_julia_dependencies` | `pub fn extract_julia_dependencies (file : & Path , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Result < Vec < PathBuf > > { static INCLUDE_RE : Lazy < Regex > = Lazy :: new (\| \| Regex :: new (r#"include\s*\(\s*["']([^"']+)["']"#) . unwrap ()) ; static MMSB_USING_RE : Lazy < Regex > = Lazy :: new (\| \| { Regex :: new (r#"(?m)^\s*(?:using\|import)\s+MMSB\.([A-Za-z0-9_\.]+)"#) . unwrap () }) ; static MMSB_SYMBOL_RE : Lazy < Regex > = Lazy :: new (\| \| { Regex :: new (r#"(?m)^\s*(?:using\|import)\s+MMSB\s*:\s*([A-Za-z0-9_,\s]+)"#) . unwrap () }) ; static LOCAL_USING_RE : Lazy < Regex > = Lazy :: new (\| \| { Regex :: new (r#"(?m)^\s*(?:using\|import)\s+\.\s*([A-Za-z0-9_\.]+)"#) . unwrap () }) ; static PLAIN_USING_RE : Lazy < Regex > = Lazy :: new (\| \| { Regex :: new (r#"(?m)^\s*(?:using\|import)\s+([A-Za-z_][A-Za-z0-9_\.]*)"#) . unwrap () }) ; fn resolve_module_name (module : & str , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Option < PathBuf > { let primary = module . split ('.') . next () . unwrap_or (module) ; resolve_module (primary , file_set , module_map) } let content = fs :: read_to_string (file) . with_context (\| \| format ! ("Unable to read {:?}" , file)) ? ; let mut deps = Vec :: new () ; for cap in INCLUDE_RE . captures_iter (& content) { if let Some (path_match) = cap . get (1) { let raw = path_match . as_str () ; let mut candidate = PathBuf :: from (raw) ; if candidate . extension () . is_none () { candidate . set_extension ("jl") ; } let resolved = if candidate . is_absolute () { candidate } else { file . parent () . map (\| p \| p . join (& candidate)) . unwrap_or (candidate) } ; if let Some (path) = crate :: cluster_011 :: resolve_path (& resolved , file_set , module_map) { deps . push (path) ; } } } for cap in MMSB_USING_RE . captures_iter (& content) { if let Some (module_match) = cap . get (1) { if let Some (path) = resolve_module_name (module_match . as_str () , file_set , module_map) { deps . push (path) ; } } } for cap in MMSB_SYMBOL_RE . captures_iter (& content) { if let Some (symbols) = cap . get (1) { for symbol in symbols . as_str () . split (',') . map (\| s \| s . trim ()) . filter (\| s \| ! s . is_empty ()) { if let Some (path) = resolve_module_name (symbol , file_set , module_map) { deps . push (path) ; } } } } for cap in LOCAL_USING_RE . captures_iter (& content) { if let Some (module_match) = cap . get (1) { if let Some (path) = resolve_module_name (module_match . as_str () , file_set , module_map) { deps . push (path) ; } } } for cap in PLAIN_USING_RE . captures_iter (& content) { if let Some (module_match) = cap . get (1) { let module = module_match . as_str () ; if module . starts_with ("MMSB") { continue ; } if let Some (path) = resolve_module_name (module , file_set , module_map) { deps . push (path) ; } } } Ok (deps) } . sig` | 0.90 | intra 5, inter 0 | same 0, other 0 | ok | - |
| `resolve_module_name` | `fn resolve_module_name (module : & str , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Option < PathBuf > { let primary = module . split ('.') . next () . unwrap_or (module) ; resolve_module (primary , file_set , module_map) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `build_dependency_map` | `pub fn build_dependency_map (files : & [PathBuf] , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Result < HashMap < PathBuf , Vec < PathBuf > > > { let mut dep_map : HashMap < PathBuf , Vec < PathBuf > > = HashMap :: new () ; for file in files { let deps = extract_dependencies (file , file_set , module_map) . with_context (\| \| format ! ("Failed to extract dependencies for {:?}" , file)) ? ; dep_map . insert (file . clone () , deps) ; } Ok (dep_map) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `extract_dependencies` | `pub (crate) fn extract_dependencies (file : & Path , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Result < Vec < PathBuf > > { let ext = file . extension () . and_then (\| s \| s . to_str ()) . unwrap_or ("") ; match ext { "rs" => extract_rust_dependencies (file , file_set , module_map) , "jl" => extract_julia_dependencies (file , file_set , module_map) , _ => Ok (Vec :: new ()) , } } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |

## File: src/070_cluster_011.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `export_program_cfg_to_path` | `pub fn export_program_cfg_to_path (result : & crate :: types :: AnalysisResult , call_edges : & [(String , String)] , output_path : & Path ,) -> std :: io :: Result < () > { use crate :: types :: ProgramCFG ; let mut program_cfg = ProgramCFG { functions : HashMap :: new () , call_edges : Vec :: new () , } ; for cfg in & result . cfgs { program_cfg . functions . insert (cfg . function . clone () , cfg . clone ()) ; } for (caller , callee) in call_edges { let caller_name = caller . split ("::") . last () . unwrap_or (caller) . to_string () ; let callee_name = callee . split ("::") . last () . unwrap_or (callee) . to_string () ; if program_cfg . functions . contains_key (& caller_name) && program_cfg . functions . contains_key (& callee_name) { program_cfg . call_edges . push ((caller_name , callee_name)) ; } } let cfg_dir = output_path . join ("30_cfg") ; std :: fs :: create_dir_all (& cfg_dir) ? ; let dot_path = cfg_dir . join ("complete_program.dot") ; crate :: cluster_001 :: export_complete_program_dot (& program_cfg , dot_path . to_string_lossy () . as_ref () ,) ? ; # [cfg (feature = "png")] { let png_path = cfg_dir . join ("complete_program.png") ; if let (Some (dot_path_str) , Some (png_path_str)) = (dot_path . to_str () , png_path . to_str ()) { let _ = std :: process :: Command :: new ("dot") . args (["-Tpng" , dot_path_str , "-o" , png_path_str]) . status () ; } } Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `build_module_map` | `pub fn build_module_map (files : & [PathBuf]) -> HashMap < String , PathBuf > { let mut map = HashMap :: new () ; for file in files { if let Some (stem) = file . file_stem () . and_then (\| s \| s . to_str ()) { let normalized = crate :: cluster_010 :: normalize_module_name (stem) ; map . insert (normalized . clone () , file . clone ()) ; if stem == "mod" { if let Some (parent) = file . parent () . and_then (\| p \| p . file_name ()) { if let Some (name) = parent . to_str () { map . insert (crate :: cluster_010 :: normalize_module_name (name) , file . clone ()) ; } } } } } map } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `resolve_path` | `pub fn resolve_path (candidate : & Path , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Option < PathBuf > { if file_set . contains (candidate) { return Some (candidate . to_path_buf ()) ; } if let Some (file_name) = candidate . file_stem () . and_then (\| s \| s . to_str ()) { let key = crate :: cluster_010 :: normalize_module_name (file_name) ; if let Some (path) = module_map . get (& key) { return Some (path . clone ()) ; } } None } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `build_directory_dag` | `pub fn build_directory_dag (dir : & PathBuf) -> Result < DiGraph < PathBuf , () > > { let files : Vec < PathBuf > = walkdir :: WalkDir :: new (dir) . into_iter () . filter_map (\| e \| e . ok ()) . filter (\| e \| { e . path () . extension () . and_then (\| ext \| ext . to_str ()) . map (\| ext \| ext == "rs" \|\| ext == "jl") . unwrap_or (false) }) . map (\| entry \| entry . into_path ()) . collect () ; let file_set : HashSet < PathBuf > = files . iter () . cloned () . collect () ; let module_map = build_module_map (& files) ; let dep_map = crate :: cluster_010 :: build_dependency_map (& files , & file_set , & module_map) ? ; let (graph , _) = build_file_dag (& files , & dep_map) ; Ok (graph) } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |
| `build_file_dependency_graph` | `pub fn build_file_dependency_graph (files : & [PathBuf]) -> Result < DiGraph < PathBuf , () > > { let file_set : HashSet < PathBuf > = files . iter () . cloned () . collect () ; let module_map = build_module_map (files) ; let dep_map = crate :: cluster_010 :: build_dependency_map (files , & file_set , & module_map) ? ; let (graph , _) = build_file_dag (files , & dep_map) ; Ok (graph) } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |
| `build_file_dag` | `pub (crate) fn build_file_dag (files : & [PathBuf] , dep_map : & HashMap < PathBuf , Vec < PathBuf > > ,) -> (DiGraph < PathBuf , () > , HashMap < PathBuf , NodeIndex >) { let mut graph = DiGraph :: new () ; let mut node_map = HashMap :: new () ; for file in files { let node = graph . add_node (file . clone ()) ; node_map . insert (file . clone () , node) ; } for (file , deps) in dep_map { if let Some (& file_node) = node_map . get (file) { for dep in deps { if let Some (& dep_node) = node_map . get (dep) { graph . add_edge (dep_node , file_node , ()) ; } } } } (graph , node_map) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/090_dependency.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `collect_roots` | `pub fn collect_roots (tree : & UseTree , state : RootState , acc : & mut BTreeSet < String >) { match tree { UseTree :: Path (path) => { let ident = path . ident . to_string () ; if state == RootState :: Start && matches ! (ident . as_str () , "crate" \| "self" \| "super") { collect_roots (& path . tree , RootState :: AfterRoot , acc) ; } else if state == RootState :: AfterRoot { acc . insert (ident) ; } else { acc . insert (ident) ; } } UseTree :: Group (group) => { for tree in & group . items { collect_roots (tree , state , acc) ; } } UseTree :: Name (name) => { acc . insert (name . ident . to_string ()) ; } UseTree :: Rename (rename) => { acc . insert (rename . ident . to_string ()) ; } UseTree :: Glob (_) => { } } } . sig` | 0.90 | intra 2, inter 0 | same 4, other 0 | ok | - |

## File: src/110_cluster_006.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `collect_directory_moves` | `pub fn collect_directory_moves (ordering : & crate :: types :: FileOrderingResult , root_path : & Path ,) -> Vec < crate :: file_ordering :: DirectoryMove > { let mut moves = Vec :: new () ; let mut by_parent : BTreeMap < PathBuf , Vec < PathBuf > > = BTreeMap :: new () ; let src_dir = root_path . join ("src") ; for dir in & ordering . ordered_directories { if dir == root_path { continue ; } if dir == & src_dir { continue ; } if let Some (parent) = dir . parent () { by_parent . entry (parent . to_path_buf ()) . or_default () . push (dir . clone ()) ; } } for (parent , mut dirs) in by_parent { dirs . sort_by (\| a , b \| crate :: cluster_008 :: compare_dir_layers (a , b)) ; for (idx , dir) in dirs . iter () . enumerate () { let Some (name) = dir . file_name () . and_then (\| n \| n . to_str ()) else { continue ; } ; let clean = strip_numeric_prefix (name) ; let suggested = format ! ("{:03}_{}" , idx * 10 , clean) ; if name == suggested { continue ; } let to = parent . join (& suggested) ; moves . push (crate :: file_ordering :: DirectoryMove { from : dir . clone () , to , }) ; } } moves } . sig` | 0.60 | intra 1, inter 0 | same 0, other 3 | ok | - |
| `compute_cohesion_score` | `pub fn compute_cohesion_score (func : & FunctionInfo , functions : & [FunctionInfo] , outgoing : & HashMap < usize , usize > , file_layers : & HashMap < String , String > , call_analysis : & crate :: types :: CallAnalysis ,) -> f64 { let mut total_calls = 0usize ; let mut intra_calls = 0usize ; let mut external_calls = 0usize ; let mut layer_ok = 0usize ; for (callee_idx , count) in outgoing { total_calls += count ; let callee = & functions [* callee_idx] ; if callee . file_path == func . file_path { intra_calls += count ; } else { external_calls += count ; } let current_layer = file_layers . get (& func . file_path) . cloned () . unwrap_or_else (\| \| func . layer . clone ()) ; let target_layer = file_layers . get (& callee . file_path) . cloned () . unwrap_or_else (\| \| callee . layer . clone ()) ; if crate :: cluster_008 :: layer_adheres (& current_layer , & target_layer) { layer_ok += count ; } } let total_calls_f = total_calls as f64 ; let call_locality = if total_calls == 0 { 1.0 } else { intra_calls as f64 / total_calls_f } ; let layer_adherence = if total_calls == 0 { 1.0 } else { layer_ok as f64 / total_calls_f } ; let cross_file_calls = if total_calls == 0 { 0.0 } else { external_calls as f64 / total_calls_f } ; let total_type_refs = call_analysis . same_file_type_refs + call_analysis . other_file_type_refs ; let type_coupling = if total_type_refs == 0 { 1.0 } else { call_analysis . same_file_type_refs as f64 / total_type_refs as f64 } ; let score = 0.4 * call_locality + 0.3 * type_coupling + 0.2 * layer_adherence - 0.1 * cross_file_calls ; score . clamp (0.0 , 1.0) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | orphaned | - (suggest module utilities) |
| `layer_prefix_value` | `# [doc = " Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)"] pub fn layer_prefix_value (layer : & str) -> Option < i32 > { let mut chars = layer . chars () ; let mut digits = String :: new () ; while let Some (ch) = chars . next () { if ch . is_ascii_digit () { digits . push (ch) ; } else { break ; } } if digits . is_empty () { None } else { digits . parse :: < i32 > () . ok () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `order_directories` | `pub fn order_directories (files : & [PathBuf] , dep_map : & HashMap < PathBuf , Vec < PathBuf > > ,) -> Vec < PathBuf > { let root = common_root (files) ; let mut dirs : HashSet < PathBuf > = HashSet :: new () ; for file in files { let mut current = file . parent () . map (Path :: to_path_buf) ; while let Some (dir) = current { if let Some (ref root_path) = root { if ! dir . starts_with (root_path) { break ; } } dirs . insert (dir . clone ()) ; current = dir . parent () . map (Path :: to_path_buf) ; } } let mut ordered : Vec < PathBuf > = dirs . into_iter () . collect () ; ordered . sort_by (\| a , b \| crate :: cluster_008 :: compare_path_components (a , b)) ; let mut node_map = HashMap :: new () ; for (idx , dir) in ordered . iter () . enumerate () { node_map . insert (dir . clone () , idx) ; } let mut adjacency : Vec < BTreeSet < usize > > = vec ! [BTreeSet :: new () ; ordered . len ()] ; let mut indegree = vec ! [0usize ; ordered . len ()] ; for (file , deps) in dep_map { let Some (from_dir) = file . parent () . map (Path :: to_path_buf) else { continue ; } ; let Some (& from_idx) = node_map . get (& from_dir) else { continue ; } ; for dep in deps { let Some (to_dir) = dep . parent () . map (Path :: to_path_buf) else { continue ; } ; if to_dir == from_dir { continue ; } let Some (& to_idx) = node_map . get (& to_dir) else { continue ; } ; if adjacency [to_idx] . insert (from_idx) { indegree [from_idx] += 1 ; } } } let mut queue : BTreeSet < usize > = indegree . iter () . enumerate () . filter_map (\| (idx , & deg) \| if deg == 0 { Some (idx) } else { None }) . collect () ; let mut result = Vec :: with_capacity (ordered . len ()) ; while let Some (& idx) = queue . iter () . next () { queue . remove (& idx) ; result . push (ordered [idx] . clone ()) ; let neighbors = adjacency [idx] . clone () ; for neighbor in neighbors { let entry = & mut indegree [neighbor] ; if * entry > 0 { * entry -= 1 ; if * entry == 0 { queue . insert (neighbor) ; } } } } if result . len () < ordered . len () { for (idx , dir) in ordered . iter () . enumerate () { if indegree [idx] > 0 { result . push (dir . clone ()) ; } } } result } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `common_root` | `pub fn common_root (files : & [PathBuf]) -> Option < PathBuf > { let mut iter = files . iter () ; let first = iter . next () ? . components () . collect :: < Vec < _ > > () ; let mut prefix_len = first . len () ; for path in iter { let comps = path . components () . collect :: < Vec < _ > > () ; let mut idx = 0 ; while idx < prefix_len && idx < comps . len () && comps [idx] == first [idx] { idx += 1 ; } prefix_len = idx ; } if prefix_len == 0 { None } else { let mut root = PathBuf :: new () ; for comp in first . into_iter () . take (prefix_len) { root . push (comp . as_os_str ()) ; } Some (root) } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `strip_numeric_prefix` | `pub (crate) fn strip_numeric_prefix (name : & str) -> & str { use once_cell :: sync :: Lazy ; use regex :: Regex ; static PREFIX_RE : Lazy < Regex > = Lazy :: new (\| \| Regex :: new (r"^\d+_(.*)$") . unwrap ()) ; PREFIX_RE . captures (name) . and_then (\| cap \| cap . get (1)) . map (\| m \| m . as_str ()) . unwrap_or (name) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `generate_canonical_name` | `pub fn generate_canonical_name (path : & Path , number : usize) -> String { let stem = path . file_stem () . and_then (\| s \| s . to_str ()) . unwrap_or ("unknown") ; let ext = path . extension () . and_then (\| s \| s . to_str ()) . unwrap_or ("") ; let clean_stem = strip_numeric_prefix (stem) ; if ext . is_empty () { format ! ("{:03}_{}" , number , clean_stem) } else { format ! ("{:03}_{}.{}" , number , clean_stem , ext) } } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |

## File: src/160_layer_utilities.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `run_dead_code_pipeline` | `pub fn run_dead_code_pipeline (elements : & [CodeElement] , config : & DeadCodeRunConfig ,) -> Result < DeadCodeReportWithMeta > { let rust_files = gather_rust_files (& config . root) ; let mut intent_map = HashMap :: new () ; let mut test_boundaries = TestBoundaries :: default () ; for file in & rust_files { let intents = detect_intent_signals (file , config . policy . as_ref ()) ; merge_intent_map (& mut intent_map , intents) ; let test_modules = detect_test_modules (file) ; test_boundaries . test_modules . extend (test_modules) ; let test_symbols = detect_test_symbols (file) ; test_boundaries . test_symbols . extend (test_symbols) ; if is_test_path (file) { test_boundaries . test_files . insert (file . clone ()) ; } } let call_graph = build_call_graph (elements) ; let entrypoints = collect_entrypoints (elements , config . policy . as_ref ()) ; let exports = collect_exports (& config . root) ; let mut items = Vec :: new () ; for element in elements { if element . element_type != ElementType :: Function { continue ; } if element . language != Language :: Rust { continue ; } let category = classify_symbol (& element . name , & call_graph , & intent_map , & test_boundaries , & entrypoints , config . policy . as_ref () ,) ; let intent_tag = intent_map . contains_key (& element . name) ; let test_reference = test_boundaries . test_symbols . contains (& element . name) ; let call_graph_proven = category == DeadCodeCategory :: Unreachable && is_reachable (& element . name , & call_graph , & entrypoints) == false ; let mut item = DeadCodeItem { symbol : element . name . clone () , file : PathBuf :: from (& element . file_path) , line : element . line_number , category , confidence : crate :: dead_code_types :: ConfidenceLevel :: Heuristic , action : crate :: dead_code_types :: RecommendedAction :: ManualReview , reason : reason_for_category (category , intent_tag , test_reference) , } ; let confidence = assign_confidence (& item , & Evidence { intent_tag , test_reference , call_graph_proven , } ,) ; item . confidence = confidence ; let public_api = is_public_api (& element . name , & exports) \|\| matches ! (element . visibility , Visibility :: Public) ; item . action = recommend_action (category , confidence , public_api) ; items . push (item) ; } let metadata = DeadCodeReportMetadata { analyzer_version : env ! ("CARGO_PKG_VERSION") . to_string () , project_root : config . root . display () . to_string () , entrypoints_found : entrypoints . len () , } ; let report = build_report (chrono :: Local :: now () . to_rfc3339 () , items , metadata ,) ; write_outputs (& report , config) ? ; Ok (report) } . sig` | 0.09 | intra 0, inter 9 | same 0, other 13 | orphaned | - (suggest module utilities) |
| `main` | `pub fn main () -> Result < () > { let args = Args :: parse () ; let root_path = std :: env :: current_dir () ? . join (& args . root) . canonicalize () ? ; let output_path = std :: env :: current_dir () ? . join (& args . output) . canonicalize () . unwrap_or_else (\| _ \| { let p = std :: env :: current_dir () . unwrap () . join (& args . output) ; std :: fs :: create_dir_all (& p) . ok () ; p . canonicalize () . unwrap_or (p) }) ; run_analysis (& root_path , & output_path , args . verbose , args . skip_julia , args . dead_code , args . dead_code_filter , args . dead_code_json , args . dead_code_summary , args . dead_code_summary_limit , args . dead_code_policy , args . correction_intelligence , args . correction_json , args . verification_policy_json , args . correction_path_slice , args . correction_path_slice_dir , args . correction_visibility_slice , args . correction_visibility_slice_dir ,) } . sig` | 0.20 | intra 0, inter 1 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `resolve_source_root` | `# [doc = " Resolves the source root directory from a given root path"] pub fn resolve_source_root (root : & Path) -> PathBuf { let src_candidate = root . join ("src") ; if src_candidate . exists () && src_candidate . is_dir () { src_candidate } else { root . to_path_buf () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `allow_analysis_dir` | `# [doc = " Checks if a directory should be included in analysis"] pub fn allow_analysis_dir (root : & Path , dir : & Path) -> bool { let name = dir . file_name () . and_then (\| n \| n . to_str ()) . unwrap_or ("") ; if name . starts_with ('.') \|\| name == "target" \|\| name == "node_modules" { return false ; } if let Ok (rel) = dir . strip_prefix (root) { if rel . components () . any (\| c \| { let s = c . as_os_str () . to_str () . unwrap_or ("") ; s . starts_with ('.') \|\| s == "target" \|\| s == "node_modules" }) { return false ; } } true } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/170_invariant_reporter.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `export_json` | `# [doc = " Export invariants to JSON for agent consumption"] pub fn export_json (result : & InvariantAnalysisResult , output_dir : & Path ,) -> Result < () , std :: io :: Error > { let json_path = output_dir . join ("invariants.json") ; let json = serde_json :: to_string_pretty (result) . map_err (\| e \| std :: io :: Error :: new (std :: io :: ErrorKind :: Other , e)) ? ; fs :: write (& json_path , json) ? ; println ! ("ðŸ“„ JSON export written to: {}" , json_path . display ()) ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `export_constraints_json` | `# [doc = " Export refactoring constraints to JSON"] pub fn export_constraints_json (constraints : & [RefactorConstraint] , output_dir : & Path ,) -> Result < () , std :: io :: Error > { let constraints_dir = output_dir . join ("96_constraints") ; fs :: create_dir_all (& constraints_dir) ? ; let json_path = constraints_dir . join ("refactor_constraints.json") ; let json = serde_json :: to_string_pretty (constraints) . map_err (\| e \| std :: io :: Error :: new (std :: io :: ErrorKind :: Other , e)) ? ; fs :: write (& json_path , json) ? ; println ! ("ðŸ“„ Constraints written to: {}" , json_path . display ()) ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |

## File: src/180_conscience_graph.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `generate_conscience_map` | `# [doc = " Generate conscience map showing protection levels per function"] pub fn generate_conscience_map (invariants : & [Invariant] , output_path : & Path ,) -> std :: io :: Result < () > { let mut content = String :: new () ; content . push_str ("# Conscience Map\n\n") ; content . push_str ("## Overview\n\n") ; content . push_str ("This map shows which functions are protected by mechanical constraints.\n") ; content . push_str ("Functions with blocking invariants cannot be refactored without violating proven properties.\n\n") ; let mut by_function : HashMap < String , Vec < & Invariant > > = HashMap :: new () ; for inv in invariants { by_function . entry (inv . target . clone ()) . or_default () . push (inv) ; } let total_functions = by_function . len () ; let protected_functions = by_function . values () . filter (\| invs \| invs . iter () . any (\| i \| i . is_blocking ())) . count () ; content . push_str (& format ! ("**Total Functions**: {}\n\n" , total_functions)) ; content . push_str (& format ! ("**Protected Functions**: {} ({:.1}%)\n\n" , protected_functions , (protected_functions as f64 / total_functions as f64) * 100.0)) ; let mut funcs : Vec < _ > = by_function . into_iter () . collect () ; funcs . sort_by_key (\| (_ , invs) \| { - (invs . iter () . filter (\| i \| i . is_blocking ()) . count () as i32) }) ; content . push_str ("---\n\n") ; content . push_str ("## Functions by Protection Level\n\n") ; for (func , invs) in funcs { let blocking_count = invs . iter () . filter (\| i \| i . is_blocking ()) . count () ; let total_count = invs . len () ; if blocking_count == 0 { continue ; } let protection_percent = (blocking_count * 100) / total_count ; content . push_str (& format ! ("### `{}` ({}% protected)\n\n" , func , protection_percent)) ; if let Some (inv) = invs . first () { if ! inv . file_path . is_empty () { content . push_str (& format ! ("**File**: `{}`\n\n" , inv . file_path)) ; } } for inv in invs . iter () . filter (\| i \| i . is_blocking ()) { content . push_str (& format ! ("- ðŸ”’ {} **{}**: {}\n" , strength_emoji (inv) , kind_name (inv) , inv . description)) ; } content . push_str ("\n") ; } content . push_str ("---\n\n") ; content . push_str ("## Legend\n\n") ; content . push_str ("- âœ“ **PROVEN**: Mathematical certainty from graph topology\n") ; content . push_str ("- â—† **EMPIRICAL**: Observed across multiple paths (high confidence)\n") ; content . push_str ("- ? **HEURISTIC**: Name-based guess (LOW CONFIDENCE - verify manually)\n\n") ; content . push_str ("- ðŸ”’ **Blocking**: Constraint mechanically enforced\n\n") ; std :: fs :: write (output_path , content) ? ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | orphaned | - (suggest module utilities) |

## File: src/190_action_validator.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `validate_action` | `# [doc = " Validate action against all constraints"] pub fn validate_action (action : & AgentAction , constraints : & [RefactorConstraint] ,) -> Result < () , Vec < ConstraintViolation > > { let mut violations = Vec :: new () ; for (idx , constraint) in constraints . iter () . enumerate () { match (action , constraint) { (AgentAction :: MoveFunction { name , from , to } , RefactorConstraint :: NoMove { target , reason , strength , } ,) if target == name => { violations . push (ConstraintViolation { constraint_id : idx , invariant_id : 0 , reason : format ! ("Cannot move {}: {} (strength: {:?})" , name , reason , strength) , severity : ViolationSeverity :: Critical , blocking : true , }) ; } (AgentAction :: MoveFunction { name , from , to } , RefactorConstraint :: FixedLayer { target , layer , strength , } ,) if target == name => { let from_layer = extract_layer (from) ; let to_layer = extract_layer (to) ; if from_layer != to_layer { violations . push (ConstraintViolation { constraint_id : idx , invariant_id : 0 , reason : format ! ("Cannot move {} across layers: layer {} fixed (strength: {:?})" , name , layer , strength) , severity : ViolationSeverity :: Critical , blocking : true , }) ; } } (AgentAction :: ChangeSignature { name , old_sig , .. } , RefactorConstraint :: PreserveSignature { target , signature , strength , } ,) if target == name => { violations . push (ConstraintViolation { constraint_id : idx , invariant_id : 0 , reason : format ! ("Cannot change signature of {}: type-stable (strength: {:?})" , name , strength) , severity : ViolationSeverity :: High , blocking : true , }) ; } (AgentAction :: DeleteFunction { name , .. } , RefactorConstraint :: NoMove { target , reason , strength , } ,) if target == name && reason . contains ("utility") => { violations . push (ConstraintViolation { constraint_id : idx , invariant_id : 0 , reason : format ! ("Cannot delete {}: widely used utility function (strength: {:?})" , name , strength) , severity : ViolationSeverity :: Critical , blocking : true , }) ; } _ => { } } } if violations . is_empty () { Ok (()) } else { Err (violations) } } . sig` | 0.82 | intra 2, inter 0 | same 14, other 5 | ok | - |
| `extract_layer` | `# [doc = " Extract layer number from file path (e.g., \"src/040_test.rs\" -> Some(40))"] fn extract_layer (path : & PathBuf) -> Option < usize > { path . file_name () . and_then (\| n \| n . to_str ()) . and_then (\| s \| { let parts : Vec < & str > = s . split ('_') . collect () ; if ! parts . is_empty () { parts [0] . parse :: < usize > () . ok () } else { None } }) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/210_utilities.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `collect_directory_files` | `pub fn collect_directory_files (directory : & DirectoryAnalysis , out : & mut Vec < PathBuf >) { out . extend (directory . files . iter () . cloned ()) ; for sub in & directory . subdirectories { collect_directory_files (sub , out) ; } } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `resolve_required_layer_path` | `pub fn resolve_required_layer_path (required_layer : & str , current_file : & Path , directory : & DirectoryAnalysis , root_path : & Path ,) -> PathBuf { let mut files = Vec :: new () ; collect_directory_files (directory , & mut files) ; let candidates = files . into_iter () . filter (\| path \| { path . file_name () . and_then (\| name \| name . to_str ()) . map (\| name \| name == required_layer) . unwrap_or (false) }) . collect :: < Vec < _ > > () ; if candidates . is_empty () { return current_file . parent () . unwrap_or (root_path) . join (required_layer) ; } let current_dir = current_file . parent () . unwrap_or (root_path) ; let mut best = None ; let mut best_score = - 1isize ; for candidate in candidates { let candidate_dir = candidate . parent () . unwrap_or (root_path) ; let score = path_common_prefix_len (current_dir , candidate_dir) ; let length = candidate . components () . count () as isize ; let combined = score * 1000 - length ; if combined > best_score { best_score = combined ; best = Some (candidate) ; } } best . unwrap_or_else (\| \| { current_file . parent () . unwrap_or (root_path) . join (required_layer) }) } . sig` | 0.60 | intra 2, inter 0 | same 0, other 1 | ok | - |
| `compute_move_metrics` | `pub fn compute_move_metrics (placement : & FunctionPlacement ,) -> (usize , usize , usize , usize , Vec < PathBuf > , Vec < PathBuf >) { let incoming_calls = placement . call_analysis . calls_from_other_files . iter () . map (\| (_ , count) \| * count) . sum :: < usize > () ; let callers = placement . call_analysis . calls_from_other_files . len () ; let mut touched = BTreeSet :: new () ; touched . insert (placement . current_file . clone ()) ; let mut outgoing_files = Vec :: new () ; for (path , _) in & placement . call_analysis . inter_file_calls { touched . insert (path . clone ()) ; outgoing_files . push (path . clone ()) ; } let mut caller_files = Vec :: new () ; for (path , _) in & placement . call_analysis . calls_from_other_files { touched . insert (path . clone ()) ; caller_files . push (path . clone ()) ; } let cost = touched . len () . max (1) ; let benefit = 1 + callers ; (incoming_calls , benefit , cost , callers , caller_files , outgoing_files) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `collect_move_items` | `pub fn collect_move_items (placements : & [FunctionPlacement] , utility_names : & BTreeSet < String > , directory : & DirectoryAnalysis , root_path : & Path ,) -> Vec < PlanItem > { let mut items = Vec :: new () ; for placement in placements { match & placement . placement_status { PlacementStatus :: ShouldMove { reason , impact } => { let priority = if * impact >= 0.5 { Priority :: Critical } else if * impact >= 0.2 { Priority :: High } else if * impact >= 0.1 { Priority :: Medium } else { Priority :: Low } ; let (impact_weight , benefit , cost , callers , caller_files , outgoing_files) = compute_move_metrics (placement) ; let to = placement . suggested_file . as_ref () . map (\| p \| compress_path (p . to_string_lossy () . as_ref ())) . unwrap_or_else (\| \| "-" . to_string ()) ; items . push (PlanItem { kind : ActionKind :: Cohesion , priority , description : format ! ("`{}` from `{}` to `{}`: {} (impact {:.2})" , placement . name , compress_path (placement . current_file . to_string_lossy () . as_ref ()) , to , reason , impact) , command : String :: new () , current_layer : None , required_layer : None , is_utility : utility_names . contains (& placement . name) , impact_weight , benefit , cost , callers , caller_files , current_file : Some (placement . current_file . clone ()) , target_file : placement . suggested_file . clone () , outgoing_files , name : Some (placement . name . clone ()) , cluster_cohesion : 0.0 , member_count : 0 , }) ; } PlacementStatus :: LayerViolation { current_layer , required_layer , } => { let target_path = resolve_required_layer_path (required_layer , & placement . current_file , directory , root_path ,) ; let to = compress_path (target_path . to_string_lossy () . as_ref ()) ; let (impact_weight , benefit , cost , callers , caller_files , outgoing_files) = compute_move_metrics (placement) ; items . push (PlanItem { kind : ActionKind :: Structural , priority : Priority :: Critical , description : format ! ("`{}` from `{}` to `{}`: layer violation {} -> {}" , placement . name , compress_path (placement . current_file . to_string_lossy () . as_ref ()) , to , current_layer , required_layer) , command : String :: new () , current_layer : Some (current_layer . clone ()) , required_layer : Some (required_layer . clone ()) , is_utility : utility_names . contains (& placement . name) , impact_weight , benefit , cost , callers , caller_files , current_file : Some (placement . current_file . clone ()) , target_file : Some (target_path) , outgoing_files , name : Some (placement . name . clone ()) , cluster_cohesion : 0.0 , member_count : 0 , }) ; } _ => { } } } items } . sig` | 0.60 | intra 5, inter 0 | same 0, other 14 | ok | - |
| `write_structural_batches` | `pub fn write_structural_batches (content : & mut String , items : & [PlanItem]) { if items . is_empty () { return ; } let mut ordered_targets = Vec :: new () ; let mut batches : HashMap < PathBuf , Vec < & PlanItem > > = HashMap :: new () ; for item in items { let Some (target) = & item . target_file else { continue ; } ; let entry = batches . entry (target . clone ()) . or_default () ; if entry . is_empty () { ordered_targets . push (target . clone ()) ; } entry . push (item) ; } content . push_str ("### Phase 3 Batches\n\n") ; content . push_str ("Action: execute batches in order and verify after each batch.\n") ; content . push_str ("Note: each batch targets one destination module.\n\n") ; for (idx , target) in ordered_targets . iter () . enumerate () { let empty : Vec < & PlanItem > = Vec :: new () ; let items = batches . get (target) . unwrap_or (& empty) ; content . push_str (& format ! ("#### Batch {}: target `{}`\n\n" , idx + 1 , compress_path (target . to_string_lossy () . as_ref ()))) ; content . push_str ("Action: move the listed functions into the target module.\n") ; content . push_str ("Note: use the rg commands to locate definitions and callers.\n\n") ; let mut commands : Vec < String > = Vec :: new () ; if ! target . exists () { let target_label = compress_path (target . to_string_lossy () . as_ref ()) ; content . push_str (& format ! ("- Create target file: `{}`\n" , target_label)) ; commands . push (format ! ("touch \"{}\"" , target . to_string_lossy ())) ; } for item in items { let name = item . name . as_deref () . unwrap_or ("function") ; let current = item . current_file . as_ref () . map (\| p \| compress_path (p . to_string_lossy () . as_ref ())) . unwrap_or_else (\| \| "-" . to_string ()) ; let ratio = if item . cost == 0 { 0.0 } else { item . benefit as f64 / item . cost as f64 } ; let caller_hint = if item . callers == 0 { "no external callers" . to_string () } else { format ! ("update {} caller files" , item . callers) } ; content . push_str (& format ! ("- Move `{}` from `{}` (impact {}, benefit/cost {:.2}, touches {} files; {})\n" , name , current , item . impact_weight , ratio , item . cost , caller_hint)) ; if let Some (current_file) = & item . current_file { commands . push (format ! ("rg -n \"{}\" \"{}\"" , name , current_file . to_string_lossy ())) ; } let mut callers = item . caller_files . clone () ; callers . sort () ; callers . dedup () ; if ! callers . is_empty () { content . push_str ("- Update imports in:\n") ; for caller in callers { content . push_str (& format ! ("  - `{}`\n" , compress_path (caller . to_string_lossy () . as_ref ()))) ; commands . push (format ! ("rg -n \"{}\" \"{}\"" , name , caller . to_string_lossy ())) ; } } } content . push_str ("- Verification gate: `cargo test`\n") ; if ! commands . is_empty () { content . push_str ("\n```bash\n") ; for command in commands { content . push_str (& format ! ("{}\n" , command)) ; } content . push_str ("```\n") ; } content . push ('\n') ; } } . sig` | 0.60 | intra 2, inter 0 | same 0, other 3 | ok | - |
| `write_cluster_batches` | `pub fn write_cluster_batches (content : & mut String , plans : & [ClusterPlan] , root_path : & Path) { if plans . is_empty () { return ; } content . push_str ("### Phase 2 Batches\n\n") ; content . push_str ("Action: execute batches in order and verify after each batch.\n") ; content . push_str ("Note: each batch creates or fills a cluster file.\n\n") ; for (idx , plan) in plans . iter () . enumerate () { content . push_str (& format ! ("#### Batch {}: target `{}`\n\n" , idx + 1 , compress_path (plan . target . to_string_lossy () . as_ref ()))) ; content . push_str ("Action: move the listed functions into the target module.\n") ; content . push_str ("Note: use the rg commands to locate definitions and callers.\n\n") ; let mut commands = Vec :: new () ; if ! plan . target . exists () { content . push_str (& format ! ("- Create target file: `{}`\n" , compress_path (plan . target . to_string_lossy () . as_ref ()))) ; commands . push (format ! ("touch \"{}\"" , plan . target . to_string_lossy ())) ; } content . push_str (& format ! ("- Cluster cohesion {:.2}, {} functions\n" , plan . cohesion , plan . members . len ())) ; for member in & plan . members { let file = compress_path (member . file . to_string_lossy () . as_ref ()) ; content . push_str (& format ! ("- Move `{}` from `{}`\n" , member . name , file)) ; commands . push (format ! ("rg -n \"{}\" \"{}\"" , member . name , member . file . to_string_lossy ())) ; commands . push (format ! ("rg -n \"{}\" \"{}\"" , member . name , root_path . to_string_lossy ())) ; } content . push_str ("- Verification gate: `cargo test`\n") ; if ! commands . is_empty () { content . push_str ("\n```bash\n") ; for command in commands { content . push_str (& format ! ("{}\n" , command)) ; } content . push_str ("```\n") ; } content . push ('\n') ; } } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `compress_path` | `# [doc = " Compress absolute paths to MMSB-relative format"] pub fn compress_path (path : & str) -> String { if let Some (idx) = path . find ("/MMSB/") { return format ! ("MMSB{}" , & path [idx + 5 ..]) ; } if path . starts_with ("MMSB/") { return path . to_string () ; } if let Some (idx) = path . rfind ("/src/") { return format ! ("MMSB/src{}" , & path [idx + 4 ..]) ; } path . to_string () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `path_common_prefix_len` | `pub fn path_common_prefix_len (a : & Path , b : & Path) -> isize { let mut count = 0isize ; for (a_comp , b_comp) in a . components () . zip (b . components ()) { if a_comp == b_comp { count += 1 ; } else { break ; } } count } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/211_dead_code_attribute_parser.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `scan_doc_comments` | `pub fn scan_doc_comments (file : & Path) -> HashMap < String , Vec < IntentMarker > > { let contents = std :: fs :: read_to_string (file) . unwrap_or_default () ; let parsed = match syn :: parse_file (& contents) { Ok (file) => file , Err (_) => return HashMap :: new () , } ; let mut map : HashMap < String , Vec < IntentMarker > > = HashMap :: new () ; for item in & parsed . items { let Some (symbol) = item_name (item) else { continue ; } ; let markers = extract_doc_markers (item_attrs (item)) ; if markers . is_empty () { continue ; } map . entry (symbol) . or_default () . extend (markers) ; } map } . sig` | 0.10 | intra 0, inter 2 | same 0, other 2 | move | - (cohesion 0.10 below threshold 0.60 (impact 0.50)) |
| `parse_mmsb_latent_attr` | `pub fn parse_mmsb_latent_attr (path : & Path) -> HashMap < String , Vec < IntentMetadata > > { let contents = std :: fs :: read_to_string (path) . unwrap_or_default () ; let file = match syn :: parse_file (& contents) { Ok (file) => file , Err (_) => return HashMap :: new () , } ; let mut map : HashMap < String , Vec < IntentMetadata > > = HashMap :: new () ; for item in & file . items { let Some (name) = item_name (item) else { continue ; } ; let tags = collect_latent_attrs (item_attrs (item)) ; if tags . is_empty () { continue ; } map . entry (name) . or_default () . extend (tags) ; } map } . sig` | 0.23 | intra 1, inter 2 | same 0, other 2 | move | - (cohesion 0.23 below threshold 0.60 (impact 0.37)) |
| `scan_file_attributes` | `pub fn scan_file_attributes (path : & Path) -> Vec < IntentTag > { let contents = std :: fs :: read_to_string (path) . unwrap_or_default () ; let file = match syn :: parse_file (& contents) { Ok (file) => file , Err (_) => return Vec :: new () , } ; let mut tags = Vec :: new () ; for item in & file . items { let Some (symbol) = item_name (item) else { continue ; } ; for meta in collect_latent_attrs (item_attrs (item)) { tags . push (IntentTag { symbol : symbol . clone () , file : path . to_path_buf () , line : None , marker : meta . marker , source : meta . source , value : meta . value . clone () , }) ; } } tags } . sig` | 0.23 | intra 1, inter 2 | same 0, other 2 | move | - (cohesion 0.23 below threshold 0.60 (impact 0.37)) |
| `detect_intent_signals` | `pub fn detect_intent_signals (file : & Path , policy : Option < & DeadCodePolicy >) -> IntentMap { let attrs = parse_mmsb_latent_attr (file) ; let doc_map = scan_doc_comments (file) ; let docs = merge_doc_intent (doc_map) ; let dir_map = planned_directory_intent (file , policy) ; merge_intent_sources (attrs , docs , dir_map) } . sig` | 0.30 | intra 2, inter 2 | same 0, other 1 | move | - (cohesion 0.30 below threshold 0.60 (impact 0.30)) |
| `scan_intent_tags` | `pub fn scan_intent_tags (file : & Path , policy : Option < & DeadCodePolicy >) -> Vec < IntentTag > { let mut tags = Vec :: new () ; let attrs = parse_mmsb_latent_attr (file) ; for (symbol , items) in attrs { for meta in items { tags . push (IntentTag { symbol : symbol . clone () , file : file . to_path_buf () , line : None , marker : meta . marker , source : meta . source , value : meta . value . clone () , }) ; } } let doc_map = scan_doc_comments (file) ; for (symbol , markers) in doc_map { for marker in markers { tags . push (IntentTag { symbol : symbol . clone () , file : file . to_path_buf () , line : None , marker , source : IntentSource :: DocComment , value : None , }) ; } } if check_planned_directory (file , policy) { for symbol in collect_symbols (file) { tags . push (IntentTag { symbol , file : file . to_path_buf () , line : None , marker : IntentMarker :: Planned , source : IntentSource :: Directory , value : None , }) ; } } tags } . sig` | 0.35 | intra 2, inter 1 | same 0, other 8 | move | - (cohesion 0.35 below threshold 0.60 (impact 0.25)) |
| `is_cfg_test_item` | `pub fn is_cfg_test_item (item : & Item) -> bool { item_attrs (item) . iter () . any (\| attr \| { if ! attr . path () . is_ident ("cfg") { return false ; } let mut found = false ; let _ = attr . parse_nested_meta (\| meta \| { if meta . path . is_ident ("test") { found = true ; return Ok (()) ; } if meta . path . is_ident ("any") { meta . parse_nested_meta (\| nested \| { if nested . path . is_ident ("test") { found = true ; } Ok (()) }) ? ; } Ok (()) }) ; found }) } . sig` | 0.40 | intra 0, inter 2 | same 0, other 0 | move | - (cohesion 0.40 below threshold 0.60 (impact 0.20)) |
| `collect_latent_attrs` | `fn collect_latent_attrs (attrs : & [Attribute]) -> Vec < IntentMetadata > { let mut markers = Vec :: new () ; for attr in attrs { if ! attr . path () . is_ident ("mmsb_latent") { continue ; } let mut marker = IntentMarker :: Latent ; let mut value = None ; let mut saw_nested = false ; let _ = attr . parse_nested_meta (\| meta \| { saw_nested = true ; if meta . path . is_ident ("planned") { marker = IntentMarker :: Planned ; } else if meta . path . is_ident ("future") { marker = IntentMarker :: Future ; } else if meta . path . is_ident ("deprecated_planned") \|\| meta . path . is_ident ("deprecated-planned") { marker = IntentMarker :: DeprecatedPlanned ; } else if meta . path . is_ident ("reason") \|\| meta . path . is_ident ("note") { let value_meta = meta . value () ? ; let lit : syn :: LitStr = value_meta . parse () ? ; value = Some (lit . value ()) ; } else if meta . path . is_ident ("marker") { let value_meta = meta . value () ? ; let lit : syn :: LitStr = value_meta . parse () ? ; marker = marker_from_str (& lit . value ()) ; } Ok (()) }) ; if ! saw_nested { if let Ok (lit) = attr . parse_args :: < syn :: LitStr > () { value = Some (lit . value ()) ; } } markers . push (IntentMetadata { marker , source : IntentSource :: Attribute , value , }) ; } markers } . sig` | 0.60 | intra 1, inter 0 | same 0, other 7 | ok | - |
| `marker_from_str` | `fn marker_from_str (raw : & str) -> IntentMarker { match raw . to_ascii_lowercase () . as_str () { "planned" => IntentMarker :: Planned , "future" => IntentMarker :: Future , "deprecated_planned" \| "deprecated-planned" => IntentMarker :: DeprecatedPlanned , _ => IntentMarker :: Latent , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 5 | ok | - |
| `detect_test_symbols` | `pub fn detect_test_symbols (file : & Path) -> HashSet < String > { let contents = std :: fs :: read_to_string (file) . unwrap_or_default () ; let parsed = match syn :: parse_file (& contents) { Ok (file) => file , Err (_) => return HashSet :: new () , } ; let mut symbols = HashSet :: new () ; for item in & parsed . items { if let Item :: Fn (item_fn) = item { if has_test_attr (& item_fn . attrs) { symbols . insert (item_fn . sig . ident . to_string ()) ; } } if let Item :: Mod (item_mod) = item { if is_cfg_test_item (item) { symbols . insert (item_mod . ident . to_string ()) ; if let Some ((_ , items)) = & item_mod . content { for nested in items { if let Item :: Fn (nested_fn) = nested { symbols . insert (nested_fn . sig . ident . to_string ()) ; } } } } } } symbols } . sig` | 0.65 | intra 1, inter 1 | same 0, other 0 | ok | - |
| `extract_attribute_value` | `pub fn extract_attribute_value (attr : & Attribute , key : & str) -> Option < String > { let mut found = None ; let _ = attr . parse_nested_meta (\| meta \| { if meta . path . is_ident (key) { let value = meta . value () ? ; let lit : syn :: LitStr = value . parse () ? ; found = Some (lit . value ()) ; } Ok (()) }) ; found } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `detect_test_modules` | `pub fn detect_test_modules (file : & Path) -> HashSet < String > { let contents = std :: fs :: read_to_string (file) . unwrap_or_default () ; let parsed = match syn :: parse_file (& contents) { Ok (file) => file , Err (_) => return HashSet :: new () , } ; let mut modules = HashSet :: new () ; for item in & parsed . items { if let Item :: Mod (item_mod) = item { if is_cfg_test_item (item) { modules . insert (item_mod . ident . to_string ()) ; } } } modules } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |

## File: src/260_file_ordering.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `parallel_build_file_dag` | `# [allow (dead_code)] pub fn parallel_build_file_dag (directories : & [PathBuf]) -> Result < DiGraph < PathBuf , () > > { let subgraphs : Vec < DiGraph < PathBuf , () > > = directories . par_iter () . map (\| dir \| crate :: dependency :: build_directory_dag (dir)) . collect :: < Result < _ > > () ? ; let mut merged = DiGraph :: new () ; let mut node_map : HashMap < PathBuf , NodeIndex > = HashMap :: new () ; for subgraph in subgraphs { for node in subgraph . node_indices () { let file = subgraph [node] . clone () ; node_map . entry (file . clone ()) . or_insert_with (\| \| merged . add_node (file)) ; } for edge in subgraph . edge_indices () { if let Some ((src , dst)) = subgraph . edge_endpoints (edge) { let src_file = subgraph [src] . clone () ; let dst_file = subgraph [dst] . clone () ; let src_node = * node_map . get (& src_file) . expect ("missing source node") ; let dst_node = * node_map . get (& dst_file) . expect ("missing target node") ; merged . add_edge (src_node , dst_node , ()) ; } } } Ok (merged) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/320_main.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `main` | `fn main () -> Result < () > { let args : Vec < String > = std :: env :: args () . collect () ; if args . len () > 1 && args [1] == "agent" { return agent_cli :: run_agent_cli () ; } crate :: layer_utilities :: main () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/330_agent_cli.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `check_action` | `# [doc = " Check if an action is allowed"] fn check_action (action_path : & PathBuf , conscience_path : & PathBuf) -> Result < () > { let action_json = std :: fs :: read_to_string (action_path) ? ; let action : AgentAction = serde_json :: from_str (& action_json) ? ; let invariants = load_invariants (conscience_path) ? ; let conscience = AgentConscience :: new (invariants) ; let result = conscience . check_action (& action) ; let output = serde_json :: to_string_pretty (& result) ? ; println ! ("{}" , output) ; std :: process :: exit (if result . allowed { 0 } else { 1 }) ; } . sig` | 0.60 | intra 2, inter 0 | same 0, other 2 | ok | - |
| `query_function` | `# [doc = " Query allowed actions for a function"] fn query_function (function : & str , conscience_path : & PathBuf) -> Result < () > { let invariants = load_invariants (conscience_path) ? ; let conscience = AgentConscience :: new (invariants) ; let allowed = conscience . query_allowed_actions (function) ; let output = serde_json :: to_string_pretty (& allowed) ? ; println ! ("{}" , output) ; Ok (()) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `show_stats` | `# [doc = " Show conscience statistics"] fn show_stats (conscience_path : & PathBuf) -> Result < () > { let invariants = load_invariants (conscience_path) ? ; let conscience = AgentConscience :: new (invariants) ; let stats = conscience . stats () ; println ! ("Conscience Statistics") ; println ! ("====================\n") ; println ! ("Total invariants:    {}" , stats . total_invariants) ; println ! ("Blocking invariants: {}" , stats . blocking_invariants) ; println ! ("Total constraints:   {}" , stats . total_constraints) ; println ! () ; println ! ("By strength:") ; println ! ("  Proven:     {}" , stats . proven_count) ; println ! ("  Empirical:  {}" , stats . empirical_count) ; println ! ("  Heuristic:  {}" , stats . heuristic_count) ; Ok (()) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `load_invariants` | `# [doc = " Load invariants from JSON file"] fn load_invariants (path : & PathBuf) -> Result < Vec < Invariant > > { let json = std :: fs :: read_to_string (path) ? ; Ok (serde_json :: from_str (& json) ?) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `run_agent_cli` | `pub fn run_agent_cli () -> Result < () > { let cli = AgentCli :: parse () ; match cli . command { AgentCommand :: Check { action , conscience } => { check_action (& action , & conscience) ? ; } AgentCommand :: Query { function , conscience , } => { query_function (& function , & conscience) ? ; } AgentCommand :: Invariants { conscience , blocking_only , } => { list_invariants (& conscience , blocking_only) ? ; } AgentCommand :: Stats { conscience } => { show_stats (& conscience) ? ; } } Ok (()) } . sig` | 0.90 | intra 4, inter 0 | same 5, other 0 | ok | - |
| `list_invariants` | `# [doc = " List all invariants"] fn list_invariants (conscience_path : & PathBuf , blocking_only : bool) -> Result < () > { let invariants = load_invariants (conscience_path) ? ; let filtered : Vec < _ > = if blocking_only { invariants . iter () . filter (\| i \| i . is_blocking ()) . cloned () . collect () } else { invariants } ; println ! ("Total invariants: {}" , filtered . len ()) ; if blocking_only { println ! ("(Showing only blocking invariants)\n") ; } let output = serde_json :: to_string_pretty (& filtered) ? ; println ! ("{}" , output) ; Ok (()) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |

## File: src/370_dead_code_doc_comment_parser.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `detect_latent_markers` | `pub fn detect_latent_markers (comment : & str) -> Option < IntentMarker > { IntentMarker :: from_comment (comment) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | ok | - |
| `merge_doc_intent` | `pub fn merge_doc_intent (map : HashMap < String , Vec < IntentMarker > >) -> IntentMap { let mut merged = IntentMap :: new () ; for (symbol , markers) in map { let mut uniques = HashSet :: new () ; for marker in markers { if ! uniques . insert (marker) { continue ; } merged . entry (symbol . clone ()) . or_default () . push (crate :: dead_code_types :: IntentMetadata { marker , source : crate :: dead_code_types :: IntentSource :: DocComment , value : None , }) ; } } merged } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `extract_doc_markers` | `pub (crate) fn extract_doc_markers (attrs : & [Attribute]) -> Vec < IntentMarker > { let mut markers = Vec :: new () ; for attr in attrs { if ! attr . path () . is_ident ("doc") { continue ; } let Meta :: NameValue (MetaNameValue { value , .. }) = & attr . meta else { continue ; } ; let syn :: Expr :: Lit (expr_lit) = value else { continue ; } ; let syn :: Lit :: Str (lit) = & expr_lit . lit else { continue ; } ; if let Some (marker) = detect_latent_markers (& lit . value ()) { markers . push (marker) ; } } markers } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `item_name` | `pub (crate) fn item_name (item : & Item) -> Option < String > { match item { Item :: Fn (item_fn) => Some (item_fn . sig . ident . to_string ()) , Item :: Struct (item_struct) => Some (item_struct . ident . to_string ()) , Item :: Enum (item_enum) => Some (item_enum . ident . to_string ()) , Item :: Mod (item_mod) => Some (item_mod . ident . to_string ()) , Item :: Trait (item_trait) => Some (item_trait . ident . to_string ()) , _ => None , } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `item_attrs` | `pub (crate) fn item_attrs (item : & Item) -> & [Attribute] { match item { Item :: Fn (item_fn) => & item_fn . attrs , Item :: Struct (item_struct) => & item_struct . attrs , Item :: Enum (item_enum) => & item_enum . attrs , Item :: Mod (item_mod) => & item_mod . attrs , Item :: Trait (item_trait) => & item_trait . attrs , _ => & [] , } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/380_dead_code_call_graph.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `classify_symbol` | `pub fn classify_symbol (symbol : & str , call_graph : & CallGraph , intent_map : & IntentMap , test_boundaries : & TestBoundaries , entrypoints : & HashSet < String > , _policy : Option < & DeadCodePolicy > ,) -> DeadCodeCategory { if intent_map . contains_key (symbol) { return DeadCodeCategory :: LatentPlanned ; } if is_test_only (symbol , call_graph , test_boundaries) { return DeadCodeCategory :: TestOnly ; } if ! is_reachable (symbol , call_graph , entrypoints) { return DeadCodeCategory :: Unreachable ; } DeadCodeCategory :: ReachableUnused } . sig` | 0.43 | intra 2, inter 1 | same 0, other 7 | move | - (cohesion 0.43 below threshold 0.60 (impact 0.17)) |
| `build_call_graph` | `pub fn build_call_graph (elements : & [CodeElement]) -> CallGraph { let mut graph : CallGraph = HashMap :: new () ; for element in elements { if element . element_type != ElementType :: Function { continue ; } if element . language != Language :: Rust { continue ; } let entry = graph . entry (element . name . clone ()) . or_default () ; entry . extend (element . calls . iter () . cloned ()) ; } graph } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `is_test_only` | `pub fn is_test_only (symbol : & str , call_graph : & CallGraph , test_boundaries : & TestBoundaries ,) -> bool { if test_boundaries . test_symbols . contains (symbol) { return true ; } let reverse = build_reverse_call_graph (call_graph) ; let callers = reverse . get (symbol) ; let Some (callers) = callers else { return false ; } ; if callers . is_empty () { return false ; } callers . iter () . all (\| caller \| test_boundaries . test_symbols . contains (caller)) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `build_reverse_call_graph` | `pub fn build_reverse_call_graph (graph : & CallGraph) -> CallGraph { let mut reverse : CallGraph = HashMap :: new () ; for (caller , callees) in graph { for callee in callees { reverse . entry (callee . clone ()) . or_default () . push (caller . clone ()) ; } } reverse } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `compute_reachability` | `pub fn compute_reachability (graph : & CallGraph , entrypoints : & HashSet < String >) -> HashSet < String > { let mut reachable = HashSet :: new () ; let mut queue : VecDeque < String > = entrypoints . iter () . cloned () . collect () ; while let Some (node) = queue . pop_front () { if ! reachable . insert (node . clone ()) { continue ; } if let Some (callees) = graph . get (& node) { for callee in callees { if ! reachable . contains (callee) { queue . push_back (callee . clone ()) ; } } } } reachable } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `is_reachable` | `pub fn is_reachable (symbol : & str , graph : & CallGraph , entrypoints : & HashSet < String > ,) -> bool { if entrypoints . is_empty () { return false ; } compute_reachability (graph , entrypoints) . contains (symbol) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |

## File: src/390_dead_code_intent.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `planned_directory_intent` | `pub (crate) fn planned_directory_intent (file : & Path , policy : Option < & DeadCodePolicy >) -> IntentMap { if ! check_planned_directory (file , policy) { return IntentMap :: new () ; } let mut map : IntentMap = HashMap :: new () ; for symbol in collect_symbols (file) { map . entry (symbol) . or_default () . push (IntentMetadata { marker : IntentMarker :: Planned , source : IntentSource :: Directory , value : None , }) ; } map } . sig` | 0.68 | intra 2, inter 0 | same 1, other 3 | ok | - |
| `check_planned_directory` | `pub fn check_planned_directory (path : & Path , policy : Option < & DeadCodePolicy >) -> bool { let Some (policy) = policy else { return false ; } ; for dir in & policy . planned_directories { if path . starts_with (dir) { return true ; } } false } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | ok | - |
| `merge_intent_sources` | `pub fn merge_intent_sources (attrs : IntentMap , docs : IntentMap , dir : IntentMap ,) -> IntentMap { let mut merged = IntentMap :: new () ; for (symbol , items) in attrs { merged . entry (symbol) . or_default () . extend (items) ; } for (symbol , items) in docs { merged . entry (symbol) . or_default () . extend (items) ; } for (symbol , items) in dir { merged . entry (symbol) . or_default () . extend (items) ; } merged } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `collect_symbols` | `pub (crate) fn collect_symbols (file : & Path) -> Vec < String > { let contents = std :: fs :: read_to_string (file) . unwrap_or_default () ; let parsed = match syn :: parse_file (& contents) { Ok (file) => file , Err (_) => return Vec :: new () , } ; parsed . items . iter () . filter_map (item_name) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/400_dead_code_test_boundaries.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `find_test_callers` | `pub fn find_test_callers (symbol : & str , call_graph : & CallGraph , test_symbols : & HashSet < String > ,) -> Vec < String > { if test_symbols . is_empty () { return Vec :: new () ; } let reverse = build_reverse_call_graph (call_graph) ; let mut callers = Vec :: new () ; let mut visited = HashSet :: new () ; let mut queue : VecDeque < String > = reverse . get (symbol) . cloned () . unwrap_or_default () . into_iter () . collect () ; while let Some (caller) = queue . pop_front () { if ! visited . insert (caller . clone ()) { continue ; } if test_symbols . contains (& caller) { callers . push (caller . clone ()) ; } if let Some (next) = reverse . get (& caller) { for parent in next { if ! visited . contains (parent) { queue . push_back (parent . clone ()) ; } } } } callers } . sig` | 0.20 | intra 0, inter 1 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `has_test_attr` | `pub (crate) fn has_test_attr (attrs : & [Attribute]) -> bool { attrs . iter () . any (\| attr \| { let path = attr . path () ; if path . is_ident ("test") { return true ; } let last = path . segments . last () . map (\| seg \| seg . ident . to_string ()) ; matches ! (last . as_deref () , Some ("test")) }) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `item_attrs` | `fn item_attrs (item : & Item) -> & [Attribute] { match item { Item :: Fn (item_fn) => & item_fn . attrs , Item :: Struct (item_struct) => & item_struct . attrs , Item :: Enum (item_enum) => & item_enum . attrs , Item :: Mod (item_mod) => & item_mod . attrs , Item :: Trait (item_trait) => & item_trait . attrs , _ => & [] , } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/410_dead_code_entrypoints.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `collect_entrypoints` | `pub fn collect_entrypoints (elements : & [CodeElement] , policy : Option < & DeadCodePolicy > ,) -> HashSet < String > { let mut entrypoints = HashSet :: new () ; if let Some (policy) = policy { for symbol in & policy . entrypoint_symbols { entrypoints . insert (symbol . clone ()) ; } } for element in elements { if element . element_type != ElementType :: Function { continue ; } if element . name == "main" { entrypoints . insert (element . name . clone ()) ; continue ; } if matches ! (element . visibility , Visibility :: Public) && treat_public_as_entrypoint (policy) { entrypoints . insert (element . name . clone ()) ; } } entrypoints } . sig` | 0.60 | intra 1, inter 0 | same 0, other 4 | ok | - |
| `collect_exports` | `pub fn collect_exports (root : & Path) -> HashSet < String > { let mut exports = HashSet :: new () ; let src_dir = root . join ("src") ; for entry in WalkDir :: new (src_dir) . into_iter () . filter_map (Result :: ok) { let path = entry . path () ; if ! path . is_file () { continue ; } if path . extension () . and_then (\| e \| e . to_str ()) != Some ("rs") { continue ; } let contents = std :: fs :: read_to_string (path) . unwrap_or_default () ; let parsed = match syn :: parse_file (& contents) { Ok (file) => file , Err (_) => continue , } ; for item in parsed . items { match item { syn :: Item :: Use (item_use) => { if matches ! (item_use . vis , syn :: Visibility :: Public (_)) { collect_use_tree_idents (& item_use . tree , & mut exports) ; } } syn :: Item :: Mod (item_mod) => { if matches ! (item_mod . vis , syn :: Visibility :: Public (_)) { exports . insert (item_mod . ident . to_string ()) ; } } _ => { } } } } exports } . sig` | 0.60 | intra 1, inter 0 | same 0, other 2 | ok | - |
| `treat_public_as_entrypoint` | `fn treat_public_as_entrypoint (policy : Option < & DeadCodePolicy >) -> bool { policy . map (\| p \| p . treat_public_as_entrypoint) . unwrap_or (true) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `is_public_api` | `pub fn is_public_api (symbol : & str , exports : & HashSet < String >) -> bool { exports . contains (symbol) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `collect_use_tree_idents` | `fn collect_use_tree_idents (tree : & syn :: UseTree , exports : & mut HashSet < String >) { match tree { syn :: UseTree :: Name (name) => { exports . insert (name . ident . to_string ()) ; } syn :: UseTree :: Rename (rename) => { exports . insert (rename . rename . to_string ()) ; } syn :: UseTree :: Path (path) => { collect_use_tree_idents (& path . tree , exports) ; } syn :: UseTree :: Group (group) => { for item in & group . items { collect_use_tree_idents (item , exports) ; } } syn :: UseTree :: Glob (_) => { exports . insert ("*" . to_string ()) ; } } } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |

## File: src/420_dead_code_classifier.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `is_reachable` | `pub fn is_reachable (symbol : & str , call_graph : & CallGraph , entrypoints : & HashSet < String >) -> bool { if entrypoints . is_empty () { return false ; } let reachable = crate :: dead_code_call_graph :: compute_reachability (call_graph , entrypoints) ; reachable . contains (symbol) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/430_dead_code_confidence.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `assign_confidence` | `pub fn assign_confidence (item : & DeadCodeItem , evidence : & Evidence) -> ConfidenceLevel { if evidence . intent_tag \|\| matches ! (item . category , DeadCodeCategory :: LatentPlanned) { return ConfidenceLevel :: IntentTag ; } if evidence . test_reference \|\| matches ! (item . category , DeadCodeCategory :: TestOnly) { return ConfidenceLevel :: TestReference ; } if evidence . call_graph_proven \|\| matches ! (item . category , DeadCodeCategory :: Unreachable) { return ConfidenceLevel :: CallGraph ; } ConfidenceLevel :: Heuristic } . sig` | 0.63 | intra 0, inter 0 | same 1, other 10 | ok | - |

## File: src/440_dead_code_actions.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `recommend_action` | `pub fn recommend_action (category : DeadCodeCategory , confidence : ConfidenceLevel , is_public_api : bool ,) -> RecommendedAction { match (category , confidence) { (DeadCodeCategory :: Unreachable , ConfidenceLevel :: CallGraph) if ! is_public_api => { RecommendedAction :: DeleteSafe } (DeadCodeCategory :: Unreachable , _) => RecommendedAction :: ManualReview , (DeadCodeCategory :: ReachableUnused , _) => RecommendedAction :: Quarantine , (DeadCodeCategory :: TestOnly , _) => RecommendedAction :: RelocateTests , (DeadCodeCategory :: LatentPlanned , _) => RecommendedAction :: Keep , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 14 | ok | - |

## File: src/460_dead_code_report.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `write_outputs` | `pub (crate) fn write_outputs (report : & DeadCodeReportWithMeta , config : & DeadCodeRunConfig) -> Result < () > { let json_path = config . write_json . clone () . unwrap_or_else (\| \| config . output_dir . join ("dead_code_full.json")) ; if let Some (parent) = json_path . parent () { std :: fs :: create_dir_all (parent) ? ; } write_report (& json_path , report) ? ; let summary_path = config . write_summary . clone () . unwrap_or_else (\| \| config . output_dir . join ("dead_code_summary.md")) ; if let Some (parent) = summary_path . parent () { std :: fs :: create_dir_all (parent) ? ; } write_summary_markdown (& summary_path , report , config . summary_limit) ? ; let plans_dir = summary_path . parent () . map (\| p \| p . to_path_buf ()) . unwrap_or_else (\| \| config . output_dir . clone ()) ; let plans_path = plans_dir . join ("dead_code_plans.md") ; write_plan_markdown (& plans_path , report , config . summary_limit) ? ; Ok (()) } . sig` | 0.42 | intra 1, inter 1 | same 1, other 1 | move | - (cohesion 0.42 below threshold 0.60 (impact 0.18)) |
| `build_basic_report` | `pub fn build_basic_report (timestamp : String , items : Vec < DeadCodeItem >) -> DeadCodeReport { let summary = DeadCodeSummary :: from_items (& items) ; DeadCodeReport { timestamp , summary , items , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | orphaned | - (suggest module utilities) |
| `build_report` | `pub fn build_report (timestamp : String , items : Vec < DeadCodeItem > , metadata : DeadCodeReportMetadata ,) -> DeadCodeReportWithMeta { let summary = DeadCodeSummary :: from_items (& items) ; DeadCodeReportWithMeta { timestamp , summary , items , metadata , } } . sig` | 0.78 | intra 0, inter 0 | same 3, other 2 | ok | - |
| `write_report` | `pub fn write_report (path : & Path , report : & DeadCodeReportWithMeta) -> std :: io :: Result < () > { let json = serde_json :: to_string_pretty (report) ? ; std :: fs :: write (path , json) } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | ok | - |

## File: src/470_dead_code_filter.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `filter_dead_code_elements` | `pub fn filter_dead_code_elements (elements : & [CodeElement] , report : & DeadCodeReportWithMeta ,) -> Vec < CodeElement > { let excluded = collect_excluded_symbols (report) ; elements . iter () . filter (\| el \| ! excluded . contains (& el . name)) . cloned () . collect () } . sig` | 0.60 | intra 1, inter 0 | same 0, other 3 | ok | - |
| `should_exclude_from_analysis` | `pub fn should_exclude_from_analysis (category : DeadCodeCategory) -> bool { matches ! (category , DeadCodeCategory :: Unreachable \| DeadCodeCategory :: TestOnly) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `collect_excluded_symbols` | `fn collect_excluded_symbols (report : & DeadCodeReportWithMeta) -> HashSet < String > { report . items . iter () . filter (\| item \| should_exclude_from_analysis (item . category)) . map (\| item \| item . symbol . clone ()) . collect () } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |

## File: src/490_dead_code_cli.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `merge_intent_map` | `pub (crate) fn merge_intent_map (base : & mut HashMap < String , Vec < crate :: dead_code_types :: IntentMetadata > > , next : HashMap < String , Vec < crate :: dead_code_types :: IntentMetadata > >) { for (symbol , items) in next { base . entry (symbol) . or_default () . extend (items) ; } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | ok | - |
| `reason_for_category` | `pub (crate) fn reason_for_category (category : DeadCodeCategory , intent_tag : bool , test_reference : bool) -> String { match category { DeadCodeCategory :: LatentPlanned => { if intent_tag { "Intent tag present" . to_string () } else { "Intent directory policy" . to_string () } } DeadCodeCategory :: TestOnly => { if test_reference { "Called only by test symbols" . to_string () } else { "Defined in test-only module" . to_string () } } DeadCodeCategory :: Unreachable => "No callers reachable from entrypoints" . to_string () , DeadCodeCategory :: ReachableUnused => "Reachable but unused in execution" . to_string () , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 5 | ok | - |
| `is_test_path` | `pub (crate) fn is_test_path (path : & Path) -> bool { path . components () . any (\| c \| { let name = c . as_os_str () . to_str () . unwrap_or ("") ; name == "tests" \|\| name == "test" \|\| name == "benches" }) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/510_dead_code_policy.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `load_policy` | `pub fn load_policy (path : & Path) -> std :: io :: Result < DeadCodePolicy > { let contents = std :: fs :: read_to_string (path) ? ; Ok (parse_policy (& contents , path . parent () . unwrap_or (path))) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `parse_policy` | `pub fn parse_policy (contents : & str , base : & Path) -> DeadCodePolicy { let mut planned_directories = Vec :: new () ; let mut public_api_roots = Vec :: new () ; let mut entrypoint_symbols = Vec :: new () ; let mut treat_public_as_entrypoint = true ; for line in contents . lines () { let trimmed = line . trim () ; if trimmed . is_empty () \|\| trimmed . starts_with ('#') \|\| trimmed . starts_with ("//") { continue ; } let Some ((key , value)) = trimmed . split_once ('=') else { continue ; } ; let key = key . trim () ; let value = value . trim () ; match key { "planned_directories" => { planned_directories = parse_list (value) . into_iter () . map (\| p \| base . join (p)) . collect () ; } "public_api_roots" => { public_api_roots = parse_list (value) . into_iter () . map (\| p \| base . join (p)) . collect () ; } "entrypoint_symbols" => { entrypoint_symbols = parse_list (value) ; } "treat_public_as_entrypoint" => { treat_public_as_entrypoint = parse_bool (value) . unwrap_or (true) ; } _ => { } } } DeadCodePolicy { planned_directories , public_api_roots , entrypoint_symbols , treat_public_as_entrypoint , } } . sig` | 0.60 | intra 4, inter 0 | same 0, other 2 | ok | - |
| `parse_list` | `fn parse_list (value : & str) -> Vec < String > { let mut trimmed = value . trim () . to_string () ; if let Some (stripped) = trimmed . strip_prefix ('[') { trimmed = stripped . to_string () ; } if let Some (stripped) = trimmed . strip_suffix (']') { trimmed = stripped . to_string () ; } trimmed . split (',') . map (\| s \| s . trim () . trim_matches ('"') . trim_matches ('\'') . to_string ()) . filter (\| s \| ! s . is_empty ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `parse_bool` | `fn parse_bool (value : & str) -> Option < bool > { match value . trim () . to_ascii_lowercase () . as_str () { "true" => Some (true) , "false" => Some (false) , _ => None , } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/520_violation_predictor.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `generate_intelligence_report` | `pub fn generate_intelligence_report (actions : & [RefactorAction] , state : & IntelligenceState < '_ > ,) -> CorrectionIntelligenceReport { let mut plans = Vec :: new () ; let mut policies = Vec :: new () ; let mut criteria = Vec :: new () ; let mut deltas = Vec :: new () ; for action in actions { let mut predictions = predict_violations (action , state . invariants , state . call_graph , state . elements) ; fill_prediction_confidence (& mut predictions) ; let mut plan = generate_correction_plan (action , & predictions) ; augment_path_coherence_strategies (& mut plan , action , & state . root) ; let policy = plan_verification_scope (action , & plan) ; let rollback = build_rollback_criteria (action , & plan) ; let delta = estimate_impact (action , & ImpactState { metrics : state . metrics . clone () , }) ; plans . push (plan) ; policies . push (policy) ; criteria . push (rollback) ; deltas . push (delta) ; } let summary = compute_summary (& plans , & deltas) ; CorrectionIntelligenceReport { version : "1.0" . to_string () , timestamp : chrono :: Utc :: now () . to_rfc3339 () , project_root : state . root . clone () , actions_analyzed : actions . len () , correction_plans : plans , verification_policies : policies , rollback_criteria : criteria , quality_deltas : deltas , summary , } } . sig` | 0.16 | intra 1, inter 5 | same 0, other 4 | move | - (cohesion 0.16 below threshold 0.60 (impact 0.44)) |
| `predict_violations` | `pub fn predict_violations (action : & RefactorAction , invariants : & InvariantAnalysisResult , call_graph : & HashMap < String , CallGraphNode > , elements : & [CodeElement] ,) -> Vec < ViolationPrediction > { let mut predictions = Vec :: new () ; match action { RefactorAction :: MoveFunction { function , from , to , required_layer } => { let callers = find_callers (function , call_graph , elements) ; if ! callers . is_empty () { predictions . push (ViolationPrediction { violation_type : ViolationType :: UnresolvedImport , affected_files : callers , severity : Severity :: Critical , confidence : 0.95 , }) ; } if let Some (layer) = required_layer { if ! layer . is_empty () { predictions . push (ViolationPrediction { violation_type : ViolationType :: LayerViolation , affected_files : vec ! [to . clone ()] , severity : Severity :: High , confidence : 1.0 , }) ; } } else if move_violates_invariant (function , from , to , invariants) { predictions . push (ViolationPrediction { violation_type : ViolationType :: LayerViolation , affected_files : vec ! [to . clone ()] , severity : Severity :: High , confidence : 0.9 , }) ; } } RefactorAction :: RenameFunction { old_name , new_name , file } => { if symbol_exists (new_name , elements) { predictions . push (ViolationPrediction { violation_type : ViolationType :: NameCollision , affected_files : vec ! [file . clone ()] , severity : Severity :: Critical , confidence : 1.0 , }) ; } let references = find_reference_files (old_name , call_graph , elements) ; if ! references . is_empty () { predictions . push (ViolationPrediction { violation_type : ViolationType :: BrokenReference , affected_files : references , severity : Severity :: Critical , confidence : 0.85 , }) ; } } RefactorAction :: RenameFile { from , to } => { predictions . push (ViolationPrediction { violation_type : ViolationType :: BrokenReference , affected_files : vec ! [from . clone () , to . clone ()] , severity : Severity :: High , confidence : 0.7 , }) ; } RefactorAction :: CreateFile { path } => { predictions . push (ViolationPrediction { violation_type : ViolationType :: UnresolvedImport , affected_files : vec ! [path . clone ()] , severity : Severity :: Low , confidence : 0.5 , }) ; } RefactorAction :: AdjustVisibility { file , .. } => { predictions . push (ViolationPrediction { violation_type : ViolationType :: VisibilityMismatch , affected_files : vec ! [file . clone ()] , severity : Severity :: Low , confidence : 0.8 , }) ; } } predictions } . sig` | 0.60 | intra 4, inter 0 | same 0, other 34 | ok | - |
| `find_callers` | `fn find_callers (function : & str , call_graph : & HashMap < String , CallGraphNode > , elements : & [CodeElement] ,) -> Vec < PathBuf > { let mut files = HashSet :: new () ; if let Some (node) = call_graph . get (function) { for caller in & node . called_by { if let Some (file) = find_element_file (caller , elements) { files . insert (file) ; } } } files . into_iter () . collect () } . sig` | 0.60 | intra 1, inter 0 | same 0, other 2 | ok | - |
| `find_reference_files` | `fn find_reference_files (function : & str , call_graph : & HashMap < String , CallGraphNode > , elements : & [CodeElement] ,) -> Vec < PathBuf > { let mut files = HashSet :: new () ; for (caller , node) in call_graph { if node . calls . iter () . any (\| c \| c == function) { if let Some (file) = find_element_file (caller , elements) { files . insert (file) ; } } } files . into_iter () . collect () } . sig` | 0.60 | intra 1, inter 0 | same 0, other 2 | ok | - |
| `find_element_file` | `fn find_element_file (function : & str , elements : & [CodeElement]) -> Option < PathBuf > { elements . iter () . find (\| el \| el . name == function) . map (\| el \| PathBuf :: from (& el . file_path)) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `symbol_exists` | `fn symbol_exists (symbol : & str , elements : & [CodeElement]) -> bool { elements . iter () . any (\| el \| el . name == symbol) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `move_violates_invariant` | `fn move_violates_invariant (_function : & str , _from : & PathBuf , _to : & PathBuf , _invariants : & InvariantAnalysisResult ,) -> bool { false } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |

## File: src/530_dead_code_report_split.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `write_summary_markdown` | `pub fn write_summary_markdown (path : & Path , report : & DeadCodeReportWithMeta , limit : usize ,) -> std :: io :: Result < () > { let mut content = String :: new () ; content . push_str ("# Dead Code Summary\n\n") ; content . push_str (& format ! ("Generated: {}\n\n" , report . timestamp)) ; content . push_str ("## Summary Counts\n\n") ; content . push_str (& format ! ("- Unreachable: {}\n- Reachable-unused: {}\n- Test-only: {}\n- Latent/planned: {}\n- Total analyzed: {}\n\n" , report . summary . unreachable , report . summary . reachable_unused , report . summary . test_only , report . summary . latent_planned , report . summary . total_analyzed)) ; let items = top_items (& report . items , limit) ; content . push_str ("## Top Findings\n\n") ; if items . is_empty () { content . push_str ("- None.\n") ; } else { for item in items { content . push_str (& format ! ("- `{}` in `{}` â€” {:?} / {:?} / {:?}\n" , item . symbol , item . file . display () , item . category , item . confidence , item . action)) ; } } content . push ('\n') ; std :: fs :: write (path , content) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `write_plan_markdown` | `pub fn write_plan_markdown (path : & Path , report : & DeadCodeReportWithMeta , limit : usize ,) -> std :: io :: Result < () > { let mut content = String :: new () ; content . push_str ("# Dead Code Plans (Review Only)\n\n") ; content . push_str (& format ! ("Generated: {}\n\n" , report . timestamp)) ; content . push_str ("Policy: review_only. No automatic deletion or moves.\n") ; content . push_str ("Guards: never delete public API; delete_safe requires manual confirmation + compiler dead_code warnings.\n\n") ; let items = top_items (& report . items , limit) ; content . push_str ("## Planned Items\n\n") ; if items . is_empty () { content . push_str ("- None.\n") ; } else { for item in items { let plan = plan_options (& item) ; content . push_str (& format ! ("- `{}` in `{}` â€” {:?} / {:?} / {:?}\n  Plan: {}\n" , item . symbol , item . file . display () , item . category , item . confidence , item . action , plan)) ; } } content . push ('\n') ; std :: fs :: write (path , content) } . sig` | 0.60 | intra 2, inter 0 | same 0, other 1 | ok | - |
| `top_items` | `fn top_items (items : & [DeadCodeItem] , limit : usize) -> Vec < DeadCodeItem > { let mut items = items . to_vec () ; items . sort_by_key (\| item \| item . action as u8) ; if items . len () > limit { items . truncate (limit) ; } items } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | ok | - |
| `plan_options` | `fn plan_options (item : & DeadCodeItem) -> String { let options = match item . category { DeadCodeCategory :: Unreachable => "keep \| quarantine \| delete_safe (manual confirm)" , DeadCodeCategory :: ReachableUnused => "keep \| quarantine \| annotate_intent" , DeadCodeCategory :: TestOnly => "relocate_tests \| keep" , DeadCodeCategory :: LatentPlanned => "keep \| annotate_intent" , } ; let mut plan = format ! ("review_only; options: {}" , options) ; if item . action == RecommendedAction :: DeleteSafe { plan . push_str ("; requires dead_code warning") ; } plan } . sig` | 0.60 | intra 0, inter 0 | same 0, other 6 | ok | - |

## File: src/540_tier_classifier.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `classify_tier` | `pub fn classify_tier (violation : & ViolationPrediction) -> ErrorTier { match (& violation . violation_type , & violation . severity) { (ViolationType :: UnresolvedImport , _) => ErrorTier :: Trivial , (ViolationType :: BrokenReference , Severity :: Low \| Severity :: Medium) => ErrorTier :: Trivial , (ViolationType :: NameCollision , _) => ErrorTier :: Moderate , (ViolationType :: LayerViolation , _) => ErrorTier :: Moderate , (ViolationType :: VisibilityMismatch , Severity :: Low \| Severity :: Medium) => ErrorTier :: Trivial , (ViolationType :: VisibilityMismatch , Severity :: High \| Severity :: Critical) => { ErrorTier :: Moderate } (ViolationType :: TypeMismatch , _) => ErrorTier :: Complex , (ViolationType :: OwnershipIssue , _) => ErrorTier :: Complex , (ViolationType :: BrokenReference , Severity :: Critical \| Severity :: High) => ErrorTier :: Complex , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 28 | orphaned | - (suggest module utilities) |

## File: src/550_confidence_scorer.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `compute_confidence` | `pub fn compute_confidence (prediction : & ViolationPrediction , context : & PredictionContext ,) -> f64 { let base : f64 = match prediction . violation_type { ViolationType :: UnresolvedImport => 0.95 , ViolationType :: NameCollision => 1.0 , ViolationType :: LayerViolation => 1.0 , ViolationType :: VisibilityMismatch => 0.8 , ViolationType :: BrokenReference => 0.85 , ViolationType :: TypeMismatch => 0.6 , ViolationType :: OwnershipIssue => 0.5 , } ; let multiplier : f64 = if context . has_test_coverage { 1.1 } else { 0.9 } ; (base * multiplier) . min (1.0) } . sig` | 0.63 | intra 0, inter 0 | same 1, other 8 | orphaned | - (suggest module utilities) |

## File: src/560_correction_plan_generator.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `generate_correction_plan` | `pub fn generate_correction_plan (action : & RefactorAction , predictions : & [ViolationPrediction] ,) -> CorrectionPlan { let mut strategies = Vec :: new () ; for prediction in predictions { match prediction . violation_type { ViolationType :: UnresolvedImport => { if let Some (symbol) = action_symbol (action) { strategies . push (CorrectionStrategy :: AddImport { module_path : action_module_path (action) , symbol , }) ; } } ViolationType :: BrokenReference => { match action { RefactorAction :: RenameFile { .. } => { if let Some ((old_ref , new_ref)) = action_refs (action) { strategies . push (CorrectionStrategy :: UpdatePath { old_path : old_ref , new_path : new_ref , }) ; } } _ => { if let Some ((old_ref , new_ref)) = action_refs (action) { for file in & prediction . affected_files { strategies . push (CorrectionStrategy :: UpdateCaller { caller_file : file . clone () , old_ref : old_ref . clone () , new_ref : new_ref . clone () , }) ; } } } } } ViolationType :: NameCollision => { if let Some (symbol) = action_symbol (action) { strategies . push (CorrectionStrategy :: RenameWithSuffix { original : symbol , suffix : "_v2" . to_string () , }) ; } } ViolationType :: LayerViolation => { if let Some (layer) = action_target_layer (action) { if let Some (function) = action_function (action) { strategies . push (CorrectionStrategy :: MoveToLayer { function , target_layer : layer , }) ; if let Some (function) = action_function (action) { if let Some (layer) = action_target_layer (action) { strategies . push (CorrectionStrategy :: EnsureImports { function , target_layer : layer , }) ; } } } } } ViolationType :: VisibilityMismatch => { if let Some ((symbol , file , from , to , reason)) = action_visibility (action) { if from == to \|\| reason . starts_with ("review:") { let options = vec ! [VisibilityPlanOption { policy : "keep_public" . to_string () , target : crate :: types :: Visibility :: Public , requires_consent : false , description : "Keep public (treat as external API)." . to_string () , } , VisibilityPlanOption { policy : "downgrade_pub_crate" . to_string () , target : crate :: types :: Visibility :: Crate , requires_consent : true , description : "Narrow to pub(crate) (internal API only)." . to_string () , } , VisibilityPlanOption { policy : "downgrade_private" . to_string () , target : crate :: types :: Visibility :: Private , requires_consent : true , description : "Narrow to private (file-local)." . to_string () , } ,] ; strategies . push (CorrectionStrategy :: VisibilityPlan { symbol , file , current : from , default_policy : "review_only" . to_string () , options , notes : reason , }) ; } else { strategies . push (CorrectionStrategy :: AdjustVisibility { symbol , file , from , to , reason , }) ; } } } ViolationType :: TypeMismatch \| ViolationType :: OwnershipIssue => { strategies . push (CorrectionStrategy :: ManualReview { reason : format ! ("{:?} requires semantic analysis" , prediction . violation_type) , context : format ! ("{:?}" , action) , }) ; } } } let tier = predictions . iter () . map (classify_tier) . max () . unwrap_or (ErrorTier :: Trivial) ; CorrectionPlan { action_id : action . action_id () , tier , predicted_violations : predictions . to_vec () , strategies , confidence : average_confidence (predictions) , estimated_fix_time_seconds : estimate_fix_time (predictions . len ()) , } } . sig` | 0.60 | intra 12, inter 0 | same 0, other 28 | ok | - |
| `average_confidence` | `fn average_confidence (predictions : & [ViolationPrediction]) -> f64 { if predictions . is_empty () { return 1.0 ; } let total : f64 = predictions . iter () . map (\| p \| p . confidence) . sum () ; total / predictions . len () as f64 } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `action_symbol` | `fn action_symbol (action : & RefactorAction) -> Option < String > { match action { RefactorAction :: MoveFunction { function , .. } => Some (function . clone ()) , RefactorAction :: RenameFunction { new_name , .. } => Some (new_name . clone ()) , RefactorAction :: AdjustVisibility { symbol , .. } => Some (symbol . clone ()) , _ => None , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | ok | - |
| `action_function` | `fn action_function (action : & RefactorAction) -> Option < String > { match action { RefactorAction :: MoveFunction { function , .. } => Some (function . clone ()) , _ => None , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | ok | - |
| `action_module_path` | `fn action_module_path (action : & RefactorAction) -> String { match action { RefactorAction :: MoveFunction { to , .. } => to . display () . to_string () , RefactorAction :: RenameFile { to , .. } => to . display () . to_string () , RefactorAction :: CreateFile { path } => path . display () . to_string () , RefactorAction :: AdjustVisibility { file , .. } => file . display () . to_string () , _ => "crate" . to_string () , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 5 | ok | - |
| `action_refs` | `fn action_refs (action : & RefactorAction) -> Option < (String , String) > { match action { RefactorAction :: RenameFunction { old_name , new_name , .. } => { Some ((old_name . clone () , new_name . clone ())) } RefactorAction :: RenameFile { from , to } => { Some ((from . display () . to_string () , to . display () . to_string ())) } _ => None , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `action_target_layer` | `fn action_target_layer (action : & RefactorAction) -> Option < String > { match action { RefactorAction :: MoveFunction { required_layer , .. } => required_layer . clone () , _ => None , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | ok | - |
| `action_visibility` | `fn action_visibility (action : & RefactorAction ,) -> Option < (String , std :: path :: PathBuf , crate :: types :: Visibility , crate :: types :: Visibility , String ,) > { match action { RefactorAction :: AdjustVisibility { symbol , file , from , to , reason , } => Some ((symbol . clone () , file . clone () , from . clone () , to . clone () , reason . clone ())) , _ => None , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | ok | - |
| `estimate_fix_time` | `fn estimate_fix_time (count : usize) -> u32 { 10 + (count as u32 * 5) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/570_verification_scope_planner.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `plan_verification_scope` | `pub fn plan_verification_scope (action : & RefactorAction , correction_plan : & CorrectionPlan ,) -> VerificationPolicy { let scope = match correction_plan . tier { ErrorTier :: Trivial if correction_plan . predicted_violations . len () <= 3 => { VerificationScope :: SyntaxOnly { files : affected_files (action) , } } ErrorTier :: Trivial \| ErrorTier :: Moderate => VerificationScope :: ModuleLocal { module : action_module (action) , transitive_depth : 2 , } , ErrorTier :: Complex => VerificationScope :: FullWorkspace , } ; let mut required_checks = vec ! [VerificationCheck :: CargoCheck] ; if matches ! (correction_plan . tier , ErrorTier :: Moderate \| ErrorTier :: Complex) { required_checks . push (VerificationCheck :: CargoTest { filter : None }) ; } VerificationPolicy { action_id : correction_plan . action_id . clone () , scope , required_checks , incremental_eligible : matches ! (correction_plan . tier , ErrorTier :: Trivial) , estimated_time_seconds : estimate_verification_time (& correction_plan . tier) , } } . sig` | 0.60 | intra 3, inter 0 | same 0, other 16 | ok | - |
| `affected_files` | `fn affected_files (action : & RefactorAction) -> Vec < std :: path :: PathBuf > { match action { RefactorAction :: MoveFunction { from , to , .. } => vec ! [from . clone () , to . clone ()] , RefactorAction :: RenameFunction { file , .. } => vec ! [file . clone ()] , RefactorAction :: RenameFile { from , to } => vec ! [from . clone () , to . clone ()] , RefactorAction :: CreateFile { path } => vec ! [path . clone ()] , RefactorAction :: AdjustVisibility { file , .. } => vec ! [file . clone ()] , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 6 | ok | - |
| `action_module` | `fn action_module (action : & RefactorAction) -> String { match action { RefactorAction :: MoveFunction { to , .. } => to . display () . to_string () , RefactorAction :: RenameFunction { file , .. } => file . display () . to_string () , RefactorAction :: RenameFile { to , .. } => to . display () . to_string () , RefactorAction :: CreateFile { path } => path . display () . to_string () , RefactorAction :: AdjustVisibility { file , .. } => file . display () . to_string () , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 6 | ok | - |
| `estimate_verification_time` | `fn estimate_verification_time (tier : & ErrorTier) -> u32 { match tier { ErrorTier :: Trivial => 10 , ErrorTier :: Moderate => 60 , ErrorTier :: Complex => 180 , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | ok | - |

## File: src/580_rollback_criteria_builder.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `build_rollback_criteria` | `pub fn build_rollback_criteria (action : & RefactorAction , correction_plan : & CorrectionPlan ,) -> RollbackCriteria { let mut mandatory = vec ! [RollbackCondition :: BuildFailed] ; let mut suggested = vec ! [RollbackCondition :: QualityDecreased { threshold : 0.05 }] ; match correction_plan . tier { ErrorTier :: Complex => { mandatory . push (RollbackCondition :: Tier3Error { error_type : ViolationType :: TypeMismatch , }) ; mandatory . push (RollbackCondition :: ManualReviewRequired) ; } ErrorTier :: Moderate => { suggested . push (RollbackCondition :: TestsFailed { critical_tests : extract_critical_tests (action) , }) ; } ErrorTier :: Trivial => { } } for prediction in & correction_plan . predicted_violations { if prediction . violation_type == ViolationType :: LayerViolation { mandatory . push (RollbackCondition :: InvariantViolated { invariant_ids : vec ! ["layer_ordering" . to_string ()] , }) ; } } RollbackCriteria { action_id : correction_plan . action_id . clone () , mandatory_rollback_if : mandatory , suggested_rollback_if : suggested , } } . sig` | 0.60 | intra 1, inter 0 | same 0, other 15 | ok | - |
| `extract_critical_tests` | `fn extract_critical_tests (_action : & RefactorAction) -> Vec < String > { Vec :: new () } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |

## File: src/590_quality_delta_calculator.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `estimate_impact` | `pub fn estimate_impact (action : & RefactorAction , current_state : & AnalysisState) -> QualityDelta { let simulated = simulate_action (action , current_state) ; calculate_quality_delta (action , & current_state . metrics , & simulated . metrics) } . sig` | 0.35 | intra 1, inter 1 | same 0, other 3 | move | - (cohesion 0.35 below threshold 0.60 (impact 0.25)) |
| `calculate_quality_delta` | `pub fn calculate_quality_delta (action : & RefactorAction , current : & Metrics , simulated : & Metrics ,) -> QualityDelta { let cohesion_delta = simulated . cohesion - current . cohesion ; let violation_delta = simulated . violations as i32 - current . violations as i32 ; let complexity_delta = simulated . complexity - current . complexity ; let overall = 0.5 * cohesion_delta - 0.3 * violation_delta as f64 - 0.2 * complexity_delta ; let acceptable = overall > - 0.05 && violation_delta <= 0 ; let reason = if acceptable { "Quality improved or maintained" . to_string () } else if overall < - 0.1 { "Quality degradation exceeds threshold" . to_string () } else if violation_delta > 0 { format ! ("Introduced {} new violations" , violation_delta) } else { "Quality barely acceptable" . to_string () } ; QualityDelta { action_id : action . action_id () , cohesion_delta , violation_delta , complexity_delta , overall_score_delta : overall , acceptable , reason , } } . sig` | 0.72 | intra 0, inter 0 | same 2, other 3 | ok | - |

## File: src/600_action_impact_estimator.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `simulate_action` | `pub (crate) fn simulate_action (_action : & RefactorAction , state : & AnalysisState) -> AnalysisState { state . clone () } . sig` | 0.80 | intra 0, inter 0 | same 2, other 1 | ok | - |

## File: src/610_correction_plan_serializer.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `write_intelligence_outputs_at` | `pub fn write_intelligence_outputs_at (report : & CorrectionIntelligenceReport , output_dir : & Path , correction_json : Option < & Path > , verification_policy_json : Option < & Path > ,) -> std :: io :: Result < () > { std :: fs :: create_dir_all (output_dir) ? ; let json_path = correction_json . map (\| p \| p . to_path_buf ()) . unwrap_or_else (\| \| output_dir . join ("correction_intelligence.json")) ; if let Some (parent) = json_path . parent () { std :: fs :: create_dir_all (parent) ? ; } let contract = serialize_correction_plans (report) ; std :: fs :: write (& json_path , serde_json :: to_string_pretty (& contract) ?) ? ; let policy_path = verification_policy_json . map (\| p \| p . to_path_buf ()) . unwrap_or_else (\| \| output_dir . join ("verification_policy.json")) ; if let Some (parent) = policy_path . parent () { std :: fs :: create_dir_all (parent) ? ; } emit_verification_policy (& report . verification_policies , & policy_path) ? ; Ok (()) } . sig` | 0.35 | intra 1, inter 1 | same 0, other 1 | move | - (cohesion 0.35 below threshold 0.60 (impact 0.25)) |
| `serialize_correction_plan` | `pub fn serialize_correction_plan (plan : & CorrectionPlan , verification : & VerificationPolicy , rollback : & RollbackCriteria ,) -> Value { json ! ({ "action_id" : plan . action_id , "tier" : format ! ("{:?}" , plan . tier) , "confidence" : plan . confidence , "estimated_fix_time_seconds" : plan . estimated_fix_time_seconds , "predicted_violations" : plan . predicted_violations . iter () . map (\| v \| json ! ({ "type" : format ! ("{:?}" , v . violation_type) , "severity" : format ! ("{:?}" , v . severity) , "affected_files" : v . affected_files , "confidence" : v . confidence , })) . collect ::< Vec < _ >> () , "correction_strategies" : plan . strategies . iter () . map (serialize_strategy) . collect ::< Vec < _ >> () , "verification_policy" : { "scope" : serialize_scope (& verification . scope) , "required_checks" : verification . required_checks . iter () . map (serialize_check) . collect ::< Vec < _ >> () , "incremental_eligible" : verification . incremental_eligible , "estimated_time_seconds" : verification . estimated_time_seconds , } , "rollback_criteria" : { "mandatory" : rollback . mandatory_rollback_if . iter () . map (\| c \| format ! ("{:?}" , c)) . collect ::< Vec < _ >> () , "suggested" : rollback . suggested_rollback_if . iter () . map (\| c \| format ! ("{:?}" , c)) . collect ::< Vec < _ >> () , } }) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `serialize_correction_plans` | `pub fn serialize_correction_plans (report : & CorrectionIntelligenceReport ,) -> serde_json :: Value { let items = report . correction_plans . iter () . zip (report . verification_policies . iter ()) . zip (report . rollback_criteria . iter ()) . map (\| ((plan , policy) , rollback) \| serialize_correction_plan (plan , policy , rollback)) . collect :: < Vec < _ > > () ; json ! ({ "version" : report . version , "timestamp" : report . timestamp , "project_root" : report . project_root , "actions_analyzed" : report . actions_analyzed , "correction_plans" : items , "quality_deltas" : report . quality_deltas , "summary" : report . summary , }) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |

## File: src/620_verification_policy_emitter.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `emit_verification_policy` | `pub fn emit_verification_policy (policies : & [VerificationPolicy] , output_path : & Path ,) -> std :: io :: Result < () > { let policy_file = json ! ({ "version" : "1.0" , "policies" : policies . iter () . map (\| p \| json ! ({ "action_id" : p . action_id , "scope" : serialize_scope (& p . scope) , "checks" : p . required_checks . iter () . map (serialize_check) . collect ::< Vec < _ >> () , "incremental" : p . incremental_eligible , "estimated_time_seconds" : p . estimated_time_seconds , })) . collect ::< Vec < _ >> () }) ; std :: fs :: write (output_path , serde_json :: to_string_pretty (& policy_file) ?) ? ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |

## File: src/630_correction_intelligence_report.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `write_intelligence_outputs` | `pub fn write_intelligence_outputs (report : & CorrectionIntelligenceReport , output_dir : & Path ,) -> std :: io :: Result < () > { write_intelligence_outputs_at (report , output_dir , None , None) } . sig` | 0.20 | intra 0, inter 1 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `augment_path_coherence_strategies` | `pub (crate) fn augment_path_coherence_strategies (plan : & mut CorrectionPlan , action : & RefactorAction , root : & Path ,) { let RefactorAction :: RenameFile { from , to } = action else { return ; } ; let Some (old_mod) = module_name_from_path (from) else { return ; } ; let Some (new_mod) = module_name_from_path (to) else { return ; } ; let old_file_name = from . file_name () . and_then (\| s \| s . to_str ()) . unwrap_or ("") ; let new_file_name = to . file_name () . and_then (\| s \| s . to_str ()) . unwrap_or ("") ; let replace_mod = old_mod != new_mod ; let mod_re = if replace_mod { Regex :: new (& format ! (r"^\s*(pub\s+)?mod\s+{}\s*;" , regex :: escape (& old_mod))) . ok () } else { None } ; let use_re = if replace_mod { Regex :: new (& format ! (r"^\s*use\s+.*\b{}\b" , regex :: escape (& old_mod))) . ok () } else { None } ; let path_re = if ! old_file_name . is_empty () && ! new_file_name . is_empty () { Regex :: new (r#"^\s*#\s*\[\s*path\s*=\s*"([^"]+)"\s*\]"#) . ok () } else { None } ; let mut updates = Vec :: new () ; let mut seen = HashSet :: new () ; let rust_files = crate :: cluster_010 :: gather_rust_files (root) ; for file in rust_files { let Ok (contents) = fs :: read_to_string (& file) else { continue ; } ; for line in contents . lines () { if let Some (re) = & mod_re { if re . is_match (line) { let new_line = line . replace (& old_mod , & new_mod) ; if new_line != line { let key = (file . clone () , line . to_string () , new_line . clone ()) ; if seen . insert (key . clone ()) { updates . push (key) ; } } continue ; } } if let Some (re) = & use_re { if re . is_match (line) { let new_line = line . replace (& old_mod , & new_mod) ; if new_line != line { let key = (file . clone () , line . to_string () , new_line . clone ()) ; if seen . insert (key . clone ()) { updates . push (key) ; } } continue ; } } if let Some (re) = & path_re { if re . is_match (line) && line . contains (old_file_name) { let new_line = line . replace (old_file_name , new_file_name) ; if new_line != line { let key = (file . clone () , line . to_string () , new_line . clone ()) ; if seen . insert (key . clone ()) { updates . push (key) ; } } } } } } updates . sort_by (\| a , b \| { a . 0 . cmp (& b . 0) . then_with (\| \| a . 1 . cmp (& b . 1)) . then_with (\| \| a . 2 . cmp (& b . 2)) }) ; for (file , old_ref , new_ref) in updates { plan . strategies . push (CorrectionStrategy :: UpdateCaller { caller_file : file , old_ref , new_ref , }) ; } } . sig` | 0.60 | intra 2, inter 0 | same 0, other 4 | ok | - |
| `fill_prediction_confidence` | `pub (crate) fn fill_prediction_confidence (predictions : & mut [ViolationPrediction]) { for prediction in predictions { if prediction . confidence <= 0.0 { prediction . confidence = default_confidence (& prediction . violation_type) ; } } } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `default_confidence` | `fn default_confidence (violation_type : & crate :: correction_plan_types :: ViolationType) -> f64 { match violation_type { crate :: correction_plan_types :: ViolationType :: UnresolvedImport => 0.95 , crate :: correction_plan_types :: ViolationType :: NameCollision => 1.0 , crate :: correction_plan_types :: ViolationType :: LayerViolation => 0.9 , crate :: correction_plan_types :: ViolationType :: VisibilityMismatch => 0.8 , crate :: correction_plan_types :: ViolationType :: BrokenReference => 0.85 , crate :: correction_plan_types :: ViolationType :: TypeMismatch => 0.6 , crate :: correction_plan_types :: ViolationType :: OwnershipIssue => 0.5 , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 8 | ok | - |
| `compute_summary` | `pub (crate) fn compute_summary (plans : & [CorrectionPlan] , deltas : & [QualityDelta]) -> CorrectionSummary { let mut trivial = 0 ; let mut moderate = 0 ; let mut complex = 0 ; let mut total_violations = 0 ; let mut total_confidence = 0.0 ; let mut total_time = 0 ; for plan in plans { match plan . tier { ErrorTier :: Trivial => trivial += 1 , ErrorTier :: Moderate => moderate += 1 , ErrorTier :: Complex => complex += 1 , } total_violations += plan . predicted_violations . len () ; total_confidence += plan . confidence ; total_time += plan . estimated_fix_time_seconds ; } let avg_conf = if plans . is_empty () { 0.0 } else { total_confidence / plans . len () as f64 } ; let _ = deltas ; CorrectionSummary { trivial_count : trivial , moderate_count : moderate , complex_count : complex , total_predicted_violations : total_violations , average_confidence : avg_conf , estimated_total_fix_time_seconds : total_time , } } . sig` | 0.69 | intra 0, inter 0 | same 2, other 5 | ok | - |
| `build_state` | `pub fn build_state < 'a > (root : & 'a Path , analysis : & 'a AnalysisResult , metrics : Metrics ,) -> IntelligenceState < 'a > { IntelligenceState { root : root . to_path_buf () , invariants : & analysis . invariants , call_graph : & analysis . call_graph , elements : & analysis . elements , metrics , } } . sig` | 0.75 | intra 0, inter 0 | same 2, other 2 | orphaned | - (suggest module utilities) |
| `filter_visibility_report` | `pub fn filter_visibility_report (report : & CorrectionIntelligenceReport ,) -> CorrectionIntelligenceReport { let mut plans = Vec :: new () ; let mut policies = Vec :: new () ; let mut criteria = Vec :: new () ; let mut deltas = Vec :: new () ; for (idx , plan) in report . correction_plans . iter () . enumerate () { let mut has_visibility = false ; for strategy in & plan . strategies { match strategy { CorrectionStrategy :: AdjustVisibility { .. } => { has_visibility = true ; break ; } CorrectionStrategy :: VisibilityPlan { .. } => { has_visibility = true ; break ; } CorrectionStrategy :: ManualReview { reason , .. } if reason . starts_with ("review:") => { has_visibility = true ; break ; } _ => { } } } if ! has_visibility { continue ; } plans . push (plan . clone ()) ; if let Some (policy) = report . verification_policies . get (idx) { policies . push (policy . clone ()) ; } if let Some (rollback) = report . rollback_criteria . get (idx) { criteria . push (rollback . clone ()) ; } if let Some (delta) = report . quality_deltas . get (idx) { deltas . push (delta . clone ()) ; } } let summary = compute_summary (& plans , & deltas) ; CorrectionIntelligenceReport { version : report . version . clone () , timestamp : report . timestamp . clone () , project_root : report . project_root . clone () , actions_analyzed : plans . len () , correction_plans : plans , verification_policies : policies , rollback_criteria : criteria , quality_deltas : deltas , summary , } } . sig` | 0.75 | intra 1, inter 0 | same 3, other 3 | ok | - |
| `filter_path_coherence_report` | `pub fn filter_path_coherence_report (report : & CorrectionIntelligenceReport ,) -> CorrectionIntelligenceReport { let mut plans = Vec :: new () ; let mut policies = Vec :: new () ; let mut criteria = Vec :: new () ; let mut deltas = Vec :: new () ; for (idx , plan) in report . correction_plans . iter () . enumerate () { let mut has_path_coherence = false ; for strategy in & plan . strategies { match strategy { CorrectionStrategy :: UpdatePath { .. } => { has_path_coherence = true ; break ; } CorrectionStrategy :: UpdateCaller { old_ref , .. } => { let trimmed = old_ref . trim_start () ; if trimmed . starts_with ("mod ") \|\| trimmed . starts_with ("pub mod ") \|\| trimmed . starts_with ("use ") \|\| trimmed . starts_with ("#[path") { has_path_coherence = true ; break ; } } _ => { } } } let is_rename_file = plan . action_id . starts_with ("rename_file_") ; if ! (has_path_coherence \|\| is_rename_file) { continue ; } plans . push (plan . clone ()) ; if let Some (policy) = report . verification_policies . get (idx) { policies . push (policy . clone ()) ; } if let Some (rollback) = report . rollback_criteria . get (idx) { criteria . push (rollback . clone ()) ; } if let Some (delta) = report . quality_deltas . get (idx) { deltas . push (delta . clone ()) ; } } let summary = compute_summary (& plans , & deltas) ; CorrectionIntelligenceReport { version : report . version . clone () , timestamp : report . timestamp . clone () , project_root : report . project_root . clone () , actions_analyzed : plans . len () , correction_plans : plans , verification_policies : policies , rollback_criteria : criteria , quality_deltas : deltas , summary , } } . sig` | 0.78 | intra 1, inter 0 | same 3, other 2 | ok | - |
| `module_name_from_path` | `fn module_name_from_path (path : & Path) -> Option < String > { let stem = path . file_stem () . and_then (\| s \| s . to_str ()) ? ; let name = if stem == "mod" { path . parent () . and_then (\| p \| p . file_name ()) . and_then (\| n \| n . to_str ()) ? . to_string () } else { stem . to_string () } ; Some (crate :: cluster_010 :: normalize_module_name (& name)) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## Orphaned Functions (Review Only)

Action: review each item for expected usage. Delete only if it also appears under "Delete Candidates (Orphaned + Dead Code)".
Note: excludes public symbols referenced by other modules and entry points. Delete candidates require dead_code warnings.

- `export_json` in `src/170_invariant_reporter.rs`
- `export_constraints_json` in `src/170_invariant_reporter.rs`
- `extract_attribute_value` in `src/211_dead_code_attribute_parser.rs`
- `parallel_build_file_dag` in `src/260_file_ordering.rs`
- `main` in `src/320_main.rs`
- `find_test_callers` in `src/400_dead_code_test_boundaries.rs`
- `build_basic_report` in `src/460_dead_code_report.rs`
- `compute_confidence` in `src/550_confidence_scorer.rs`
- `write_intelligence_outputs` in `src/630_correction_intelligence_report.rs`

## Delete Candidates (Orphaned + Dead Code)

- None detected.

## Utility Module Candidates

- None detected.

## Function Clusters

- cohesion 0.67, suggested `src/330_agent_cli.rs`
  - src/330_agent_cli.rs::run_agent_cli, src/330_agent_cli.rs::query_function, src/330_agent_cli.rs::list_invariants, src/330_agent_cli.rs::load_invariants
- cohesion 1.00, suggested `src/470_dead_code_filter.rs`
  - src/470_dead_code_filter.rs::filter_dead_code_elements, src/470_dead_code_filter.rs::should_exclude_from_analysis, src/470_dead_code_filter.rs::collect_excluded_symbols
- cohesion 1.00, suggested `src/510_dead_code_policy.rs`
  - src/510_dead_code_policy.rs::load_policy, src/510_dead_code_policy.rs::parse_policy, src/510_dead_code_policy.rs::parse_list, src/510_dead_code_policy.rs::parse_bool
- cohesion 1.00, suggested `src/070_cluster_011.rs`
  - src/070_cluster_011.rs::build_module_map, src/070_cluster_011.rs::build_file_dependency_graph, src/070_cluster_011.rs::build_file_dag
- cohesion 0.41, suggested `src/000_cluster_001.rs`
  - src/000_cluster_001.rs::build_directory_entry_map, src/000_cluster_001.rs::collect_naming_warnings, src/000_cluster_001.rs::detect_layer, src/000_cluster_001.rs::order_rust_files_by_dependency, src/000_cluster_001.rs::build_entries, src/000_cluster_001.rs::analyze_file_ordering, src/000_cluster_001.rs::naming_score_for_file, src/000_cluster_001.rs::detect_cycles, src/000_cluster_001.rs::detect_violations
- cohesion 1.00, suggested `src/210_utilities.rs`
  - src/210_utilities.rs::compress_path, src/210_utilities.rs::collect_directory_files, src/210_utilities.rs::path_common_prefix_len, src/210_utilities.rs::resolve_required_layer_path, src/210_utilities.rs::compute_move_metrics, src/210_utilities.rs::collect_move_items, src/210_utilities.rs::write_structural_batches, src/210_utilities.rs::write_cluster_batches
- cohesion 0.60, suggested `src/380_dead_code_call_graph.rs`
  - src/020_cluster_010.rs::gather_rust_files, src/160_layer_utilities.rs::resolve_source_root, src/160_layer_utilities.rs::allow_analysis_dir, src/211_dead_code_attribute_parser.rs::is_cfg_test_item, src/380_dead_code_call_graph.rs::build_reverse_call_graph, src/380_dead_code_call_graph.rs::compute_reachability, src/400_dead_code_test_boundaries.rs::find_test_callers
- cohesion 0.00, suggested `src/050_cluster_010.rs`
  - src/000_cluster_001.rs::collect_julia_dependencies, src/050_cluster_010.rs::contains_tools, src/050_cluster_010.rs::extract_rust_dependencies, src/050_cluster_010.rs::build_dependency_map
- cohesion 0.40, suggested `src/390_dead_code_intent.rs`
  - src/211_dead_code_attribute_parser.rs::detect_intent_signals, src/370_dead_code_doc_comment_parser.rs::merge_doc_intent, src/390_dead_code_intent.rs::check_planned_directory, src/390_dead_code_intent.rs::merge_intent_sources
- cohesion 0.56, suggested `src/211_dead_code_attribute_parser.rs`
  - src/211_dead_code_attribute_parser.rs::scan_intent_tags, src/211_dead_code_attribute_parser.rs::scan_doc_comments, src/370_dead_code_doc_comment_parser.rs::extract_doc_markers, src/370_dead_code_doc_comment_parser.rs::item_name, src/390_dead_code_intent.rs::collect_symbols, src/400_dead_code_test_boundaries.rs::item_attrs
- cohesion 1.00, suggested `src/560_correction_plan_generator.rs`
  - src/520_violation_predictor.rs::predict_violations, src/520_violation_predictor.rs::find_callers, src/520_violation_predictor.rs::find_reference_files, src/520_violation_predictor.rs::find_element_file, src/520_violation_predictor.rs::symbol_exists, src/520_violation_predictor.rs::move_violates_invariant, src/520_violation_predictor.rs::generate_intelligence_report, src/560_correction_plan_generator.rs::generate_correction_plan, src/560_correction_plan_generator.rs::average_confidence, src/560_correction_plan_generator.rs::estimate_fix_time, src/560_correction_plan_generator.rs::action_symbol, src/560_correction_plan_generator.rs::action_function, src/560_correction_plan_generator.rs::action_module_path, src/560_correction_plan_generator.rs::action_refs, src/560_correction_plan_generator.rs::action_target_layer, src/560_correction_plan_generator.rs::action_visibility, src/570_verification_scope_planner.rs::plan_verification_scope, src/570_verification_scope_planner.rs::affected_files, src/570_verification_scope_planner.rs::action_module, src/570_verification_scope_planner.rs::estimate_verification_time, src/580_rollback_criteria_builder.rs::build_rollback_criteria, src/580_rollback_criteria_builder.rs::extract_critical_tests, src/590_quality_delta_calculator.rs::calculate_quality_delta, src/590_quality_delta_calculator.rs::estimate_impact, src/600_action_impact_estimator.rs::simulate_action, src/630_correction_intelligence_report.rs::filter_path_coherence_report, src/630_correction_intelligence_report.rs::filter_visibility_report, src/630_correction_intelligence_report.rs::augment_path_coherence_strategies, src/630_correction_intelligence_report.rs::module_name_from_path, src/630_correction_intelligence_report.rs::compute_summary, src/630_correction_intelligence_report.rs::fill_prediction_confidence, src/630_correction_intelligence_report.rs::default_confidence
- cohesion 0.67, suggested `src/380_dead_code_call_graph.rs`
  - src/000_cluster_001.rs::run_analysis, src/030_refactor_constraints.rs::generate_constraints, src/070_cluster_011.rs::export_program_cfg_to_path, src/160_layer_utilities.rs::main, src/380_dead_code_call_graph.rs::build_call_graph
- cohesion 0.07, suggested `src/010_cluster_008.rs`
  - src/000_cluster_001.rs::layer_constrained_sort, src/000_cluster_001.rs::topo_sort_within, src/010_cluster_008.rs::is_layer_violation, src/010_cluster_008.rs::compare_dir_layers, src/010_cluster_008.rs::layer_adheres, src/010_cluster_008.rs::parse_cluster_members, src/010_cluster_008.rs::is_core_module_path
- cohesion 0.60, suggested `src/530_dead_code_report_split.rs`
  - src/460_dead_code_report.rs::write_outputs, src/530_dead_code_report_split.rs::write_plan_markdown, src/530_dead_code_report_split.rs::top_items, src/530_dead_code_report_split.rs::plan_options
- cohesion 0.00, suggested `src/000_cluster_001.rs`
  - src/000_cluster_001.rs::rust_entry_paths, src/000_cluster_001.rs::collect_rust_dependencies, src/000_cluster_001.rs::build_file_layers, src/000_cluster_001.rs::topological_sort, src/000_cluster_001.rs::ordered_by_name
- cohesion 0.18, suggested `src/050_cluster_010.rs`
  - src/000_cluster_001.rs::order_julia_files_by_dependency, src/050_cluster_010.rs::normalize_module_name, src/050_cluster_010.rs::build_module_root_map, src/050_cluster_010.rs::extract_julia_dependencies, src/050_cluster_010.rs::extract_dependencies
- cohesion 0.85, suggested `src/010_cluster_008.rs`
  - src/010_cluster_008.rs::build_result, src/010_cluster_008.rs::adjacency_from_edges, src/010_cluster_008.rs::topo_sort, src/010_cluster_008.rs::layer_rank_map, src/010_cluster_008.rs::insert_sorted, src/010_cluster_008.rs::is_mmsb_main, src/010_cluster_008.rs::layer_prefix_value, src/010_cluster_008.rs::compare_path_components, src/010_cluster_008.rs::detect_layer_violation, src/010_cluster_008.rs::cluster_target_path, src/010_cluster_008.rs::collect_cluster_plans, src/110_cluster_006.rs::layer_prefix_value
- cohesion 0.73, suggested `src/410_dead_code_entrypoints.rs`
  - src/000_cluster_001.rs::gather_julia_files, src/160_layer_utilities.rs::run_dead_code_pipeline, src/211_dead_code_attribute_parser.rs::detect_test_modules, src/211_dead_code_attribute_parser.rs::detect_test_symbols, src/380_dead_code_call_graph.rs::is_reachable, src/380_dead_code_call_graph.rs::classify_symbol, src/380_dead_code_call_graph.rs::is_test_only, src/400_dead_code_test_boundaries.rs::has_test_attr, src/410_dead_code_entrypoints.rs::collect_entrypoints, src/410_dead_code_entrypoints.rs::collect_exports, src/410_dead_code_entrypoints.rs::is_public_api, src/410_dead_code_entrypoints.rs::collect_use_tree_idents, src/410_dead_code_entrypoints.rs::treat_public_as_entrypoint, src/420_dead_code_classifier.rs::is_reachable, src/430_dead_code_confidence.rs::assign_confidence, src/440_dead_code_actions.rs::recommend_action, src/460_dead_code_report.rs::build_report, src/490_dead_code_cli.rs::merge_intent_map, src/490_dead_code_cli.rs::reason_for_category, src/490_dead_code_cli.rs::is_test_path
- cohesion 1.00, suggested `src/110_cluster_006.rs`
  - src/110_cluster_006.rs::strip_numeric_prefix, src/110_cluster_006.rs::generate_canonical_name, src/110_cluster_006.rs::collect_directory_moves
- cohesion 0.45, suggested `src/211_dead_code_attribute_parser.rs`
  - src/211_dead_code_attribute_parser.rs::parse_mmsb_latent_attr, src/211_dead_code_attribute_parser.rs::scan_file_attributes, src/211_dead_code_attribute_parser.rs::collect_latent_attrs, src/211_dead_code_attribute_parser.rs::marker_from_str, src/370_dead_code_doc_comment_parser.rs::detect_latent_markers, src/370_dead_code_doc_comment_parser.rs::item_attrs, src/390_dead_code_intent.rs::planned_directory_intent
- cohesion 1.00, suggested `src/610_correction_plan_serializer.rs`
  - src/610_correction_plan_serializer.rs::serialize_correction_plan, src/610_correction_plan_serializer.rs::serialize_correction_plans, src/610_correction_plan_serializer.rs::write_intelligence_outputs_at, src/620_verification_policy_emitter.rs::emit_verification_policy, src/630_correction_intelligence_report.rs::write_intelligence_outputs

