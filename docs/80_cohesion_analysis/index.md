# Function Cohesion Analysis

Generated: 2025-12-30 22:29:05

## Metrics

- Avg cohesion: 0.76
- Move suggestions: 7
- Orphaned functions: 76
- Layer violations: 10

## File: src/000_cluster_001.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `build_directory_entry_map` | `pub fn build_directory_entry_map (files : & [PathBuf] ,) -> Result < HashMap < PathBuf , crate :: types :: FileOrderEntry > > { use crate :: file_ordering :: { build_dependency_map , build_entries , build_file_dag , detect_cycles , ordered_by_name , topological_sort , } ; use crate :: layer_core :: layer_constrained_sort ; use crate :: layer_utilities :: build_file_layers ; use crate :: types :: FileOrderingResult ; use std :: collections :: HashSet ; const DEFAULT_STEP : usize = 10 ; if files . is_empty () { return Ok (HashMap :: new ()) ; } let file_set : HashSet < PathBuf > = files . iter () . cloned () . collect () ; let module_map = crate :: cluster_011 :: build_module_map (files) ; let dep_map = build_dependency_map (files , & file_set , & module_map) ? ; let file_layers = build_file_layers (files) ; let (graph , node_map) = build_file_dag (files , & dep_map) ; let cycles = detect_cycles (& graph , files) ; let ordered_nodes = if cycles . is_empty () { layer_constrained_sort (& graph , & file_layers) . unwrap_or_else (\| _ \| { topological_sort (& graph) . unwrap_or_else (\| _ \| ordered_by_name (files , & node_map)) }) } else { ordered_by_name (files , & node_map) } ; let ordered_files = ordered_nodes . into_iter () . map (\| idx \| graph [idx] . clone ()) . collect :: < Vec < _ > > () ; let ordering = FileOrderingResult { ordered_files : build_entries (& ordered_files , DEFAULT_STEP) , violations : Vec :: new () , layer_violations : Vec :: new () , ordered_directories : Vec :: new () , cycles , } ; let mut map = HashMap :: new () ; for entry in ordering . ordered_files { map . insert (entry . current_path . clone () , entry) ; } Ok (map) } . sig` | 0.49 | intra 7, inter 2 | same 0, other 3 | move | - (cohesion 0.49 below threshold 0.60 (impact 0.11)) |
| `layer_constrained_sort` | `pub fn layer_constrained_sort (graph : & DiGraph < PathBuf , () > , file_layers : & HashMap < PathBuf , String > ,) -> Result < Vec < NodeIndex > > { use crate :: cluster_006 :: layer_prefix_value ; let mut layer_nodes : BTreeMap < i32 , Vec < NodeIndex > > = BTreeMap :: new () ; for node in graph . node_indices () { let file = & graph [node] ; let layer_name = file_layers . get (file) . cloned () . unwrap_or_else (\| \| "root" . to_string ()) ; let layer_value = layer_prefix_value (& layer_name) . unwrap_or (0) ; layer_nodes . entry (layer_value) . or_default () . push (node) ; } let mut ordered = Vec :: new () ; for (_layer , nodes) in layer_nodes { let sorted = topo_sort_within (graph , & nodes) ? ; ordered . extend (sorted) ; } Ok (ordered) } . sig` | 0.57 | intra 1, inter 2 | same 0, other 0 | move | - (cohesion 0.57 below threshold 0.60 (impact 0.03)) |
| `collect_naming_warnings` | `pub fn collect_naming_warnings (directory : & crate :: types :: DirectoryAnalysis , config : & crate :: report :: ReportConfig , warnings : & mut Vec < String > ,) -> Result < () > { use crate :: utilities :: compress_path ; use crate :: dependency :: naming_score_for_file ; if directory . path . components () . any (\| comp \| comp . as_os_str () == "_old") { return Ok (()) ; } let file_map = build_directory_entry_map (& directory . files) ? ; for file in & directory . files { if file . components () . any (\| comp \| comp . as_os_str () == "_old") { continue ; } let entry = file_map . get (file) ; if let Some (score) = naming_score_for_file (file , entry) { if score < config . naming_score_warning { let suggested = entry . map (\| e \| e . suggested_name . as_str ()) . unwrap_or ("suggested name unavailable") ; warnings . push (format ! ("File `{}` has naming score {:.0}; consider renaming to `{}`." , compress_path (file . to_string_lossy () . as_ref ()) , score , suggested ,)) ; } } } for child in & directory . subdirectories { collect_naming_warnings (child , config , warnings) ? ; } Ok (()) } . sig` | 0.60 | intra 3, inter 0 | same 0, other 2 | ok | - |
| `order_rust_files_by_dependency` | `# [doc = " Order Rust files by dependency and capture layer graph details."] pub fn order_rust_files_by_dependency (files : & [PathBuf] , root : & Path ,) -> Result < (Vec < PathBuf > , LayerGraph) > { let module_map = crate :: cluster_010 :: build_module_root_map (root) ? ; let entry_files = rust_entry_paths (root) ; let mut file_layers : HashMap < PathBuf , String > = HashMap :: new () ; let mut nodes : BTreeSet < String > = BTreeSet :: new () ; let mut edges_map : BTreeMap < (String , String) , BTreeSet < ReferenceDetail > > = BTreeMap :: new () ; let mut unresolved = Vec :: new () ; for file in files { let layer = detect_layer (file) ; nodes . insert (layer . clone ()) ; file_layers . insert (file . clone () , layer . clone ()) ; let deps = collect_rust_dependencies (file) . with_context (\| \| format ! ("Failed to collect dependencies for {:?}" , file)) ? ; for dep in deps { if let Some (info) = module_map . get (& dep . root) { nodes . insert (info . layer . clone ()) ; if info . layer != layer { edges_map . entry ((info . layer . clone () , layer . clone ())) . or_default () . insert (ReferenceDetail { file : file . clone () , reference : dep . detail . clone () , }) ; } } else { unresolved . push (UnresolvedDependency { file : file . clone () , reference : dep . detail . clone () , }) ; } } } crate :: cluster_008 :: build_result (files , file_layers , nodes , edges_map , unresolved , & entry_files ,) } . sig` | 0.60 | intra 3, inter 0 | same 0, other 4 | ok | - |
| `build_entries` | `# [doc = " Builds file ordering entries with canonical names and rename flags"] pub fn build_entries (ordered : & [PathBuf] , step : usize) -> Vec < crate :: types :: FileOrderEntry > { ordered . iter () . enumerate () . map (\| (idx , path) \| { let canonical_order = idx * step ; let suggested_name = crate :: cluster_006 :: generate_canonical_name (path , canonical_order) ; let needs_rename = path . file_name () . and_then (\| n \| n . to_str ()) . map (\| name \| name != suggested_name) . unwrap_or (false) ; crate :: types :: FileOrderEntry { current_path : path . clone () , canonical_order , suggested_name , needs_rename , } }) . collect () } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | ok | - |
| `analyze_file_ordering` | `pub fn analyze_file_ordering (files : & [PathBuf] , step : Option < usize > ,) -> Result < crate :: types :: FileOrderingResult > { let step = step . unwrap_or (10) ; let file_set : HashSet < PathBuf > = files . iter () . cloned () . collect () ; let module_map = crate :: cluster_011 :: build_module_map (files) ; let dep_map = crate :: cluster_010 :: build_dependency_map (files , & file_set , & module_map) ? ; let file_layers = build_file_layers (files) ; let ordered_directories = crate :: layer_core :: order_directories (files , & dep_map) ; let (graph , node_map) = crate :: cluster_011 :: build_file_dag (files , & dep_map) ; let layer_violations = crate :: cluster_008 :: detect_layer_violations (& graph , & file_layers) ; let cycles = detect_cycles (& graph , files) ; let ordered_nodes = if cycles . is_empty () { crate :: layer_core :: layer_constrained_sort (& graph , & file_layers) . unwrap_or_else (\| _ \| { topological_sort (& graph) . unwrap_or_else (\| _ \| ordered_by_name (files , & node_map)) }) } else { ordered_by_name (files , & node_map) } ; let ordered_files = ordered_nodes . into_iter () . map (\| idx \| graph [idx] . clone ()) . collect :: < Vec < _ > > () ; let file_entries = build_entries (& ordered_files , step) ; let violations = detect_violations (& file_entries , & dep_map) ; Ok (crate :: types :: FileOrderingResult { ordered_files : file_entries , violations , layer_violations , ordered_directories , cycles , }) } . sig` | 0.60 | intra 7, inter 0 | same 0, other 2 | ok | - |
| `naming_score_for_file` | `pub fn naming_score_for_file (file : & Path , order_entry : Option < & crate :: types :: FileOrderEntry > ,) -> Option < f64 > { let name = file . file_name () ? . to_string_lossy () ; let stem = file . file_stem () ? . to_string_lossy () ; let mut score = 1.0f64 ; if stem . len () < 3 { score -= 0.2 ; } if stem . len () > 40 { score -= 0.1 ; } if stem . chars () . any (\| c \| c . is_uppercase ()) { score -= 0.1 ; } if ! stem . chars () . all (\| c \| c . is_ascii_lowercase () \|\| c . is_ascii_digit () \|\| c == '_') { score -= 0.1 ; } if name . contains ("__") { score -= 0.1 ; } if let Some (entry) = order_entry { let expected = entry . suggested_name . as_str () ; let actual = name . as_ref () ; if expected != actual { score -= 0.3 ; } else { score += 0.1 ; } } if let Ok (contents) = fs :: read_to_string (file) { let mut ident_counts : HashMap < String , usize > = HashMap :: new () ; let ident_re = match Regex :: new (r"[A-Za-z_][A-Za-z0-9_]*") { Ok (regex) => regex , Err (_) => return None , } ; for cap in ident_re . captures_iter (& contents) { let Some (m) = cap . get (0) else { continue ; } ; let ident = m . as_str () . to_lowercase () ; if matches ! (ident . as_str () , "fn" \| "pub" \| "use" \| "struct" \| "enum" \| "impl" \| "mod" \| "let" \| "mut" \| "ref" \| "self" \| "crate" \| "super" \| "where" \| "trait" \| "type" \| "const" \| "static" \| "match" \| "if" \| "else" \| "for" \| "while" \| "loop" \| "return" \| "async" \| "await" \| "move" \| "dyn" \| "as") { continue ; } * ident_counts . entry (ident) . or_insert (0) += 1 ; } let mut idents = ident_counts . into_iter () . collect :: < Vec < _ > > () ; idents . sort_by (\| a , b \| b . 1 . cmp (& a . 1)) ; let top_idents = idents . into_iter () . take (8) . map (\| (k , _) \| k) . collect :: < Vec < _ > > () ; let name_tokens = stem . split ('_') . map (\| s \| s . to_lowercase ()) . filter (\| s \| ! s . is_empty () && ! s . chars () . all (\| c \| c . is_ascii_digit ())) . collect :: < Vec < _ > > () ; let overlap = top_idents . iter () . filter (\| ident \| name_tokens . iter () . any (\| t \| t == * ident)) . count () ; if overlap == 0 { score -= 0.1 ; } else if overlap >= 2 { score += 0.1 ; } } if score < 0.0 { score = 0.0 ; } if score > 1.0 { score = 1.0 ; } Some (score * 100.0) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `detect_violations` | `pub (crate) fn detect_violations (ordered_files : & [crate :: types :: FileOrderEntry] , dep_map : & HashMap < PathBuf , Vec < PathBuf > > ,) -> Vec < crate :: types :: OrderViolation > { let mut alpha = ordered_files . to_vec () ; alpha . sort_by (\| a , b \| a . current_path . cmp (& b . current_path)) ; let alpha_positions : HashMap < PathBuf , usize > = alpha . iter () . enumerate () . map (\| (idx , entry) \| (entry . current_path . clone () , idx)) . collect () ; let canonical_positions : HashMap < PathBuf , usize > = ordered_files . iter () . enumerate () . map (\| (idx , entry) \| (entry . current_path . clone () , idx)) . collect () ; let mut violations = Vec :: new () ; for entry in ordered_files { let Some (& alpha_pos) = alpha_positions . get (& entry . current_path) else { continue ; } ; let Some (& required_pos) = canonical_positions . get (& entry . current_path) else { continue ; } ; if alpha_pos != required_pos { let blocking_dependencies = dep_map . get (& entry . current_path) . map (\| deps \| { deps . iter () . filter (\| dep \| { let dep_alpha = alpha_positions . get (* dep) . copied () . unwrap_or (0) ; dep_alpha > alpha_pos }) . cloned () . collect :: < Vec < _ > > () }) . unwrap_or_default () ; violations . push (crate :: types :: OrderViolation { file : entry . current_path . clone () , current_position : alpha_pos , required_position : required_pos , blocking_dependencies , }) ; } } violations } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `export_complete_program_dot` | `# [doc = " Exports a complete program CFG to DOT format"] pub fn export_complete_program_dot (program : & crate :: types :: ProgramCFG , path : & str ,) -> std :: io :: Result < () > { use std :: collections :: HashMap ; use std :: fmt :: Write ; fn escape_dot (s : & str) -> String { s . replace ('\\' , "\\\\") . replace ('"' , "\\\"") . replace ('\n' , "\\n") } let mut dot = String :: new () ; writeln ! (dot , "digraph ProgramCFG {{") . unwrap () ; writeln ! (dot , "  rankdir=TB;") . unwrap () ; writeln ! (dot , "  compound=true;") . unwrap () ; writeln ! (dot , "  newrank=true;") . unwrap () ; writeln ! (dot , "  label=\"Complete Program CFG - {} functions\";" , program . functions . len ()) . unwrap () ; writeln ! (dot , "  labelloc=t;") . unwrap () ; writeln ! (dot , "  fontsize=16;") . unwrap () ; writeln ! (dot , "") . unwrap () ; let mut funcs : Vec < _ > = program . functions . iter () . collect () ; funcs . sort_by_key (\| (fid , _) \| fid . as_str ()) ; let mut func_to_cluster : HashMap < & String , usize > = HashMap :: new () ; for (cluster_idx , (func_id , cfg)) in funcs . iter () . enumerate () { let safe_name = func_id . replace (['!' , '?' , '*'] , "_") ; let cc = crate :: cluster_008 :: cyclomatic_complexity (cfg) ; func_to_cluster . insert (func_id , cluster_idx) ; writeln ! (dot , "  subgraph cluster_{} {{" , cluster_idx) . unwrap () ; writeln ! (dot , "    label=\"{} (CC={})\";" , safe_name , cc) . unwrap () ; writeln ! (dot , "    style=filled;") . unwrap () ; writeln ! (dot , "    fillcolor=lightgray;") . unwrap () ; writeln ! (dot , "    color=black;") . unwrap () ; writeln ! (dot , "") . unwrap () ; for node in & cfg . nodes { let (shape , color , style) = crate :: cluster_008 :: node_style (& node . node_type) ; let mut label = node . label . clone () ; if ! node . lines . is_empty () { let lines_str : String = node . lines . iter () . map (\| l \| l . to_string ()) . collect :: < Vec < _ > > () . join (",") ; label = format ! ("{} L{}" , label , lines_str) ; } let url = format ! ("http://127.0.0.1:8081/run?f={}" , func_id) ; writeln ! (dot , "    f{}_n{} [label=\"{}\", shape={}, fillcolor={}, style={}, URL=\"{}\"];" , cluster_idx , node . id , escape_dot (& label) , shape , color , style , url) . unwrap () ; } writeln ! (dot , "") . unwrap () ; for edge in & cfg . edges { let mut attrs = Vec :: new () ; if let Some (cond) = edge . condition { let label = if cond { "T" } else { "F" } ; let color = if cond { "darkgreen" } else { "red" } ; attrs . push (format ! ("label=\"{}\"" , label)) ; attrs . push (format ! ("color=\"{}\"" , color)) ; } let attr_str = if attrs . is_empty () { "" . to_string () } else { format ! (" [{}]" , attrs . join (", ")) } ; writeln ! (dot , "    f{}_n{} -> f{}_n{}{};" , cluster_idx , edge . from , cluster_idx , edge . to , attr_str) . unwrap () ; } writeln ! (dot , "  }}") . unwrap () ; writeln ! (dot , "") . unwrap () ; } writeln ! (dot , "  // Inter-function calls") . unwrap () ; writeln ! (dot , "  edge [style=dashed, color=blue, penwidth=2];") . unwrap () ; writeln ! (dot , "") . unwrap () ; for (caller , callee) in & program . call_edges { if let (Some (& caller_idx) , Some (& callee_idx)) = (func_to_cluster . get (caller) , func_to_cluster . get (callee)) { if let (Some (caller_cfg) , Some (callee_cfg)) = (program . functions . get (caller) , program . functions . get (callee)) { writeln ! (dot , "  f{}_n{} -> f{}_n{} [ltail=cluster_{}, lhead=cluster_{}, label=\"call\"];" , caller_idx , caller_cfg . exit_id , callee_idx , callee_cfg . entry_id , caller_idx , callee_idx) . unwrap () ; } } } writeln ! (dot , "}}") . unwrap () ; std :: fs :: write (path , dot) ? ; println ! ("Complete program CFG exported to {}" , path) ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | orphaned | - (suggest module utilities) |
| `temp_dir` | `# [cfg (test)] fn temp_dir (name : & str) -> PathBuf { let mut dir = std :: env :: temp_dir () ; dir . push (format ! ("mmsb_analyzer_{}_{}" , name , std :: time :: SystemTime :: now () . duration_since (std :: time :: UNIX_EPOCH) . unwrap () . as_nanos ())) ; dir } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `detects_cycles` | `# [cfg (test)] fn detects_cycles () -> Result < () > { use crate :: dependency :: analyze_file_ordering ; use std :: fs :: { create_dir_all , write } ; let dir = temp_dir ("cycle") ; create_dir_all (& dir) ? ; let a = dir . join ("a.rs") ; let b = dir . join ("b.rs") ; write (& a , "use crate::b; pub fn a() {}") ? ; write (& b , "use crate::a; pub fn b() {}") ? ; let result = analyze_file_ordering (& [a . clone () , b . clone ()] , Some (10)) ? ; assert ! (! result . cycles . is_empty ()) ; Ok (()) } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |
| `generates_canonical_names_and_violations` | `# [cfg (test)] fn generates_canonical_names_and_violations () -> Result < () > { use crate :: dependency :: analyze_file_ordering ; use std :: fs :: { create_dir_all , write } ; let dir = temp_dir ("names") ; create_dir_all (& dir) ? ; let a = dir . join ("a.rs") ; let b = dir . join ("b.rs") ; write (& a , "use crate::b; pub fn a() {}") ? ; write (& b , "pub fn b() {}") ? ; let result = analyze_file_ordering (& [a . clone () , b . clone ()] , Some (10)) ? ; let entries = & result . ordered_files ; assert_eq ! (entries [0] . suggested_name , "000_b.rs") ; assert_eq ! (entries [1] . suggested_name , "010_a.rs") ; assert ! (! result . violations . is_empty ()) ; Ok (()) } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |
| `topo_sort_orders_dependencies` | `# [cfg (test)] # [allow (dead_code)] fn topo_sort_orders_dependencies () -> Result < () > { use crate :: dependency :: analyze_file_ordering ; use std :: fs :: { create_dir_all , write } ; let dir = temp_dir ("topo") ; create_dir_all (& dir) ? ; let a = dir . join ("a.rs") ; let b = dir . join ("b.rs") ; let c = dir . join ("c.rs") ; write (& a , "pub fn a() {}") ? ; write (& b , "use crate::a; pub fn b() {}") ? ; write (& c , "use crate::b; pub fn c() {}") ? ; let result = analyze_file_ordering (& [c . clone () , b . clone () , a . clone ()] , Some (10)) ? ; let ordered : Vec < _ > = result . ordered_files . iter () . map (\| entry \| entry . current_path . clone ()) . collect () ; assert_eq ! (ordered , vec ! [a , b , c]) ; assert ! (result . cycles . is_empty ()) ; Ok (()) } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |
| `topo_sort_within` | `pub fn topo_sort_within (graph : & DiGraph < PathBuf , () > , nodes : & [NodeIndex] ,) -> Result < Vec < NodeIndex > > { let node_set : HashSet < NodeIndex > = nodes . iter () . copied () . collect () ; let mut indegree : HashMap < NodeIndex , usize > = HashMap :: new () ; for & node in nodes { indegree . insert (node , 0) ; } for & node in nodes { let incoming = graph . neighbors_directed (node , petgraph :: Direction :: Incoming) . filter (\| n \| node_set . contains (n)) . count () ; indegree . insert (node , incoming) ; } let mut queue = std :: collections :: VecDeque :: new () ; for & node in nodes { if indegree . get (& node) . copied () . unwrap_or (0) == 0 { queue . push_back (node) ; } } let mut ordered = Vec :: new () ; while let Some (node) = queue . pop_front () { ordered . push (node) ; for neighbor in graph . neighbors_directed (node , petgraph :: Direction :: Outgoing) { if ! node_set . contains (& neighbor) { continue ; } if let Some (entry) = indegree . get_mut (& neighbor) { * entry = entry . saturating_sub (1) ; if * entry == 0 { queue . push_back (neighbor) ; } } } } if ordered . len () != nodes . len () { return Err (anyhow :: anyhow ! ("Cycle detected within layer group")) ; } Ok (ordered) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `detect_layer` | `# [doc = " Detects the layer identifier from a path by finding first digit-prefixed component"] pub fn detect_layer (path : & Path) -> String { for component in path . components () { if let Some (name) = component . as_os_str () . to_str () { if let Some (first) = name . chars () . next () { if first . is_ascii_digit () { if let Some (pos) = name . find ('_') { if name [.. pos] . chars () . all (\| c \| c . is_ascii_digit ()) { return name . to_string () ; } } } } } } "root" . to_string () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `rust_entry_paths` | `pub fn rust_entry_paths (root : & Path) -> BTreeSet < PathBuf > { let src_dir = crate :: layer_utilities :: resolve_source_root (root) ; ["lib.rs" , "main.rs"] . iter () . map (\| rel \| src_dir . join (rel)) . filter (\| p \| p . exists ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `collect_rust_dependencies` | `fn collect_rust_dependencies (path : & Path) -> Result < Vec < RustDependency > > { let content = fs :: read_to_string (path) . with_context (\| \| format ! ("Unable to read Rust file {:?}" , path)) ? ; let syntax = syn :: parse_file (& content) . with_context (\| \| format ! ("Unable to parse Rust file {:?}" , path)) ? ; let mut collector = UseCollector :: default () ; collector . visit_file (& syntax) ; Ok (collector . deps) } . sig` | 0.90 | intra 0, inter 0 | same 2, other 0 | ok | - |
| `collect_roots_from_crate` | `fn collect_roots_from_crate (tree : & UseTree , state : CrateRootState , acc : & mut BTreeSet < String >) { match tree { UseTree :: Path (path) => { let ident = path . ident . to_string () ; if state == CrateRootState :: Start && ident == "crate" { collect_roots_from_crate (& path . tree , CrateRootState :: AfterCrate , acc) ; } else if state == CrateRootState :: AfterCrate { acc . insert (ident) ; } else { collect_roots_from_crate (& path . tree , state , acc) ; } } UseTree :: Group (group) => { for tree in & group . items { collect_roots_from_crate (tree , state , acc) ; } } UseTree :: Name (name) => { if state == CrateRootState :: AfterCrate { acc . insert (name . ident . to_string ()) ; } } UseTree :: Rename (rename) => { if state == CrateRootState :: AfterCrate { acc . insert (rename . ident . to_string ()) ; } } UseTree :: Glob (_) => { } } } . sig` | 0.90 | intra 3, inter 0 | same 6, other 0 | ok | - |
| `collect_julia_dependencies` | `pub (crate) fn collect_julia_dependencies (path : & Path) -> Result < Vec < JuliaDependency > > { let content = fs :: read_to_string (path) . with_context (\| \| format ! ("Unable to read Julia file {:?}" , path)) ? ; let mut deps = Vec :: new () ; for cap in INCLUDE_REGEX . captures_iter (& content) { if let Some (path_match) = cap . get (1) { let relative = PathBuf :: from (path_match . as_str ()) ; let detail = cap . get (0) . map (\| m \| m . as_str () . trim () . to_string ()) . unwrap_or_default () ; deps . push (JuliaDependency { target : JuliaTarget :: Include (relative) , detail , }) ; } } for cap in USING_REGEX . captures_iter (& content) { if let Some (module_match) = cap . get (1) { let module = module_match . as_str () ; let primary = module . split ('.') . next () . unwrap_or (module) . to_string () ; let detail = cap . get (0) . map (\| m \| m . as_str () . trim () . to_string ()) . unwrap_or_default () ; deps . push (JuliaDependency { target : JuliaTarget :: Module (primary) , detail , }) ; } } for cap in ROOT_USING_REGEX . captures_iter (& content) { if let Some (symbols) = cap . get (1) { let detail = cap . get (0) . map (\| m \| m . as_str () . trim () . to_string ()) . unwrap_or_default () ; for symbol in symbols . as_str () . split (',') . map (\| s \| s . trim ()) . filter (\| s \| ! s . is_empty ()) { let primary = symbol . split ('.') . next () . unwrap_or (symbol) . to_string () ; deps . push (JuliaDependency { target : JuliaTarget :: Module (primary) , detail : detail . clone () , }) ; } } } for cap in LOCAL_USING_REGEX . captures_iter (& content) { if let Some (module_match) = cap . get (1) { let module = module_match . as_str () ; let detail = cap . get (0) . map (\| m \| m . as_str () . trim () . to_string ()) . unwrap_or_default () ; deps . push (JuliaDependency { target : JuliaTarget :: Module (module . to_string ()) , detail , }) ; } } Ok (deps) } . sig` | 0.90 | intra 0, inter 0 | same 9, other 0 | ok | - |
| `julia_entry_paths` | `pub fn julia_entry_paths (root : & Path) -> BTreeSet < PathBuf > { let src_dir = crate :: layer_utilities :: resolve_source_root (root) ; ["MMSB.jl" , "API.jl" , "MMSB/API.jl"] . iter () . map (\| rel \| src_dir . join (rel)) . filter (\| p \| p . exists ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `build_file_layers` | `pub fn build_file_layers (files : & [PathBuf]) -> HashMap < PathBuf , String > { let mut layers = HashMap :: new () ; for file in files { layers . insert (file . clone () , detect_layer (file)) ; } layers } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `gather_julia_files` | `pub fn gather_julia_files (root : & Path) -> Vec < PathBuf > { use walkdir :: WalkDir ; let src_root = crate :: layer_utilities :: resolve_source_root (root) ; WalkDir :: new (& src_root) . into_iter () . filter_entry (\| entry \| { if entry . depth () == 0 { return true ; } if ! entry . file_type () . is_dir () { return true ; } crate :: layer_utilities :: allow_analysis_dir (& src_root , entry . path ()) }) . filter_map (\| e \| e . ok ()) . filter (\| e \| e . path () . extension () . map_or (false , \| ext \| ext == "jl")) . filter (\| e \| { let rel = e . path () . strip_prefix (& src_root) . unwrap_or (e . path ()) ; rel . components () . count () == 1 \|\| e . path () . starts_with (src_root . join ("src")) }) . map (\| entry \| entry . into_path ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `topological_sort` | `pub fn topological_sort (graph : & DiGraph < PathBuf , () >) -> Result < Vec < NodeIndex > > { use petgraph :: Direction ; use std :: collections :: VecDeque ; let mut indegree = vec ! [0usize ; graph . node_count ()] ; for node in graph . node_indices () { indegree [node . index ()] = graph . neighbors_directed (node , Direction :: Incoming) . count () ; } let mut queue = VecDeque :: new () ; for node in graph . node_indices () { if indegree [node . index ()] == 0 { queue . push_back (node) ; } } let mut ordered = Vec :: new () ; while let Some (node) = queue . pop_front () { ordered . push (node) ; for neighbor in graph . neighbors_directed (node , Direction :: Outgoing) { let entry = & mut indegree [neighbor . index ()] ; * entry = entry . saturating_sub (1) ; if * entry == 0 { queue . push_back (neighbor) ; } } } if ordered . len () != graph . node_count () { return Err (anyhow :: anyhow ! ("Cycle detected in dependency graph")) ; } Ok (ordered) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `ordered_by_name` | `pub fn ordered_by_name (files : & [PathBuf] , node_map : & HashMap < PathBuf , NodeIndex > ,) -> Vec < NodeIndex > { let mut sorted = files . to_vec () ; sorted . sort () ; sorted . into_iter () . filter_map (\| path \| node_map . get (& path) . copied ()) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `detect_cycles` | `pub (crate) fn detect_cycles (graph : & DiGraph < PathBuf , () > , files : & [PathBuf] ,) -> Vec < Vec < PathBuf > > { let sccs = tarjan_scc (graph) ; let mut cycles = Vec :: new () ; for scc in sccs { if scc . len () > 1 { cycles . push (scc . into_iter () . map (\| idx \| graph [idx] . clone ()) . collect ()) ; } } if cycles . is_empty () { return cycles ; } if cycles . iter () . all (\| cycle \| cycle . is_empty ()) { let mut fallback = files . to_vec () ; fallback . sort () ; cycles . push (fallback) ; } cycles } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `escape_dot` | `fn escape_dot (s : & str) -> String { s . replace ('\\' , "\\\\") . replace ('"' , "\\\"") . replace ('\n' , "\\n") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `test_detects_cycles` | `# [test] fn test_detects_cycles () { detects_cycles () . unwrap () ; } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `test_generates_canonical_names_and_violations` | `# [test] fn test_generates_canonical_names_and_violations () { generates_canonical_names_and_violations () . unwrap () ; } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |

## File: src/000_invariant_types.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `test_confidence_from_strength` | `# [test] fn test_confidence_from_strength () { assert_eq ! (Confidence :: from_strength (& InvariantStrength :: Proven) . value () , 1.0) ; assert_eq ! (Confidence :: from_strength (& InvariantStrength :: Empirical { paths_checked : 150 }) . value () , 0.9) ; assert_eq ! (Confidence :: from_strength (& InvariantStrength :: Heuristic) . value () , 0.3) ; } . sig` | 0.90 | intra 0, inter 0 | same 6, other 0 | orphaned | - (suggest module utilities) |
| `test_is_blocking` | `# [test] fn test_is_blocking () { let proven_inv = Invariant :: new ("test_fn" . to_string () , "test.rs" . to_string () , InvariantKind :: Structural (StructuralInvariant :: LayerFixed { layer : 0 }) , InvariantStrength :: Proven , "Test invariant" . to_string () ,) ; assert ! (proven_inv . is_blocking ()) ; let heuristic_inv = Invariant :: new ("test_fn" . to_string () , "test.rs" . to_string () , InvariantKind :: Semantic (SemanticInvariant :: PureFunction) , InvariantStrength :: Heuristic , "Test heuristic" . to_string () ,) ; assert ! (! heuristic_inv . is_blocking ()) ; } . sig` | 0.90 | intra 0, inter 0 | same 8, other 0 | orphaned | - (suggest module utilities) |
| `test_stats_calculation` | `# [test] fn test_stats_calculation () { let mut result = InvariantAnalysisResult :: new () ; result . add_invariant (Invariant :: new ("fn1" . to_string () , "test.rs" . to_string () , InvariantKind :: Structural (StructuralInvariant :: Leaf) , InvariantStrength :: Proven , "Leaf node" . to_string () ,)) ; result . add_invariant (Invariant :: new ("fn2" . to_string () , "test.rs" . to_string () , InvariantKind :: Semantic (SemanticInvariant :: PureFunction) , InvariantStrength :: Heuristic , "Pure function" . to_string () ,)) ; result . stats . update_totals () ; assert_eq ! (result . stats . total_count , 2) ; assert_eq ! (result . stats . proven_count , 1) ; assert_eq ! (result . stats . heuristic_count , 1) ; assert_eq ! (result . stats . proven_percentage () , 50.0) ; } . sig` | 0.90 | intra 0, inter 0 | same 9, other 0 | orphaned | - (suggest module utilities) |

## File: src/005_refactor_constraints.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `test_check_move_allowed_blocking` | `# [test] fn test_check_move_allowed_blocking () { let constraints = vec ! [RefactorConstraint :: NoMove { target : "test_fn" . to_string () , reason : "layer fixed" . to_string () , strength : InvariantStrength :: Proven , }] ; let result = check_move_allowed ("test_fn" , "old.rs" , "new.rs" , & constraints) ; assert ! (result . is_err ()) ; assert ! (result . unwrap_err () . contains ("layer fixed")) ; } . sig` | 0.50 | intra 1, inter 1 | same 1, other 1 | move | - (cohesion 0.50 below threshold 0.60 (impact 0.10)) |
| `test_check_move_allowed_non_blocking` | `# [test] fn test_check_move_allowed_non_blocking () { let constraints = vec ! [RefactorConstraint :: NoMove { target : "test_fn" . to_string () , reason : "heuristic" . to_string () , strength : InvariantStrength :: Heuristic , }] ; let result = check_move_allowed ("test_fn" , "old.rs" , "new.rs" , & constraints) ; assert ! (result . is_ok ()) ; } . sig` | 0.50 | intra 1, inter 1 | same 1, other 1 | move | - (cohesion 0.50 below threshold 0.60 (impact 0.10)) |
| `test_from_invariant_layer_fixed` | `# [test] fn test_from_invariant_layer_fixed () { let inv = Invariant :: new ("test_fn" . to_string () , "test.rs" . to_string () , InvariantKind :: Structural (StructuralInvariant :: LayerFixed { layer : 5 }) , InvariantStrength :: Proven , "Layer is fixed" . to_string () ,) ; let constraint = from_invariant (& inv) . unwrap () ; match constraint { RefactorConstraint :: FixedLayer { target , layer , .. } => { assert_eq ! (target , "test_fn") ; assert_eq ! (layer , 5) ; } _ => panic ! ("Wrong constraint type") , } } . sig` | 0.66 | intra 1, inter 0 | same 1, other 4 | ok | - |
| `from_invariant` | `# [doc = " Convert an invariant to a refactoring constraint"] pub fn from_invariant (invariant : & Invariant) -> Option < RefactorConstraint > { match & invariant . kind { InvariantKind :: Structural (s) => match s { StructuralInvariant :: LayerFixed { layer } => Some (RefactorConstraint :: FixedLayer { target : invariant . target . clone () , layer : * layer , strength : invariant . strength , }) , StructuralInvariant :: DegreeStable { in_degree , out_degree } => { Some (RefactorConstraint :: PreserveDegree { target : invariant . target . clone () , in_degree : * in_degree , out_degree : * out_degree , strength : invariant . strength , }) } StructuralInvariant :: Leaf \| StructuralInvariant :: Root => { Some (RefactorConstraint :: NoMove { target : invariant . target . clone () , reason : "graph topology fixed" . to_string () , strength : invariant . strength , }) } StructuralInvariant :: Bridge => Some (RefactorConstraint :: NoDelete { target : invariant . target . clone () , dependents : vec ! ["graph connectivity" . to_string ()] , strength : invariant . strength , }) , StructuralInvariant :: SccMembership { .. } => Some (RefactorConstraint :: NoMove { target : invariant . target . clone () , reason : "SCC membership fixed" . to_string () , strength : invariant . strength , }) , } , InvariantKind :: Semantic (s) => match s { SemanticInvariant :: TypeStable { signature } => { Some (RefactorConstraint :: PreserveSignature { target : invariant . target . clone () , signature : signature . clone () , strength : invariant . strength , }) } SemanticInvariant :: PureFunction \| SemanticInvariant :: Idempotent => { Some (RefactorConstraint :: PreserveSignature { target : invariant . target . clone () , signature : "effects must remain pure" . to_string () , strength : invariant . strength , }) } SemanticInvariant :: EffectStable { .. } => Some (RefactorConstraint :: PreserveSignature { target : invariant . target . clone () , signature : "effect signature fixed" . to_string () , strength : invariant . strength , }) , } , InvariantKind :: Delta (d) => match d { DeltaInvariant :: Monotonic { .. } => Some (RefactorConstraint :: PreserveOrdering { target : invariant . target . clone () , must_come_before : Vec :: new () , strength : invariant . strength , }) , _ => None , } , InvariantKind :: PathIntersection (p) => Some (RefactorConstraint :: MustPreserve { target : invariant . target . clone () , facts : p . facts . iter () . cloned () . collect () , strength : invariant . strength , }) , } } . sig` | 0.72 | intra 0, inter 0 | same 11, other 16 | ok | - |
| `generate_constraints` | `# [doc = " Generate all constraints from an invariant analysis result"] pub fn generate_constraints (analysis : & InvariantAnalysisResult) -> Vec < RefactorConstraint > { analysis . invariants . iter () . filter_map (from_invariant) . collect () } . sig` | 0.75 | intra 0, inter 0 | same 1, other 1 | ok | - |
| `test_constraint_is_blocking` | `# [test] fn test_constraint_is_blocking () { let proven = RefactorConstraint :: NoMove { target : "fn1" . to_string () , reason : "test" . to_string () , strength : InvariantStrength :: Proven , } ; assert ! (proven . is_blocking ()) ; let heuristic = RefactorConstraint :: NoMove { target : "fn2" . to_string () , reason : "test" . to_string () , strength : InvariantStrength :: Heuristic , } ; assert ! (! heuristic . is_blocking ()) ; } . sig` | 0.75 | intra 0, inter 0 | same 2, other 2 | orphaned | - (suggest module utilities) |
| `check_move_allowed` | `# [doc = " Check if a move operation is allowed by constraints"] pub fn check_move_allowed (target : & str , current_file : & str , suggested_file : & str , constraints : & [RefactorConstraint] ,) -> Result < () , String > { for constraint in constraints { if constraint . target () == target && constraint . is_blocking () { match constraint { RefactorConstraint :: NoMove { reason , .. } => { return Err (format ! ("Cannot move {} from {} to {}: {}" , target , current_file , suggested_file , reason)) ; } RefactorConstraint :: FixedLayer { layer , .. } => { return Err (format ! ("Cannot move {} from {} to {}: layer {} is fixed" , target , current_file , suggested_file , layer)) ; } _ => { } } } } Ok (()) } . sig` | 0.90 | intra 0, inter 0 | same 3, other 0 | ok | - |

## File: src/010_cluster_008.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `detect_layer_violations` | `pub fn detect_layer_violations (graph : & DiGraph < PathBuf , () > , file_layers : & HashMap < PathBuf , String > ,) -> Vec < FileLayerViolation > { let mut violations = Vec :: new () ; for edge in graph . edge_references () { let from = & graph [edge . source ()] ; let to = & graph [edge . target ()] ; let from_layer = file_layers . get (from) . cloned () . unwrap_or_else (\| \| "root" . to_string ()) ; let to_layer = file_layers . get (to) . cloned () . unwrap_or_else (\| \| "root" . to_string ()) ; if let (Some (from_val) , Some (to_val)) = (layer_prefix_value (& from_layer) , layer_prefix_value (& to_layer)) { if from_val > to_val { violations . push (FileLayerViolation { from : from . clone () , to : to . clone () , from_layer , to_layer , }) ; } } } violations } . sig` | 0.35 | intra 2, inter 1 | same 0, other 2 | move | - (cohesion 0.35 below threshold 0.60 (impact 0.25)) |
| `cluster_target_path` | `pub fn cluster_target_path (target : PathBuf , members : & [crate :: report :: ClusterMember] , root_path : & Path , idx : usize ,) -> PathBuf { if ! is_core_module_path (& target) { return target ; } let prefix = target . file_stem () . and_then (\| name \| name . to_str ()) . and_then (\| stem \| layer_prefix_value (stem)) . unwrap_or (900) ; let file_name = format ! ("{:03}_cluster_{:03}.rs" , prefix , idx + 1) ; let dir = members . first () . and_then (\| member \| member . file . parent ()) . unwrap_or (root_path) ; dir . join (file_name) } . sig` | 0.43 | intra 2, inter 1 | same 0, other 1 | move | - (cohesion 0.43 below threshold 0.60 (impact 0.17)) |
| `build_result` | `pub fn build_result (files : & [PathBuf] , file_layers : HashMap < PathBuf , String > , nodes : BTreeSet < String > , edges_map : BTreeMap < (String , String) , BTreeSet < ReferenceDetail > > , unresolved : Vec < UnresolvedDependency > , entry_files : & BTreeSet < PathBuf > ,) -> Result < (Vec < PathBuf > , LayerGraph) > { let adjacency = adjacency_from_edges (& edges_map) ; let (mut ordered_layers , cycles) = topo_sort (& nodes , & adjacency) ; if let Some (pos) = ordered_layers . iter () . position (\| layer \| layer == "root") { let root_layer = ordered_layers . remove (pos) ; ordered_layers . insert (0 , root_layer) ; } let rank = layer_rank_map (& ordered_layers) ; let mut ordered_files = files . to_vec () ; ordered_files . sort_by (\| a , b \| { let mmsb_a = is_mmsb_main (a) ; let mmsb_b = is_mmsb_main (b) ; if mmsb_a && ! mmsb_b { return Ordering :: Less ; } else if mmsb_b && ! mmsb_a { return Ordering :: Greater ; } let entry_a = entry_files . contains (a) ; let entry_b = entry_files . contains (b) ; if entry_a && ! entry_b { return Ordering :: Less ; } else if entry_b && ! entry_a { return Ordering :: Greater ; } let layer_a = file_layers . get (a) . cloned () . unwrap_or_else (\| \| "root" . to_string ()) ; let layer_b = file_layers . get (b) . cloned () . unwrap_or_else (\| \| "root" . to_string ()) ; let rank_a = rank . get (& layer_a) . cloned () . unwrap_or (ordered_layers . len ()) ; let rank_b = rank . get (& layer_b) . cloned () . unwrap_or (ordered_layers . len ()) ; rank_a . cmp (& rank_b) . then_with (\| \| layer_a . cmp (& layer_b)) . then_with (\| \| a . cmp (b)) }) ; let edges = edges_map . into_iter () . map (\| ((from , to) , references) \| LayerEdge { violation : is_layer_violation (& from , & to) , from , to , references : references . into_iter () . collect () , }) . collect () ; let graph = LayerGraph { ordered_layers , edges , cycles , unresolved , } ; Ok ((ordered_files , graph)) } . sig` | 0.60 | intra 6, inter 0 | same 0, other 5 | ok | - |
| `adjacency_from_edges` | `fn adjacency_from_edges (edges_map : & BTreeMap < (String , String) , BTreeSet < ReferenceDetail > > ,) -> HashMap < String , BTreeSet < String > > { let mut adjacency : HashMap < String , BTreeSet < String > > = HashMap :: new () ; for ((from , to) , _) in edges_map { adjacency . entry (from . clone ()) . or_default () . insert (to . clone ()) ; } adjacency } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `parse_cluster_members` | `pub fn parse_cluster_members (cluster : & crate :: types :: FunctionCluster ,) -> Vec < crate :: report :: ClusterMember > { cluster . members . iter () . filter_map (\| member \| { let (file , name) = member . rsplit_once ("::") ? ; Some (crate :: report :: ClusterMember { file : PathBuf :: from (file) , name : name . to_string () , }) }) . collect () } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `collect_cluster_plans` | `pub fn collect_cluster_plans (clusters : & [crate :: types :: FunctionCluster] , root_path : & Path ,) -> Vec < crate :: report :: ClusterPlan > { let mut plans = Vec :: new () ; for (idx , cluster) in clusters . iter () . enumerate () { let all_members = parse_cluster_members (cluster) ; let target = if let Some (suggested) = & cluster . suggested_file { suggested . clone () } else if let Some (first) = all_members . first () { let file_name = format ! ("900_cluster_{:03}.rs" , idx + 1) ; first . file . parent () . unwrap_or (root_path) . join (file_name) } else { let file_name = format ! ("900_cluster_{:03}.rs" , idx + 1) ; root_path . join (file_name) } ; let target = cluster_target_path (target , & all_members , root_path , idx) ; let members = all_members . into_iter () . filter (\| member \| member . file != target) . collect :: < Vec < _ > > () ; if members . len () < 2 { continue ; } plans . push (crate :: report :: ClusterPlan { target , cohesion : cluster . cohesion , members , }) ; } plans . sort_by (\| a , b \| { use std :: cmp :: Ordering ; b . cohesion . partial_cmp (& a . cohesion) . unwrap_or (Ordering :: Equal) . then_with (\| \| b . members . len () . cmp (& a . members . len ())) . then_with (\| \| a . target . cmp (& b . target)) }) ; plans } . sig` | 0.60 | intra 2, inter 0 | same 0, other 3 | ok | - |
| `node_style` | `pub fn node_style (node_type : & NodeType) -> (& str , & str , & str) { match node_type { NodeType :: Entry => ("ellipse" , "lightgreen" , "\"filled,bold\"") , NodeType :: Exit => ("doubleoctagon" , "lightcoral" , "\"filled,bold\"") , NodeType :: BasicBlock => ("box" , "lightblue" , "filled") , NodeType :: Branch => ("diamond" , "yellow" , "filled") , NodeType :: LoopHeader => ("box" , "orange" , "\"filled,rounded\"") , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 6 | orphaned | - (suggest module utilities) |
| `cyclomatic_complexity` | `pub fn cyclomatic_complexity (cfg : & crate :: types :: FunctionCfg) -> usize { let edges = cfg . edges . len () as isize ; let nodes = cfg . nodes . len () as isize ; let exits = 1isize ; let cc = edges - nodes + 2 * exits ; if cc <= 0 { 1 } else { cc as usize } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `is_layer_violation` | `# [doc = " Checks if a dependency from one layer to another violates layer ordering"] # [doc = " Returns true if from_layer > to_layer (violation: higher depends on lower)"] pub fn is_layer_violation (from : & str , to : & str) -> bool { match (layer_prefix_value (from) , layer_prefix_value (to)) { (Some (a) , Some (b)) => a > b , _ => false , } } . sig` | 0.65 | intra 2, inter 1 | same 0, other 0 | ok | - |
| `compare_dir_layers` | `pub fn compare_dir_layers (a : & Path , b : & Path) -> Ordering { let a_name = a . file_name () . and_then (\| n \| n . to_str ()) . unwrap_or ("") ; let b_name = b . file_name () . and_then (\| n \| n . to_str ()) . unwrap_or ("") ; let a_layer = layer_prefix_value (a_name) . unwrap_or (i32 :: MAX) ; let b_layer = layer_prefix_value (b_name) . unwrap_or (i32 :: MAX) ; a_layer . cmp (& b_layer) . then_with (\| \| a_name . cmp (b_name)) } . sig` | 0.65 | intra 2, inter 1 | same 0, other 0 | ok | - |
| `compare_path_components` | `pub fn compare_path_components (a : & Path , b : & Path) -> Ordering { let a_components : Vec < _ > = a . components () . collect () ; let b_components : Vec < _ > = b . components () . collect () ; let min_len = a_components . len () . min (b_components . len ()) ; for idx in 0 .. min_len { let a_name = a_components [idx] . as_os_str () . to_string_lossy () ; let b_name = b_components [idx] . as_os_str () . to_string_lossy () ; let a_prefix = layer_prefix_value (& a_name) ; let b_prefix = layer_prefix_value (& b_name) ; let cmp = match (a_prefix , b_prefix) { (Some (a_val) , Some (b_val)) => a_val . cmp (& b_val) , _ => a_name . cmp (& b_name) , } ; if cmp != Ordering :: Equal { return cmp ; } } a_components . len () . cmp (& b_components . len ()) } . sig` | 0.65 | intra 2, inter 1 | same 0, other 0 | ok | - |
| `layer_adheres` | `pub fn layer_adheres (current_layer : & str , target_layer : & str) -> bool { match (layer_prefix_value (current_layer) , layer_prefix_value (target_layer)) { (Some (curr) , Some (target)) => curr <= target , _ => true , } } . sig` | 0.65 | intra 2, inter 1 | same 0, other 0 | ok | - |
| `structural_layer_value` | `pub (crate) fn structural_layer_value (layer : & Option < String > , default : i32) -> i32 { layer . as_ref () . and_then (\| value \| layer_prefix_value (value)) . unwrap_or (default) } . sig` | 0.65 | intra 1, inter 1 | same 0, other 0 | ok | - |
| `detect_layer_violation` | `pub fn detect_layer_violation (func : & FunctionInfo , functions : & [FunctionInfo] , outgoing : & HashMap < usize , usize > , file_layers : & HashMap < String , String > ,) -> Option < (String , String) > { let current_layer = file_layers . get (& func . file_path) . cloned () . unwrap_or_else (\| \| func . layer . clone ()) ; let current_value = layer_prefix_value (& current_layer) ? ; let mut violation : Option < (i32 , String) > = None ; for (callee_idx , _) in outgoing { let callee = & functions [* callee_idx] ; let target_layer = file_layers . get (& callee . file_path) . cloned () . unwrap_or_else (\| \| callee . layer . clone ()) ; if let Some (target_value) = layer_prefix_value (& target_layer) { if target_value < current_value { match violation { Some ((best_value , _)) if target_value >= best_value => { } _ => { violation = Some ((target_value , target_layer)) ; } } } } } violation . map (\| (_ , target_layer) \| (current_layer , target_layer)) } . sig` | 0.65 | intra 2, inter 1 | same 2, other 0 | ok | - |
| `topo_sort` | `fn topo_sort (nodes : & BTreeSet < String > , adjacency : & HashMap < String , BTreeSet < String > > ,) -> (Vec < String > , Vec < String >) { let mut indegree : HashMap < String , usize > = HashMap :: new () ; for node in nodes { indegree . entry (node . clone ()) . or_insert (0) ; } for targets in adjacency . values () { for target in targets { * indegree . entry (target . clone ()) . or_insert (0) += 1 ; } } let mut queue : VecDeque < String > = indegree . iter () . filter_map (\| (node , & deg) \| if deg == 0 { Some (node . clone ()) } else { None }) . collect () ; queue . make_contiguous () . sort () ; let mut order = Vec :: new () ; while let Some (node) = queue . pop_front () { order . push (node . clone ()) ; if let Some (targets) = adjacency . get (& node) { for target in targets { if let Some (entry) = indegree . get_mut (target) { * entry -= 1 ; if * entry == 0 { insert_sorted (& mut queue , target . clone ()) ; } } } } } if order . len () != nodes . len () { let mut remaining : Vec < _ > = nodes . iter () . filter (\| layer \| ! order . contains (layer)) . cloned () . collect () ; remaining . sort () ; let cycles = remaining . clone () ; order . extend (remaining) ; return (order , cycles) ; } (order , Vec :: new ()) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `layer_rank_map` | `fn layer_rank_map (order : & [String]) -> HashMap < String , usize > { let mut rank = HashMap :: new () ; for (idx , layer) in order . iter () . enumerate () { rank . insert (layer . clone () , idx) ; } rank } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `insert_sorted` | `fn insert_sorted (queue : & mut VecDeque < String > , value : String) { let mut inserted = false ; for idx in 0 .. queue . len () { if value < queue [idx] { queue . insert (idx , value . clone ()) ; inserted = true ; break ; } } if ! inserted { queue . push_back (value) ; } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `is_mmsb_main` | `fn is_mmsb_main (path : & Path) -> bool { path . file_name () . and_then (\| n \| n . to_str ()) . map (\| n \| n == "MMSB.jl") . unwrap_or (false) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `layer_prefix_value` | `# [doc = " Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)"] fn layer_prefix_value (layer : & str) -> Option < i32 > { let mut chars = layer . chars () ; let mut digits = String :: new () ; while let Some (ch) = chars . next () { if ch . is_ascii_digit () { digits . push (ch) ; } else { break ; } } if digits . is_empty () { None } else { digits . parse :: < i32 > () . ok () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `is_core_module_path` | `pub fn is_core_module_path (path : & Path) -> bool { let Some (stem) = path . file_stem () . and_then (\| name \| name . to_str ()) else { return false ; } ; stem . starts_with ("040_dependency") \|\| stem . starts_with ("060_layer_core") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/010_scc_compressor.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `test_scc_compression_dag` | `# [test] fn test_scc_compression_dag () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; let compression = SccCompression :: new (graph) ; assert ! (compression . is_dag ()) ; assert_eq ! (compression . count_trivial_sccs () , 3) ; assert_eq ! (compression . count_nontrivial_sccs () , 0) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `test_scc_compression_cycle` | `# [test] fn test_scc_compression_cycle () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; graph . add_edge (c , a , ()) ; let compression = SccCompression :: new (graph) ; assert ! (compression . is_dag ()) ; assert_eq ! (compression . count_nontrivial_sccs () , 1) ; assert_eq ! (compression . compressed_graph . node_count () , 1) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `test_scc_compression_mixed` | `# [test] fn test_scc_compression_mixed () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; let d = graph . add_node ("D" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; graph . add_edge (c , b , ()) ; graph . add_edge (a , d , ()) ; let compression = SccCompression :: new (graph) ; assert ! (compression . is_dag ()) ; assert_eq ! (compression . count_nontrivial_sccs () , 1) ; assert_eq ! (compression . count_trivial_sccs () , 2) ; assert_eq ! (compression . compressed_graph . node_count () , 3) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |

## File: src/020_cluster_010.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `order_julia_files_by_dependency` | `pub fn order_julia_files_by_dependency (files : & [PathBuf] , root : & Path ,) -> Result < (Vec < PathBuf > , crate :: dependency :: LayerGraph) > { use crate :: cluster_001 :: { collect_julia_dependencies , JuliaTarget } ; use crate :: dependency :: ReferenceDetail ; let mut file_layers : HashMap < PathBuf , String > = HashMap :: new () ; let mut nodes : BTreeSet < String > = BTreeSet :: new () ; let mut edges_map : BTreeMap < (String , String) , BTreeSet < ReferenceDetail > > = BTreeMap :: new () ; let mut unresolved = Vec :: new () ; let resolver = LayerResolver :: build (root) ? ; let entry_files = crate :: cluster_001 :: julia_entry_paths (root) ; for file in files { let layer = crate :: cluster_001 :: detect_layer (file) ; nodes . insert (layer . clone ()) ; file_layers . insert (file . clone () , layer . clone ()) ; let references = collect_julia_dependencies (file) . with_context (\| \| format ! ("Failed to analyze Julia dependencies for {:?}" , file)) ? ; for dep in references { match dep . target { JuliaTarget :: Include (include_path) => { let resolved = if include_path . is_absolute () { include_path . clone () } else { file . parent () . map (\| p \| p . join (& include_path)) . unwrap_or (include_path . clone ()) } ; if resolved . exists () { let target_layer = crate :: cluster_001 :: detect_layer (& resolved) ; nodes . insert (target_layer . clone ()) ; if target_layer != layer { edges_map . entry ((target_layer . clone () , layer . clone ())) . or_default () . insert (ReferenceDetail { file : file . clone () , reference : dep . detail . clone () , }) ; } } else { unresolved . push (crate :: dependency :: UnresolvedDependency { file : file . clone () , reference : dep . detail . clone () , }) ; } } JuliaTarget :: Module (module) => { if let Some (target_layer) = resolver . resolve_module (& module) { nodes . insert (target_layer . clone ()) ; if target_layer != layer { edges_map . entry ((target_layer . clone () , layer . clone ())) . or_default () . insert (ReferenceDetail { file : file . clone () , reference : dep . detail . clone () , }) ; } } else { unresolved . push (crate :: dependency :: UnresolvedDependency { file : file . clone () , reference : dep . detail . clone () , }) ; } } } } } crate :: cluster_008 :: build_result (files , file_layers , nodes , edges_map , unresolved , & entry_files ,) } . sig` | 0.28 | intra 1, inter 1 | same 1, other 10 | layer violation | - (020_cluster_010.rs -> 000_cluster_001.rs) |
| `extract_rust_dependencies` | `pub fn extract_rust_dependencies (file : & Path , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Result < Vec < PathBuf > > { # [derive (Default)] struct UseCollector { roots : BTreeSet < String > , mods : BTreeSet < String > , } impl < 'ast > Visit < 'ast > for UseCollector { fn visit_item_use (& mut self , node : & 'ast ItemUse) { crate :: dependency :: collect_roots (& node . tree , RootState :: Start , & mut self . roots) ; } fn visit_item_mod (& mut self , node : & 'ast syn :: ItemMod) { if node . content . is_none () { self . mods . insert (node . ident . to_string ()) ; } } } let content = fs :: read_to_string (file) . with_context (\| \| format ! ("Unable to read {:?}" , file)) ? ; let syntax = syn :: parse_file (& content) . with_context (\| \| format ! ("Unable to parse Rust file {:?}" , file)) ? ; let mut collector = UseCollector :: default () ; collector . visit_file (& syntax) ; let mut deps = Vec :: new () ; for root in collector . roots { if let Some (path) = resolve_module (& root , file_set , module_map) { deps . push (path) ; } } for module in collector . mods { if let Some (path) = resolve_module (& module , file_set , module_map) { deps . push (path) ; } } Ok (deps) } . sig` | 0.82 | intra 2, inter 0 | same 3, other 1 | ok | - |
| `normalize_module_name` | `pub fn normalize_module_name (name : & str) -> String { if let Some (pos) = name . find ('_') { if name [.. pos] . chars () . all (\| c \| c . is_ascii_digit ()) { return name [pos + 1 ..] . to_string () ; } } name . to_string () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `resolve_module` | `pub fn resolve_module (root : & str , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Option < PathBuf > { let key = normalize_module_name (root) ; if let Some (path) = module_map . get (& key) { return Some (path . clone ()) ; } module_map . iter () . find (\| (name , _) \| name == & & key) . map (\| (_ , path) \| path . clone ()) . or_else (\| \| { module_map . iter () . find (\| (name , _) \| key . starts_with (name . as_str ())) . map (\| (_ , path) \| path . clone ()) }) . or_else (\| \| crate :: cluster_011 :: resolve_path (& PathBuf :: from (root) , file_set , module_map)) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `contains_tools` | `pub fn contains_tools (path : & Path) -> bool { path . components () . any (\| c \| c . as_os_str () == "tools") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `build_module_root_map` | `pub fn build_module_root_map (root : & Path) -> Result < HashMap < String , ModuleRoot > , std :: io :: Error > { let src_dir = root . join ("src") ; let mut map = HashMap :: new () ; if src_dir . is_dir () { for entry in fs :: read_dir (& src_dir) ? { let entry = entry ? ; let path = entry . path () ; if contains_tools (& path) { continue ; } let name = entry . file_name () . to_string_lossy () . to_string () . trim_end_matches (".rs") . to_string () ; if path . is_dir () { let normalized = normalize_module_name (& name) ; map . insert (normalized , ModuleRoot { layer : name . clone () , } ,) ; } else if path . extension () . map (\| ext \| ext == "rs") . unwrap_or (false) { map . insert (name . clone () , ModuleRoot { layer : crate :: cluster_001 :: detect_layer (& path) , } ,) ; } } } Ok (map) } . sig` | 0.90 | intra 2, inter 0 | same 3, other 0 | ok | - |
| `resolve_source_root` | `fn resolve_source_root (root : & Path) -> PathBuf { let src_candidate = root . join ("src") ; if src_candidate . exists () && src_candidate . is_dir () { src_candidate } else { root . to_path_buf () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `extract_julia_dependencies` | `pub fn extract_julia_dependencies (file : & Path , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Result < Vec < PathBuf > > { static INCLUDE_RE : Lazy < Regex > = Lazy :: new (\| \| Regex :: new (r#"include\s*\(\s*["']([^"']+)["']"#) . unwrap ()) ; static MMSB_USING_RE : Lazy < Regex > = Lazy :: new (\| \| { Regex :: new (r#"(?m)^\s*(?:using\|import)\s+MMSB\.([A-Za-z0-9_\.]+)"#) . unwrap () }) ; static MMSB_SYMBOL_RE : Lazy < Regex > = Lazy :: new (\| \| { Regex :: new (r#"(?m)^\s*(?:using\|import)\s+MMSB\s*:\s*([A-Za-z0-9_,\s]+)"#) . unwrap () }) ; static LOCAL_USING_RE : Lazy < Regex > = Lazy :: new (\| \| { Regex :: new (r#"(?m)^\s*(?:using\|import)\s+\.\s*([A-Za-z0-9_\.]+)"#) . unwrap () }) ; static PLAIN_USING_RE : Lazy < Regex > = Lazy :: new (\| \| { Regex :: new (r#"(?m)^\s*(?:using\|import)\s+([A-Za-z_][A-Za-z0-9_\.]*)"#) . unwrap () }) ; fn resolve_module_name (module : & str , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Option < PathBuf > { let primary = module . split ('.') . next () . unwrap_or (module) ; resolve_module (primary , file_set , module_map) } let content = fs :: read_to_string (file) . with_context (\| \| format ! ("Unable to read {:?}" , file)) ? ; let mut deps = Vec :: new () ; for cap in INCLUDE_RE . captures_iter (& content) { if let Some (path_match) = cap . get (1) { let raw = path_match . as_str () ; let mut candidate = PathBuf :: from (raw) ; if candidate . extension () . is_none () { candidate . set_extension ("jl") ; } let resolved = if candidate . is_absolute () { candidate } else { file . parent () . map (\| p \| p . join (& candidate)) . unwrap_or (candidate) } ; if let Some (path) = crate :: cluster_011 :: resolve_path (& resolved , file_set , module_map) { deps . push (path) ; } } } for cap in MMSB_USING_RE . captures_iter (& content) { if let Some (module_match) = cap . get (1) { if let Some (path) = resolve_module_name (module_match . as_str () , file_set , module_map) { deps . push (path) ; } } } for cap in MMSB_SYMBOL_RE . captures_iter (& content) { if let Some (symbols) = cap . get (1) { for symbol in symbols . as_str () . split (',') . map (\| s \| s . trim ()) . filter (\| s \| ! s . is_empty ()) { if let Some (path) = resolve_module_name (symbol , file_set , module_map) { deps . push (path) ; } } } } for cap in LOCAL_USING_RE . captures_iter (& content) { if let Some (module_match) = cap . get (1) { if let Some (path) = resolve_module_name (module_match . as_str () , file_set , module_map) { deps . push (path) ; } } } for cap in PLAIN_USING_RE . captures_iter (& content) { if let Some (module_match) = cap . get (1) { let module = module_match . as_str () ; if module . starts_with ("MMSB") { continue ; } if let Some (path) = resolve_module_name (module , file_set , module_map) { deps . push (path) ; } } } Ok (deps) } . sig` | 0.90 | intra 5, inter 0 | same 0, other 0 | ok | - |
| `resolve_module_name` | `fn resolve_module_name (module : & str , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Option < PathBuf > { let primary = module . split ('.') . next () . unwrap_or (module) ; resolve_module (primary , file_set , module_map) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `build_dependency_map` | `pub fn build_dependency_map (files : & [PathBuf] , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Result < HashMap < PathBuf , Vec < PathBuf > > > { let mut dep_map : HashMap < PathBuf , Vec < PathBuf > > = HashMap :: new () ; for file in files { let deps = extract_dependencies (file , file_set , module_map) . with_context (\| \| format ! ("Failed to extract dependencies for {:?}" , file)) ? ; dep_map . insert (file . clone () , deps) ; } Ok (dep_map) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `extract_dependencies` | `pub (crate) fn extract_dependencies (file : & Path , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Result < Vec < PathBuf > > { let ext = file . extension () . and_then (\| s \| s . to_str ()) . unwrap_or ("") ; match ext { "rs" => extract_rust_dependencies (file , file_set , module_map) , "jl" => extract_julia_dependencies (file , file_set , module_map) , _ => Ok (Vec :: new ()) , } } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |

## File: src/020_layer_inference.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `infer_layers` | `# [doc = " Infer layers from call graph structure"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `graph` - Call graph (edge from caller to callee)"] # [doc = " * `max_iterations` - Maximum fixpoint iterations (default: 100)"] # [doc = ""] # [doc = " # Returns"] # [doc = " HashMap mapping node name to layer information"] pub fn infer_layers (graph : & DiGraph < String , () > , max_iterations : usize) -> HashMap < String , LayerInfo > { let mut layers : HashMap < NodeIndex , usize > = HashMap :: new () ; let mut result : HashMap < String , LayerInfo > = HashMap :: new () ; for node_idx in graph . node_indices () { let out_degree = graph . neighbors_directed (node_idx , Direction :: Outgoing) . count () ; if out_degree == 0 { layers . insert (node_idx , 0) ; } } let mut changed = true ; let mut iteration = 0 ; while changed && iteration < max_iterations { changed = false ; iteration += 1 ; for node_idx in graph . node_indices () { if layers . contains_key (& node_idx) { continue ; } let callees : Vec < NodeIndex > = graph . neighbors_directed (node_idx , Direction :: Outgoing) . collect () ; let all_assigned = callees . iter () . all (\| callee \| layers . contains_key (callee)) ; if all_assigned && ! callees . is_empty () { let max_callee_layer = callees . iter () . filter_map (\| callee \| layers . get (callee)) . max () . copied () . unwrap_or (0) ; layers . insert (node_idx , max_callee_layer + 1) ; changed = true ; } } } let max_layer = layers . values () . max () . copied () . unwrap_or (0) ; for node_idx in graph . node_indices () { if ! layers . contains_key (& node_idx) { layers . insert (node_idx , max_layer + 1) ; } } for (node_idx , & layer) in & layers { let name = graph [* node_idx] . clone () ; let dependencies : Vec < String > = graph . neighbors_directed (* node_idx , Direction :: Outgoing) . map (\| callee_idx \| graph [callee_idx] . clone ()) . collect () ; let max_dependency_layer = if dependencies . is_empty () { None } else { dependencies . iter () . filter_map (\| dep_name \| { graph . node_indices () . find (\| idx \| & graph [* idx] == dep_name) . and_then (\| idx \| layers . get (& idx)) . copied () }) . max () } ; result . insert (name . clone () , LayerInfo { name , layer , dependencies , max_dependency_layer , } ,) ; } result } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `detect_layer_violations` | `# [doc = " Detect layer violations in the call graph"] # [doc = ""] # [doc = " A violation occurs when a lower-layer function calls a higher-layer function"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `layer_assignments` - Layer information for each node"] # [doc = " * `graph` - The call graph"] # [doc = ""] # [doc = " # Returns"] # [doc = " Vec of (caller, callee, caller_layer, callee_layer) tuples"] pub fn detect_layer_violations (layer_assignments : & HashMap < String , LayerInfo > , graph : & DiGraph < String , () > ,) -> Vec < (String , String , usize , usize) > { let mut violations = Vec :: new () ; for edge in graph . edge_references () { let caller = & graph [edge . source ()] ; let callee = & graph [edge . target ()] ; if let (Some (caller_info) , Some (callee_info)) = (layer_assignments . get (caller) , layer_assignments . get (callee)) { if caller_info . layer <= callee_info . layer { violations . push ((caller . clone () , callee . clone () , caller_info . layer , callee_info . layer ,)) ; } } } violations } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `test_detect_layer_violations_none` | `# [test] fn test_detect_layer_violations_none () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; let layers = infer_layers (& graph , 100) ; let violations = detect_layer_violations (& layers , & graph) ; assert_eq ! (violations . len () , 0) ; } . sig` | 0.67 | intra 2, inter 1 | same 0, other 0 | layer violation | - (020_layer_inference.rs -> 010_cluster_008.rs) |
| `test_layer_inference_simple_dag` | `# [test] fn test_layer_inference_simple_dag () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; let layers = infer_layers (& graph , 100) ; assert_eq ! (layers . get ("C") . unwrap () . layer , 0) ; assert_eq ! (layers . get ("B") . unwrap () . layer , 1) ; assert_eq ! (layers . get ("A") . unwrap () . layer , 2) ; } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `test_layer_inference_diamond` | `# [test] fn test_layer_inference_diamond () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; let d = graph . add_node ("D" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (a , c , ()) ; graph . add_edge (b , d , ()) ; graph . add_edge (c , d , ()) ; let layers = infer_layers (& graph , 100) ; assert_eq ! (layers . get ("D") . unwrap () . layer , 0) ; assert_eq ! (layers . get ("B") . unwrap () . layer , 1) ; assert_eq ! (layers . get ("C") . unwrap () . layer , 1) ; assert_eq ! (layers . get ("A") . unwrap () . layer , 2) ; } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |

## File: src/030_cluster_011.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `export_program_cfg_to_path` | `pub fn export_program_cfg_to_path (result : & crate :: types :: AnalysisResult , call_edges : & [(String , String)] , output_path : & Path ,) -> std :: io :: Result < () > { use crate :: types :: ProgramCFG ; let mut program_cfg = ProgramCFG { functions : HashMap :: new () , call_edges : Vec :: new () , } ; for cfg in & result . cfgs { program_cfg . functions . insert (cfg . function . clone () , cfg . clone ()) ; } for (caller , callee) in call_edges { let caller_name = caller . split ("::") . last () . unwrap_or (caller) . to_string () ; let callee_name = callee . split ("::") . last () . unwrap_or (callee) . to_string () ; if program_cfg . functions . contains_key (& caller_name) && program_cfg . functions . contains_key (& callee_name) { program_cfg . call_edges . push ((caller_name , callee_name)) ; } } let cfg_dir = output_path . join ("30_cfg") ; std :: fs :: create_dir_all (& cfg_dir) ? ; let dot_path = cfg_dir . join ("complete_program.dot") ; crate :: cluster_001 :: export_complete_program_dot (& program_cfg , dot_path . to_string_lossy () . as_ref () ,) ? ; # [cfg (feature = "png")] { let png_path = cfg_dir . join ("complete_program.png") ; if let (Some (dot_path_str) , Some (png_path_str)) = (dot_path . to_str () , png_path . to_str ()) { let _ = std :: process :: Command :: new ("dot") . args (["-Tpng" , dot_path_str , "-o" , png_path_str]) . status () ; } } Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | ok | - |
| `build_module_map` | `pub fn build_module_map (files : & [PathBuf]) -> HashMap < String , PathBuf > { let mut map = HashMap :: new () ; for file in files { if let Some (stem) = file . file_stem () . and_then (\| s \| s . to_str ()) { let normalized = crate :: cluster_010 :: normalize_module_name (stem) ; map . insert (normalized . clone () , file . clone ()) ; if stem == "mod" { if let Some (parent) = file . parent () . and_then (\| p \| p . file_name ()) { if let Some (name) = parent . to_str () { map . insert (crate :: cluster_010 :: normalize_module_name (name) , file . clone ()) ; } } } } } map } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `resolve_path` | `pub fn resolve_path (candidate : & Path , file_set : & HashSet < PathBuf > , module_map : & HashMap < String , PathBuf > ,) -> Option < PathBuf > { if file_set . contains (candidate) { return Some (candidate . to_path_buf ()) ; } if let Some (file_name) = candidate . file_stem () . and_then (\| s \| s . to_str ()) { let key = crate :: cluster_010 :: normalize_module_name (file_name) ; if let Some (path) = module_map . get (& key) { return Some (path . clone ()) ; } } None } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `build_directory_dag` | `pub fn build_directory_dag (dir : & PathBuf) -> Result < DiGraph < PathBuf , () > > { let files : Vec < PathBuf > = walkdir :: WalkDir :: new (dir) . into_iter () . filter_map (\| e \| e . ok ()) . filter (\| e \| { e . path () . extension () . and_then (\| ext \| ext . to_str ()) . map (\| ext \| ext == "rs" \|\| ext == "jl") . unwrap_or (false) }) . map (\| entry \| entry . into_path ()) . collect () ; let file_set : HashSet < PathBuf > = files . iter () . cloned () . collect () ; let module_map = build_module_map (& files) ; let dep_map = crate :: cluster_010 :: build_dependency_map (& files , & file_set , & module_map) ? ; let (graph , _) = build_file_dag (& files , & dep_map) ; Ok (graph) } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |
| `build_file_dependency_graph` | `pub fn build_file_dependency_graph (files : & [PathBuf]) -> Result < DiGraph < PathBuf , () > > { let file_set : HashSet < PathBuf > = files . iter () . cloned () . collect () ; let module_map = build_module_map (files) ; let dep_map = crate :: cluster_010 :: build_dependency_map (files , & file_set , & module_map) ? ; let (graph , _) = build_file_dag (files , & dep_map) ; Ok (graph) } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |
| `build_file_dag` | `pub (crate) fn build_file_dag (files : & [PathBuf] , dep_map : & HashMap < PathBuf , Vec < PathBuf > > ,) -> (DiGraph < PathBuf , () > , HashMap < PathBuf , NodeIndex >) { let mut graph = DiGraph :: new () ; let mut node_map = HashMap :: new () ; for file in files { let node = graph . add_node (file . clone ()) ; node_map . insert (file . clone () , node) ; } for (file , deps) in dep_map { if let Some (& file_node) = node_map . get (file) { for dep in deps { if let Some (& dep_node) = node_map . get (dep) { graph . add_edge (dep_node , file_node , ()) ; } } } } (graph , node_map) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/030_fixpoint_solver.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `propagate_to_fixpoint` | `# [doc = " Propagate symbolic abstractions until fixpoint"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `graph` - Call graph"] # [doc = " * `initial` - Initial abstractions for each node"] # [doc = " * `max_iterations` - Maximum iterations before giving up"] # [doc = ""] # [doc = " # Returns"] # [doc = " FixpointResult containing final abstractions and metadata"] pub fn propagate_to_fixpoint (graph : & DiGraph < String , () > , initial : HashMap < String , SymbolicAbstraction > , max_iterations : usize ,) -> FixpointResult { let mut name_to_idx : HashMap < String , NodeIndex > = HashMap :: new () ; for idx in graph . node_indices () { name_to_idx . insert (graph [idx] . clone () , idx) ; } let mut current : HashMap < NodeIndex , SymbolicAbstraction > = HashMap :: new () ; for (name , abstraction) in & initial { if let Some (& idx) = name_to_idx . get (name) { current . insert (idx , abstraction . clone ()) ; } } let mut iteration = 0 ; let mut converged = false ; while iteration < max_iterations { iteration += 1 ; let mut next = current . clone () ; let mut changed = false ; for node_idx in graph . node_indices () { let mut new_abstraction = current . get (& node_idx) . cloned () . unwrap_or_else (SymbolicAbstraction :: new) ; for callee_idx in graph . neighbors_directed (node_idx , Direction :: Outgoing) { if let Some (callee_abs) = current . get (& callee_idx) { new_abstraction . merge (callee_abs) ; } } if let Some (old) = current . get (& node_idx) { if ! old . approx_eq (& new_abstraction) { changed = true ; } } else { changed = true ; } next . insert (node_idx , new_abstraction) ; } current = next ; if ! changed { converged = true ; break ; } } let mut abstractions = HashMap :: new () ; let mut stable_nodes = Vec :: new () ; for (idx , abstraction) in & current { let name = graph [* idx] . clone () ; if let Some (initial_abs) = initial . get (& name) { if initial_abs . approx_eq (abstraction) { stable_nodes . push (name . clone ()) ; } } abstractions . insert (name , abstraction . clone ()) ; } FixpointResult { abstractions , iterations : iteration , converged , stable_nodes , } } . sig` | 0.90 | intra 0, inter 0 | same 6, other 0 | ok | - |
| `test_symbolic_abstraction_merge` | `# [test] fn test_symbolic_abstraction_merge () { let mut abs1 = SymbolicAbstraction :: new () ; abs1 . type_sig = Some ("String" . to_string ()) ; abs1 . effects . insert ("IO" . to_string ()) ; abs1 . layer = Some (1) ; let mut abs2 = SymbolicAbstraction :: new () ; abs2 . effects . insert ("Mutation" . to_string ()) ; abs2 . layer = Some (2) ; abs1 . merge (& abs2) ; assert_eq ! (abs1 . type_sig , Some ("String" . to_string ())) ; assert ! (abs1 . effects . contains ("IO")) ; assert ! (abs1 . effects . contains ("Mutation")) ; assert_eq ! (abs1 . layer , Some (2)) ; } . sig` | 0.90 | intra 0, inter 0 | same 2, other 0 | orphaned | - (suggest module utilities) |
| `test_fixpoint_simple` | `# [test] fn test_fixpoint_simple () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; let mut initial = HashMap :: new () ; let mut c_abs = SymbolicAbstraction :: new () ; c_abs . effects . insert ("Pure" . to_string ()) ; initial . insert ("C" . to_string () , c_abs) ; initial . insert ("A" . to_string () , SymbolicAbstraction :: new ()) ; initial . insert ("B" . to_string () , SymbolicAbstraction :: new ()) ; let result = propagate_to_fixpoint (& graph , initial , 100) ; assert ! (result . converged) ; assert ! (result . abstractions ["A"] . effects . contains ("Pure")) ; assert ! (result . abstractions ["B"] . effects . contains ("Pure")) ; } . sig` | 0.90 | intra 1, inter 0 | same 3, other 0 | ok | - |
| `test_fixpoint_convergence` | `# [test] fn test_fixpoint_convergence () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; graph . add_edge (a , b , ()) ; let mut initial = HashMap :: new () ; initial . insert ("A" . to_string () , SymbolicAbstraction :: new ()) ; initial . insert ("B" . to_string () , SymbolicAbstraction :: new ()) ; let result = propagate_to_fixpoint (& graph , initial , 100) ; assert ! (result . converged) ; assert ! (result . iterations < 10) ; } . sig` | 0.90 | intra 1, inter 0 | same 2, other 0 | ok | - |

## File: src/040_dependency.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `collect_roots` | `pub fn collect_roots (tree : & UseTree , state : RootState , acc : & mut BTreeSet < String >) { match tree { UseTree :: Path (path) => { let ident = path . ident . to_string () ; if state == RootState :: Start && matches ! (ident . as_str () , "crate" \| "self" \| "super") { collect_roots (& path . tree , RootState :: AfterRoot , acc) ; } else if state == RootState :: AfterRoot { acc . insert (ident) ; } else { acc . insert (ident) ; } } UseTree :: Group (group) => { for tree in & group . items { collect_roots (tree , state , acc) ; } } UseTree :: Name (name) => { acc . insert (name . ident . to_string ()) ; } UseTree :: Rename (rename) => { acc . insert (rename . ident . to_string ()) ; } UseTree :: Glob (_) => { } } } . sig` | 0.90 | intra 2, inter 0 | same 4, other 0 | ok | - |

## File: src/040_structural_detector.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `test_detect_leaf_root` | `# [test] fn test_detect_leaf_root () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; let detector = StructuralDetector :: new (graph) ; let invariants = detector . detect_leaf_root () ; let roots : Vec < _ > = invariants . iter () . filter (\| inv \| matches ! (inv . kind , InvariantKind :: Structural (StructuralInvariant :: Root))) . collect () ; let leaves : Vec < _ > = invariants . iter () . filter (\| inv \| matches ! (inv . kind , InvariantKind :: Structural (StructuralInvariant :: Leaf))) . collect () ; assert_eq ! (roots . len () , 1) ; assert_eq ! (leaves . len () , 1) ; assert_eq ! (roots [0] . target , "A") ; assert_eq ! (leaves [0] . target , "C") ; } . sig` | 0.66 | intra 0, inter 0 | same 1, other 4 | orphaned | - (suggest module utilities) |
| `test_detect_degree_stable` | `# [test] fn test_detect_degree_stable () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; graph . add_edge (a , b , ()) ; let detector = StructuralDetector :: new (graph) ; let invariants = detector . detect_degree_stable () ; assert_eq ! (invariants . len () , 2) ; let a_inv = invariants . iter () . find (\| inv \| inv . target == "A") . unwrap () ; if let InvariantKind :: Structural (StructuralInvariant :: DegreeStable { in_degree , out_degree , }) = & a_inv . kind { assert_eq ! (* in_degree , 0) ; assert_eq ! (* out_degree , 1) ; } else { panic ! ("Wrong invariant kind") ; } } . sig` | 0.70 | intra 0, inter 0 | same 1, other 2 | orphaned | - (suggest module utilities) |
| `test_all_structural_invariants_proven` | `# [test] fn test_all_structural_invariants_proven () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; graph . add_edge (a , b , ()) ; let detector = StructuralDetector :: new (graph) ; let invariants = detector . detect_all () ; let proven_count = invariants . iter () . filter (\| inv \| matches ! (inv . strength , InvariantStrength :: Proven)) . count () ; assert ! (proven_count > 0) ; } . sig` | 0.75 | intra 0, inter 0 | same 1, other 1 | orphaned | - (suggest module utilities) |

## File: src/050_cluster_006.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `collect_directory_moves` | `pub fn collect_directory_moves (ordering : & crate :: types :: FileOrderingResult , root_path : & Path ,) -> Vec < crate :: file_ordering :: DirectoryMove > { let mut moves = Vec :: new () ; let mut by_parent : BTreeMap < PathBuf , Vec < PathBuf > > = BTreeMap :: new () ; let src_dir = root_path . join ("src") ; for dir in & ordering . ordered_directories { if dir == root_path { continue ; } if dir == & src_dir { continue ; } if let Some (parent) = dir . parent () { by_parent . entry (parent . to_path_buf ()) . or_default () . push (dir . clone ()) ; } } for (parent , mut dirs) in by_parent { dirs . sort_by (\| a , b \| crate :: cluster_008 :: compare_dir_layers (a , b)) ; for (idx , dir) in dirs . iter () . enumerate () { let Some (name) = dir . file_name () . and_then (\| n \| n . to_str ()) else { continue ; } ; let clean = strip_numeric_prefix (name) ; let suggested = format ! ("{:03}_{}" , idx * 10 , clean) ; if name == suggested { continue ; } let to = parent . join (& suggested) ; moves . push (crate :: file_ordering :: DirectoryMove { from : dir . clone () , to , }) ; } } moves } . sig` | 0.60 | intra 1, inter 0 | same 0, other 3 | ok | - |
| `compute_cohesion_score` | `pub fn compute_cohesion_score (func : & FunctionInfo , functions : & [FunctionInfo] , outgoing : & HashMap < usize , usize > , file_layers : & HashMap < String , String > , call_analysis : & crate :: types :: CallAnalysis ,) -> f64 { let mut total_calls = 0usize ; let mut intra_calls = 0usize ; let mut external_calls = 0usize ; let mut layer_ok = 0usize ; for (callee_idx , count) in outgoing { total_calls += count ; let callee = & functions [* callee_idx] ; if callee . file_path == func . file_path { intra_calls += count ; } else { external_calls += count ; } let current_layer = file_layers . get (& func . file_path) . cloned () . unwrap_or_else (\| \| func . layer . clone ()) ; let target_layer = file_layers . get (& callee . file_path) . cloned () . unwrap_or_else (\| \| callee . layer . clone ()) ; if crate :: cluster_008 :: layer_adheres (& current_layer , & target_layer) { layer_ok += count ; } } let total_calls_f = total_calls as f64 ; let call_locality = if total_calls == 0 { 1.0 } else { intra_calls as f64 / total_calls_f } ; let layer_adherence = if total_calls == 0 { 1.0 } else { layer_ok as f64 / total_calls_f } ; let cross_file_calls = if total_calls == 0 { 0.0 } else { external_calls as f64 / total_calls_f } ; let total_type_refs = call_analysis . same_file_type_refs + call_analysis . other_file_type_refs ; let type_coupling = if total_type_refs == 0 { 1.0 } else { call_analysis . same_file_type_refs as f64 / total_type_refs as f64 } ; let score = 0.4 * call_locality + 0.3 * type_coupling + 0.2 * layer_adherence - 0.1 * cross_file_calls ; score . clamp (0.0 , 1.0) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | orphaned | - (suggest module utilities) |
| `layer_prefix_value` | `# [doc = " Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)"] pub fn layer_prefix_value (layer : & str) -> Option < i32 > { let mut chars = layer . chars () ; let mut digits = String :: new () ; while let Some (ch) = chars . next () { if ch . is_ascii_digit () { digits . push (ch) ; } else { break ; } } if digits . is_empty () { None } else { digits . parse :: < i32 > () . ok () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `order_directories` | `pub fn order_directories (files : & [PathBuf] , dep_map : & HashMap < PathBuf , Vec < PathBuf > > ,) -> Vec < PathBuf > { let root = common_root (files) ; let mut dirs : HashSet < PathBuf > = HashSet :: new () ; for file in files { let mut current = file . parent () . map (Path :: to_path_buf) ; while let Some (dir) = current { if let Some (ref root_path) = root { if ! dir . starts_with (root_path) { break ; } } dirs . insert (dir . clone ()) ; current = dir . parent () . map (Path :: to_path_buf) ; } } let mut ordered : Vec < PathBuf > = dirs . into_iter () . collect () ; ordered . sort_by (\| a , b \| crate :: cluster_008 :: compare_path_components (a , b)) ; let mut node_map = HashMap :: new () ; for (idx , dir) in ordered . iter () . enumerate () { node_map . insert (dir . clone () , idx) ; } let mut adjacency : Vec < BTreeSet < usize > > = vec ! [BTreeSet :: new () ; ordered . len ()] ; let mut indegree = vec ! [0usize ; ordered . len ()] ; for (file , deps) in dep_map { let Some (from_dir) = file . parent () . map (Path :: to_path_buf) else { continue ; } ; let Some (& from_idx) = node_map . get (& from_dir) else { continue ; } ; for dep in deps { let Some (to_dir) = dep . parent () . map (Path :: to_path_buf) else { continue ; } ; if to_dir == from_dir { continue ; } let Some (& to_idx) = node_map . get (& to_dir) else { continue ; } ; if adjacency [to_idx] . insert (from_idx) { indegree [from_idx] += 1 ; } } } let mut queue : BTreeSet < usize > = indegree . iter () . enumerate () . filter_map (\| (idx , & deg) \| if deg == 0 { Some (idx) } else { None }) . collect () ; let mut result = Vec :: with_capacity (ordered . len ()) ; while let Some (& idx) = queue . iter () . next () { queue . remove (& idx) ; result . push (ordered [idx] . clone ()) ; let neighbors = adjacency [idx] . clone () ; for neighbor in neighbors { let entry = & mut indegree [neighbor] ; if * entry > 0 { * entry -= 1 ; if * entry == 0 { queue . insert (neighbor) ; } } } } if result . len () < ordered . len () { for (idx , dir) in ordered . iter () . enumerate () { if indegree [idx] > 0 { result . push (dir . clone ()) ; } } } result } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `common_root` | `pub fn common_root (files : & [PathBuf]) -> Option < PathBuf > { let mut iter = files . iter () ; let first = iter . next () ? . components () . collect :: < Vec < _ > > () ; let mut prefix_len = first . len () ; for path in iter { let comps = path . components () . collect :: < Vec < _ > > () ; let mut idx = 0 ; while idx < prefix_len && idx < comps . len () && comps [idx] == first [idx] { idx += 1 ; } prefix_len = idx ; } if prefix_len == 0 { None } else { let mut root = PathBuf :: new () ; for comp in first . into_iter () . take (prefix_len) { root . push (comp . as_os_str ()) ; } Some (root) } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `strip_numeric_prefix` | `pub (crate) fn strip_numeric_prefix (name : & str) -> & str { use once_cell :: sync :: Lazy ; use regex :: Regex ; static PREFIX_RE : Lazy < Regex > = Lazy :: new (\| \| Regex :: new (r"^\d+_(.*)$") . unwrap ()) ; PREFIX_RE . captures (name) . and_then (\| cap \| cap . get (1)) . map (\| m \| m . as_str ()) . unwrap_or (name) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `generate_canonical_name` | `pub fn generate_canonical_name (path : & Path , number : usize) -> String { let stem = path . file_stem () . and_then (\| s \| s . to_str ()) . unwrap_or ("unknown") ; let ext = path . extension () . and_then (\| s \| s . to_str ()) . unwrap_or ("") ; let clean_stem = strip_numeric_prefix (stem) ; if ext . is_empty () { format ! ("{:03}_{}" , number , clean_stem) } else { format ! ("{:03}_{}.{}" , number , clean_stem , ext) } } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |

## File: src/050_semantic_detector.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `make_function` | `fn make_function (name : & str , signature : & str , calls : Vec < String >) -> CodeElement { CodeElement { name : name . to_string () , file_path : "test.rs" . to_string () , line_number : 1 , element_type : ElementType :: Function , signature : signature . to_string () , visibility : Visibility :: Public , generic_params : Vec :: new () , language : Language :: Rust , layer : "0" . to_string () , calls , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 5 | orphaned | - (suggest module utilities) |
| `test_detect_idempotent_heuristic` | `# [test] fn test_detect_idempotent_heuristic () { let elements = vec ! [make_function ("set_value" , "fn set_value(x: i32)" , Vec :: new ()) , make_function ("reset_state" , "fn reset_state()" , Vec :: new ()) ,] ; let detector = SemanticDetector :: new (& elements) ; let invariants = detector . detect_idempotent () ; assert_eq ! (invariants . len () , 2) ; for inv in & invariants { assert ! (matches ! (inv . strength , InvariantStrength :: Heuristic)) ; assert ! (matches ! (inv . kind , InvariantKind :: Semantic (SemanticInvariant :: Idempotent))) ; } } . sig` | 0.68 | intra 0, inter 0 | same 1, other 3 | orphaned | - (suggest module utilities) |
| `test_detect_type_stable` | `# [test] fn test_detect_type_stable () { let elements = vec ! [make_function ("test_fn" , "fn test_fn(x: i32) -> i32" , Vec :: new () ,)] ; let detector = SemanticDetector :: new (& elements) ; let invariants = detector . detect_type_stable () ; assert_eq ! (invariants . len () , 1) ; assert_eq ! (invariants [0] . target , "test_fn") ; assert ! (matches ! (invariants [0] . strength , InvariantStrength :: Empirical { .. })) ; } . sig` | 0.75 | intra 0, inter 0 | same 1, other 1 | orphaned | - (suggest module utilities) |
| `test_detect_pure_function_heuristic` | `# [test] fn test_detect_pure_function_heuristic () { let elements = vec ! [make_function ("is_valid" , "fn is_valid(x: i32) -> bool" , Vec :: new ()) , make_function ("get_value" , "fn get_value() -> i32" , Vec :: new ()) ,] ; let detector = SemanticDetector :: new (& elements) ; let invariants = detector . detect_pure_function () ; assert ! (invariants . len () >= 2) ; for inv in & invariants { assert ! (matches ! (inv . strength , InvariantStrength :: Heuristic)) ; } } . sig` | 0.75 | intra 0, inter 0 | same 1, other 1 | orphaned | - (suggest module utilities) |
| `test_no_pure_for_mutable` | `# [test] fn test_no_pure_for_mutable () { let elements = vec ! [make_function ("is_valid" , "fn is_valid(x: &mut i32) -> bool" , Vec :: new () ,)] ; let detector = SemanticDetector :: new (& elements) ; let invariants = detector . detect_pure_function () ; let pure_count = invariants . iter () . filter (\| inv \| inv . target == "is_valid") . count () ; assert_eq ! (pure_count , 0) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |

## File: src/060_layer_core.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `structural_cmp` | `pub fn structural_cmp (a : & crate :: report :: PlanItem , b : & crate :: report :: PlanItem) -> std :: cmp :: Ordering { let a_required = structural_layer_value (& a . required_layer , i32 :: MAX) ; let b_required = structural_layer_value (& b . required_layer , i32 :: MAX) ; let a_current = structural_layer_value (& a . current_layer , i32 :: MIN) ; let b_current = structural_layer_value (& b . current_layer , i32 :: MIN) ; let a_benefit = if a . cost == 0 { 0 } else { (a . benefit . saturating_mul (1000)) / a . cost } ; let b_benefit = if b . cost == 0 { 0 } else { (b . benefit . saturating_mul (1000)) / b . cost } ; a_required . cmp (& b_required) . then_with (\| \| b . is_utility . cmp (& a . is_utility)) . then_with (\| \| b_benefit . cmp (& a_benefit)) . then_with (\| \| b . impact_weight . cmp (& a . impact_weight)) . then_with (\| \| b_current . cmp (& a_current)) . then_with (\| \| a . description . cmp (& b . description)) } . sig` | 0.00 | intra 0, inter 1 | same 0, other 2 | layer violation | - (060_layer_core.rs -> 010_cluster_008.rs) |
| `sort_structural_items` | `pub fn sort_structural_items (items : & mut Vec < crate :: report :: PlanItem >) { use std :: collections :: HashMap ; use std :: path :: PathBuf ; if items . len () <= 1 { return ; } let count = items . len () ; let mut edges : Vec < Vec < usize > > = vec ! [Vec :: new () ; count] ; let mut indegree = vec ! [0usize ; count] ; let mut file_to_items : HashMap < PathBuf , Vec < usize > > = HashMap :: new () ; for (idx , item) in items . iter () . enumerate () { if let Some (path) = & item . current_file { file_to_items . entry (path . clone ()) . or_default () . push (idx) ; } } for i in 0 .. count { for j in (i + 1) .. count { let req_i = structural_layer_value (& items [i] . required_layer , i32 :: MAX) ; let req_j = structural_layer_value (& items [j] . required_layer , i32 :: MAX) ; let mut edge = None ; if req_i != req_j { edge = if req_i < req_j { Some ((i , j)) } else { Some ((j , i)) } ; } else if items [i] . is_utility != items [j] . is_utility { edge = if items [i] . is_utility { Some ((i , j)) } else { Some ((j , i)) } ; } if let Some ((from , to)) = edge { edges [from] . push (to) ; indegree [to] += 1 ; } } } for (idx , item) in items . iter () . enumerate () { for file in & item . outgoing_files { if let Some (dependents) = file_to_items . get (file) { for & dependent_idx in dependents { if dependent_idx == idx { continue ; } edges [dependent_idx] . push (idx) ; indegree [idx] += 1 ; } } } } let mut ordered_indices = Vec :: with_capacity (count) ; let mut available : Vec < usize > = (0 .. count) . filter (\| & i \| indegree [i] == 0) . collect () ; while ! available . is_empty () { available . sort_by (\| & a , & b \| structural_cmp (& items [a] , & items [b])) ; let next = available . remove (0) ; ordered_indices . push (next) ; for & neighbor in & edges [next] { indegree [neighbor] = indegree [neighbor] . saturating_sub (1) ; if indegree [neighbor] == 0 { available . push (neighbor) ; } } } if ordered_indices . len () != count { items . sort_by (structural_cmp) ; return ; } let mut reordered = Vec :: with_capacity (count) ; for idx in ordered_indices { reordered . push (items [idx] . clone ()) ; } * items = reordered ; } . sig` | 0.13 | intra 1, inter 1 | same 0, other 1 | layer violation | - (060_layer_core.rs -> 010_cluster_008.rs) |

## File: src/060_path_detector.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `test_path_detector_diamond` | `# [test] fn test_path_detector_diamond () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; let d = graph . add_node ("D" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (a , c , ()) ; graph . add_edge (b , d , ()) ; graph . add_edge (c , d , ()) ; let detector = PathDetector :: new (graph) ; let invariants = detector . detect_all (10 , 100) ; let a_invs : Vec < _ > = invariants . iter () . filter (\| inv \| inv . target == "A") . collect () ; if ! a_invs . is_empty () { let inv = a_invs [0] ; if let InvariantKind :: PathIntersection (ref pi) = inv . kind { assert_eq ! (pi . paths_analyzed , 2) ; assert ! (pi . facts . contains ("visits_D")) ; } } } . sig` | 0.75 | intra 0, inter 0 | same 1, other 1 | orphaned | - (suggest module utilities) |
| `test_path_detector_simple` | `# [test] fn test_path_detector_simple () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; let detector = PathDetector :: new (graph) ; let invariants = detector . detect_all (10 , 100) ; assert ! (invariants . len () >= 0) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `test_extract_facts_from_path` | `# [test] fn test_extract_facts_from_path () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; let c = graph . add_node ("C" . to_string ()) ; graph . add_edge (a , b , ()) ; graph . add_edge (b , c , ()) ; let detector = PathDetector :: new (graph) ; let path = vec ! [a , b , c] ; let facts = detector . extract_facts_from_path (& path) ; assert ! (facts . contains ("visits_A")) ; assert ! (facts . contains ("visits_B")) ; assert ! (facts . contains ("visits_C")) ; assert ! (facts . contains ("reaches_leaf")) ; assert ! (facts . contains ("path_length_3")) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `test_path_stats` | `# [test] fn test_path_stats () { let mut graph = DiGraph :: new () ; let a = graph . add_node ("A" . to_string ()) ; let b = graph . add_node ("B" . to_string ()) ; graph . add_edge (a , b , ()) ; let detector = PathDetector :: new (graph) ; let stats = detector . get_stats () ; assert_eq ! (stats . original_nodes , 2) ; assert ! (stats . is_dag) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |

## File: src/070_invariant_integrator.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `make_simple_analysis` | `fn make_simple_analysis () -> AnalysisResult { let mut result = AnalysisResult :: new () ; result . add_element (CodeElement { name : "test_fn" . to_string () , file_path : "test.rs" . to_string () , line_number : 1 , element_type : ElementType :: Function , signature : "fn test_fn() -> i32" . to_string () , visibility : Visibility :: Public , generic_params : Vec :: new () , language : Language :: Rust , layer : "0" . to_string () , calls : Vec :: new () , }) ; let mut call_graph = HashMap :: new () ; call_graph . insert ("test_fn" . to_string () , CallGraphNode { function_name : "test_fn" . to_string () , file_path : "test.rs" . to_string () , calls : Vec :: new () , called_by : Vec :: new () , } ,) ; result . call_graph = call_graph ; result } . sig` | 0.60 | intra 0, inter 0 | same 0, other 7 | ok | - |
| `test_invariant_detector_creation` | `# [test] fn test_invariant_detector_creation () { let analysis = make_simple_analysis () ; let detector = InvariantDetector :: new (& analysis , & analysis . call_graph) ; assert_eq ! (detector . call_graph . node_count () , 1) ; } . sig` | 0.90 | intra 1, inter 0 | same 1, other 0 | ok | - |
| `test_detect_all` | `# [test] fn test_detect_all () { let analysis = make_simple_analysis () ; let detector = InvariantDetector :: new (& analysis , & analysis . call_graph) ; let result = detector . detect_all () ; assert ! (result . stats . structural_count > 0) ; } . sig` | 0.90 | intra 1, inter 0 | same 1, other 0 | ok | - |

## File: src/070_layer_utilities.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `run_analysis` | `pub fn run_analysis (root_path : & Path , output_path : & Path , verbose : bool , skip_julia : bool ,) -> Result < () > { use crate :: control_flow :: ControlFlowAnalyzer ; use crate :: cohesion_analyzer :: FunctionCohesionAnalyzer ; use crate :: dependency :: LayerGraph ; use crate :: directory_analyzer :: DirectoryAnalyzer ; use crate :: dot_exporter :: export_program_cfg_to_path ; use crate :: julia_parser :: JuliaAnalyzer ; use crate :: report :: ReportGenerator ; use crate :: rust_parser :: RustAnalyzer ; use crate :: types :: { AnalysisResult , FileOrderingResult } ; let julia_script_path = root_path . join ("src/000_main.jl") ; println ! ("MMSB Intelligence Substrate Analyzer") ; println ! ("=====================================\n") ; println ! ("Root directory: {:?}" , root_path) ; println ! ("Output directory: {:?}" , output_path) ; println ! ("Julia script: {:?}\n" , julia_script_path) ; let rust_analyzer = RustAnalyzer :: new (root_path . to_string_lossy () . to_string ()) ; let mut combined_result = AnalysisResult :: new () ; println ! ("Scanning Rust files (dependency-ordered)...") ; let mut rust_count = 0 ; let rust_files = gather_rust_files (root_path) ; let (ordered_rust_files , rust_layer_graph) = crate :: dependency :: order_rust_files_by_dependency (& rust_files , root_path) . context ("Failed to resolve Rust dependency order") ? ; let rust_file_ordering = crate :: dependency :: analyze_file_ordering (& rust_files , None) . context ("Failed to analyze Rust file ordering") ? ; let julia_file_ordering = FileOrderingResult { ordered_files : Vec :: new () , violations : Vec :: new () , layer_violations : Vec :: new () , ordered_directories : Vec :: new () , cycles : Vec :: new () , } ; for path in ordered_rust_files { if verbose { println ! ("  Analyzing: {:?}" , path) ; } match rust_analyzer . analyze_file (& path) { Ok (result) => { rust_count += 1 ; combined_result . merge (result) ; } Err (e) => { eprintln ! ("Warning: Failed to analyze {:?}: {}" , path , e) ; } } } println ! ("  Analyzed {} Rust files\n" , rust_count) ; let mut julia_count = 0 ; let mut julia_layer_graph = LayerGraph { ordered_layers : Vec :: new () , edges : Vec :: new () , cycles : Vec :: new () , unresolved : Vec :: new () , } ; if ! skip_julia { println ! ("Scanning Julia files (dependency-ordered)...") ; let julia_files = gather_julia_files (root_path) ; let (ordered_julia_files , jlg) = crate :: dependency :: order_julia_files_by_dependency (& julia_files , root_path) . context ("Failed to resolve Julia dependency order") ? ; julia_layer_graph = jlg ; if julia_script_path . exists () { let julia_analyzer = JuliaAnalyzer :: new (root_path . to_path_buf () , julia_script_path . clone () , output_path . join ("30_cfg/dots") ,) ; for path in ordered_julia_files { if verbose { println ! ("  Analyzing: {:?}" , path) ; } match julia_analyzer . analyze_file (& path) { Ok (result) => { julia_count += 1 ; combined_result . merge (result) ; } Err (e) => { eprintln ! ("Warning: Failed to analyze {:?}: {}" , path , e) ; } } } } else { println ! ("  Skipping Julia analysis (script not found)") ; } println ! ("  Analyzed {} Julia files\n" , julia_count) ; } println ! ("Building call graph...") ; let mut cf_analyzer = ControlFlowAnalyzer :: new () ; cf_analyzer . build_call_graph (& combined_result) ; use crate :: invariant_integrator :: InvariantDetector ; println ! ("Detecting invariants...") ; let invariants_result = { let invariant_detector = InvariantDetector :: new (& combined_result , & combined_result . call_graph ,) ; invariant_detector . detect_all () } ; let constraints = { let invariant_detector = InvariantDetector :: new (& combined_result , & combined_result . call_graph ,) ; invariant_detector . generate_constraints (& invariants_result) } ; combined_result . invariants = invariants_result ; combined_result . constraints = constraints ; println ! ("Analyzing function cohesion...") ; let cohesion_analyzer = FunctionCohesionAnalyzer :: new () ; let placements = cohesion_analyzer . analyze (& combined_result) ? ; let clusters = cohesion_analyzer . detect_clusters (& combined_result) ? ; println ! ("Analyzing directory structure...") ; let dir_analyzer = DirectoryAnalyzer :: new (root_path . to_path_buf ()) ; let dir_analysis = dir_analyzer . analyze () ? ; println ! ("\nGenerating reports...") ; let report_gen = ReportGenerator :: new (output_path . to_string_lossy () . to_string ()) ; report_gen . generate_all (& combined_result , & cf_analyzer , & rust_layer_graph , & julia_layer_graph , & rust_file_ordering , & julia_file_ordering , & placements , & clusters , & dir_analysis , root_path ,) . context ("Failed to generate reports") ? ; println ! ("\nExporting program CFG...") ; export_program_cfg_to_path (& combined_result , & cf_analyzer . call_edges () , output_path) ? ; println ! ("\nGenerating invariant report...") ; use crate :: invariant_reporter ; invariant_reporter :: generate_invariant_report (& combined_result . invariants , output_path) . context ("Failed to generate invariant report") ? ; invariant_reporter :: export_constraints_json (& combined_result . constraints , output_path) . context ("Failed to export constraints") ? ; println ! ("\n Analysis complete!") ; println ! ("  Total elements: {}" , combined_result . elements . len ()) ; println ! ("  Rust files: {}" , rust_count) ; println ! ("  Julia files: {}" , julia_count) ; println ! ("  Output: {}\n" , output_path . display ()) ; Ok (()) } . sig` | 0.08 | intra 1, inter 3 | same 0, other 21 | layer violation | - (070_layer_utilities.rs -> 000_cluster_001.rs) |
| `gather_rust_files` | `pub fn gather_rust_files (root : & Path) -> Vec < PathBuf > { use walkdir :: WalkDir ; let src_root = resolve_source_root (root) ; WalkDir :: new (& src_root) . into_iter () . filter_entry (\| entry \| { if entry . depth () == 0 { return true ; } if ! entry . file_type () . is_dir () { return true ; } allow_analysis_dir (& src_root , entry . path ()) }) . filter_map (\| e \| e . ok ()) . filter (\| e \| e . path () . extension () . map_or (false , \| ext \| ext == "rs")) . filter (\| e \| { let rel = e . path () . strip_prefix (& src_root) . unwrap_or (e . path ()) ; rel . components () . count () == 1 \|\| e . path () . starts_with (src_root . join ("src")) }) . map (\| entry \| entry . into_path ()) . collect () } . sig` | 0.67 | intra 2, inter 1 | same 0, other 0 | layer violation | - (070_layer_utilities.rs -> 020_cluster_010.rs) |
| `resolve_source_root` | `# [doc = " Resolves the source root directory from a given root path"] pub fn resolve_source_root (root : & Path) -> PathBuf { let src_candidate = root . join ("src") ; if src_candidate . exists () && src_candidate . is_dir () { src_candidate } else { root . to_path_buf () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `allow_analysis_dir` | `# [doc = " Checks if a directory should be included in analysis"] pub fn allow_analysis_dir (root : & Path , dir : & Path) -> bool { let name = dir . file_name () . and_then (\| n \| n . to_str ()) . unwrap_or ("") ; if name . starts_with ('.') \|\| name == "target" \|\| name == "node_modules" { return false ; } if let Ok (rel) = dir . strip_prefix (root) { if rel . components () . any (\| c \| { let s = c . as_os_str () . to_str () . unwrap_or ("") ; s . starts_with ('.') \|\| s == "target" \|\| s == "node_modules" }) { return false ; } } true } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `main` | `pub fn main () -> Result < () > { let args = Args :: parse () ; let root_path = std :: env :: current_dir () ? . join (& args . root) . canonicalize () ? ; let output_path = std :: env :: current_dir () ? . join (& args . output) . canonicalize () . unwrap_or_else (\| _ \| { let p = std :: env :: current_dir () . unwrap () . join (& args . output) ; std :: fs :: create_dir_all (& p) . ok () ; p . canonicalize () . unwrap_or (p) }) ; run_analysis (& root_path , & output_path , args . verbose , args . skip_julia) } . sig` | 0.90 | intra 1, inter 0 | same 1, other 0 | ok | - |

## File: src/080_invariant_reporter.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `generate_invariant_report` | `# [doc = " Generate invariant report in markdown format"] pub fn generate_invariant_report (result : & InvariantAnalysisResult , output_dir : & Path ,) -> Result < () , std :: io :: Error > { let report_dir = output_dir . join ("95_invariants") ; fs :: create_dir_all (& report_dir) ? ; let report_path = report_dir . join ("index.md") ; let mut report = String :: new () ; report . push_str ("# Invariant Analysis Report\n\n") ; report . push_str (& format ! ("Generated: {}\n\n" , chrono :: Local :: now () . format ("%Y-%m-%d %H:%M:%S"))) ; report . push_str ("## Summary\n\n") ; report . push_str (& format ! ("- **Total Invariants**: {}\n" , result . stats . total_count)) ; report . push_str (& format ! ("- **Proven**: {} ({:.1}%)\n" , result . stats . proven_count , result . stats . proven_percentage ())) ; report . push_str (& format ! ("- **Empirical**: {}\n" , result . stats . empirical_count)) ; report . push_str (& format ! ("- **Heuristic**: {} ({:.1}%)  LOW CONFIDENCE\n" , result . stats . heuristic_count , result . stats . heuristic_percentage ())) ; report . push_str (& format ! ("- **Violations**: {}\n\n" , result . stats . violation_count)) ; report . push_str ("### By Kind\n\n") ; report . push_str (& format ! ("- **Structural**: {}\n" , result . stats . structural_count)) ; report . push_str (& format ! ("- **Semantic**: {}\n" , result . stats . semantic_count)) ; report . push_str (& format ! ("- **Delta**: {}\n" , result . stats . delta_count)) ; report . push_str (& format ! ("- **Path-Intersection**: {}\n\n" , result . stats . path_intersection_count)) ; report . push_str ("## Proven Invariants (Mechanical Truth)\n\n") ; report . push_str ("These invariants are mathematically proven from graph structure and should **always block refactorings**.\n\n") ; let proven : Vec < _ > = result . invariants . iter () . filter (\| inv \| matches ! (inv . strength , InvariantStrength :: Proven)) . collect () ; if proven . is_empty () { report . push_str ("*None detected*\n\n") ; } else { for inv in & proven { report . push_str (& format ! ("### {}\n\n" , inv . target)) ; report . push_str (& format ! ("- **Type**: {}\n" , inv . kind)) ; report . push_str (& format ! ("- **File**: {}\n" , inv . file_path)) ; report . push_str (& format ! ("- **Description**: {}\n" , inv . description)) ; if ! inv . evidence . is_empty () { report . push_str ("- **Evidence**:\n") ; for e in & inv . evidence { report . push_str (& format ! ("  - {}\n" , e)) ; } } report . push_str ("\n") ; } } report . push_str ("## Empirical Invariants (High Confidence)\n\n") ; report . push_str ("These invariants were observed across multiple paths/samples and have high confidence.\n\n") ; let empirical : Vec < _ > = result . invariants . iter () . filter (\| inv \| matches ! (inv . strength , InvariantStrength :: Empirical { .. })) . collect () ; if empirical . is_empty () { report . push_str ("*None detected*\n\n") ; } else { for inv in empirical . iter () . take (20) { report . push_str (& format ! ("### {}\n\n" , inv . target)) ; report . push_str (& format ! ("- **Type**: {}\n" , inv . kind)) ; report . push_str (& format ! ("- **Strength**: {}\n" , inv . strength)) ; report . push_str (& format ! ("- **Confidence**: {:.2}\n" , inv . confidence . value ())) ; report . push_str (& format ! ("- **Description**: {}\n\n" , inv . description)) ; } if empirical . len () > 20 { report . push_str (& format ! ("*... and {} more*\n\n" , empirical . len () - 20)) ; } } report . push_str ("## Heuristic Signals (Low Confidence - Review Required)\n\n") ; report . push_str (" **WARNING**: These are based on naming patterns and heuristics. They require manual verification and should **NOT block refactorings**.\n\n") ; let heuristic : Vec < _ > = result . invariants . iter () . filter (\| inv \| matches ! (inv . strength , InvariantStrength :: Heuristic)) . collect () ; if heuristic . is_empty () { report . push_str ("*None detected*\n\n") ; } else { for inv in heuristic . iter () . take (10) { report . push_str (& format ! ("- **{}**: {} ({})\n" , inv . target , inv . description , inv . file_path)) ; } if heuristic . len () > 10 { report . push_str (& format ! ("\n*... and {} more (see JSON export)*\n\n" , heuristic . len () - 10)) ; } else { report . push_str ("\n") ; } } if ! result . violations . is_empty () { report . push_str ("## Violations\n\n") ; report . push_str ("Detected violations of invariants, grouped by severity.\n\n") ; let mut critical : Vec < _ > = result . violations . iter () . filter (\| v \| matches ! (v . severity , ViolationSeverity :: Critical)) . collect () ; critical . sort_by_key (\| v \| & v . invariant . target) ; if ! critical . is_empty () { report . push_str ("### Critical\n\n") ; for violation in critical { report . push_str (& format ! ("- **{}**: {}\n" , violation . invariant . target , violation . violation_description)) ; if let Some (ref fix) = violation . suggested_fix { report . push_str (& format ! ("  - *Suggested fix*: {}\n" , fix)) ; } } report . push_str ("\n") ; } } if ! result . layer_assignments . is_empty () { report . push_str ("## Layer Assignments (Inferred from Call Graph)\n\n") ; report . push_str ("Layers are **NOT** based on filename prefixes. They are computed from the call graph structure.\n\n") ; let mut layers : Vec < _ > = result . layer_assignments . iter () . collect () ; layers . sort_by_key (\| (_ , info) \| info . layer) ; for (name , info) in layers . iter () . take (20) { report . push_str (& format ! ("- **{}**: Layer {} (dependencies: {})\n" , name , info . layer , info . dependencies . len ())) ; } if layers . len () > 20 { report . push_str (& format ! ("\n*... and {} more (see JSON export)*\n\n" , layers . len () - 20)) ; } else { report . push_str ("\n") ; } } fs :: write (& report_path , report) ? ; println ! (" Invariant report written to: {}" , report_path . display ()) ; export_json (result , & report_dir) ? ; let conscience_map_path = report_dir . join ("conscience_map.md") ; crate :: conscience_graph :: generate_conscience_map (& result . invariants , & conscience_map_path) ? ; println ! (" Conscience map written to: {}" , conscience_map_path . display ()) ; Ok (()) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 10 | ok | - |
| `export_json` | `# [doc = " Export invariants to JSON for agent consumption"] pub fn export_json (result : & InvariantAnalysisResult , output_dir : & Path ,) -> Result < () , std :: io :: Error > { let json_path = output_dir . join ("invariants.json") ; let json = serde_json :: to_string_pretty (result) . map_err (\| e \| std :: io :: Error :: new (std :: io :: ErrorKind :: Other , e)) ? ; fs :: write (& json_path , json) ? ; println ! (" JSON export written to: {}" , json_path . display ()) ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `export_constraints_json` | `# [doc = " Export refactoring constraints to JSON"] pub fn export_constraints_json (constraints : & [RefactorConstraint] , output_dir : & Path ,) -> Result < () , std :: io :: Error > { let constraints_dir = output_dir . join ("96_constraints") ; fs :: create_dir_all (& constraints_dir) ? ; let json_path = constraints_dir . join ("refactor_constraints.json") ; let json = serde_json :: to_string_pretty (constraints) . map_err (\| e \| std :: io :: Error :: new (std :: io :: ErrorKind :: Other , e)) ? ; fs :: write (& json_path , json) ? ; println ! (" Constraints written to: {}" , json_path . display ()) ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `test_generate_report` | `# [test] fn test_generate_report () { let result = InvariantAnalysisResult { invariants : vec ! [Invariant :: new ("test_fn" . to_string () , "test.rs" . to_string () , InvariantKind :: Structural (StructuralInvariant :: Leaf) , InvariantStrength :: Proven , "Test invariant" . to_string () ,) ,] , violations : Vec :: new () , layer_assignments : HashMap :: new () , stats : InvariantStats { total_count : 1 , proven_count : 1 , empirical_count : 0 , heuristic_count : 0 , structural_count : 1 , semantic_count : 0 , delta_count : 0 , path_intersection_count : 0 , violation_count : 0 , } , } ; let temp_dir = std :: env :: temp_dir () . join ("mmsb_test_invariants") ; fs :: create_dir_all (& temp_dir) . unwrap () ; let res = generate_invariant_report (& result , & temp_dir) ; assert ! (res . is_ok ()) ; let _ = fs :: remove_dir_all (& temp_dir) ; } . sig` | 0.60 | intra 1, inter 0 | same 0, other 6 | ok | - |

## File: src/082_conscience_graph.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `test_strength_emoji` | `# [test] fn test_strength_emoji () { let proven = make_test_invariant ("test" , InvariantKind :: Structural (StructuralInvariant :: Leaf) , InvariantStrength :: Proven ,) ; let empirical = make_test_invariant ("test" , InvariantKind :: Semantic (SemanticInvariant :: TypeStable { signature : "sig" . to_string () , }) , InvariantStrength :: Empirical { paths_checked : 5 } ,) ; let heuristic = make_test_invariant ("test" , InvariantKind :: Semantic (SemanticInvariant :: PureFunction) , InvariantStrength :: Heuristic ,) ; assert_eq ! (strength_emoji (& proven) , "") ; assert_eq ! (strength_emoji (& empirical) , "") ; assert_eq ! (strength_emoji (& heuristic) , "?") ; } . sig` | 0.35 | intra 3, inter 1 | same 0, other 9 | move | - (cohesion 0.35 below threshold 0.60 (impact 0.25)) |
| `generate_conscience_map` | `# [doc = " Generate conscience map showing protection levels per function"] pub fn generate_conscience_map (invariants : & [Invariant] , output_path : & Path ,) -> std :: io :: Result < () > { let mut content = String :: new () ; content . push_str ("# Conscience Map\n\n") ; content . push_str ("## Overview\n\n") ; content . push_str ("This map shows which functions are protected by mechanical constraints.\n") ; content . push_str ("Functions with blocking invariants cannot be refactored without violating proven properties.\n\n") ; let mut by_function : HashMap < String , Vec < & Invariant > > = HashMap :: new () ; for inv in invariants { by_function . entry (inv . target . clone ()) . or_default () . push (inv) ; } let total_functions = by_function . len () ; let protected_functions = by_function . values () . filter (\| invs \| invs . iter () . any (\| i \| i . is_blocking ())) . count () ; content . push_str (& format ! ("**Total Functions**: {}\n\n" , total_functions)) ; content . push_str (& format ! ("**Protected Functions**: {} ({:.1}%)\n\n" , protected_functions , (protected_functions as f64 / total_functions as f64) * 100.0)) ; let mut funcs : Vec < _ > = by_function . into_iter () . collect () ; funcs . sort_by_key (\| (_ , invs) \| { - (invs . iter () . filter (\| i \| i . is_blocking ()) . count () as i32) }) ; content . push_str ("---\n\n") ; content . push_str ("## Functions by Protection Level\n\n") ; for (func , invs) in funcs { let blocking_count = invs . iter () . filter (\| i \| i . is_blocking ()) . count () ; let total_count = invs . len () ; if blocking_count == 0 { continue ; } let protection_percent = (blocking_count * 100) / total_count ; content . push_str (& format ! ("### `{}` ({}% protected)\n\n" , func , protection_percent)) ; if let Some (inv) = invs . first () { if ! inv . file_path . is_empty () { content . push_str (& format ! ("**File**: `{}`\n\n" , inv . file_path)) ; } } for inv in invs . iter () . filter (\| i \| i . is_blocking ()) { content . push_str (& format ! ("-  {} **{}**: {}\n" , strength_emoji (inv) , kind_name (inv) , inv . description)) ; } content . push_str ("\n") ; } content . push_str ("---\n\n") ; content . push_str ("## Legend\n\n") ; content . push_str ("-  **PROVEN**: Mathematical certainty from graph topology\n") ; content . push_str ("-  **EMPIRICAL**: Observed across multiple paths (high confidence)\n") ; content . push_str ("- ? **HEURISTIC**: Name-based guess (LOW CONFIDENCE - verify manually)\n\n") ; content . push_str ("-  **Blocking**: Constraint mechanically enforced\n\n") ; std :: fs :: write (output_path , content) ? ; Ok (()) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | orphaned | - (suggest module utilities) |
| `strength_emoji` | `# [doc = " Get emoji for invariant strength"] fn strength_emoji (inv : & Invariant) -> & 'static str { match inv . strength { InvariantStrength :: Proven => "" , InvariantStrength :: Empirical { .. } => "" , InvariantStrength :: Heuristic => "?" , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | orphaned | - (suggest module utilities) |
| `kind_name` | `# [doc = " Get short name for invariant kind"] fn kind_name (inv : & Invariant) -> String { match & inv . kind { InvariantKind :: Structural (s) => match s { StructuralInvariant :: LayerFixed { layer } => format ! ("LayerFixed({})" , layer) , StructuralInvariant :: DegreeStable { in_degree , out_degree } => { format ! ("DegreeStable(in={}, out={})" , in_degree , out_degree) } StructuralInvariant :: Leaf => "Leaf" . to_string () , StructuralInvariant :: Root => "Root" . to_string () , StructuralInvariant :: Bridge => "Bridge" . to_string () , StructuralInvariant :: SccMembership { scc_id , scc_size } => { format ! ("SCC({}, size={})" , scc_id , scc_size) } } , InvariantKind :: Semantic (s) => match s { SemanticInvariant :: TypeStable { .. } => "TypeStable" . to_string () , SemanticInvariant :: PureFunction => "PureFunction" . to_string () , SemanticInvariant :: Idempotent => "Idempotent" . to_string () , SemanticInvariant :: EffectStable { .. } => "EffectStable" . to_string () , } , InvariantKind :: Delta (_) => "Delta" . to_string () , InvariantKind :: PathIntersection { .. } => "PathIntersection" . to_string () , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 15 | orphaned | - (suggest module utilities) |
| `make_test_invariant` | `fn make_test_invariant (name : & str , kind : InvariantKind , strength : InvariantStrength ,) -> Invariant { Invariant :: new (name . to_string () , "test.rs" . to_string () , kind , strength , "test" . to_string () ,) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | ok | - |
| `test_generate_stats` | `# [test] fn test_generate_stats () { let invariants = vec ! [make_test_invariant ("fn1" , InvariantKind :: Structural (StructuralInvariant :: LayerFixed { layer : 0 }) , InvariantStrength :: Proven ,) , make_test_invariant ("fn1" , InvariantKind :: Semantic (SemanticInvariant :: PureFunction) , InvariantStrength :: Heuristic ,) , make_test_invariant ("fn2" , InvariantKind :: Structural (StructuralInvariant :: Leaf) , InvariantStrength :: Proven ,) ,] ; let stats = generate_conscience_stats (& invariants) ; assert_eq ! (stats . total_functions , 2) ; assert_eq ! (stats . total_invariants , 3) ; assert_eq ! (stats . proven_count , 2) ; assert_eq ! (stats . heuristic_count , 1) ; } . sig` | 0.60 | intra 1, inter 0 | same 0, other 9 | ok | - |
| `generate_conscience_stats` | `# [doc = " Generate summary statistics"] pub fn generate_conscience_stats (invariants : & [Invariant]) -> ConscienceStats { let mut by_function : HashMap < String , Vec < & Invariant > > = HashMap :: new () ; for inv in invariants { by_function . entry (inv . target . clone ()) . or_default () . push (inv) ; } let total_functions = by_function . len () ; let protected_functions = by_function . values () . filter (\| invs \| invs . iter () . any (\| i \| i . is_blocking ())) . count () ; let proven_count = invariants . iter () . filter (\| i \| matches ! (i . strength , InvariantStrength :: Proven)) . count () ; let empirical_count = invariants . iter () . filter (\| i \| matches ! (i . strength , InvariantStrength :: Empirical { .. })) . count () ; let heuristic_count = invariants . iter () . filter (\| i \| matches ! (i . strength , InvariantStrength :: Heuristic)) . count () ; ConscienceStats { total_functions , protected_functions , total_invariants : invariants . len () , blocking_invariants : invariants . iter () . filter (\| i \| i . is_blocking ()) . count () , proven_count , empirical_count , heuristic_count , } } . sig` | 0.69 | intra 0, inter 0 | same 2, other 5 | ok | - |

## File: src/083_action_validator.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `test_check_move_allowed` | `# [test] fn test_check_move_allowed () { let constraints = vec ! [RefactorConstraint :: NoMove { target : "test_fn" . to_string () , reason : "critical function" . to_string () , strength : InvariantStrength :: Proven , }] ; let result = check_move_allowed ("test_fn" , & PathBuf :: from ("a.rs") , & PathBuf :: from ("b.rs") , & constraints ,) ; assert ! (result . is_err ()) ; assert ! (result . unwrap_err () . contains ("critical function")) ; } . sig` | 0.25 | intra 1, inter 1 | same 0, other 2 | layer violation | - (083_action_validator.rs -> 005_refactor_constraints.rs) |
| `matches_function` | `# [doc = " Check if function name matches constraint target"] fn matches_function (action_name : & str , constraint : & RefactorConstraint) -> bool { constraint . target () == action_name } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `test_validate_layer_fixed_constraint` | `# [test] fn test_validate_layer_fixed_constraint () { let constraints = vec ! [RefactorConstraint :: FixedLayer { target : "test_fn" . to_string () , layer : 0 , strength : InvariantStrength :: Proven , }] ; let action = AgentAction :: MoveFunction { name : "test_fn" . to_string () , from : PathBuf :: from ("src/000_test.rs") , to : PathBuf :: from ("src/010_test.rs") , } ; let result = validate_action (& action , & constraints) ; assert ! (result . is_err ()) ; } . sig` | 0.70 | intra 1, inter 0 | same 1, other 2 | ok | - |
| `test_validate_preserve_signature` | `# [test] fn test_validate_preserve_signature () { let constraints = vec ! [RefactorConstraint :: PreserveSignature { target : "test_fn" . to_string () , signature : "fn test_fn() -> i32" . to_string () , strength : InvariantStrength :: Empirical { paths_checked : 5 } , }] ; let action = AgentAction :: ChangeSignature { name : "test_fn" . to_string () , old_sig : "fn test_fn() -> i32" . to_string () , new_sig : "fn test_fn() -> String" . to_string () , file : PathBuf :: from ("src/test.rs") , } ; let result = validate_action (& action , & constraints) ; assert ! (result . is_err ()) ; } . sig` | 0.70 | intra 1, inter 0 | same 1, other 2 | ok | - |
| `test_validate_allowed_action` | `# [test] fn test_validate_allowed_action () { let constraints = vec ! [RefactorConstraint :: NoMove { target : "other_fn" . to_string () , reason : "layer fixed" . to_string () , strength : InvariantStrength :: Proven , }] ; let action = AgentAction :: MoveFunction { name : "test_fn" . to_string () , from : PathBuf :: from ("src/000_test.rs") , to : PathBuf :: from ("src/010_test.rs") , } ; let result = validate_action (& action , & constraints) ; assert ! (result . is_ok ()) ; } . sig` | 0.70 | intra 1, inter 0 | same 1, other 2 | ok | - |
| `check_move_allowed` | `# [doc = " Check if a specific move is allowed"] pub fn check_move_allowed (name : & str , from : & PathBuf , to : & PathBuf , constraints : & [RefactorConstraint] ,) -> Result < () , String > { let action = AgentAction :: MoveFunction { name : name . to_string () , from : from . clone () , to : to . clone () , } ; match validate_action (& action , constraints) { Ok (_) => Ok (()) , Err (violations) => { let reasons : Vec < String > = violations . iter () . map (\| v \| v . reason . clone ()) . collect () ; Err (reasons . join ("; ")) } } } . sig` | 0.75 | intra 1, inter 0 | same 1, other 1 | ok | - |
| `test_validate_no_move_constraint` | `# [test] fn test_validate_no_move_constraint () { let constraints = vec ! [RefactorConstraint :: NoMove { target : "test_fn" . to_string () , reason : "layer 0 is fixed" . to_string () , strength : InvariantStrength :: Proven , }] ; let action = AgentAction :: MoveFunction { name : "test_fn" . to_string () , from : PathBuf :: from ("src/000_test.rs") , to : PathBuf :: from ("src/010_test.rs") , } ; let result = validate_action (& action , & constraints) ; assert ! (result . is_err ()) ; let violations = result . unwrap_err () ; assert_eq ! (violations . len () , 1) ; assert_eq ! (violations [0] . severity , ViolationSeverity :: Critical) ; assert ! (violations [0] . blocking) ; } . sig` | 0.75 | intra 1, inter 0 | same 2, other 2 | ok | - |
| `validate_action` | `# [doc = " Validate action against all constraints"] pub fn validate_action (action : & AgentAction , constraints : & [RefactorConstraint] ,) -> Result < () , Vec < ConstraintViolation > > { let mut violations = Vec :: new () ; for (idx , constraint) in constraints . iter () . enumerate () { match (action , constraint) { (AgentAction :: MoveFunction { name , from , to } , RefactorConstraint :: NoMove { target , reason , strength , } ,) if target == name => { violations . push (ConstraintViolation { constraint_id : idx , invariant_id : 0 , reason : format ! ("Cannot move {}: {} (strength: {:?})" , name , reason , strength) , severity : ViolationSeverity :: Critical , blocking : true , }) ; } (AgentAction :: MoveFunction { name , from , to } , RefactorConstraint :: FixedLayer { target , layer , strength , } ,) if target == name => { let from_layer = extract_layer (from) ; let to_layer = extract_layer (to) ; if from_layer != to_layer { violations . push (ConstraintViolation { constraint_id : idx , invariant_id : 0 , reason : format ! ("Cannot move {} across layers: layer {} fixed (strength: {:?})" , name , layer , strength) , severity : ViolationSeverity :: Critical , blocking : true , }) ; } } (AgentAction :: ChangeSignature { name , old_sig , .. } , RefactorConstraint :: PreserveSignature { target , signature , strength , } ,) if target == name => { violations . push (ConstraintViolation { constraint_id : idx , invariant_id : 0 , reason : format ! ("Cannot change signature of {}: type-stable (strength: {:?})" , name , strength) , severity : ViolationSeverity :: High , blocking : true , }) ; } (AgentAction :: DeleteFunction { name , .. } , RefactorConstraint :: NoMove { target , reason , strength , } ,) if target == name && reason . contains ("utility") => { violations . push (ConstraintViolation { constraint_id : idx , invariant_id : 0 , reason : format ! ("Cannot delete {}: widely used utility function (strength: {:?})" , name , strength) , severity : ViolationSeverity :: Critical , blocking : true , }) ; } _ => { } } } if violations . is_empty () { Ok (()) } else { Err (violations) } } . sig` | 0.82 | intra 2, inter 0 | same 14, other 5 | ok | - |
| `extract_layer` | `# [doc = " Extract layer number from file path (e.g., \"src/040_test.rs\" -> Some(40))"] fn extract_layer (path : & PathBuf) -> Option < usize > { path . file_name () . and_then (\| n \| n . to_str ()) . and_then (\| s \| { let parts : Vec < & str > = s . split ('_') . collect () ; if ! parts . is_empty () { parts [0] . parse :: < usize > () . ok () } else { None } }) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `test_extract_layer` | `# [test] fn test_extract_layer () { assert_eq ! (extract_layer (& PathBuf :: from ("src/040_test.rs")) , Some (40)) ; assert_eq ! (extract_layer (& PathBuf :: from ("src/000_utils.rs")) , Some (0)) ; assert_eq ! (extract_layer (& PathBuf :: from ("src/test.rs")) , None) ; } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/085_agent_conscience.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `test_conscience_blocks_invalid_move` | `# [test] fn test_conscience_blocks_invalid_move () { let inv = make_test_invariant ("test_fn" , 0 , InvariantStrength :: Proven) ; let conscience = AgentConscience :: new (vec ! [inv]) ; let action = AgentAction :: MoveFunction { name : "test_fn" . to_string () , from : PathBuf :: from ("src/000_test.rs") , to : PathBuf :: from ("src/010_test.rs") , } ; let result = conscience . check_action (& action) ; assert ! (! result . allowed) ; assert ! (! result . violations . is_empty ()) ; } . sig` | 0.30 | intra 1, inter 2 | same 1, other 2 | layer violation | - (085_agent_conscience.rs -> 082_conscience_graph.rs) |
| `test_conscience_allows_valid_action` | `# [test] fn test_conscience_allows_valid_action () { let inv = make_test_invariant ("other_fn" , 0 , InvariantStrength :: Proven) ; let conscience = AgentConscience :: new (vec ! [inv]) ; let action = AgentAction :: MoveFunction { name : "test_fn" . to_string () , from : PathBuf :: from ("src/000_test.rs") , to : PathBuf :: from ("src/010_test.rs") , } ; let result = conscience . check_action (& action) ; assert ! (result . allowed) ; assert ! (result . violations . is_empty ()) ; } . sig` | 0.30 | intra 1, inter 2 | same 1, other 2 | layer violation | - (085_agent_conscience.rs -> 082_conscience_graph.rs) |
| `test_query_allowed_actions` | `# [test] fn test_query_allowed_actions () { let inv = make_test_invariant ("test_fn" , 0 , InvariantStrength :: Proven) ; let conscience = AgentConscience :: new (vec ! [inv]) ; let allowed = conscience . query_allowed_actions ("test_fn") ; assert ! (! allowed . is_empty ()) ; } . sig` | 0.40 | intra 1, inter 1 | same 1, other 1 | layer violation | - (085_agent_conscience.rs -> 082_conscience_graph.rs) |
| `make_test_invariant` | `fn make_test_invariant (name : & str , layer : usize , strength : InvariantStrength) -> Invariant { Invariant :: new (name . to_string () , format ! ("src/{:03}_test.rs" , layer * 10) , InvariantKind :: Structural (StructuralInvariant :: LayerFixed { layer }) , strength , format ! ("Layer {} fixed" , layer) ,) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 5 | ok | - |
| `test_conscience_stats` | `# [test] fn test_conscience_stats () { let invariants = vec ! [make_test_invariant ("fn1" , 0 , InvariantStrength :: Proven) , make_test_invariant ("fn2" , 1 , InvariantStrength :: Empirical { paths_checked : 3 }) , make_test_invariant ("fn3" , 2 , InvariantStrength :: Heuristic) ,] ; let conscience = AgentConscience :: new (invariants) ; let stats = conscience . stats () ; assert_eq ! (stats . total_invariants , 3) ; assert_eq ! (stats . proven_count , 1) ; assert_eq ! (stats . empirical_count , 1) ; assert_eq ! (stats . heuristic_count , 1) ; } . sig` | 0.68 | intra 0, inter 0 | same 1, other 3 | orphaned | - (suggest module utilities) |

## File: src/090_utilities.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `collect_directory_files` | `pub fn collect_directory_files (directory : & DirectoryAnalysis , out : & mut Vec < PathBuf >) { out . extend (directory . files . iter () . cloned ()) ; for sub in & directory . subdirectories { collect_directory_files (sub , out) ; } } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `resolve_required_layer_path` | `pub fn resolve_required_layer_path (required_layer : & str , current_file : & Path , directory : & DirectoryAnalysis , root_path : & Path ,) -> PathBuf { let mut files = Vec :: new () ; collect_directory_files (directory , & mut files) ; let candidates = files . into_iter () . filter (\| path \| { path . file_name () . and_then (\| name \| name . to_str ()) . map (\| name \| name == required_layer) . unwrap_or (false) }) . collect :: < Vec < _ > > () ; if candidates . is_empty () { return current_file . parent () . unwrap_or (root_path) . join (required_layer) ; } let current_dir = current_file . parent () . unwrap_or (root_path) ; let mut best = None ; let mut best_score = - 1isize ; for candidate in candidates { let candidate_dir = candidate . parent () . unwrap_or (root_path) ; let score = path_common_prefix_len (current_dir , candidate_dir) ; let length = candidate . components () . count () as isize ; let combined = score * 1000 - length ; if combined > best_score { best_score = combined ; best = Some (candidate) ; } } best . unwrap_or_else (\| \| { current_file . parent () . unwrap_or (root_path) . join (required_layer) }) } . sig` | 0.60 | intra 2, inter 0 | same 0, other 1 | ok | - |
| `compute_move_metrics` | `pub fn compute_move_metrics (placement : & FunctionPlacement ,) -> (usize , usize , usize , usize , Vec < PathBuf > , Vec < PathBuf >) { let incoming_calls = placement . call_analysis . calls_from_other_files . iter () . map (\| (_ , count) \| * count) . sum :: < usize > () ; let callers = placement . call_analysis . calls_from_other_files . len () ; let mut touched = BTreeSet :: new () ; touched . insert (placement . current_file . clone ()) ; let mut outgoing_files = Vec :: new () ; for (path , _) in & placement . call_analysis . inter_file_calls { touched . insert (path . clone ()) ; outgoing_files . push (path . clone ()) ; } let mut caller_files = Vec :: new () ; for (path , _) in & placement . call_analysis . calls_from_other_files { touched . insert (path . clone ()) ; caller_files . push (path . clone ()) ; } let cost = touched . len () . max (1) ; let benefit = 1 + callers ; (incoming_calls , benefit , cost , callers , caller_files , outgoing_files) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `collect_move_items` | `pub fn collect_move_items (placements : & [FunctionPlacement] , utility_names : & BTreeSet < String > , directory : & DirectoryAnalysis , root_path : & Path ,) -> Vec < PlanItem > { let mut items = Vec :: new () ; for placement in placements { match & placement . placement_status { PlacementStatus :: ShouldMove { reason , impact } => { let priority = if * impact >= 0.5 { Priority :: Critical } else if * impact >= 0.2 { Priority :: High } else if * impact >= 0.1 { Priority :: Medium } else { Priority :: Low } ; let (impact_weight , benefit , cost , callers , caller_files , outgoing_files) = compute_move_metrics (placement) ; let to = placement . suggested_file . as_ref () . map (\| p \| compress_path (p . to_string_lossy () . as_ref ())) . unwrap_or_else (\| \| "-" . to_string ()) ; items . push (PlanItem { kind : ActionKind :: Cohesion , priority , description : format ! ("`{}` from `{}` to `{}`: {} (impact {:.2})" , placement . name , compress_path (placement . current_file . to_string_lossy () . as_ref ()) , to , reason , impact) , command : String :: new () , current_layer : None , required_layer : None , is_utility : utility_names . contains (& placement . name) , impact_weight , benefit , cost , callers , caller_files , current_file : Some (placement . current_file . clone ()) , target_file : placement . suggested_file . clone () , outgoing_files , name : Some (placement . name . clone ()) , cluster_cohesion : 0.0 , member_count : 0 , }) ; } PlacementStatus :: LayerViolation { current_layer , required_layer , } => { let target_path = resolve_required_layer_path (required_layer , & placement . current_file , directory , root_path ,) ; let to = compress_path (target_path . to_string_lossy () . as_ref ()) ; let (impact_weight , benefit , cost , callers , caller_files , outgoing_files) = compute_move_metrics (placement) ; items . push (PlanItem { kind : ActionKind :: Structural , priority : Priority :: Critical , description : format ! ("`{}` from `{}` to `{}`: layer violation {} -> {}" , placement . name , compress_path (placement . current_file . to_string_lossy () . as_ref ()) , to , current_layer , required_layer) , command : String :: new () , current_layer : Some (current_layer . clone ()) , required_layer : Some (required_layer . clone ()) , is_utility : utility_names . contains (& placement . name) , impact_weight , benefit , cost , callers , caller_files , current_file : Some (placement . current_file . clone ()) , target_file : Some (target_path) , outgoing_files , name : Some (placement . name . clone ()) , cluster_cohesion : 0.0 , member_count : 0 , }) ; } _ => { } } } items } . sig` | 0.60 | intra 5, inter 0 | same 0, other 14 | ok | - |
| `write_structural_batches` | `pub fn write_structural_batches (content : & mut String , items : & [PlanItem]) { if items . is_empty () { return ; } let mut ordered_targets = Vec :: new () ; let mut batches : HashMap < PathBuf , Vec < & PlanItem > > = HashMap :: new () ; for item in items { let Some (target) = & item . target_file else { continue ; } ; let entry = batches . entry (target . clone ()) . or_default () ; if entry . is_empty () { ordered_targets . push (target . clone ()) ; } entry . push (item) ; } content . push_str ("### Phase 3 Batches\n\n") ; content . push_str ("Action: execute batches in order and verify after each batch.\n") ; content . push_str ("Note: each batch targets one destination module.\n\n") ; for (idx , target) in ordered_targets . iter () . enumerate () { let empty : Vec < & PlanItem > = Vec :: new () ; let items = batches . get (target) . unwrap_or (& empty) ; content . push_str (& format ! ("#### Batch {}: target `{}`\n\n" , idx + 1 , compress_path (target . to_string_lossy () . as_ref ()))) ; content . push_str ("Action: move the listed functions into the target module.\n") ; content . push_str ("Note: use the rg commands to locate definitions and callers.\n\n") ; let mut commands : Vec < String > = Vec :: new () ; if ! target . exists () { let target_label = compress_path (target . to_string_lossy () . as_ref ()) ; content . push_str (& format ! ("- Create target file: `{}`\n" , target_label)) ; commands . push (format ! ("touch \"{}\"" , target . to_string_lossy ())) ; } for item in items { let name = item . name . as_deref () . unwrap_or ("function") ; let current = item . current_file . as_ref () . map (\| p \| compress_path (p . to_string_lossy () . as_ref ())) . unwrap_or_else (\| \| "-" . to_string ()) ; let ratio = if item . cost == 0 { 0.0 } else { item . benefit as f64 / item . cost as f64 } ; let caller_hint = if item . callers == 0 { "no external callers" . to_string () } else { format ! ("update {} caller files" , item . callers) } ; content . push_str (& format ! ("- Move `{}` from `{}` (impact {}, benefit/cost {:.2}, touches {} files; {})\n" , name , current , item . impact_weight , ratio , item . cost , caller_hint)) ; if let Some (current_file) = & item . current_file { commands . push (format ! ("rg -n \"{}\" \"{}\"" , name , current_file . to_string_lossy ())) ; } let mut callers = item . caller_files . clone () ; callers . sort () ; callers . dedup () ; if ! callers . is_empty () { content . push_str ("- Update imports in:\n") ; for caller in callers { content . push_str (& format ! ("  - `{}`\n" , compress_path (caller . to_string_lossy () . as_ref ()))) ; commands . push (format ! ("rg -n \"{}\" \"{}\"" , name , caller . to_string_lossy ())) ; } } } content . push_str ("- Verification gate: `cargo test`\n") ; if ! commands . is_empty () { content . push_str ("\n```bash\n") ; for command in commands { content . push_str (& format ! ("{}\n" , command)) ; } content . push_str ("```\n") ; } content . push ('\n') ; } } . sig` | 0.60 | intra 2, inter 0 | same 0, other 3 | ok | - |
| `write_cluster_batches` | `pub fn write_cluster_batches (content : & mut String , plans : & [ClusterPlan] , root_path : & Path) { if plans . is_empty () { return ; } content . push_str ("### Phase 2 Batches\n\n") ; content . push_str ("Action: execute batches in order and verify after each batch.\n") ; content . push_str ("Note: each batch creates or fills a cluster file.\n\n") ; for (idx , plan) in plans . iter () . enumerate () { content . push_str (& format ! ("#### Batch {}: target `{}`\n\n" , idx + 1 , compress_path (plan . target . to_string_lossy () . as_ref ()))) ; content . push_str ("Action: move the listed functions into the target module.\n") ; content . push_str ("Note: use the rg commands to locate definitions and callers.\n\n") ; let mut commands = Vec :: new () ; if ! plan . target . exists () { content . push_str (& format ! ("- Create target file: `{}`\n" , compress_path (plan . target . to_string_lossy () . as_ref ()))) ; commands . push (format ! ("touch \"{}\"" , plan . target . to_string_lossy ())) ; } content . push_str (& format ! ("- Cluster cohesion {:.2}, {} functions\n" , plan . cohesion , plan . members . len ())) ; for member in & plan . members { let file = compress_path (member . file . to_string_lossy () . as_ref ()) ; content . push_str (& format ! ("- Move `{}` from `{}`\n" , member . name , file)) ; commands . push (format ! ("rg -n \"{}\" \"{}\"" , member . name , member . file . to_string_lossy ())) ; commands . push (format ! ("rg -n \"{}\" \"{}\"" , member . name , root_path . to_string_lossy ())) ; } content . push_str ("- Verification gate: `cargo test`\n") ; if ! commands . is_empty () { content . push_str ("\n```bash\n") ; for command in commands { content . push_str (& format ! ("{}\n" , command)) ; } content . push_str ("```\n") ; } content . push ('\n') ; } } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `compress_path` | `# [doc = " Compress absolute paths to MMSB-relative format"] pub fn compress_path (path : & str) -> String { if let Some (idx) = path . find ("/MMSB/") { return format ! ("MMSB{}" , & path [idx + 5 ..]) ; } if path . starts_with ("MMSB/") { return path . to_string () ; } if let Some (idx) = path . rfind ("/src/") { return format ! ("MMSB/src{}" , & path [idx + 4 ..]) ; } path . to_string () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `path_common_prefix_len` | `pub fn path_common_prefix_len (a : & Path , b : & Path) -> isize { let mut count = 0isize ; for (a_comp , b_comp) in a . components () . zip (b . components ()) { if a_comp == b_comp { count += 1 ; } else { break ; } } count } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/110_cohesion_analyzer.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `collect_functions` | `fn collect_functions (result : & AnalysisResult) -> Vec < FunctionInfo > { result . elements . iter () . filter (\| elem \| matches ! (elem . element_type , ElementType :: Function)) . map (\| elem \| FunctionInfo { name : elem . name . clone () , signature : elem . signature . clone () , file_path : elem . file_path . clone () , layer : elem . layer . clone () , calls : elem . calls . clone () , }) . collect () } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | ok | - |
| `build_call_edges` | `fn build_call_edges (result : & AnalysisResult ,) -> (Vec < FunctionInfo > , Vec < HashMap < usize , usize > > , Vec < HashMap < usize , usize > > ,) { let functions = collect_functions (result) ; let name_map = build_name_map (& functions) ; let mut outgoing_counts : Vec < HashMap < usize , usize > > = vec ! [HashMap :: new () ; functions . len ()] ; let mut incoming_counts : Vec < HashMap < usize , usize > > = vec ! [HashMap :: new () ; functions . len ()] ; for (idx , func) in functions . iter () . enumerate () { for call in & func . calls { if let Some (callee_idxs) = name_map . get (call) { for & callee_idx in callee_idxs { * outgoing_counts [idx] . entry (callee_idx) . or_insert (0) += 1 ; * incoming_counts [callee_idx] . entry (idx) . or_insert (0) += 1 ; } } } } (functions , outgoing_counts , incoming_counts) } . sig` | 0.60 | intra 2, inter 0 | same 0, other 2 | ok | - |
| `build_function_layers` | `fn build_function_layers (functions : & [FunctionInfo]) -> HashMap < String , String > { let mut map = HashMap :: new () ; for func in functions { map . entry (func . file_path . clone ()) . or_insert_with (\| \| func . layer . clone ()) ; } map } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `build_type_maps` | `fn build_type_maps (result : & AnalysisResult ,) -> (HashMap < String , HashSet < String > > , HashSet < String >) { let mut file_types : HashMap < String , HashSet < String > > = HashMap :: new () ; let mut all_types : HashSet < String > = HashSet :: new () ; for elem in & result . elements { if matches ! (elem . element_type , ElementType :: Struct \| ElementType :: Enum \| ElementType :: Trait) { let name = elem . name . clone () ; all_types . insert (name . clone ()) ; file_types . entry (elem . file_path . clone ()) . or_default () . insert (name) ; } } (file_types , all_types) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | orphaned | - (suggest module utilities) |
| `build_name_map` | `fn build_name_map (functions : & [FunctionInfo]) -> HashMap < String , Vec < usize > > { let mut map : HashMap < String , Vec < usize > > = HashMap :: new () ; for (idx , func) in functions . iter () . enumerate () { map . entry (func . name . clone ()) . or_default () . push (idx) ; } map } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `build_call_analysis` | `fn build_call_analysis (func : & FunctionInfo , functions : & [FunctionInfo] , outgoing : & HashMap < usize , usize > , incoming : & HashMap < usize , usize > , file_types : & HashMap < String , HashSet < String > > , all_types : & HashSet < String > ,) -> CallAnalysis { let mut intra_file_calls = 0usize ; let mut inter_file_calls : BTreeMap < PathBuf , usize > = BTreeMap :: new () ; for (callee_idx , count) in outgoing { let callee = & functions [* callee_idx] ; if callee . file_path == func . file_path { intra_file_calls += count ; } else { * inter_file_calls . entry (PathBuf :: from (& callee . file_path)) . or_insert (0) += count ; } } let mut calls_from_same_file = 0usize ; let mut calls_from_other_files : BTreeMap < PathBuf , usize > = BTreeMap :: new () ; for (caller_idx , count) in incoming { let caller = & functions [* caller_idx] ; if caller . file_path == func . file_path { calls_from_same_file += count ; } else { * calls_from_other_files . entry (PathBuf :: from (& caller . file_path)) . or_insert (0) += count ; } } let (same_file_type_refs , other_file_type_refs) = compute_type_coupling (func , file_types , all_types) ; CallAnalysis { intra_file_calls , inter_file_calls : inter_file_calls . into_iter () . collect () , calls_from_same_file , calls_from_other_files : calls_from_other_files . into_iter () . collect () , same_file_type_refs , other_file_type_refs , } } . sig` | 0.60 | intra 1, inter 0 | same 0, other 4 | ok | - |
| `determine_status` | `fn determine_status (call_analysis : & CallAnalysis , cohesion_score : f64 , threshold : f64 , layer_violation : Option < (String , String) > ,) -> PlacementStatus { let total_activity = call_analysis . intra_file_calls + call_analysis . calls_from_same_file ; let external_activity : usize = call_analysis . calls_from_other_files . iter () . map (\| (_ , count) \| * count) . sum () ; if total_activity == 0 && external_activity == 0 { return PlacementStatus :: Orphaned { suggested_module : "utilities" . to_string () , } ; } if let Some ((current_layer , required_layer)) = layer_violation { return PlacementStatus :: LayerViolation { current_layer , required_layer , } ; } if cohesion_score >= threshold { return PlacementStatus :: Correct ; } let mut impact = threshold - cohesion_score ; if impact < 0.0 { impact = 0.0 ; } PlacementStatus :: ShouldMove { reason : format ! ("cohesion {:.2} below threshold {:.2}" , cohesion_score , threshold) , impact , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 6 | orphaned | - (suggest module utilities) |
| `suggest_file` | `fn suggest_file (call_analysis : & CallAnalysis) -> Option < PathBuf > { let mut best_file : Option < PathBuf > = None ; let mut best_score = 0usize ; for (file , count) in & call_analysis . calls_from_other_files { if * count > best_score { best_score = * count ; best_file = Some (file . clone ()) ; } } if best_score == 0 { return None ; } if let Some (candidate) = & best_file { let outgoing_to_candidate = call_analysis . inter_file_calls . iter () . find (\| (path , _) \| path == candidate) . map (\| (_ , count) \| * count) . unwrap_or (0) ; if outgoing_to_candidate == 0 && call_analysis . intra_file_calls >= best_score { return None ; } } best_file } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `suggest_cluster_file` | `fn suggest_cluster_file (members : & [usize] , functions : & [FunctionInfo] ,) -> Option < PathBuf > { let mut counts : HashMap < & str , usize > = HashMap :: new () ; for idx in members { let path = functions [* idx] . file_path . as_str () ; * counts . entry (path) . or_insert (0) += 1 ; } counts . into_iter () . max_by_key (\| (_ , count) \| * count) . map (\| (path , _) \| PathBuf :: from (path)) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `compute_type_coupling` | `fn compute_type_coupling (func : & FunctionInfo , file_types : & HashMap < String , HashSet < String > > , all_types : & HashSet < String > ,) -> (usize , usize) { let mut same_file = 0usize ; let mut other_file = 0usize ; let tokens = extract_identifiers (& func . signature) ; let same_set = file_types . get (& func . file_path) ; for token in tokens { if let Some (types) = same_set { if types . contains (& token) { same_file += 1 ; continue ; } } if all_types . contains (& token) { other_file += 1 ; } } (same_file , other_file) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `compute_cluster_cohesion` | `fn compute_cluster_cohesion (members : & [usize] , outgoing_counts : & [HashMap < usize , usize >] ,) -> f64 { let member_set : HashSet < usize > = members . iter () . copied () . collect () ; let mut internal = 0usize ; let mut total = 0usize ; for idx in members { if let Some (outgoing) = outgoing_counts . get (* idx) { for (target , count) in outgoing { total += count ; if member_set . contains (target) { internal += count ; } } } } if total == 0 { 0.0 } else { internal as f64 / total as f64 } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `extract_identifiers` | `fn extract_identifiers (text : & str) -> Vec < String > { let mut tokens = Vec :: new () ; let mut current = String :: new () ; for ch in text . chars () { if ch . is_ascii_alphanumeric () \|\| ch == '_' { current . push (ch) ; } else if ! current . is_empty () { tokens . push (current . clone ()) ; current . clear () ; } } if ! current . is_empty () { tokens . push (current) ; } tokens } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `louvain_communities` | `fn louvain_communities (outgoing_counts : & [HashMap < usize , usize >]) -> Vec < usize > { let n = outgoing_counts . len () ; if n == 0 { return Vec :: new () ; } let (neighbors , degrees , total_weight) = build_undirected_graph (outgoing_counts) ; if total_weight == 0 { return (0 .. n) . collect () ; } let two_m = (2 * total_weight) as f64 ; let mut community : Vec < usize > = (0 .. n) . collect () ; let mut sum_tot = degrees . clone () ; let max_iters = 25 ; for iter in 0 .. max_iters { let mut moved = false ; for node in 0 .. n { let current = community [node] ; let mut neighbor_comms : HashMap < usize , usize > = HashMap :: new () ; for (neighbor , weight) in & neighbors [node] { let comm = community [* neighbor] ; * neighbor_comms . entry (comm) . or_insert (0) += * weight ; } let mut best_comm = current ; let mut best_gain = 0.0f64 ; for (comm , k_i_in) in neighbor_comms { if comm == current { continue ; } let gain = (k_i_in as f64 - (degrees [node] as f64 * sum_tot [comm] as f64) / two_m) / two_m ; if gain > best_gain { best_gain = gain ; best_comm = comm ; } } if best_comm != current && best_gain > 0.0 { community [node] = best_comm ; sum_tot [current] = sum_tot [current] . saturating_sub (degrees [node]) ; sum_tot [best_comm] += degrees [node] ; moved = true ; } } if ! moved { break ; } if iter % 5 == 0 { println ! ("  Louvain pass {}..." , iter + 1) ; } } community } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `build_undirected_graph` | `fn build_undirected_graph (outgoing_counts : & [HashMap < usize , usize >] ,) -> (Vec < Vec < (usize , usize) > > , Vec < usize > , usize) { let n = outgoing_counts . len () ; let mut edge_weights : HashMap < (usize , usize) , usize > = HashMap :: new () ; for (src , outgoing) in outgoing_counts . iter () . enumerate () { for (dst , weight) in outgoing { let (a , b) = if src <= * dst { (src , * dst) } else { (* dst , src) } ; * edge_weights . entry ((a , b)) . or_insert (0) += * weight ; } } let mut neighbors = vec ! [Vec :: new () ; n] ; let mut degrees = vec ! [0usize ; n] ; let mut total_weight = 0usize ; for ((a , b) , weight) in edge_weights { if a == b { continue ; } neighbors [a] . push ((b , weight)) ; neighbors [b] . push ((a , weight)) ; degrees [a] += weight ; degrees [b] += weight ; total_weight += weight ; } (neighbors , degrees , total_weight) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/120_directory_analyzer.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `is_source_file` | `fn is_source_file (path : & Path) -> bool { matches ! (path . extension () . and_then (\| e \| e . to_str ()) , Some ("rs") \| Some ("jl")) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `should_skip_path` | `fn should_skip_path (path : & Path) -> bool { let Some (name) = path . file_name () else { return false ; } ; name == "target" \|\| name == ".git" \|\| name == "tools" \|\| name == "examples" \|\| name == "docs" \|\| name == "xtask" \|\| name == ".julia" \|\| name == "test" \|\| name == "tests" \|\| name == "benches" } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/130_control_flow.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `sanitize_identifier` | `fn sanitize_identifier (name : & str) -> String { name . chars () . map (\| c \| if c . is_ascii_alphanumeric () { c } else { '_' }) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/140_file_ordering.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `parallel_build_file_dag` | `# [allow (dead_code)] pub fn parallel_build_file_dag (directories : & [PathBuf]) -> Result < DiGraph < PathBuf , () > > { let subgraphs : Vec < DiGraph < PathBuf , () > > = directories . par_iter () . map (\| dir \| crate :: dependency :: build_directory_dag (dir)) . collect :: < Result < _ > > () ? ; let mut merged = DiGraph :: new () ; let mut node_map : HashMap < PathBuf , NodeIndex > = HashMap :: new () ; for subgraph in subgraphs { for node in subgraph . node_indices () { let file = subgraph [node] . clone () ; node_map . entry (file . clone ()) . or_insert_with (\| \| merged . add_node (file)) ; } for edge in subgraph . edge_indices () { if let Some ((src , dst)) = subgraph . edge_endpoints (edge) { let src_file = subgraph [src] . clone () ; let dst_file = subgraph [dst] . clone () ; let src_node = * node_map . get (& src_file) . expect ("missing source node") ; let dst_node = * node_map . get (& dst_file) . expect ("missing target node") ; merged . add_edge (src_node , dst_node , ()) ; } } } Ok (merged) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/150_julia_parser.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `slugify_relative` | `fn slugify_relative (root : & Path , path : & Path) -> String { let relative = path . strip_prefix (root) . unwrap_or (path) ; relative . components () . map (\| c \| c . as_os_str () . to_string_lossy () . replace ('.' , "_")) . collect :: < Vec < _ > > () . join ("-") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `resolve_julia_binary` | `fn resolve_julia_binary () -> PathBuf { if let Ok (custom) = env :: var ("JULIA_BINARY") { let candidate = PathBuf :: from (custom) ; if candidate . exists () { return candidate ; } } if let Ok (home) = env :: var ("HOME") { let juliaup_root = Path :: new (& home) . join (".julia/juliaup") ; if let Ok (entries) = fs :: read_dir (& juliaup_root) { for entry in entries . flatten () { let path = entry . path () ; if ! path . is_dir () { continue ; } if let Some (name) = path . file_name () . and_then (\| n \| n . to_str ()) { if name . starts_with ("julia-") { let candidate = path . join ("bin/julia") ; if candidate . exists () { return candidate ; } } } } } } let alt = PathBuf :: from ("/home/cicero-arch-omen/git/julia/usr/bin/julia") ; if alt . exists () { return alt ; } PathBuf :: from ("julia") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `find_julia_project_dir` | `fn find_julia_project_dir (script_path : & Path) -> PathBuf { let mut current = script_path . parent () ; while let Some (dir) = current { if dir . join ("Project.toml") . exists () { return dir . to_path_buf () ; } current = dir . parent () ; } script_path . parent () . unwrap_or_else (\| \| Path :: new (".")) . to_path_buf () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `parse_module_name` | `fn parse_module_name (line : & str) -> Option < String > { if line . starts_with ("module ") { return line . split_whitespace () . nth (1) . map (\| name \| name . trim_end_matches (';') . to_string ()) ; } if line . starts_with ("baremodule ") { return line . split_whitespace () . nth (1) . map (\| name \| name . trim_end_matches (';') . to_string ()) ; } None } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `parse_struct_name` | `fn parse_struct_name (line : & str) -> Option < String > { if line . starts_with ("mutable struct ") \|\| line . starts_with ("struct ") { let offset = if line . starts_with ("mutable struct ") { 2 } else { 1 } ; let tokens : Vec < & str > = line . split_whitespace () . collect () ; return tokens . get (offset) . map (\| name \| { name . split ("<:") . next () . unwrap_or (name) . trim_end_matches ('{') . to_string () }) ; } None } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `relativize_path` | `fn relativize_path (path : & Path , root : & Path) -> String { if let Ok (stripped) = path . strip_prefix (root) { stripped . to_string_lossy () . to_string () } else { path . to_string_lossy () . to_string () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `extract_calls_from_lines` | `fn extract_calls_from_lines (lines : & [String]) -> Vec < String > { let joined = lines . join ("\n") ; extract_calls_from_text (& joined) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `extract_calls_from_text` | `fn extract_calls_from_text (text : & str) -> Vec < String > { let mut calls = Vec :: new () ; for capture in CALL_RE . captures_iter (text) { if let Some (name) = capture . get (1) { let identifier = name . as_str () ; if is_reserved (identifier) { continue ; } calls . push (identifier . to_string ()) ; } } calls . sort () ; calls . dedup () ; calls } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `is_reserved` | `fn is_reserved (name : & str) -> bool { matches ! (name , "if" \| "for" \| "while" \| "begin" \| "let" \| "struct" \| "mutable" \| "quote" \| "macro" \| "module" \| "end" \| "baremodule" \| "function") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `paren_balance` | `fn paren_balance (input : & str) -> i32 { let mut balance = 0i32 ; for ch in input . chars () { if ch == '(' { balance += 1 ; } else if ch == ')' { balance -= 1 ; } } balance } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/160_rust_parser.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `relativize_path` | `fn relativize_path (path : & Path , root : & Path) -> String { if let Ok (stripped) = path . strip_prefix (root) { stripped . to_string_lossy () . to_string () } else { path . to_string_lossy () . to_string () } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `expr_snippet` | `fn expr_snippet (expr : & syn :: Expr) -> String { truncate_label (quote :: quote ! (# expr) . to_string ()) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `pat_snippet` | `fn pat_snippet (pat : & syn :: Pat) -> String { truncate_label (quote :: quote ! (# pat) . to_string ()) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `truncate_label` | `fn truncate_label (text : String) -> String { let collapsed = text . split_whitespace () . collect :: < Vec < _ > > () . join (" ") ; let mut label = collapsed ; if label . len () > 80 { label . truncate (77) ; label . push_str ("...") ; } label } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |

## File: src/180_report.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `placement_status_label` | `fn placement_status_label (status : & PlacementStatus) -> String { match status { PlacementStatus :: Correct => "ok" . to_string () , PlacementStatus :: ShouldMove { .. } => "move" . to_string () , PlacementStatus :: Orphaned { .. } => "orphaned" . to_string () , PlacementStatus :: LayerViolation { .. } => "layer violation" . to_string () , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 5 | orphaned | - (suggest module utilities) |
| `placement_status_notes` | `fn placement_status_notes (status : & PlacementStatus) -> String { match status { PlacementStatus :: Correct => String :: new () , PlacementStatus :: ShouldMove { reason , impact } => { format ! ("{} (impact {:.2})" , reason , impact) } PlacementStatus :: Orphaned { suggested_module } => { format ! ("suggest module {}" , suggested_module) } PlacementStatus :: LayerViolation { current_layer , required_layer , } => format ! ("{} -> {}" , current_layer , required_layer) , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 5 | orphaned | - (suggest module utilities) |
| `collect_utility_candidates` | `fn collect_utility_candidates (placements : & [FunctionPlacement]) -> Vec < String > { let mut candidates = BTreeSet :: new () ; for placement in placements { let external_files = placement . call_analysis . calls_from_other_files . len () ; if external_files >= 3 { candidates . insert (format ! ("`{}` called by {} files (suggest `utilities`)" , placement . name , external_files)) ; } } candidates . into_iter () . collect () } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `is_entrypoint_main` | `fn is_entrypoint_main (entry : & FunctionPlacement) -> bool { entry . name == "main" && entry . current_file . ends_with (Path :: new ("src/190_main.rs")) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `referenced_elsewhere` | `fn referenced_elsewhere (entry : & FunctionPlacement , references : & HashMap < String , HashSet < PathBuf > > ,) -> bool { let Some (files) = references . get (& entry . name) else { return false ; } ; files . iter () . any (\| path \| ! path_matches (& entry . current_file , path)) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `is_dead_code_candidate` | `fn is_dead_code_candidate (entry : & FunctionPlacement , dead_code : & HashMap < String , HashSet < PathBuf > > ,) -> bool { let Some (paths) = dead_code . get (& entry . name) else { return false ; } ; if paths . is_empty () { return true ; } paths . iter () . any (\| path \| path_matches (& entry . current_file , path)) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `filter_orphaned` | `fn filter_orphaned < 'a > (placements : & 'a [FunctionPlacement] , root_path : & Path , output_dir : & str ,) -> (Vec < & 'a FunctionPlacement > , Vec < & 'a FunctionPlacement >) { let references = collect_symbol_references (root_path) ; let dead_code = load_cargo_warnings (output_dir) . as_deref () . map (parse_dead_code_warnings) . unwrap_or_default () ; let mut orphaned = Vec :: new () ; let mut delete_candidates = Vec :: new () ; for entry in placements . iter () . filter (\| p \| matches ! (p . placement_status , PlacementStatus :: Orphaned { .. })) { if is_entrypoint_main (entry) { continue ; } if let Some (true) = is_public_function (& entry . current_file , & entry . name) { if referenced_elsewhere (entry , & references) { continue ; } } let is_delete_candidate = is_dead_code_candidate (entry , & dead_code) ; if is_delete_candidate { delete_candidates . push (entry) ; } orphaned . push (entry) ; } (orphaned , delete_candidates) } . sig` | 0.60 | intra 6, inter 0 | same 0, other 4 | ok | - |
| `collect_directories` | `fn collect_directories < 'a > (node : & 'a DirectoryAnalysis , acc : & mut Vec < & 'a DirectoryAnalysis >) { acc . push (node) ; for child in & node . subdirectories { collect_directories (child , acc) ; } } . sig` | 0.60 | intra 1, inter 0 | same 0, other 2 | ok | - |
| `compute_ordering_correctness` | `fn compute_ordering_correctness (rust_ordering : & FileOrderingResult , julia_ordering : & FileOrderingResult ,) -> f64 { let mut total = 0usize ; let mut correct = 0usize ; for ordering in [rust_ordering , julia_ordering] { total += ordering . ordered_files . len () ; correct += ordering . ordered_files . len () . saturating_sub (ordering . violations . len ()) ; } if total == 0 { 1.0 } else { correct as f64 / total as f64 } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 2 | orphaned | - (suggest module utilities) |
| `compute_directory_cohesion` | `fn compute_directory_cohesion (placements : & [FunctionPlacement]) -> f64 { let mut intra = 0usize ; let mut inter = 0usize ; for placement in placements { let current_dir = placement . current_file . parent () . map (\| p \| p . to_path_buf ()) ; intra += placement . call_analysis . intra_file_calls ; for (file , count) in & placement . call_analysis . inter_file_calls { let same_dir = current_dir . as_ref () . and_then (\| dir \| file . parent () . map (\| p \| p == dir)) . unwrap_or (false) ; if same_dir { intra += count ; } else { inter += count ; } } } let total = intra + inter ; if total == 0 { 1.0 } else { intra as f64 / total as f64 } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | orphaned | - (suggest module utilities) |
| `language_label` | `fn language_label (language : & Language) -> & 'static str { match language { Language :: Rust => "Rust" , Language :: Julia => "Julia" , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 3 | orphaned | - (suggest module utilities) |
| `visibility_label` | `fn visibility_label (vis : & Visibility) -> & 'static str { match vis { Visibility :: Public => "pub" , Visibility :: Crate => "pub(crate)" , Visibility :: Private => "priv" , } } . sig` | 0.60 | intra 0, inter 0 | same 0, other 4 | orphaned | - (suggest module utilities) |
| `collect_size_warnings` | `fn collect_size_warnings (directory : & DirectoryAnalysis , config : & ReportConfig , warnings : & mut Vec < String > ,) { if directory . files . len () >= config . dir_file_warning { warnings . push (format ! ("Directory `{}` has {} files; consider splitting into submodules." , compress_path (directory . path . to_string_lossy () . as_ref ()) , directory . files . len ())) ; } for file in & directory . files { if let Ok (contents) = fs :: read_to_string (file) { let lines = contents . lines () . count () ; if lines >= config . file_line_warning { warnings . push (format ! ("File `{}` has {} lines; consider extracting helpers." , compress_path (file . to_string_lossy () . as_ref ()) , lines)) ; } } } for child in & directory . subdirectories { collect_size_warnings (child , config , warnings) ; } } . sig` | 0.75 | intra 1, inter 0 | same 1, other 1 | ok | - |
| `directory_moves_to_plan` | `fn directory_moves_to_plan (label : & str , moves : Vec < DirectoryMove >) -> Vec < PlanItem > { moves . into_iter () . map (\| item \| PlanItem { kind : ActionKind :: Ordering , priority : Priority :: Medium , description : format ! ("[{}] dir `{}` -> `{}`" , label , compress_path (item . from . to_string_lossy () . as_ref ()) , compress_path (item . to . to_string_lossy () . as_ref ())) , command : format ! ("git mv \"{}\" \"{}\"" , item . from . to_string_lossy () , item . to . to_string_lossy ()) , current_layer : None , required_layer : None , is_utility : false , impact_weight : 0 , benefit : 0 , cost : 1 , callers : 0 , caller_files : Vec :: new () , current_file : Some (item . from . clone ()) , target_file : Some (item . to . clone ()) , outgoing_files : Vec :: new () , name : None , cluster_cohesion : 0.0 , member_count : 0 , }) . collect () } . sig` | 0.84 | intra 0, inter 0 | same 4, other 1 | orphaned | - (suggest module utilities) |
| `collect_rename_items` | `fn collect_rename_items (ordering : & FileOrderingResult , label : & str) -> Vec < PlanItem > { let mut layer_violation_files = BTreeSet :: new () ; for violation in & ordering . layer_violations { layer_violation_files . insert (violation . to . clone ()) ; } ordering . ordered_files . iter () . filter (\| entry \| entry . needs_rename) . map (\| entry \| { let from = entry . current_path . clone () ; let to = entry . current_path . parent () . map (\| p \| p . join (& entry . suggested_name)) . unwrap_or_else (\| \| PathBuf :: from (& entry . suggested_name)) ; let priority = if layer_violation_files . contains (& entry . current_path) { Priority :: Critical } else { Priority :: Medium } ; PlanItem { kind : ActionKind :: Ordering , priority , description : format ! ("[{}] `{}` -> `{}`" , label , compress_path (from . to_string_lossy () . as_ref ()) , compress_path (to . to_string_lossy () . as_ref ())) , command : format ! ("git mv \"{}\" \"{}\"" , from . to_string_lossy () , to . to_string_lossy ()) , current_layer : None , required_layer : None , is_utility : false , impact_weight : 0 , benefit : 0 , cost : 1 , callers : 0 , caller_files : Vec :: new () , current_file : Some (from . clone ()) , target_file : Some (to . clone ()) , outgoing_files : Vec :: new () , name : None , cluster_cohesion : 0.0 , member_count : 0 , } }) . collect () } . sig` | 0.85 | intra 0, inter 0 | same 5, other 1 | orphaned | - (suggest module utilities) |
| `display_path` | `fn display_path (path : & Path , root_path : & Path) -> String { let relative = path . strip_prefix (root_path) . unwrap_or (path) ; relative . to_string_lossy () . to_string () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `write_priority_section` | `fn write_priority_section (content : & mut String , title : & str , items : & [PlanItem]) { content . push_str (& format ! ("## {}\n\n" , title)) ; let (action , note) = match title { "Phase 1: Correctness Blockers" => ("fix these first; they block correctness or builds." , "empty means no critical blockers detected." ,) , "Phase 2: Cluster Extraction" => ("create the listed cluster files and move the grouped functions." , "use the batches below to keep changes small." ,) , "Phase 3: Structural Constraints" => ("resolve the layer violations by moving functions to target modules." , "follow batch order to avoid cascading dependency churn." ,) , "Phase 4: Cohesion Improvements" => ("optional: improve cohesion by moving functions to better-fit modules." , "safe to defer unless you are actively refactoring." ,) , "Phase 5: Ordering & Renames" => ("optional: rename files to match ordering conventions." , "update module paths and imports after renames." ,) , _ => ("review items" , "no additional guidance available.") , } ; content . push_str (& format ! ("Action: {}\n" , action)) ; content . push_str (& format ! ("Note: {}\n\n" , note)) ; if items . is_empty () { content . push_str ("- None.\n\n") ; return ; } let mut commands = Vec :: new () ; for item in items { content . push_str (& format ! ("- {}\n" , item . description)) ; if ! item . command . is_empty () { commands . push (item . command . clone ()) ; } } content . push ('\n') ; if ! commands . is_empty () { content . push_str ("```bash\n") ; for cmd in commands { content . push_str (& format ! ("{}\n" , cmd)) ; } content . push_str ("```\n\n") ; } } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `write_structural_tips` | `fn write_structural_tips (content : & mut String , items : & [PlanItem]) { if items . is_empty () { return ; } content . push_str ("### Phase 3 Tips\n\n") ; content . push_str ("Action: apply these guidelines while executing Phase 3 batches.\n") ; content . push_str ("Note: these are advisory, not checklist items.\n\n") ; content . push_str ("- Move lowest-layer helpers first; higher layers should depend on stable primitives.\n") ; content . push_str ("- Keep moves small: move one function + update imports + rerun tests.\n") ; content . push_str ("- If a target module is missing, create it before moving functions.\n") ; content . push_str ("- Prefer consolidating shared utilities into their destination layer once.\n") ; content . push_str ("- Avoid touching `_old/` unless explicitly refactoring archives.\n\n") ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `write_cluster_tips` | `fn write_cluster_tips (content : & mut String , plans : & [ClusterPlan]) { if plans . is_empty () { return ; } content . push_str ("### Phase 2 Tips\n\n") ; content . push_str ("Action: apply these guidelines while executing Phase 2 batches.\n") ; content . push_str ("Note: these are advisory, not checklist items.\n\n") ; content . push_str ("- Extract clusters as a unit; avoid splitting a cluster across files.\n") ; content . push_str ("- Prefer creating new files before moving functions to keep diffs small.\n") ; content . push_str ("- After each batch, update imports and run tests to lock in behavior.\n\n") ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `sort_plan_items` | `fn sort_plan_items (items : & mut Vec < PlanItem >) { items . sort_by (\| a , b \| { a . priority . cmp (& b . priority) . then_with (\| \| a . description . cmp (& b . description)) }) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `sort_cluster_items` | `fn sort_cluster_items (items : & mut Vec < PlanItem >) { items . sort_by (\| a , b \| { b . cluster_cohesion . partial_cmp (& a . cluster_cohesion) . unwrap_or (Ordering :: Equal) . then_with (\| \| b . member_count . cmp (& a . member_count)) . then_with (\| \| a . description . cmp (& b . description)) }) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `cluster_priority` | `fn cluster_priority (cohesion : f64) -> Priority { if cohesion >= 0.8 { Priority :: Critical } else if cohesion >= 0.6 { Priority :: High } else if cohesion >= 0.4 { Priority :: Medium } else { Priority :: Low } } . sig` | 0.90 | intra 0, inter 0 | same 5, other 0 | ok | - |
| `collect_cluster_items` | `fn collect_cluster_items (plans : & [ClusterPlan]) -> Vec < PlanItem > { plans . iter () . map (\| plan \| PlanItem { kind : ActionKind :: Cluster , priority : cluster_priority (plan . cohesion) , description : format ! ("Create cluster file `{}` with {} functions (cohesion {:.2})" , compress_path (plan . target . to_string_lossy () . as_ref ()) , plan . members . len () , plan . cohesion) , command : format ! ("touch \"{}\"" , plan . target . to_string_lossy ()) , current_layer : None , required_layer : None , is_utility : false , impact_weight : 0 , benefit : 0 , cost : 1 , callers : 0 , caller_files : Vec :: new () , current_file : None , target_file : Some (plan . target . clone ()) , outgoing_files : Vec :: new () , name : None , cluster_cohesion : plan . cohesion , member_count : plan . members . len () , }) . collect () } . sig` | 0.90 | intra 1, inter 0 | same 4, other 0 | ok | - |
| `load_cargo_warnings` | `fn load_cargo_warnings (output_dir : & str) -> Option < String > { let path = Path :: new (output_dir) . join ("cargo_warnings.txt") ; if ! path . exists () { return None ; } fs :: read_to_string (path) . ok () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `parse_dead_code_warnings` | `fn parse_dead_code_warnings (warnings : & str) -> HashMap < String , HashSet < PathBuf > > { let mut dead_code = HashMap :: new () ; let mut lines = warnings . lines () . peekable () ; while let Some (line) = lines . next () { let trimmed = line . trim () ; if ! trimmed . starts_with ("warning:") { continue ; } let Some (name_start) = trimmed . find ("function `") else { continue ; } ; let rest = & trimmed [name_start + "function `" . len () ..] ; let Some (name_end) = rest . find ('`') else { continue ; } ; let name = & rest [.. name_end] ; if ! trimmed . contains ("is never used") { continue ; } let mut warn_path : Option < PathBuf > = None ; if let Some (next) = lines . peek () { let next_trimmed = next . trim () ; if let Some (path_start) = next_trimmed . find ("--> ") { let path_part = & next_trimmed [path_start + 4 ..] ; if let Some (path_end) = path_part . find (':') { warn_path = Some (PathBuf :: from (& path_part [.. path_end])) ; } } } dead_code . entry (name . to_string ()) . or_insert_with (HashSet :: new) . extend (warn_path) ; } dead_code } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `parse_use_symbols` | `fn parse_use_symbols (line : & str) -> Vec < String > { let mut symbols = Vec :: new () ; let Some (use_idx) = line . find ("use ") else { return symbols ; } ; let mut clause = line [use_idx + 4 ..] . trim () ; if let Some (end_idx) = clause . find (';') { clause = clause [.. end_idx] . trim () ; } clause = clause . strip_prefix ("crate::") . unwrap_or (clause) ; clause = clause . strip_prefix ("self::") . unwrap_or (clause) ; if let Some (brace_start) = clause . find ('{') { let brace_end = clause . rfind ('}') . unwrap_or (clause . len ()) ; let inner = & clause [brace_start + 1 .. brace_end] ; for item in inner . split (',') { let item = item . trim () ; if item . is_empty () \|\| item == "*" \|\| item == "self" \|\| item == "super" { continue ; } let item = item . split (" as ") . next () . unwrap_or (item) . trim () ; let last = item . rsplit ("::") . next () . unwrap_or (item) ; if ! last . is_empty () { symbols . push (last . to_string ()) ; } } } else { let last = clause . rsplit ("::") . next () . unwrap_or (clause) . trim () ; if ! last . is_empty () && last != "*" && last != "self" && last != "super" { symbols . push (last . to_string ()) ; } } symbols } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `scan_crate_paths` | `fn scan_crate_paths (line : & str) -> Vec < String > { let mut symbols = Vec :: new () ; let mut idx = 0 ; while let Some (found) = line [idx ..] . find ("crate::") { let start = idx + found + "crate::" . len () ; let mut end = start ; for ch in line [start ..] . chars () { if ch . is_ascii_alphanumeric () \|\| ch == '_' \|\| ch == ':' { end += ch . len_utf8 () ; } else { break ; } } if end > start { let path = & line [start .. end] ; if let Some (last) = path . rsplit ("::") . next () { if ! last . is_empty () { symbols . push (last . to_string ()) ; } } } idx = end ; } symbols } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `collect_symbol_references` | `fn collect_symbol_references (root_path : & Path) -> HashMap < String , HashSet < PathBuf > > { let mut references : HashMap < String , HashSet < PathBuf > > = HashMap :: new () ; let src_dir = root_path . join ("src") ; for entry in WalkDir :: new (& src_dir) . into_iter () . filter_map (\| e \| e . ok ()) { let path = entry . path () ; if ! path . is_file () \|\| path . extension () . and_then (\| e \| e . to_str ()) != Some ("rs") { continue ; } let Ok (contents) = fs :: read_to_string (path) else { continue ; } ; for line in contents . lines () { if line . contains ("use crate::") { for symbol in parse_use_symbols (line) { references . entry (symbol) . or_insert_with (HashSet :: new) . insert (path . to_path_buf ()) ; } } if line . contains ("crate::") { for symbol in scan_crate_paths (line) { references . entry (symbol) . or_insert_with (HashSet :: new) . insert (path . to_path_buf ()) ; } } } } references } . sig` | 0.90 | intra 2, inter 0 | same 0, other 0 | ok | - |
| `is_public_function` | `fn is_public_function (file_path : & Path , name : & str) -> Option < bool > { let Ok (contents) = fs :: read_to_string (file_path) else { return None ; } ; let needle = format ! ("fn {}" , name) ; for line in contents . lines () { if let Some (pos) = line . find (& needle) { let prefix = line [.. pos] . trim_start () ; return Some (prefix . starts_with ("pub")) ; } } None } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `path_matches` | `fn path_matches (entry_path : & Path , candidate : & Path) -> bool { entry_path == candidate \|\| entry_path . ends_with (candidate) \|\| candidate . ends_with (entry_path) } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | ok | - |
| `load_report_config` | `fn load_report_config (output_dir : & str) -> ReportConfig { let path = Path :: new (output_dir) . join ("analyzer_config.toml") ; let mut config = ReportConfig :: defaults () ; let Ok (contents) = fs :: read_to_string (path) else { return config ; } ; for line in contents . lines () { let trimmed = line . trim () ; if trimmed . is_empty () \|\| trimmed . starts_with ('#') { continue ; } let Some ((key , value)) = trimmed . split_once ('=') else { continue ; } ; let key = key . trim () ; let value = value . trim () . trim_matches ('"') ; match key { "file_line_warning" => { if let Ok (parsed) = value . parse :: < usize > () { config . file_line_warning = parsed ; } } "dir_file_warning" => { if let Ok (parsed) = value . parse :: < usize > () { config . dir_file_warning = parsed ; } } "baseline_path" => { if ! value . is_empty () { config . baseline_path = value . to_string () ; } } "naming_score_warning" => { if let Ok (parsed) = value . parse :: < f64 > () { config . naming_score_warning = parsed ; } } _ => { } } } config } . sig` | 0.90 | intra 0, inter 0 | same 2, other 0 | orphaned | - (suggest module utilities) |
| `load_baseline_metrics` | `fn load_baseline_metrics (config : & ReportConfig , output_dir : & str) -> Option < HashMap < String , f64 > > { let path = Path :: new (output_dir) . join (& config . baseline_path) ; let Ok (contents) = fs :: read_to_string (path) else { return None ; } ; let mut metrics = HashMap :: new () ; for line in contents . lines () { let trimmed = line . trim () ; if trimmed . is_empty () \|\| trimmed . starts_with ('#') { continue ; } let Some ((key , value)) = trimmed . split_once ('=') else { continue ; } ; let key = key . trim () . to_string () ; let value = value . trim () ; if let Ok (parsed) = value . parse :: < f64 > () { metrics . insert (key , parsed) ; } } Some (metrics) } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `baseline_deltas` | `fn baseline_deltas (baseline : & HashMap < String , f64 > , dir_cohesion : f64 , ordering_correctness : f64 , avg_cohesion : f64 , renames_len : usize , relocations : usize ,) -> Vec < String > { let mut deltas = Vec :: new () ; if let Some (prev) = baseline . get ("directory_cohesion") { deltas . push (format ! ("directory_cohesion: {:.2} -> {:.2} (delta {:+.2})" , prev , dir_cohesion , dir_cohesion - prev)) ; } if let Some (prev) = baseline . get ("ordering_correctness") { let current = ordering_correctness * 100.0 ; deltas . push (format ! ("ordering_correctness: {:.1}% -> {:.1}% (delta {:+.1}%)" , prev , current , current - prev)) ; } if let Some (prev) = baseline . get ("avg_function_cohesion") { deltas . push (format ! ("avg_function_cohesion: {:.2} -> {:.2} (delta {:+.2})" , prev , avg_cohesion , avg_cohesion - prev)) ; } if let Some (prev) = baseline . get ("rename_ops_needed") { let current = renames_len as f64 ; deltas . push (format ! ("rename_ops_needed: {:.0} -> {} (delta {:+.0})" , prev , renames_len , current - prev)) ; } if let Some (prev) = baseline . get ("function_relocations") { let current = relocations as f64 ; deltas . push (format ! ("function_relocations: {:.0} -> {} (delta {:+.0})" , prev , relocations , current - prev)) ; } deltas } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `write_baseline_metrics` | `fn write_baseline_metrics (config : & ReportConfig , output_dir : & str , dir_cohesion : f64 , ordering_correctness : f64 , avg_cohesion : f64 , renames_len : usize , relocations : usize ,) { let path = Path :: new (output_dir) . join (& config . baseline_path) ; if path . exists () { return ; } let content = format ! ("directory_cohesion={:.2}\nordering_correctness={:.1}\navg_function_cohesion={:.2}\nrename_ops_needed={}\nfunction_relocations={}\n" , dir_cohesion , ordering_correctness * 100.0 , avg_cohesion , renames_len , relocations) ; let _ = fs :: write (path , content) ; } . sig` | 0.90 | intra 0, inter 0 | same 1, other 0 | orphaned | - (suggest module utilities) |
| `slugify_path` | `fn slugify_path (path : & Path) -> String { let mut slug = String :: new () ; for component in path . components () { if ! slug . is_empty () { slug . push_str ("__") ; } slug . push_str (& component . as_os_str () . to_string_lossy () . replace ('/' , "_")) ; } if slug . is_empty () { "root" . to_string () } else { slug } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `render_mermaid_graph` | `fn render_mermaid_graph (graph : & petgraph :: graph :: DiGraph < PathBuf , () >) -> String { let mut output = String :: from ("```mermaid\ngraph TD\n") ; let mut node_ids : HashMap < usize , String > = HashMap :: new () ; let mut idx = 0usize ; for node in graph . node_indices () { let node_name = graph [node] . file_name () . and_then (\| n \| n . to_str ()) . unwrap_or ("file") ; let safe_id = format ! ("F{}" , idx) ; idx += 1 ; node_ids . insert (node . index () , safe_id . clone ()) ; output . push_str (& format ! ("    {}[\"{}\"]\n" , safe_id , node_name)) ; } for edge in graph . edge_indices () { if let Some ((src , dst)) = graph . edge_endpoints (edge) { if let (Some (from) , Some (to)) = (node_ids . get (& src . index ()) , node_ids . get (& dst . index ())) { output . push_str (& format ! ("    {} --> {}\n" , from , to)) ; } } } output . push_str ("```\n") ; output } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `prefix_key_from_path` | `fn prefix_key_from_path (path : & str) -> String { let relative = path . strip_prefix ("MMSB/") . unwrap_or (path) ; if relative . is_empty () { return "root" . to_string () ; } let parts : Vec < & str > = relative . split ('/') . collect () ; if parts . len () == 1 { return "root" . to_string () ; } if parts [0] == "src" && parts . len () >= 2 { return format ! ("{}/{}" , parts [0] , parts [1]) ; } parts [0] . to_string () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `slugify_key` | `fn slugify_key (input : & str) -> String { input . chars () . map (\| c \| match c { '/' => '-' , ' ' => '_' , _ if c . is_ascii_alphanumeric () \|\| c == '-' => c . to_ascii_lowercase () , _ => '_' , }) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `group_key_cmp` | `fn group_key_cmp (a : & str , b : & str) -> Ordering { match (a == "root" , b == "root") { (true , true) => Ordering :: Equal , (true , false) => Ordering :: Less , (false , true) => Ordering :: Greater , _ => a . cmp (b) , } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `function_bucket_label` | `fn function_bucket_label (name : & str) -> & 'static str { let first = name . chars () . find (\| c \| c . is_ascii_alphabetic ()) . map (\| c \| c . to_ascii_uppercase ()) . unwrap_or ('#') ; match first { 'A' ..= 'F' => "A-F" , 'G' ..= 'M' => "G-M" , 'N' ..= 'S' => "N-S" , 'T' ..= 'Z' => "T-Z" , _ => "Other" , } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `slugify_file_path` | `fn slugify_file_path (path : & str) -> String { path . trim_start_matches ("MMSB/") . replace ('/' , "-") . replace ('.' , "_") . to_lowercase () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `short_signature` | `fn short_signature (input : & str) -> String { let collapsed = input . split_whitespace () . collect :: < Vec < _ > > () . join (" ") ; if collapsed . len () > 120 { let mut truncated = collapsed . chars () . take (117) . collect :: < String > () ; truncated . push_str ("...") ; truncated } else { collapsed } } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `normalize_use_stmt` | `fn normalize_use_stmt (stmt : & str) -> String { let collapsed = stmt . replace ('\n' , " ") ; let mut cleaned = collapsed . split_whitespace () . collect :: < Vec < _ > > () . join (" ") ; if let Some (idx) = cleaned . find (';') { cleaned . truncate (idx) ; } cleaned = cleaned . trim () . to_string () ; if cleaned . starts_with ("pub") { if let Some (pos) = cleaned . find (' ') { cleaned = cleaned [pos + 1 ..] . trim () . to_string () ; } } if let Some (stripped) = cleaned . strip_prefix ("use ") { cleaned = stripped . trim () . to_string () ; } cleaned } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `sanitize_mermaid_id` | `fn sanitize_mermaid_id (input : & str) -> String { input . chars () . map (\| c \| if c . is_ascii_alphanumeric () { c } else { '_' }) . collect () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |
| `sanitize_mermaid_label` | `fn sanitize_mermaid_label (label : & str) -> String { label . replace ('"' , "'") . replace ('`' , "'") } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/190_main.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `main` | `fn main () -> Result < () > { let args : Vec < String > = std :: env :: args () . collect () ; if args . len () > 1 && args [1] == "agent" { return agent_cli :: run_agent_cli () ; } crate :: layer_utilities :: main () } . sig` | 0.90 | intra 0, inter 0 | same 0, other 0 | orphaned | - (suggest module utilities) |

## File: src/191_agent_cli.rs

| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |
| --- | --- | --- | --- | --- | --- | --- |
| `check_action` | `# [doc = " Check if an action is allowed"] fn check_action (action_path : & PathBuf , conscience_path : & PathBuf) -> Result < () > { let action_json = std :: fs :: read_to_string (action_path) ? ; let action : AgentAction = serde_json :: from_str (& action_json) ? ; let invariants = load_invariants (conscience_path) ? ; let conscience = AgentConscience :: new (invariants) ; let result = conscience . check_action (& action) ; let output = serde_json :: to_string_pretty (& result) ? ; println ! ("{}" , output) ; std :: process :: exit (if result . allowed { 0 } else { 1 }) ; } . sig` | 0.60 | intra 2, inter 0 | same 0, other 2 | ok | - |
| `query_function` | `# [doc = " Query allowed actions for a function"] fn query_function (function : & str , conscience_path : & PathBuf) -> Result < () > { let invariants = load_invariants (conscience_path) ? ; let conscience = AgentConscience :: new (invariants) ; let allowed = conscience . query_allowed_actions (function) ; let output = serde_json :: to_string_pretty (& allowed) ? ; println ! ("{}" , output) ; Ok (()) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `show_stats` | `# [doc = " Show conscience statistics"] fn show_stats (conscience_path : & PathBuf) -> Result < () > { let invariants = load_invariants (conscience_path) ? ; let conscience = AgentConscience :: new (invariants) ; let stats = conscience . stats () ; println ! ("Conscience Statistics") ; println ! ("====================\n") ; println ! ("Total invariants:    {}" , stats . total_invariants) ; println ! ("Blocking invariants: {}" , stats . blocking_invariants) ; println ! ("Total constraints:   {}" , stats . total_constraints) ; println ! () ; println ! ("By strength:") ; println ! ("  Proven:     {}" , stats . proven_count) ; println ! ("  Empirical:  {}" , stats . empirical_count) ; println ! ("  Heuristic:  {}" , stats . heuristic_count) ; Ok (()) } . sig` | 0.60 | intra 1, inter 0 | same 0, other 1 | ok | - |
| `load_invariants` | `# [doc = " Load invariants from JSON file"] fn load_invariants (path : & PathBuf) -> Result < Vec < Invariant > > { let json = std :: fs :: read_to_string (path) ? ; Ok (serde_json :: from_str (& json) ?) } . sig` | 0.60 | intra 0, inter 0 | same 0, other 1 | ok | - |
| `run_agent_cli` | `pub fn run_agent_cli () -> Result < () > { let cli = AgentCli :: parse () ; match cli . command { AgentCommand :: Check { action , conscience } => { check_action (& action , & conscience) ? ; } AgentCommand :: Query { function , conscience , } => { query_function (& function , & conscience) ? ; } AgentCommand :: Invariants { conscience , blocking_only , } => { list_invariants (& conscience , blocking_only) ? ; } AgentCommand :: Stats { conscience } => { show_stats (& conscience) ? ; } } Ok (()) } . sig` | 0.90 | intra 4, inter 0 | same 5, other 0 | ok | - |
| `list_invariants` | `# [doc = " List all invariants"] fn list_invariants (conscience_path : & PathBuf , blocking_only : bool) -> Result < () > { let invariants = load_invariants (conscience_path) ? ; let filtered : Vec < _ > = if blocking_only { invariants . iter () . filter (\| i \| i . is_blocking ()) . cloned () . collect () } else { invariants } ; println ! ("Total invariants: {}" , filtered . len ()) ; if blocking_only { println ! ("(Showing only blocking invariants)\n") ; } let output = serde_json :: to_string_pretty (& filtered) ? ; println ! ("{}" , output) ; Ok (()) } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |
| `test_load_invariants_empty` | `# [test] fn test_load_invariants_empty () { let temp_path = std :: env :: temp_dir () . join ("test_invariants.json") ; std :: fs :: write (& temp_path , "[]") . unwrap () ; let result = load_invariants (& temp_path) ; assert ! (result . is_ok ()) ; assert_eq ! (result . unwrap () . len () , 0) ; let _ = std :: fs :: remove_file (& temp_path) ; } . sig` | 0.90 | intra 1, inter 0 | same 0, other 0 | ok | - |

## Orphaned Functions (Review Only)

Action: review each item for expected usage. Delete only if it also appears under "Delete Candidates (Orphaned + Dead Code)".
Note: excludes public symbols referenced by other modules and entry points. Delete candidates require dead_code warnings.

- `escape_dot` in `src/000_cluster_001.rs`
- `test_confidence_from_strength` in `src/000_invariant_types.rs`
- `test_is_blocking` in `src/000_invariant_types.rs`
- `test_stats_calculation` in `src/000_invariant_types.rs`
- `test_constraint_is_blocking` in `src/005_refactor_constraints.rs`
- `test_scc_compression_dag` in `src/010_scc_compressor.rs`
- `test_scc_compression_cycle` in `src/010_scc_compressor.rs`
- `test_scc_compression_mixed` in `src/010_scc_compressor.rs`
- `test_symbolic_abstraction_merge` in `src/030_fixpoint_solver.rs`
- `test_detect_leaf_root` in `src/040_structural_detector.rs`
- `test_detect_degree_stable` in `src/040_structural_detector.rs`
- `test_all_structural_invariants_proven` in `src/040_structural_detector.rs`
- `make_function` in `src/050_semantic_detector.rs`
- `test_detect_type_stable` in `src/050_semantic_detector.rs`
- `test_detect_pure_function_heuristic` in `src/050_semantic_detector.rs`
- `test_detect_idempotent_heuristic` in `src/050_semantic_detector.rs`
- `test_no_pure_for_mutable` in `src/050_semantic_detector.rs`
- `test_path_detector_simple` in `src/060_path_detector.rs`
- `test_path_detector_diamond` in `src/060_path_detector.rs`
- `test_extract_facts_from_path` in `src/060_path_detector.rs`
- `test_path_stats` in `src/060_path_detector.rs`
- `export_constraints_json` in `src/080_invariant_reporter.rs`
- `strength_emoji` in `src/082_conscience_graph.rs`
- `kind_name` in `src/082_conscience_graph.rs`
- `matches_function` in `src/083_action_validator.rs`
- `test_extract_layer` in `src/083_action_validator.rs`
- `test_conscience_stats` in `src/085_agent_conscience.rs`
- `build_function_layers` in `src/110_cohesion_analyzer.rs`
- `build_type_maps` in `src/110_cohesion_analyzer.rs`
- `determine_status` in `src/110_cohesion_analyzer.rs`
- `suggest_file` in `src/110_cohesion_analyzer.rs`
- `compute_cluster_cohesion` in `src/110_cohesion_analyzer.rs`
- `suggest_cluster_file` in `src/110_cohesion_analyzer.rs`
- `is_source_file` in `src/120_directory_analyzer.rs`
- `should_skip_path` in `src/120_directory_analyzer.rs`
- `sanitize_identifier` in `src/130_control_flow.rs`
- `parallel_build_file_dag` in `src/140_file_ordering.rs`
- `slugify_relative` in `src/150_julia_parser.rs`
- `resolve_julia_binary` in `src/150_julia_parser.rs`
- `find_julia_project_dir` in `src/150_julia_parser.rs`
- `parse_module_name` in `src/150_julia_parser.rs`
- `parse_struct_name` in `src/150_julia_parser.rs`
- `relativize_path` in `src/150_julia_parser.rs`
- `paren_balance` in `src/150_julia_parser.rs`
- `relativize_path` in `src/160_rust_parser.rs`
- `display_path` in `src/180_report.rs`
- `placement_status_label` in `src/180_report.rs`
- `placement_status_notes` in `src/180_report.rs`
- `collect_rename_items` in `src/180_report.rs`
- `collect_utility_candidates` in `src/180_report.rs`
- `directory_moves_to_plan` in `src/180_report.rs`
- `write_priority_section` in `src/180_report.rs`
- `write_structural_tips` in `src/180_report.rs`
- `write_cluster_tips` in `src/180_report.rs`
- `sort_plan_items` in `src/180_report.rs`
- `sort_cluster_items` in `src/180_report.rs`
- `parse_dead_code_warnings` in `src/180_report.rs`
- `load_report_config` in `src/180_report.rs`
- `load_baseline_metrics` in `src/180_report.rs`
- `baseline_deltas` in `src/180_report.rs`
- `write_baseline_metrics` in `src/180_report.rs`
- `slugify_path` in `src/180_report.rs`
- `render_mermaid_graph` in `src/180_report.rs`
- `compute_ordering_correctness` in `src/180_report.rs`
- `compute_directory_cohesion` in `src/180_report.rs`
- `prefix_key_from_path` in `src/180_report.rs`
- `slugify_key` in `src/180_report.rs`
- `group_key_cmp` in `src/180_report.rs`
- `function_bucket_label` in `src/180_report.rs`
- `slugify_file_path` in `src/180_report.rs`
- `language_label` in `src/180_report.rs`
- `visibility_label` in `src/180_report.rs`
- `short_signature` in `src/180_report.rs`
- `normalize_use_stmt` in `src/180_report.rs`
- `sanitize_mermaid_id` in `src/180_report.rs`
- `sanitize_mermaid_label` in `src/180_report.rs`

## Delete Candidates (Orphaned + Dead Code)

- `matches_function` in `src/083_action_validator.rs`

## Utility Module Candidates

- None detected.

## Function Clusters

- cohesion 1.00, suggested `src/070_layer_utilities.rs`
  - src/000_cluster_001.rs::gather_julia_files, src/005_refactor_constraints.rs::generate_constraints, src/020_cluster_010.rs::resolve_source_root, src/030_cluster_011.rs::export_program_cfg_to_path, src/070_layer_utilities.rs::resolve_source_root, src/070_layer_utilities.rs::allow_analysis_dir, src/070_layer_utilities.rs::gather_rust_files, src/070_layer_utilities.rs::main, src/070_layer_utilities.rs::run_analysis
- cohesion 1.00, suggested `src/080_invariant_reporter.rs`
  - src/080_invariant_reporter.rs::generate_invariant_report, src/080_invariant_reporter.rs::export_json, src/080_invariant_reporter.rs::test_generate_report
- cohesion 1.00, suggested `src/090_utilities.rs`
  - src/090_utilities.rs::compress_path, src/090_utilities.rs::collect_directory_files, src/090_utilities.rs::path_common_prefix_len, src/090_utilities.rs::resolve_required_layer_path, src/090_utilities.rs::compute_move_metrics, src/090_utilities.rs::collect_move_items, src/090_utilities.rs::write_structural_batches, src/090_utilities.rs::write_cluster_batches
- cohesion 1.00, suggested `src/110_cohesion_analyzer.rs`
  - src/110_cohesion_analyzer.rs::collect_functions, src/110_cohesion_analyzer.rs::build_call_edges, src/110_cohesion_analyzer.rs::build_name_map
- cohesion 0.22, suggested `src/020_cluster_010.rs`
  - src/000_cluster_001.rs::collect_julia_dependencies, src/020_cluster_010.rs::normalize_module_name, src/020_cluster_010.rs::build_module_root_map, src/020_cluster_010.rs::extract_julia_dependencies, src/020_cluster_010.rs::extract_dependencies
- cohesion 0.00, suggested `src/180_report.rs`
  - src/180_report.rs::load_cargo_warnings, src/180_report.rs::collect_symbol_references, src/180_report.rs::is_public_function, src/180_report.rs::is_entrypoint_main, src/180_report.rs::is_dead_code_candidate
- cohesion 1.00, suggested `src/110_cohesion_analyzer.rs`
  - src/110_cohesion_analyzer.rs::build_call_analysis, src/110_cohesion_analyzer.rs::compute_type_coupling, src/110_cohesion_analyzer.rs::extract_identifiers
- cohesion 0.40, suggested `src/085_agent_conscience.rs`
  - src/082_conscience_graph.rs::make_test_invariant, src/085_agent_conscience.rs::test_conscience_blocks_invalid_move, src/085_agent_conscience.rs::test_query_allowed_actions
- cohesion 0.50, suggested `src/020_cluster_010.rs`
  - src/020_cluster_010.rs::resolve_module, src/020_cluster_010.rs::order_julia_files_by_dependency, src/020_cluster_010.rs::resolve_module_name
- cohesion 1.00, suggested `src/030_fixpoint_solver.rs`
  - src/030_fixpoint_solver.rs::propagate_to_fixpoint, src/030_fixpoint_solver.rs::test_fixpoint_simple, src/030_fixpoint_solver.rs::test_fixpoint_convergence
- cohesion 1.00, suggested `src/150_julia_parser.rs`
  - src/150_julia_parser.rs::extract_calls_from_lines, src/150_julia_parser.rs::extract_calls_from_text, src/150_julia_parser.rs::is_reserved
- cohesion 0.50, suggested `src/083_action_validator.rs`
  - src/005_refactor_constraints.rs::check_move_allowed, src/083_action_validator.rs::extract_layer, src/083_action_validator.rs::test_check_move_allowed
- cohesion 0.29, suggested `src/180_report.rs`
  - src/180_report.rs::parse_use_symbols, src/180_report.rs::scan_crate_paths, src/180_report.rs::path_matches, src/180_report.rs::referenced_elsewhere, src/180_report.rs::filter_orphaned
- cohesion 0.50, suggested `src/191_agent_cli.rs`
  - src/085_agent_conscience.rs::test_conscience_allows_valid_action, src/191_agent_cli.rs::run_agent_cli, src/191_agent_cli.rs::query_function, src/191_agent_cli.rs::show_stats, src/191_agent_cli.rs::load_invariants, src/191_agent_cli.rs::test_load_invariants_empty
- cohesion 1.00, suggested `src/160_rust_parser.rs`
  - src/160_rust_parser.rs::expr_snippet, src/160_rust_parser.rs::pat_snippet, src/160_rust_parser.rs::truncate_label
- cohesion 1.00, suggested `src/070_invariant_integrator.rs`
  - src/070_invariant_integrator.rs::make_simple_analysis, src/070_invariant_integrator.rs::test_invariant_detector_creation, src/070_invariant_integrator.rs::test_detect_all
- cohesion 1.00, suggested `src/050_cluster_006.rs`
  - src/050_cluster_006.rs::strip_numeric_prefix, src/050_cluster_006.rs::generate_canonical_name, src/050_cluster_006.rs::collect_directory_moves
- cohesion 0.12, suggested `src/000_cluster_001.rs`
  - src/000_cluster_001.rs::generates_canonical_names_and_violations, src/000_cluster_001.rs::rust_entry_paths, src/000_cluster_001.rs::collect_rust_dependencies, src/000_cluster_001.rs::build_file_layers, src/000_cluster_001.rs::topological_sort, src/000_cluster_001.rs::ordered_by_name, src/000_cluster_001.rs::build_entries, src/000_cluster_001.rs::test_generates_canonical_names_and_violations, src/030_cluster_011.rs::build_directory_dag, src/030_cluster_011.rs::build_file_dependency_graph
- cohesion 0.14, suggested `src/010_cluster_008.rs`
  - src/000_cluster_001.rs::layer_constrained_sort, src/000_cluster_001.rs::topo_sort_within, src/010_cluster_008.rs::is_layer_violation, src/010_cluster_008.rs::compare_dir_layers, src/010_cluster_008.rs::compare_path_components, src/010_cluster_008.rs::detect_layer_violation, src/010_cluster_008.rs::parse_cluster_members, src/010_cluster_008.rs::is_core_module_path, src/020_layer_inference.rs::infer_layers, src/020_layer_inference.rs::detect_layer_violations, src/020_layer_inference.rs::test_layer_inference_simple_dag, src/020_layer_inference.rs::test_layer_inference_diamond
- cohesion 0.00, suggested `src/020_cluster_010.rs`
  - src/020_cluster_010.rs::contains_tools, src/020_cluster_010.rs::extract_rust_dependencies, src/020_cluster_010.rs::build_dependency_map
- cohesion 0.48, suggested `src/000_cluster_001.rs`
  - src/000_cluster_001.rs::build_directory_entry_map, src/000_cluster_001.rs::collect_naming_warnings, src/000_cluster_001.rs::temp_dir, src/000_cluster_001.rs::detects_cycles, src/000_cluster_001.rs::topo_sort_orders_dependencies, src/000_cluster_001.rs::detect_layer, src/000_cluster_001.rs::order_rust_files_by_dependency, src/000_cluster_001.rs::analyze_file_ordering, src/000_cluster_001.rs::naming_score_for_file, src/000_cluster_001.rs::detect_cycles, src/000_cluster_001.rs::detect_violations, src/000_cluster_001.rs::test_detects_cycles, src/030_cluster_011.rs::build_module_map, src/030_cluster_011.rs::build_file_dag
- cohesion 0.40, suggested `src/083_action_validator.rs`
  - src/005_refactor_constraints.rs::test_check_move_allowed_blocking, src/005_refactor_constraints.rs::test_check_move_allowed_non_blocking, src/083_action_validator.rs::validate_action, src/083_action_validator.rs::test_validate_no_move_constraint, src/083_action_validator.rs::test_validate_layer_fixed_constraint, src/083_action_validator.rs::test_validate_preserve_signature, src/083_action_validator.rs::test_validate_allowed_action
- cohesion 0.78, suggested `src/010_cluster_008.rs`
  - src/010_cluster_008.rs::build_result, src/010_cluster_008.rs::adjacency_from_edges, src/010_cluster_008.rs::topo_sort, src/010_cluster_008.rs::layer_rank_map, src/010_cluster_008.rs::insert_sorted, src/010_cluster_008.rs::is_mmsb_main, src/010_cluster_008.rs::layer_prefix_value, src/010_cluster_008.rs::layer_adheres, src/010_cluster_008.rs::detect_layer_violations, src/010_cluster_008.rs::cluster_target_path, src/010_cluster_008.rs::collect_cluster_plans, src/020_layer_inference.rs::test_detect_layer_violations_none, src/050_cluster_006.rs::layer_prefix_value

