{
  "version": "1.0",
  "timestamp": "2025-12-31T06:29:33.053271615+00:00",
  "actions": [
    {
      "action_id": "create_file_src/226_correction_plan_generator.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "create_file_src/233_correction_intelligence_report.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "create_file_src/216_dead_code_entrypoints.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "create_file_src/214_dead_code_intent.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "create_file_src/000_cluster_001.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "create_file_src/211_dead_code_attribute_parser.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "create_file_src/010_cluster_008.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "create_file_src/000_cluster_001.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_order_julia_files_by_dependency_to_src/000_cluster_001.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/020_cluster_010.rs",
          "original_content": "//! Cluster 010: Module resolution and dependency extraction utilities\n//!\n//! This module contains foundational functions for:\n//! - Module name resolution and path mapping\n//! - Rust dependency extraction from source files\n//! - Julia dependency extraction from source files\n//!\n//! Functions moved from src/000_dependency.rs as part of Phase 2, Batch 6 refactoring.\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::ItemUse;\nuse walkdir::WalkDir;\n\nuse crate::dependency::RootState;\n\n// ============================================================================\n// Module Resolution (from src/000_dependency.rs)\n// ============================================================================\n\npub fn normalize_module_name(name: &str) -> String {\n    if let Some(pos) = name.find('_') {\n        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n            return name[pos + 1..].to_string();\n        }\n    }\n    name.to_string()\n}\n\npub fn resolve_module(\n    root: &str,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Option<PathBuf> {\n    let key = normalize_module_name(root);\n    if let Some(path) = module_map.get(&key) {\n        return Some(path.clone());\n    }\n    module_map\n        .iter()\n        .find(|(name, _)| name == &&key)\n        .map(|(_, path)| path.clone())\n        .or_else(|| {\n            module_map\n                .iter()\n                .find(|(name, _)| key.starts_with(name.as_str()))\n                .map(|(_, path)| path.clone())\n        })\n        .or_else(|| crate::cluster_011::resolve_path(&PathBuf::from(root), file_set, module_map))\n}\n\npub fn contains_tools(path: &Path) -> bool {\n    path.components().any(|c| c.as_os_str() == \"tools\")\n}\n\n#[derive(Clone)]\npub struct ModuleRoot {\n    pub layer: String,\n}\n\npub fn build_module_root_map(root: &Path) -> Result<HashMap<String, ModuleRoot>, std::io::Error> {\n    let src_dir = root.join(\"src\");\n    let mut map = HashMap::new();\n    if src_dir.is_dir() {\n        for entry in fs::read_dir(&src_dir)? {\n            let entry = entry?;\n            let path = entry.path();\n            if contains_tools(&path) {\n                continue;\n            }\n            let name = entry\n                .file_name()\n                .to_string_lossy()\n                .to_string()\n                .trim_end_matches(\".rs\")\n                .to_string();\n            if path.is_dir() {\n                let normalized = normalize_module_name(&name);\n                map.insert(\n                    normalized,\n                    ModuleRoot {\n                        layer: name.clone(),\n                    },\n                );\n            } else if path.extension().map(|ext| ext == \"rs\").unwrap_or(false) {\n                map.insert(\n                    name.clone(),\n                    ModuleRoot {\n                        layer: crate::cluster_001::detect_layer(&path),\n                    },\n                );\n            }\n        }\n    }\n    Ok(map)\n}\n\n// ============================================================================\n// Julia Dependency Ordering (from src/020_layer_utilities.rs)\n// ============================================================================\n\nstruct LayerResolver {\n    aliases: HashMap<String, String>,\n}\n\nfn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\nimpl LayerResolver {\n    fn build(root: &Path) -> Result<Self> {\n        let mut resolver = LayerResolver {\n            aliases: HashMap::new(),\n        };\n        let src_dir = resolve_source_root(root);\n        if src_dir.is_dir() {\n            for entry in WalkDir::new(&src_dir).into_iter().filter_map(|e| e.ok()) {\n                let path = entry.path();\n                if contains_tools(path) {\n                    continue;\n                }\n                let layer = crate::cluster_001::detect_layer(path);\n                if layer == \"root\" {\n                    continue;\n                }\n                if path.is_dir() {\n                    if let Some(name) = path.file_name().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(name, &layer);\n                    }\n                } else if path.extension().map_or(false, |ext| ext == \"jl\") {\n                    if let Some(stem) = path.file_stem().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(stem, &layer);\n                    }\n                }\n            }\n        }\n        Ok(resolver)\n    }\n\n    fn add_aliases(&mut self, name: &str, layer: &str) {\n        let lower = name.to_lowercase();\n        self.aliases\n            .entry(lower.clone())\n            .or_insert_with(|| layer.to_string());\n        let condensed = lower.replace('_', \"\");\n        self.aliases\n            .entry(condensed)\n            .or_insert_with(|| layer.to_string());\n    }\n\n    fn resolve_module(&self, module: &str) -> Option<String> {\n        let key = module.to_lowercase();\n        if let Some(layer) = self.aliases.get(&key) {\n            return Some(layer.clone());\n        }\n        let condensed = key.replace('_', \"\");\n        if let Some(layer) = self.aliases.get(&condensed) {\n            return Some(layer.clone());\n        }\n        self.aliases\n            .iter()\n            .filter(|(alias, _)| !alias.is_empty())\n            .find(|(alias, _)| key.starts_with(alias.as_str()))\n            .map(|(_, layer)| layer.clone())\n    }\n}\n\npub fn order_julia_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, crate::dependency::LayerGraph)> {\n    use crate::cluster_001::{collect_julia_dependencies, JuliaTarget};\n    use crate::dependency::ReferenceDetail;\n\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n    let resolver = LayerResolver::build(root)?;\n    let entry_files = crate::cluster_001::julia_entry_paths(root);\n\n    for file in files {\n        let layer = crate::cluster_001::detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let references = collect_julia_dependencies(file)\n            .with_context(|| format!(\"Failed to analyze Julia dependencies for {:?}\", file))?;\n        for dep in references {\n            match dep.target {\n                JuliaTarget::Include(include_path) => {\n                    let resolved = if include_path.is_absolute() {\n                        include_path.clone()\n                    } else {\n                        file.parent()\n                            .map(|p| p.join(&include_path))\n                            .unwrap_or(include_path.clone())\n                    };\n\n                    if resolved.exists() {\n                        let target_layer = crate::cluster_001::detect_layer(&resolved);\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n                JuliaTarget::Module(module) => {\n                    if let Some(target_layer) = resolver.resolve_module(&module) {\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n\n// ============================================================================\n// Rust Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_rust_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    #[derive(Default)]\n    struct UseCollector {\n        roots: BTreeSet<String>,\n        mods: BTreeSet<String>,\n    }\n\n    impl<'ast> Visit<'ast> for UseCollector {\n        fn visit_item_use(&mut self, node: &'ast ItemUse) {\n            crate::dependency::collect_roots(&node.tree, RootState::Start, &mut self.roots);\n        }\n\n        fn visit_item_mod(&mut self, node: &'ast syn::ItemMod) {\n            if node.content.is_none() {\n                self.mods.insert(node.ident.to_string());\n            }\n        }\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", file))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    let mut deps = Vec::new();\n    for root in collector.roots {\n        if let Some(path) = resolve_module(&root, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    for module in collector.mods {\n        if let Some(path) = resolve_module(&module, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    Ok(deps)\n}\n\n// ============================================================================\n// Julia Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_julia_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    static INCLUDE_RE: Lazy<Regex> =\n        Lazy::new(|| Regex::new(r#\"include\\s*\\(\\s*[\"']([^\"']+)[\"']\"#).unwrap());\n    static MMSB_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static MMSB_SYMBOL_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#).unwrap()\n    });\n    static LOCAL_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+\\.\\s*([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static PLAIN_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+([A-Za-z_][A-Za-z0-9_\\.]*)\"#).unwrap()\n    });\n\n    fn resolve_module_name(\n        module: &str,\n        file_set: &HashSet<PathBuf>,\n        module_map: &HashMap<String, PathBuf>,\n    ) -> Option<PathBuf> {\n        let primary = module.split('.').next().unwrap_or(module);\n        resolve_module(primary, file_set, module_map)\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_RE.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let raw = path_match.as_str();\n            let mut candidate = PathBuf::from(raw);\n            if candidate.extension().is_none() {\n                candidate.set_extension(\"jl\");\n            }\n            let resolved = if candidate.is_absolute() {\n                candidate\n            } else {\n                file.parent()\n                    .map(|p| p.join(&candidate))\n                    .unwrap_or(candidate)\n            };\n            if let Some(path) = crate::cluster_011::resolve_path(&resolved, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_SYMBOL_RE.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                if let Some(path) = resolve_module_name(symbol, file_set, module_map) {\n                    deps.push(path);\n                }\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in PLAIN_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            if module.starts_with(\"MMSB\") {\n                continue;\n            }\n            if let Some(path) = resolve_module_name(module, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    Ok(deps)\n}\n\n// ============================================================================\n// File Dependency Mapping (from src/090_file_ordering.rs)\n// ============================================================================\n\npub fn build_dependency_map(\n    files: &[PathBuf],\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<HashMap<PathBuf, Vec<PathBuf>>> {\n    let mut dep_map: HashMap<PathBuf, Vec<PathBuf>> = HashMap::new();\n    for file in files {\n        let deps = extract_dependencies(file, file_set, module_map)\n            .with_context(|| format!(\"Failed to extract dependencies for {:?}\", file))?;\n        dep_map.insert(file.clone(), deps);\n    }\n    Ok(dep_map)\n}\n\npub(crate) fn extract_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    let ext = file.extension().and_then(|s| s.to_str()).unwrap_or(\"\");\n    match ext {\n        \"rs\" => extract_rust_dependencies(file, file_set, module_map),\n        \"jl\" => extract_julia_dependencies(file, file_set, module_map),\n        _ => Ok(Vec::new()),\n    }\n}\n",
          "updated_content": "//! Cluster 010: Module resolution and dependency extraction utilities\n//!\n//! This module contains foundational functions for:\n//! - Module name resolution and path mapping\n//! - Rust dependency extraction from source files\n//! - Julia dependency extraction from source files\n//!\n//! Functions moved from src/000_dependency.rs as part of Phase 2, Batch 6 refactoring.\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::ItemUse;\nuse walkdir::WalkDir;\n\nuse crate::dependency::RootState;\n\n// ============================================================================\n// Module Resolution (from src/000_dependency.rs)\n// ============================================================================\n\npub fn normalize_module_name(name: &str) -> String {\n    if let Some(pos) = name.find('_') {\n        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n            return name[pos + 1..].to_string();\n        }\n    }\n    name.to_string()\n}\n\npub fn resolve_module(\n    root: &str,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Option<PathBuf> {\n    let key = normalize_module_name(root);\n    if let Some(path) = module_map.get(&key) {\n        return Some(path.clone());\n    }\n    module_map\n        .iter()\n        .find(|(name, _)| name == &&key)\n        .map(|(_, path)| path.clone())\n        .or_else(|| {\n            module_map\n                .iter()\n                .find(|(name, _)| key.starts_with(name.as_str()))\n                .map(|(_, path)| path.clone())\n        })\n        .or_else(|| crate::cluster_011::resolve_path(&PathBuf::from(root), file_set, module_map))\n}\n\npub fn contains_tools(path: &Path) -> bool {\n    path.components().any(|c| c.as_os_str() == \"tools\")\n}\n\n#[derive(Clone)]\npub struct ModuleRoot {\n    pub layer: String,\n}\n\npub fn build_module_root_map(root: &Path) -> Result<HashMap<String, ModuleRoot>, std::io::Error> {\n    let src_dir = root.join(\"src\");\n    let mut map = HashMap::new();\n    if src_dir.is_dir() {\n        for entry in fs::read_dir(&src_dir)? {\n            let entry = entry?;\n            let path = entry.path();\n            if contains_tools(&path) {\n                continue;\n            }\n            let name = entry\n                .file_name()\n                .to_string_lossy()\n                .to_string()\n                .trim_end_matches(\".rs\")\n                .to_string();\n            if path.is_dir() {\n                let normalized = normalize_module_name(&name);\n                map.insert(\n                    normalized,\n                    ModuleRoot {\n                        layer: name.clone(),\n                    },\n                );\n            } else if path.extension().map(|ext| ext == \"rs\").unwrap_or(false) {\n                map.insert(\n                    name.clone(),\n                    ModuleRoot {\n                        layer: crate::cluster_001::detect_layer(&path),\n                    },\n                );\n            }\n        }\n    }\n    Ok(map)\n}\n\n// ============================================================================\n// Julia Dependency Ordering (from src/020_layer_utilities.rs)\n// ============================================================================\n\nstruct LayerResolver {\n    aliases: HashMap<String, String>,\n}\n\nfn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\nimpl LayerResolver {\n    fn build(root: &Path) -> Result<Self> {\n        let mut resolver = LayerResolver {\n            aliases: HashMap::new(),\n        };\n        let src_dir = resolve_source_root(root);\n        if src_dir.is_dir() {\n            for entry in WalkDir::new(&src_dir).into_iter().filter_map(|e| e.ok()) {\n                let path = entry.path();\n                if contains_tools(path) {\n                    continue;\n                }\n                let layer = crate::cluster_001::detect_layer(path);\n                if layer == \"root\" {\n                    continue;\n                }\n                if path.is_dir() {\n                    if let Some(name) = path.file_name().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(name, &layer);\n                    }\n                } else if path.extension().map_or(false, |ext| ext == \"jl\") {\n                    if let Some(stem) = path.file_stem().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(stem, &layer);\n                    }\n                }\n            }\n        }\n        Ok(resolver)\n    }\n\n    fn add_aliases(&mut self, name: &str, layer: &str) {\n        let lower = name.to_lowercase();\n        self.aliases\n            .entry(lower.clone())\n            .or_insert_with(|| layer.to_string());\n        let condensed = lower.replace('_', \"\");\n        self.aliases\n            .entry(condensed)\n            .or_insert_with(|| layer.to_string());\n    }\n\n    fn resolve_module(&self, module: &str) -> Option<String> {\n        let key = module.to_lowercase();\n        if let Some(layer) = self.aliases.get(&key) {\n            return Some(layer.clone());\n        }\n        let condensed = key.replace('_', \"\");\n        if let Some(layer) = self.aliases.get(&condensed) {\n            return Some(layer.clone());\n        }\n        self.aliases\n            .iter()\n            .filter(|(alias, _)| !alias.is_empty())\n            .find(|(alias, _)| key.starts_with(alias.as_str()))\n            .map(|(_, layer)| layer.clone())\n    }\n}\n\n\n\n// ============================================================================\n// Rust Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_rust_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    #[derive(Default)]\n    struct UseCollector {\n        roots: BTreeSet<String>,\n        mods: BTreeSet<String>,\n    }\n\n    impl<'ast> Visit<'ast> for UseCollector {\n        fn visit_item_use(&mut self, node: &'ast ItemUse) {\n            crate::dependency::collect_roots(&node.tree, RootState::Start, &mut self.roots);\n        }\n\n        fn visit_item_mod(&mut self, node: &'ast syn::ItemMod) {\n            if node.content.is_none() {\n                self.mods.insert(node.ident.to_string());\n            }\n        }\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", file))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    let mut deps = Vec::new();\n    for root in collector.roots {\n        if let Some(path) = resolve_module(&root, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    for module in collector.mods {\n        if let Some(path) = resolve_module(&module, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    Ok(deps)\n}\n\n// ============================================================================\n// Julia Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_julia_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    static INCLUDE_RE: Lazy<Regex> =\n        Lazy::new(|| Regex::new(r#\"include\\s*\\(\\s*[\"']([^\"']+)[\"']\"#).unwrap());\n    static MMSB_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static MMSB_SYMBOL_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#).unwrap()\n    });\n    static LOCAL_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+\\.\\s*([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static PLAIN_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+([A-Za-z_][A-Za-z0-9_\\.]*)\"#).unwrap()\n    });\n\n    fn resolve_module_name(\n        module: &str,\n        file_set: &HashSet<PathBuf>,\n        module_map: &HashMap<String, PathBuf>,\n    ) -> Option<PathBuf> {\n        let primary = module.split('.').next().unwrap_or(module);\n        resolve_module(primary, file_set, module_map)\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_RE.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let raw = path_match.as_str();\n            let mut candidate = PathBuf::from(raw);\n            if candidate.extension().is_none() {\n                candidate.set_extension(\"jl\");\n            }\n            let resolved = if candidate.is_absolute() {\n                candidate\n            } else {\n                file.parent()\n                    .map(|p| p.join(&candidate))\n                    .unwrap_or(candidate)\n            };\n            if let Some(path) = crate::cluster_011::resolve_path(&resolved, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_SYMBOL_RE.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                if let Some(path) = resolve_module_name(symbol, file_set, module_map) {\n                    deps.push(path);\n                }\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in PLAIN_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            if module.starts_with(\"MMSB\") {\n                continue;\n            }\n            if let Some(path) = resolve_module_name(module, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    Ok(deps)\n}\n\n// ============================================================================\n// File Dependency Mapping (from src/090_file_ordering.rs)\n// ============================================================================\n\npub fn build_dependency_map(\n    files: &[PathBuf],\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<HashMap<PathBuf, Vec<PathBuf>>> {\n    let mut dep_map: HashMap<PathBuf, Vec<PathBuf>> = HashMap::new();\n    for file in files {\n        let deps = extract_dependencies(file, file_set, module_map)\n            .with_context(|| format!(\"Failed to extract dependencies for {:?}\", file))?;\n        dep_map.insert(file.clone(), deps);\n    }\n    Ok(dep_map)\n}\n\npub(crate) fn extract_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    let ext = file.extension().and_then(|s| s.to_str()).unwrap_or(\"\");\n    match ext {\n        \"rs\" => extract_rust_dependencies(file, file_set, module_map),\n        \"jl\" => extract_julia_dependencies(file, file_set, module_map),\n        _ => Ok(Vec::new()),\n    }\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/000_cluster_001.rs",
          "original_content": "//! Cluster 001: Core dependency analysis and file ordering utilities\n//!\n//! This module contains fundamental functions for:\n//! - Module mapping and dependency resolution\n//! - Topological sorting and layer-constrained ordering\n//! - File gathering and layer construction\n//! - Dependency graph building and cycle detection\n//! - DOT export for program CFGs\n//! - Naming validation and warnings\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse petgraph::algo::tarjan_scc;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::{ItemUse, UseTree};\n\nuse crate::dependency::{LayerGraph, ReferenceDetail, UnresolvedDependency};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\npub fn build_directory_entry_map(\n    files: &[PathBuf],\n) -> Result<HashMap<PathBuf, crate::types::FileOrderEntry>> {\n    use crate::file_ordering::{\n        build_dependency_map, build_entries, build_file_dag, detect_cycles, ordered_by_name,\n        topological_sort,\n    };\n    use crate::layer_core::layer_constrained_sort;\n    use crate::layer_utilities::build_file_layers;\n    use crate::types::FileOrderingResult;\n    use std::collections::HashSet;\n\n    const DEFAULT_STEP: usize = 10;\n\n    if files.is_empty() {\n        return Ok(HashMap::new());\n    }\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = crate::cluster_011::build_module_map(files);\n    let dep_map = build_dependency_map(files, &file_set, &module_map)?;\n    let file_layers = build_file_layers(files);\n    let (graph, node_map) = build_file_dag(files, &dep_map);\n    let cycles = detect_cycles(&graph, files);\n\n    let ordered_nodes = if cycles.is_empty() {\n        layer_constrained_sort(&graph, &file_layers).unwrap_or_else(|_| {\n            topological_sort(&graph).unwrap_or_else(|_| ordered_by_name(files, &node_map))\n        })\n    } else {\n        ordered_by_name(files, &node_map)\n    };\n\n    let ordered_files = ordered_nodes\n        .into_iter()\n        .map(|idx| graph[idx].clone())\n        .collect::<Vec<_>>();\n\n    let ordering = FileOrderingResult {\n        ordered_files: build_entries(&ordered_files, DEFAULT_STEP),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles,\n    };\n    let mut map = HashMap::new();\n    for entry in ordering.ordered_files {\n        map.insert(entry.current_path.clone(), entry);\n    }\n    Ok(map)\n}\n\npub fn collect_naming_warnings(\n    directory: &crate::types::DirectoryAnalysis,\n    config: &crate::report::ReportConfig,\n    warnings: &mut Vec<String>,\n) -> Result<()> {\n    use crate::utilities::compress_path;\n    use crate::dependency::naming_score_for_file;\n    if directory\n        .path\n        .components()\n        .any(|comp| comp.as_os_str() == \"_old\")\n    {\n        return Ok(());\n    }\n    let file_map = build_directory_entry_map(&directory.files)?;\n    for file in &directory.files {\n        if file.components().any(|comp| comp.as_os_str() == \"_old\") {\n            continue;\n        }\n        let entry = file_map.get(file);\n        if let Some(score) = naming_score_for_file(file, entry) {\n            if score < config.naming_score_warning {\n                let suggested = entry\n                    .map(|e| e.suggested_name.as_str())\n                    .unwrap_or(\"suggested name unavailable\");\n                warnings.push(format!(\n                    \"File `{}` has naming score {:.0}; consider renaming to `{}`.\",\n                    compress_path(file.to_string_lossy().as_ref()),\n                    score,\n                    suggested,\n                ));\n            }\n        }\n    }\n    for child in &directory.subdirectories {\n        collect_naming_warnings(child, config, warnings)?;\n    }\n    Ok(())\n}\n\n#[cfg(test)]\nfn temp_dir(name: &str) -> PathBuf {\n    let mut dir = std::env::temp_dir();\n    dir.push(format!(\n        \"mmsb_analyzer_{}_{}\",\n        name,\n        std::time::SystemTime::now()\n            .duration_since(std::time::UNIX_EPOCH)\n            .unwrap()\n            .as_nanos()\n    ));\n    dir\n}\n\n#[cfg(test)]\nfn detects_cycles() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"cycle\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    write(&a, \"use crate::b; pub fn a() {}\")?;\n    write(&b, \"use crate::a; pub fn b() {}\")?;\n\n    let result = analyze_file_ordering(&[a.clone(), b.clone()], Some(10))?;\n    assert!(!result.cycles.is_empty());\n    Ok(())\n}\n\n#[cfg(test)]\nfn generates_canonical_names_and_violations() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"names\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    write(&a, \"use crate::b; pub fn a() {}\")?;\n    write(&b, \"pub fn b() {}\")?;\n\n    let result = analyze_file_ordering(&[a.clone(), b.clone()], Some(10))?;\n    let entries = &result.ordered_files;\n    assert_eq!(entries[0].suggested_name, \"000_b.rs\");\n    assert_eq!(entries[1].suggested_name, \"010_a.rs\");\n    assert!(!result.violations.is_empty());\n    Ok(())\n}\n\n#[cfg(test)]\n#[allow(dead_code)]\nfn topo_sort_orders_dependencies() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"topo\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    let c = dir.join(\"c.rs\");\n    write(&a, \"pub fn a() {}\")?;\n    write(&b, \"use crate::a; pub fn b() {}\")?;\n    write(&c, \"use crate::b; pub fn c() {}\")?;\n\n    let result = analyze_file_ordering(&[c.clone(), b.clone(), a.clone()], Some(10))?;\n    let ordered: Vec<_> = result\n        .ordered_files\n        .iter()\n        .map(|entry| entry.current_path.clone())\n        .collect();\n    assert_eq!(ordered, vec![a, b, c]);\n    assert!(result.cycles.is_empty());\n    Ok(())\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\npub fn layer_constrained_sort(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Result<Vec<NodeIndex>> {\n    use crate::cluster_006::layer_prefix_value;\n\n    let mut layer_nodes: BTreeMap<i32, Vec<NodeIndex>> = BTreeMap::new();\n    for node in graph.node_indices() {\n        let file = &graph[node];\n        let layer_name = file_layers\n            .get(file)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_value = layer_prefix_value(&layer_name).unwrap_or(0);\n        layer_nodes.entry(layer_value).or_default().push(node);\n    }\n\n    let mut ordered = Vec::new();\n    for (_layer, nodes) in layer_nodes {\n        let sorted = topo_sort_within(graph, &nodes)?;\n        ordered.extend(sorted);\n    }\n    Ok(ordered)\n}\n\npub fn topo_sort_within(\n    graph: &DiGraph<PathBuf, ()>,\n    nodes: &[NodeIndex],\n) -> Result<Vec<NodeIndex>> {\n    let node_set: HashSet<NodeIndex> = nodes.iter().copied().collect();\n    let mut indegree: HashMap<NodeIndex, usize> = HashMap::new();\n    for &node in nodes {\n        indegree.insert(node, 0);\n    }\n    for &node in nodes {\n        let incoming = graph\n            .neighbors_directed(node, petgraph::Direction::Incoming)\n            .filter(|n| node_set.contains(n))\n            .count();\n        indegree.insert(node, incoming);\n    }\n    let mut queue = std::collections::VecDeque::new();\n    for &node in nodes {\n        if indegree.get(&node).copied().unwrap_or(0) == 0 {\n            queue.push_back(node);\n        }\n    }\n    let mut ordered = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        ordered.push(node);\n        for neighbor in graph.neighbors_directed(node, petgraph::Direction::Outgoing) {\n            if !node_set.contains(&neighbor) {\n                continue;\n            }\n            if let Some(entry) = indegree.get_mut(&neighbor) {\n                *entry = entry.saturating_sub(1);\n                if *entry == 0 {\n                    queue.push_back(neighbor);\n                }\n            }\n        }\n    }\n    if ordered.len() != nodes.len() {\n        return Err(anyhow::anyhow!(\"Cycle detected within layer group\"));\n    }\n    Ok(ordered)\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs\n// ============================================================================\n\n/// Detects the layer identifier from a path by finding first digit-prefixed component\npub fn detect_layer(path: &Path) -> String {\n    for component in path.components() {\n        if let Some(name) = component.as_os_str().to_str() {\n            if let Some(first) = name.chars().next() {\n                if first.is_ascii_digit() {\n                    if let Some(pos) = name.find('_') {\n                        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n                            return name.to_string();\n                        }\n                    }\n                }\n            }\n        }\n    }\n    \"root\".to_string()\n}\n\npub fn rust_entry_paths(root: &Path) -> BTreeSet<PathBuf> {\n    let src_dir = crate::layer_utilities::resolve_source_root(root);\n    [\"lib.rs\", \"main.rs\"]\n        .iter()\n        .map(|rel| src_dir.join(rel))\n        .filter(|p| p.exists())\n        .collect()\n}\n\n#[derive(Clone)]\nstruct RustDependency {\n    root: String,\n    detail: String,\n}\n\nfn collect_rust_dependencies(path: &Path) -> Result<Vec<RustDependency>> {\n    let content =\n        fs::read_to_string(path).with_context(|| format!(\"Unable to read Rust file {:?}\", path))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", path))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    Ok(collector.deps)\n}\n\n#[derive(Default)]\nstruct UseCollector {\n    deps: Vec<RustDependency>,\n}\n\nimpl<'ast> Visit<'ast> for UseCollector {\n    fn visit_item_use(&mut self, node: &'ast ItemUse) {\n        let mut roots = BTreeSet::new();\n        collect_roots_from_crate(&node.tree, CrateRootState::Start, &mut roots);\n        let stmt = quote::quote!(#node).to_string();\n        for root in roots {\n            self.deps.push(RustDependency {\n                root,\n                detail: stmt.clone(),\n            });\n        }\n    }\n}\n\n#[derive(Copy, Clone, Eq, PartialEq)]\nenum CrateRootState {\n    Start,\n    AfterCrate,\n}\n\nfn collect_roots_from_crate(tree: &UseTree, state: CrateRootState, acc: &mut BTreeSet<String>) {\n    match tree {\n        UseTree::Path(path) => {\n            let ident = path.ident.to_string();\n            if state == CrateRootState::Start && ident == \"crate\" {\n                collect_roots_from_crate(&path.tree, CrateRootState::AfterCrate, acc);\n            } else if state == CrateRootState::AfterCrate {\n                acc.insert(ident);\n            } else {\n                collect_roots_from_crate(&path.tree, state, acc);\n            }\n        }\n        UseTree::Group(group) => {\n            for tree in &group.items {\n                collect_roots_from_crate(tree, state, acc);\n            }\n        }\n        UseTree::Name(name) => {\n            if state == CrateRootState::AfterCrate {\n                acc.insert(name.ident.to_string());\n            }\n        }\n        UseTree::Rename(rename) => {\n            if state == CrateRootState::AfterCrate {\n                acc.insert(rename.ident.to_string());\n            }\n        }\n        UseTree::Glob(_) => {}\n    }\n}\n\n/// Order Rust files by dependency and capture layer graph details.\npub fn order_rust_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let module_map = crate::cluster_010::build_module_root_map(root)?;\n    let entry_files = rust_entry_paths(root);\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n\n    for file in files {\n        let layer = detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let deps = collect_rust_dependencies(file)\n            .with_context(|| format!(\"Failed to collect dependencies for {:?}\", file))?;\n        for dep in deps {\n            if let Some(info) = module_map.get(&dep.root) {\n                nodes.insert(info.layer.clone());\n                if info.layer != layer {\n                    edges_map\n                        .entry((info.layer.clone(), layer.clone()))\n                        .or_default()\n                        .insert(ReferenceDetail {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                }\n            } else {\n                unresolved.push(UnresolvedDependency {\n                    file: file.clone(),\n                    reference: dep.detail.clone(),\n                });\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n\n// ============================================================================\n// Julia Dependency Analysis (from src/000_dependency.rs)\n// ============================================================================\n\n#[derive(Clone)]\npub(crate) struct JuliaDependency {\n    pub(crate) target: JuliaTarget,\n    pub(crate) detail: String,\n}\n\n#[derive(Clone)]\npub(crate) enum JuliaTarget {\n    Include(PathBuf),\n    Module(String),\n}\n\nstatic INCLUDE_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)include\\s*\\(\\s*[\"']([^\"'\\n]+)[\"']\"#).expect(\"failed to compile include regex\")\n});\nstatic USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#)\n        .expect(\"failed to compile using regex\")\n});\nstatic ROOT_USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#)\n        .expect(\"failed to compile root using regex\")\n});\nstatic LOCAL_USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+\\.\\s*([A-Za-z0-9_]+)\"#)\n        .expect(\"failed to compile local using regex\")\n});\n\npub(crate) fn collect_julia_dependencies(path: &Path) -> Result<Vec<JuliaDependency>> {\n    let content = fs::read_to_string(path)\n        .with_context(|| format!(\"Unable to read Julia file {:?}\", path))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_REGEX.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let relative = PathBuf::from(path_match.as_str());\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Include(relative),\n                detail,\n            });\n        }\n    }\n\n    for cap in USING_REGEX.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            let primary = module.split('.').next().unwrap_or(module).to_string();\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Module(primary),\n                detail,\n            });\n        }\n    }\n\n    for cap in ROOT_USING_REGEX.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                let primary = symbol.split('.').next().unwrap_or(symbol).to_string();\n                deps.push(JuliaDependency {\n                    target: JuliaTarget::Module(primary),\n                    detail: detail.clone(),\n                });\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_REGEX.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Module(module.to_string()),\n                detail,\n            });\n        }\n    }\n\n    Ok(deps)\n}\n\npub fn julia_entry_paths(root: &Path) -> BTreeSet<PathBuf> {\n    let src_dir = crate::layer_utilities::resolve_source_root(root);\n    [\"MMSB.jl\", \"API.jl\", \"MMSB/API.jl\"]\n        .iter()\n        .map(|rel| src_dir.join(rel))\n        .filter(|p| p.exists())\n        .collect()\n}\n\npub fn build_file_layers(files: &[PathBuf]) -> HashMap<PathBuf, String> {\n    let mut layers = HashMap::new();\n    for file in files {\n        layers.insert(file.clone(), detect_layer(file));\n    }\n    layers\n}\n\npub fn gather_julia_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = crate::layer_utilities::resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            crate::layer_utilities::allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"jl\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n    .collect()\n}\n\n// ============================================================================\n// From src/090_file_ordering.rs\n// ============================================================================\n\npub fn topological_sort(graph: &DiGraph<PathBuf, ()>) -> Result<Vec<NodeIndex>> {\n    use petgraph::Direction;\n    use std::collections::VecDeque;\n\n    let mut indegree = vec![0usize; graph.node_count()];\n    for node in graph.node_indices() {\n        indegree[node.index()] = graph\n            .neighbors_directed(node, Direction::Incoming)\n            .count();\n    }\n\n    let mut queue = VecDeque::new();\n    for node in graph.node_indices() {\n        if indegree[node.index()] == 0 {\n            queue.push_back(node);\n        }\n    }\n\n    let mut ordered = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        ordered.push(node);\n        for neighbor in graph.neighbors_directed(node, Direction::Outgoing) {\n            let entry = &mut indegree[neighbor.index()];\n            *entry = entry.saturating_sub(1);\n            if *entry == 0 {\n                queue.push_back(neighbor);\n            }\n        }\n    }\n\n    if ordered.len() != graph.node_count() {\n        return Err(anyhow::anyhow!(\"Cycle detected in dependency graph\"));\n    }\n\n    Ok(ordered)\n}\n\npub fn ordered_by_name(\n    files: &[PathBuf],\n    node_map: &HashMap<PathBuf, NodeIndex>,\n) -> Vec<NodeIndex> {\n    let mut sorted = files.to_vec();\n    sorted.sort();\n    sorted\n        .into_iter()\n        .filter_map(|path| node_map.get(&path).copied())\n        .collect()\n}\n\n/// Builds file ordering entries with canonical names and rename flags\npub fn build_entries(ordered: &[PathBuf], step: usize) -> Vec<crate::types::FileOrderEntry> {\n    ordered\n        .iter()\n        .enumerate()\n        .map(|(idx, path)| {\n            let canonical_order = idx * step;\n            let suggested_name =\n                crate::cluster_006::generate_canonical_name(path, canonical_order);\n            let needs_rename = path\n                .file_name()\n                .and_then(|n| n.to_str())\n                .map(|name| name != suggested_name)\n                .unwrap_or(false);\n            crate::types::FileOrderEntry {\n                current_path: path.clone(),\n                canonical_order,\n                suggested_name,\n                needs_rename,\n            }\n        })\n        .collect()\n}\n\npub fn analyze_file_ordering(\n    files: &[PathBuf],\n    step: Option<usize>,\n) -> Result<crate::types::FileOrderingResult> {\n    let step = step.unwrap_or(10);\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = crate::cluster_011::build_module_map(files);\n    let dep_map = crate::cluster_010::build_dependency_map(files, &file_set, &module_map)?;\n    let file_layers = build_file_layers(files);\n    let ordered_directories = crate::layer_core::order_directories(files, &dep_map);\n\n    let (graph, node_map) = crate::cluster_011::build_file_dag(files, &dep_map);\n    let layer_violations = crate::cluster_008::detect_layer_violations(&graph, &file_layers);\n    let cycles = detect_cycles(&graph, files);\n\n    let ordered_nodes = if cycles.is_empty() {\n        crate::layer_core::layer_constrained_sort(&graph, &file_layers).unwrap_or_else(|_| {\n            topological_sort(&graph).unwrap_or_else(|_| ordered_by_name(files, &node_map))\n        })\n    } else {\n        ordered_by_name(files, &node_map)\n    };\n\n    let ordered_files = ordered_nodes\n        .into_iter()\n        .map(|idx| graph[idx].clone())\n        .collect::<Vec<_>>();\n\n    let file_entries = build_entries(&ordered_files, step);\n    let violations = detect_violations(&file_entries, &dep_map);\n\n    Ok(crate::types::FileOrderingResult {\n        ordered_files: file_entries,\n        violations,\n        layer_violations,\n        ordered_directories,\n        cycles,\n    })\n}\n\npub fn naming_score_for_file(\n    file: &Path,\n    order_entry: Option<&crate::types::FileOrderEntry>,\n) -> Option<f64> {\n    let name = file.file_name()?.to_string_lossy();\n    let stem = file.file_stem()?.to_string_lossy();\n    let mut score = 1.0f64;\n\n    if stem.len() < 3 {\n        score -= 0.2;\n    }\n    if stem.len() > 40 {\n        score -= 0.1;\n    }\n    if stem.chars().any(|c| c.is_uppercase()) {\n        score -= 0.1;\n    }\n    if !stem\n        .chars()\n        .all(|c| c.is_ascii_lowercase() || c.is_ascii_digit() || c == '_')\n    {\n        score -= 0.1;\n    }\n    if name.contains(\"__\") {\n        score -= 0.1;\n    }\n\n    if let Some(entry) = order_entry {\n        let expected = entry.suggested_name.as_str();\n        let actual = name.as_ref();\n        if expected != actual {\n            score -= 0.3;\n        } else {\n            score += 0.1;\n        }\n    }\n\n    if let Ok(contents) = fs::read_to_string(file) {\n        let mut ident_counts: HashMap<String, usize> = HashMap::new();\n        let ident_re = match Regex::new(r\"[A-Za-z_][A-Za-z0-9_]*\") {\n            Ok(regex) => regex,\n            Err(_) => return None,\n        };\n        for cap in ident_re.captures_iter(&contents) {\n            let Some(m) = cap.get(0) else {\n                continue;\n            };\n            let ident = m.as_str().to_lowercase();\n            if matches!(\n                ident.as_str(),\n                \"fn\"\n                    | \"pub\"\n                    | \"use\"\n                    | \"struct\"\n                    | \"enum\"\n                    | \"impl\"\n                    | \"mod\"\n                    | \"let\"\n                    | \"mut\"\n                    | \"ref\"\n                    | \"self\"\n                    | \"crate\"\n                    | \"super\"\n                    | \"where\"\n                    | \"trait\"\n                    | \"type\"\n                    | \"const\"\n                    | \"static\"\n                    | \"match\"\n                    | \"if\"\n                    | \"else\"\n                    | \"for\"\n                    | \"while\"\n                    | \"loop\"\n                    | \"return\"\n                    | \"async\"\n                    | \"await\"\n                    | \"move\"\n                    | \"dyn\"\n                    | \"as\"\n            ) {\n                continue;\n            }\n            *ident_counts.entry(ident).or_insert(0) += 1;\n        }\n\n        let mut idents = ident_counts.into_iter().collect::<Vec<_>>();\n        idents.sort_by(|a, b| b.1.cmp(&a.1));\n        let top_idents = idents.into_iter().take(8).map(|(k, _)| k).collect::<Vec<_>>();\n        let name_tokens = stem\n            .split('_')\n            .map(|s| s.to_lowercase())\n            .filter(|s| !s.is_empty() && !s.chars().all(|c| c.is_ascii_digit()))\n            .collect::<Vec<_>>();\n        let overlap = top_idents\n            .iter()\n            .filter(|ident| name_tokens.iter().any(|t| t == *ident))\n            .count();\n\n        if overlap == 0 {\n            score -= 0.1;\n        } else if overlap >= 2 {\n            score += 0.1;\n        }\n    }\n\n    if score < 0.0 {\n        score = 0.0;\n    }\n    if score > 1.0 {\n        score = 1.0;\n    }\n    Some(score * 100.0)\n}\n\npub(crate) fn detect_cycles(\n    graph: &DiGraph<PathBuf, ()>,\n    files: &[PathBuf],\n) -> Vec<Vec<PathBuf>> {\n    let sccs = tarjan_scc(graph);\n    let mut cycles = Vec::new();\n    for scc in sccs {\n        if scc.len() > 1 {\n            cycles.push(scc.into_iter().map(|idx| graph[idx].clone()).collect());\n        }\n    }\n    if cycles.is_empty() {\n        return cycles;\n    }\n    if cycles.iter().all(|cycle| cycle.is_empty()) {\n        let mut fallback = files.to_vec();\n        fallback.sort();\n        cycles.push(fallback);\n    }\n    cycles\n}\n\npub(crate) fn detect_violations(\n    ordered_files: &[crate::types::FileOrderEntry],\n    dep_map: &HashMap<PathBuf, Vec<PathBuf>>,\n) -> Vec<crate::types::OrderViolation> {\n    let mut alpha = ordered_files.to_vec();\n    alpha.sort_by(|a, b| a.current_path.cmp(&b.current_path));\n    let alpha_positions: HashMap<PathBuf, usize> = alpha\n        .iter()\n        .enumerate()\n        .map(|(idx, entry)| (entry.current_path.clone(), idx))\n        .collect();\n\n    let canonical_positions: HashMap<PathBuf, usize> = ordered_files\n        .iter()\n        .enumerate()\n        .map(|(idx, entry)| (entry.current_path.clone(), idx))\n        .collect();\n\n    let mut violations = Vec::new();\n    for entry in ordered_files {\n        let Some(&alpha_pos) = alpha_positions.get(&entry.current_path) else {\n            continue;\n        };\n        let Some(&required_pos) = canonical_positions.get(&entry.current_path) else {\n            continue;\n        };\n        if alpha_pos != required_pos {\n            let blocking_dependencies = dep_map\n                .get(&entry.current_path)\n                .map(|deps| {\n                    deps.iter()\n                        .filter(|dep| {\n                            let dep_alpha = alpha_positions.get(*dep).copied().unwrap_or(0);\n                            dep_alpha > alpha_pos\n                        })\n                        .cloned()\n                        .collect::<Vec<_>>()\n                })\n                .unwrap_or_default();\n            violations.push(crate::types::OrderViolation {\n                file: entry.current_path.clone(),\n                current_position: alpha_pos,\n                required_position: required_pos,\n                blocking_dependencies,\n            });\n        }\n    }\n\n    violations\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\n/// Exports a complete program CFG to DOT format\npub fn export_complete_program_dot(\n    program: &crate::types::ProgramCFG,\n    path: &str,\n) -> std::io::Result<()> {\n    use std::collections::HashMap;\n    use std::fmt::Write;\n\n    fn escape_dot(s: &str) -> String {\n        s.replace('\\\\', \"\\\\\\\\\").replace('\"', \"\\\\\\\"\").replace('\\n', \"\\\\n\")\n    }\n\n    let mut dot = String::new();\n\n    writeln!(dot, \"digraph ProgramCFG {{\").unwrap();\n    writeln!(dot, \"  rankdir=TB;\").unwrap();\n    writeln!(dot, \"  compound=true;\").unwrap();\n    writeln!(dot, \"  newrank=true;\").unwrap();\n    writeln!(\n        dot,\n        \"  label=\\\"Complete Program CFG - {} functions\\\";\",\n        program.functions.len()\n    )\n    .unwrap();\n    writeln!(dot, \"  labelloc=t;\").unwrap();\n    writeln!(dot, \"  fontsize=16;\").unwrap();\n    writeln!(dot, \"\").unwrap();\n\n    let mut funcs: Vec<_> = program.functions.iter().collect();\n    funcs.sort_by_key(|(fid, _)| fid.as_str());\n\n    let mut func_to_cluster: HashMap<&String, usize> = HashMap::new();\n\n    for (cluster_idx, (func_id, cfg)) in funcs.iter().enumerate() {\n        let safe_name = func_id.replace(['!', '?', '*'], \"_\");\n        let cc = crate::cluster_008::cyclomatic_complexity(cfg);\n        func_to_cluster.insert(func_id, cluster_idx);\n\n        writeln!(dot, \"  subgraph cluster_{} {{\", cluster_idx).unwrap();\n        writeln!(dot, \"    label=\\\"{} (CC={})\\\";\", safe_name, cc).unwrap();\n        writeln!(dot, \"    style=filled;\").unwrap();\n        writeln!(dot, \"    fillcolor=lightgray;\").unwrap();\n        writeln!(dot, \"    color=black;\").unwrap();\n        writeln!(dot, \"\").unwrap();\n\n        for node in &cfg.nodes {\n            let (shape, color, style) = crate::cluster_008::node_style(&node.node_type);\n\n            let mut label = node.label.clone();\n            if !node.lines.is_empty() {\n                let lines_str: String = node\n                    .lines\n                    .iter()\n                    .map(|l| l.to_string())\n                    .collect::<Vec<_>>()\n                    .join(\",\");\n                label = format!(\"{} L{}\", label, lines_str);\n            }\n\n            let url = format!(\"http://127.0.0.1:8081/run?f={}\", func_id);\n\n            writeln!(\n                dot,\n                \"    f{}_n{} [label=\\\"{}\\\", shape={}, fillcolor={}, style={}, URL=\\\"{}\\\"];\",\n                cluster_idx,\n                node.id,\n                escape_dot(&label),\n                shape,\n                color,\n                style,\n                url\n            )\n            .unwrap();\n        }\n\n        writeln!(dot, \"\").unwrap();\n\n        for edge in &cfg.edges {\n            let mut attrs = Vec::new();\n            if let Some(cond) = edge.condition {\n                let label = if cond { \"T\" } else { \"F\" };\n                let color = if cond { \"darkgreen\" } else { \"red\" };\n                attrs.push(format!(\"label=\\\"{}\\\"\", label));\n                attrs.push(format!(\"color=\\\"{}\\\"\", color));\n            }\n            let attr_str = if attrs.is_empty() {\n                \"\".to_string()\n            } else {\n                format!(\" [{}]\", attrs.join(\", \"))\n            };\n\n            writeln!(\n                dot,\n                \"    f{}_n{} -> f{}_n{}{};\",\n                cluster_idx,\n                edge.from,\n                cluster_idx,\n                edge.to,\n                attr_str\n            )\n            .unwrap();\n        }\n\n        writeln!(dot, \"  }}\").unwrap();\n        writeln!(dot, \"\").unwrap();\n    }\n\n    writeln!(dot, \"  // Inter-function calls\").unwrap();\n    writeln!(dot, \"  edge [style=dashed, color=blue, penwidth=2];\").unwrap();\n    writeln!(dot, \"\").unwrap();\n\n    for (caller, callee) in &program.call_edges {\n        if let (Some(&caller_idx), Some(&callee_idx)) =\n            (func_to_cluster.get(caller), func_to_cluster.get(callee))\n        {\n            if let (Some(caller_cfg), Some(callee_cfg)) =\n                (program.functions.get(caller), program.functions.get(callee))\n            {\n                writeln!(\n                    dot,\n                    \"  f{}_n{} -> f{}_n{} [ltail=cluster_{}, lhead=cluster_{}, label=\\\"call\\\"];\",\n                    caller_idx,\n                    caller_cfg.exit_id,\n                    callee_idx,\n                    callee_cfg.entry_id,\n                    caller_idx,\n                    callee_idx\n                )\n                .unwrap();\n            }\n        }\n    }\n\n    writeln!(dot, \"}}\").unwrap();\n\n    std::fs::write(path, dot)?;\n    println!(\"Complete program CFG exported to {}\", path);\n    Ok(())\n}\n\n// ============================================================================\n// Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_detects_cycles() {\n        detects_cycles().unwrap();\n    }\n\n    #[test]\n    fn test_generates_canonical_names_and_violations() {\n        generates_canonical_names_and_violations().unwrap();\n    }\n}\n",
          "updated_content": "//! Cluster 001: Core dependency analysis and file ordering utilities\n//!\n//! This module contains fundamental functions for:\n//! - Module mapping and dependency resolution\n//! - Topological sorting and layer-constrained ordering\n//! - File gathering and layer construction\n//! - Dependency graph building and cycle detection\n//! - DOT export for program CFGs\n//! - Naming validation and warnings\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse petgraph::algo::tarjan_scc;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::{ItemUse, UseTree};\n\nuse crate::dependency::{LayerGraph, ReferenceDetail, UnresolvedDependency};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\npub fn build_directory_entry_map(\n    files: &[PathBuf],\n) -> Result<HashMap<PathBuf, crate::types::FileOrderEntry>> {\n    use crate::file_ordering::{\n        build_dependency_map, build_entries, build_file_dag, detect_cycles, ordered_by_name,\n        topological_sort,\n    };\n    use crate::layer_core::layer_constrained_sort;\n    use crate::layer_utilities::build_file_layers;\n    use crate::types::FileOrderingResult;\n    use std::collections::HashSet;\n\n    const DEFAULT_STEP: usize = 10;\n\n    if files.is_empty() {\n        return Ok(HashMap::new());\n    }\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = crate::cluster_011::build_module_map(files);\n    let dep_map = build_dependency_map(files, &file_set, &module_map)?;\n    let file_layers = build_file_layers(files);\n    let (graph, node_map) = build_file_dag(files, &dep_map);\n    let cycles = detect_cycles(&graph, files);\n\n    let ordered_nodes = if cycles.is_empty() {\n        layer_constrained_sort(&graph, &file_layers).unwrap_or_else(|_| {\n            topological_sort(&graph).unwrap_or_else(|_| ordered_by_name(files, &node_map))\n        })\n    } else {\n        ordered_by_name(files, &node_map)\n    };\n\n    let ordered_files = ordered_nodes\n        .into_iter()\n        .map(|idx| graph[idx].clone())\n        .collect::<Vec<_>>();\n\n    let ordering = FileOrderingResult {\n        ordered_files: build_entries(&ordered_files, DEFAULT_STEP),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles,\n    };\n    let mut map = HashMap::new();\n    for entry in ordering.ordered_files {\n        map.insert(entry.current_path.clone(), entry);\n    }\n    Ok(map)\n}\n\npub fn collect_naming_warnings(\n    directory: &crate::types::DirectoryAnalysis,\n    config: &crate::report::ReportConfig,\n    warnings: &mut Vec<String>,\n) -> Result<()> {\n    use crate::utilities::compress_path;\n    use crate::dependency::naming_score_for_file;\n    if directory\n        .path\n        .components()\n        .any(|comp| comp.as_os_str() == \"_old\")\n    {\n        return Ok(());\n    }\n    let file_map = build_directory_entry_map(&directory.files)?;\n    for file in &directory.files {\n        if file.components().any(|comp| comp.as_os_str() == \"_old\") {\n            continue;\n        }\n        let entry = file_map.get(file);\n        if let Some(score) = naming_score_for_file(file, entry) {\n            if score < config.naming_score_warning {\n                let suggested = entry\n                    .map(|e| e.suggested_name.as_str())\n                    .unwrap_or(\"suggested name unavailable\");\n                warnings.push(format!(\n                    \"File `{}` has naming score {:.0}; consider renaming to `{}`.\",\n                    compress_path(file.to_string_lossy().as_ref()),\n                    score,\n                    suggested,\n                ));\n            }\n        }\n    }\n    for child in &directory.subdirectories {\n        collect_naming_warnings(child, config, warnings)?;\n    }\n    Ok(())\n}\n\n#[cfg(test)]\nfn temp_dir(name: &str) -> PathBuf {\n    let mut dir = std::env::temp_dir();\n    dir.push(format!(\n        \"mmsb_analyzer_{}_{}\",\n        name,\n        std::time::SystemTime::now()\n            .duration_since(std::time::UNIX_EPOCH)\n            .unwrap()\n            .as_nanos()\n    ));\n    dir\n}\n\n#[cfg(test)]\nfn detects_cycles() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"cycle\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    write(&a, \"use crate::b; pub fn a() {}\")?;\n    write(&b, \"use crate::a; pub fn b() {}\")?;\n\n    let result = analyze_file_ordering(&[a.clone(), b.clone()], Some(10))?;\n    assert!(!result.cycles.is_empty());\n    Ok(())\n}\n\n#[cfg(test)]\nfn generates_canonical_names_and_violations() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"names\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    write(&a, \"use crate::b; pub fn a() {}\")?;\n    write(&b, \"pub fn b() {}\")?;\n\n    let result = analyze_file_ordering(&[a.clone(), b.clone()], Some(10))?;\n    let entries = &result.ordered_files;\n    assert_eq!(entries[0].suggested_name, \"000_b.rs\");\n    assert_eq!(entries[1].suggested_name, \"010_a.rs\");\n    assert!(!result.violations.is_empty());\n    Ok(())\n}\n\n#[cfg(test)]\n#[allow(dead_code)]\nfn topo_sort_orders_dependencies() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"topo\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    let c = dir.join(\"c.rs\");\n    write(&a, \"pub fn a() {}\")?;\n    write(&b, \"use crate::a; pub fn b() {}\")?;\n    write(&c, \"use crate::b; pub fn c() {}\")?;\n\n    let result = analyze_file_ordering(&[c.clone(), b.clone(), a.clone()], Some(10))?;\n    let ordered: Vec<_> = result\n        .ordered_files\n        .iter()\n        .map(|entry| entry.current_path.clone())\n        .collect();\n    assert_eq!(ordered, vec![a, b, c]);\n    assert!(result.cycles.is_empty());\n    Ok(())\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\npub fn layer_constrained_sort(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Result<Vec<NodeIndex>> {\n    use crate::cluster_006::layer_prefix_value;\n\n    let mut layer_nodes: BTreeMap<i32, Vec<NodeIndex>> = BTreeMap::new();\n    for node in graph.node_indices() {\n        let file = &graph[node];\n        let layer_name = file_layers\n            .get(file)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_value = layer_prefix_value(&layer_name).unwrap_or(0);\n        layer_nodes.entry(layer_value).or_default().push(node);\n    }\n\n    let mut ordered = Vec::new();\n    for (_layer, nodes) in layer_nodes {\n        let sorted = topo_sort_within(graph, &nodes)?;\n        ordered.extend(sorted);\n    }\n    Ok(ordered)\n}\n\npub fn topo_sort_within(\n    graph: &DiGraph<PathBuf, ()>,\n    nodes: &[NodeIndex],\n) -> Result<Vec<NodeIndex>> {\n    let node_set: HashSet<NodeIndex> = nodes.iter().copied().collect();\n    let mut indegree: HashMap<NodeIndex, usize> = HashMap::new();\n    for &node in nodes {\n        indegree.insert(node, 0);\n    }\n    for &node in nodes {\n        let incoming = graph\n            .neighbors_directed(node, petgraph::Direction::Incoming)\n            .filter(|n| node_set.contains(n))\n            .count();\n        indegree.insert(node, incoming);\n    }\n    let mut queue = std::collections::VecDeque::new();\n    for &node in nodes {\n        if indegree.get(&node).copied().unwrap_or(0) == 0 {\n            queue.push_back(node);\n        }\n    }\n    let mut ordered = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        ordered.push(node);\n        for neighbor in graph.neighbors_directed(node, petgraph::Direction::Outgoing) {\n            if !node_set.contains(&neighbor) {\n                continue;\n            }\n            if let Some(entry) = indegree.get_mut(&neighbor) {\n                *entry = entry.saturating_sub(1);\n                if *entry == 0 {\n                    queue.push_back(neighbor);\n                }\n            }\n        }\n    }\n    if ordered.len() != nodes.len() {\n        return Err(anyhow::anyhow!(\"Cycle detected within layer group\"));\n    }\n    Ok(ordered)\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs\n// ============================================================================\n\n/// Detects the layer identifier from a path by finding first digit-prefixed component\npub fn detect_layer(path: &Path) -> String {\n    for component in path.components() {\n        if let Some(name) = component.as_os_str().to_str() {\n            if let Some(first) = name.chars().next() {\n                if first.is_ascii_digit() {\n                    if let Some(pos) = name.find('_') {\n                        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n                            return name.to_string();\n                        }\n                    }\n                }\n            }\n        }\n    }\n    \"root\".to_string()\n}\n\npub fn rust_entry_paths(root: &Path) -> BTreeSet<PathBuf> {\n    let src_dir = crate::layer_utilities::resolve_source_root(root);\n    [\"lib.rs\", \"main.rs\"]\n        .iter()\n        .map(|rel| src_dir.join(rel))\n        .filter(|p| p.exists())\n        .collect()\n}\n\n#[derive(Clone)]\nstruct RustDependency {\n    root: String,\n    detail: String,\n}\n\nfn collect_rust_dependencies(path: &Path) -> Result<Vec<RustDependency>> {\n    let content =\n        fs::read_to_string(path).with_context(|| format!(\"Unable to read Rust file {:?}\", path))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", path))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    Ok(collector.deps)\n}\n\n#[derive(Default)]\nstruct UseCollector {\n    deps: Vec<RustDependency>,\n}\n\nimpl<'ast> Visit<'ast> for UseCollector {\n    fn visit_item_use(&mut self, node: &'ast ItemUse) {\n        let mut roots = BTreeSet::new();\n        collect_roots_from_crate(&node.tree, CrateRootState::Start, &mut roots);\n        let stmt = quote::quote!(#node).to_string();\n        for root in roots {\n            self.deps.push(RustDependency {\n                root,\n                detail: stmt.clone(),\n            });\n        }\n    }\n}\n\n#[derive(Copy, Clone, Eq, PartialEq)]\nenum CrateRootState {\n    Start,\n    AfterCrate,\n}\n\nfn collect_roots_from_crate(tree: &UseTree, state: CrateRootState, acc: &mut BTreeSet<String>) {\n    match tree {\n        UseTree::Path(path) => {\n            let ident = path.ident.to_string();\n            if state == CrateRootState::Start && ident == \"crate\" {\n                collect_roots_from_crate(&path.tree, CrateRootState::AfterCrate, acc);\n            } else if state == CrateRootState::AfterCrate {\n                acc.insert(ident);\n            } else {\n                collect_roots_from_crate(&path.tree, state, acc);\n            }\n        }\n        UseTree::Group(group) => {\n            for tree in &group.items {\n                collect_roots_from_crate(tree, state, acc);\n            }\n        }\n        UseTree::Name(name) => {\n            if state == CrateRootState::AfterCrate {\n                acc.insert(name.ident.to_string());\n            }\n        }\n        UseTree::Rename(rename) => {\n            if state == CrateRootState::AfterCrate {\n                acc.insert(rename.ident.to_string());\n            }\n        }\n        UseTree::Glob(_) => {}\n    }\n}\n\n/// Order Rust files by dependency and capture layer graph details.\npub fn order_rust_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let module_map = crate::cluster_010::build_module_root_map(root)?;\n    let entry_files = rust_entry_paths(root);\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n\n    for file in files {\n        let layer = detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let deps = collect_rust_dependencies(file)\n            .with_context(|| format!(\"Failed to collect dependencies for {:?}\", file))?;\n        for dep in deps {\n            if let Some(info) = module_map.get(&dep.root) {\n                nodes.insert(info.layer.clone());\n                if info.layer != layer {\n                    edges_map\n                        .entry((info.layer.clone(), layer.clone()))\n                        .or_default()\n                        .insert(ReferenceDetail {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                }\n            } else {\n                unresolved.push(UnresolvedDependency {\n                    file: file.clone(),\n                    reference: dep.detail.clone(),\n                });\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n\n// ============================================================================\n// Julia Dependency Analysis (from src/000_dependency.rs)\n// ============================================================================\n\n#[derive(Clone)]\npub(crate) struct JuliaDependency {\n    pub(crate) target: JuliaTarget,\n    pub(crate) detail: String,\n}\n\n#[derive(Clone)]\npub(crate) enum JuliaTarget {\n    Include(PathBuf),\n    Module(String),\n}\n\nstatic INCLUDE_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)include\\s*\\(\\s*[\"']([^\"'\\n]+)[\"']\"#).expect(\"failed to compile include regex\")\n});\nstatic USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#)\n        .expect(\"failed to compile using regex\")\n});\nstatic ROOT_USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#)\n        .expect(\"failed to compile root using regex\")\n});\nstatic LOCAL_USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+\\.\\s*([A-Za-z0-9_]+)\"#)\n        .expect(\"failed to compile local using regex\")\n});\n\npub(crate) fn collect_julia_dependencies(path: &Path) -> Result<Vec<JuliaDependency>> {\n    let content = fs::read_to_string(path)\n        .with_context(|| format!(\"Unable to read Julia file {:?}\", path))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_REGEX.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let relative = PathBuf::from(path_match.as_str());\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Include(relative),\n                detail,\n            });\n        }\n    }\n\n    for cap in USING_REGEX.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            let primary = module.split('.').next().unwrap_or(module).to_string();\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Module(primary),\n                detail,\n            });\n        }\n    }\n\n    for cap in ROOT_USING_REGEX.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                let primary = symbol.split('.').next().unwrap_or(symbol).to_string();\n                deps.push(JuliaDependency {\n                    target: JuliaTarget::Module(primary),\n                    detail: detail.clone(),\n                });\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_REGEX.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Module(module.to_string()),\n                detail,\n            });\n        }\n    }\n\n    Ok(deps)\n}\n\npub fn julia_entry_paths(root: &Path) -> BTreeSet<PathBuf> {\n    let src_dir = crate::layer_utilities::resolve_source_root(root);\n    [\"MMSB.jl\", \"API.jl\", \"MMSB/API.jl\"]\n        .iter()\n        .map(|rel| src_dir.join(rel))\n        .filter(|p| p.exists())\n        .collect()\n}\n\npub fn build_file_layers(files: &[PathBuf]) -> HashMap<PathBuf, String> {\n    let mut layers = HashMap::new();\n    for file in files {\n        layers.insert(file.clone(), detect_layer(file));\n    }\n    layers\n}\n\npub fn gather_julia_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = crate::layer_utilities::resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            crate::layer_utilities::allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"jl\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n    .collect()\n}\n\n// ============================================================================\n// From src/090_file_ordering.rs\n// ============================================================================\n\npub fn topological_sort(graph: &DiGraph<PathBuf, ()>) -> Result<Vec<NodeIndex>> {\n    use petgraph::Direction;\n    use std::collections::VecDeque;\n\n    let mut indegree = vec![0usize; graph.node_count()];\n    for node in graph.node_indices() {\n        indegree[node.index()] = graph\n            .neighbors_directed(node, Direction::Incoming)\n            .count();\n    }\n\n    let mut queue = VecDeque::new();\n    for node in graph.node_indices() {\n        if indegree[node.index()] == 0 {\n            queue.push_back(node);\n        }\n    }\n\n    let mut ordered = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        ordered.push(node);\n        for neighbor in graph.neighbors_directed(node, Direction::Outgoing) {\n            let entry = &mut indegree[neighbor.index()];\n            *entry = entry.saturating_sub(1);\n            if *entry == 0 {\n                queue.push_back(neighbor);\n            }\n        }\n    }\n\n    if ordered.len() != graph.node_count() {\n        return Err(anyhow::anyhow!(\"Cycle detected in dependency graph\"));\n    }\n\n    Ok(ordered)\n}\n\npub fn ordered_by_name(\n    files: &[PathBuf],\n    node_map: &HashMap<PathBuf, NodeIndex>,\n) -> Vec<NodeIndex> {\n    let mut sorted = files.to_vec();\n    sorted.sort();\n    sorted\n        .into_iter()\n        .filter_map(|path| node_map.get(&path).copied())\n        .collect()\n}\n\n/// Builds file ordering entries with canonical names and rename flags\npub fn build_entries(ordered: &[PathBuf], step: usize) -> Vec<crate::types::FileOrderEntry> {\n    ordered\n        .iter()\n        .enumerate()\n        .map(|(idx, path)| {\n            let canonical_order = idx * step;\n            let suggested_name =\n                crate::cluster_006::generate_canonical_name(path, canonical_order);\n            let needs_rename = path\n                .file_name()\n                .and_then(|n| n.to_str())\n                .map(|name| name != suggested_name)\n                .unwrap_or(false);\n            crate::types::FileOrderEntry {\n                current_path: path.clone(),\n                canonical_order,\n                suggested_name,\n                needs_rename,\n            }\n        })\n        .collect()\n}\n\npub fn analyze_file_ordering(\n    files: &[PathBuf],\n    step: Option<usize>,\n) -> Result<crate::types::FileOrderingResult> {\n    let step = step.unwrap_or(10);\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = crate::cluster_011::build_module_map(files);\n    let dep_map = crate::cluster_010::build_dependency_map(files, &file_set, &module_map)?;\n    let file_layers = build_file_layers(files);\n    let ordered_directories = crate::layer_core::order_directories(files, &dep_map);\n\n    let (graph, node_map) = crate::cluster_011::build_file_dag(files, &dep_map);\n    let layer_violations = crate::cluster_008::detect_layer_violations(&graph, &file_layers);\n    let cycles = detect_cycles(&graph, files);\n\n    let ordered_nodes = if cycles.is_empty() {\n        crate::layer_core::layer_constrained_sort(&graph, &file_layers).unwrap_or_else(|_| {\n            topological_sort(&graph).unwrap_or_else(|_| ordered_by_name(files, &node_map))\n        })\n    } else {\n        ordered_by_name(files, &node_map)\n    };\n\n    let ordered_files = ordered_nodes\n        .into_iter()\n        .map(|idx| graph[idx].clone())\n        .collect::<Vec<_>>();\n\n    let file_entries = build_entries(&ordered_files, step);\n    let violations = detect_violations(&file_entries, &dep_map);\n\n    Ok(crate::types::FileOrderingResult {\n        ordered_files: file_entries,\n        violations,\n        layer_violations,\n        ordered_directories,\n        cycles,\n    })\n}\n\npub fn naming_score_for_file(\n    file: &Path,\n    order_entry: Option<&crate::types::FileOrderEntry>,\n) -> Option<f64> {\n    let name = file.file_name()?.to_string_lossy();\n    let stem = file.file_stem()?.to_string_lossy();\n    let mut score = 1.0f64;\n\n    if stem.len() < 3 {\n        score -= 0.2;\n    }\n    if stem.len() > 40 {\n        score -= 0.1;\n    }\n    if stem.chars().any(|c| c.is_uppercase()) {\n        score -= 0.1;\n    }\n    if !stem\n        .chars()\n        .all(|c| c.is_ascii_lowercase() || c.is_ascii_digit() || c == '_')\n    {\n        score -= 0.1;\n    }\n    if name.contains(\"__\") {\n        score -= 0.1;\n    }\n\n    if let Some(entry) = order_entry {\n        let expected = entry.suggested_name.as_str();\n        let actual = name.as_ref();\n        if expected != actual {\n            score -= 0.3;\n        } else {\n            score += 0.1;\n        }\n    }\n\n    if let Ok(contents) = fs::read_to_string(file) {\n        let mut ident_counts: HashMap<String, usize> = HashMap::new();\n        let ident_re = match Regex::new(r\"[A-Za-z_][A-Za-z0-9_]*\") {\n            Ok(regex) => regex,\n            Err(_) => return None,\n        };\n        for cap in ident_re.captures_iter(&contents) {\n            let Some(m) = cap.get(0) else {\n                continue;\n            };\n            let ident = m.as_str().to_lowercase();\n            if matches!(\n                ident.as_str(),\n                \"fn\"\n                    | \"pub\"\n                    | \"use\"\n                    | \"struct\"\n                    | \"enum\"\n                    | \"impl\"\n                    | \"mod\"\n                    | \"let\"\n                    | \"mut\"\n                    | \"ref\"\n                    | \"self\"\n                    | \"crate\"\n                    | \"super\"\n                    | \"where\"\n                    | \"trait\"\n                    | \"type\"\n                    | \"const\"\n                    | \"static\"\n                    | \"match\"\n                    | \"if\"\n                    | \"else\"\n                    | \"for\"\n                    | \"while\"\n                    | \"loop\"\n                    | \"return\"\n                    | \"async\"\n                    | \"await\"\n                    | \"move\"\n                    | \"dyn\"\n                    | \"as\"\n            ) {\n                continue;\n            }\n            *ident_counts.entry(ident).or_insert(0) += 1;\n        }\n\n        let mut idents = ident_counts.into_iter().collect::<Vec<_>>();\n        idents.sort_by(|a, b| b.1.cmp(&a.1));\n        let top_idents = idents.into_iter().take(8).map(|(k, _)| k).collect::<Vec<_>>();\n        let name_tokens = stem\n            .split('_')\n            .map(|s| s.to_lowercase())\n            .filter(|s| !s.is_empty() && !s.chars().all(|c| c.is_ascii_digit()))\n            .collect::<Vec<_>>();\n        let overlap = top_idents\n            .iter()\n            .filter(|ident| name_tokens.iter().any(|t| t == *ident))\n            .count();\n\n        if overlap == 0 {\n            score -= 0.1;\n        } else if overlap >= 2 {\n            score += 0.1;\n        }\n    }\n\n    if score < 0.0 {\n        score = 0.0;\n    }\n    if score > 1.0 {\n        score = 1.0;\n    }\n    Some(score * 100.0)\n}\n\npub(crate) fn detect_cycles(\n    graph: &DiGraph<PathBuf, ()>,\n    files: &[PathBuf],\n) -> Vec<Vec<PathBuf>> {\n    let sccs = tarjan_scc(graph);\n    let mut cycles = Vec::new();\n    for scc in sccs {\n        if scc.len() > 1 {\n            cycles.push(scc.into_iter().map(|idx| graph[idx].clone()).collect());\n        }\n    }\n    if cycles.is_empty() {\n        return cycles;\n    }\n    if cycles.iter().all(|cycle| cycle.is_empty()) {\n        let mut fallback = files.to_vec();\n        fallback.sort();\n        cycles.push(fallback);\n    }\n    cycles\n}\n\npub(crate) fn detect_violations(\n    ordered_files: &[crate::types::FileOrderEntry],\n    dep_map: &HashMap<PathBuf, Vec<PathBuf>>,\n) -> Vec<crate::types::OrderViolation> {\n    let mut alpha = ordered_files.to_vec();\n    alpha.sort_by(|a, b| a.current_path.cmp(&b.current_path));\n    let alpha_positions: HashMap<PathBuf, usize> = alpha\n        .iter()\n        .enumerate()\n        .map(|(idx, entry)| (entry.current_path.clone(), idx))\n        .collect();\n\n    let canonical_positions: HashMap<PathBuf, usize> = ordered_files\n        .iter()\n        .enumerate()\n        .map(|(idx, entry)| (entry.current_path.clone(), idx))\n        .collect();\n\n    let mut violations = Vec::new();\n    for entry in ordered_files {\n        let Some(&alpha_pos) = alpha_positions.get(&entry.current_path) else {\n            continue;\n        };\n        let Some(&required_pos) = canonical_positions.get(&entry.current_path) else {\n            continue;\n        };\n        if alpha_pos != required_pos {\n            let blocking_dependencies = dep_map\n                .get(&entry.current_path)\n                .map(|deps| {\n                    deps.iter()\n                        .filter(|dep| {\n                            let dep_alpha = alpha_positions.get(*dep).copied().unwrap_or(0);\n                            dep_alpha > alpha_pos\n                        })\n                        .cloned()\n                        .collect::<Vec<_>>()\n                })\n                .unwrap_or_default();\n            violations.push(crate::types::OrderViolation {\n                file: entry.current_path.clone(),\n                current_position: alpha_pos,\n                required_position: required_pos,\n                blocking_dependencies,\n            });\n        }\n    }\n\n    violations\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\n/// Exports a complete program CFG to DOT format\npub fn export_complete_program_dot(\n    program: &crate::types::ProgramCFG,\n    path: &str,\n) -> std::io::Result<()> {\n    use std::collections::HashMap;\n    use std::fmt::Write;\n\n    fn escape_dot(s: &str) -> String {\n        s.replace('\\\\', \"\\\\\\\\\").replace('\"', \"\\\\\\\"\").replace('\\n', \"\\\\n\")\n    }\n\n    let mut dot = String::new();\n\n    writeln!(dot, \"digraph ProgramCFG {{\").unwrap();\n    writeln!(dot, \"  rankdir=TB;\").unwrap();\n    writeln!(dot, \"  compound=true;\").unwrap();\n    writeln!(dot, \"  newrank=true;\").unwrap();\n    writeln!(\n        dot,\n        \"  label=\\\"Complete Program CFG - {} functions\\\";\",\n        program.functions.len()\n    )\n    .unwrap();\n    writeln!(dot, \"  labelloc=t;\").unwrap();\n    writeln!(dot, \"  fontsize=16;\").unwrap();\n    writeln!(dot, \"\").unwrap();\n\n    let mut funcs: Vec<_> = program.functions.iter().collect();\n    funcs.sort_by_key(|(fid, _)| fid.as_str());\n\n    let mut func_to_cluster: HashMap<&String, usize> = HashMap::new();\n\n    for (cluster_idx, (func_id, cfg)) in funcs.iter().enumerate() {\n        let safe_name = func_id.replace(['!', '?', '*'], \"_\");\n        let cc = crate::cluster_008::cyclomatic_complexity(cfg);\n        func_to_cluster.insert(func_id, cluster_idx);\n\n        writeln!(dot, \"  subgraph cluster_{} {{\", cluster_idx).unwrap();\n        writeln!(dot, \"    label=\\\"{} (CC={})\\\";\", safe_name, cc).unwrap();\n        writeln!(dot, \"    style=filled;\").unwrap();\n        writeln!(dot, \"    fillcolor=lightgray;\").unwrap();\n        writeln!(dot, \"    color=black;\").unwrap();\n        writeln!(dot, \"\").unwrap();\n\n        for node in &cfg.nodes {\n            let (shape, color, style) = crate::cluster_008::node_style(&node.node_type);\n\n            let mut label = node.label.clone();\n            if !node.lines.is_empty() {\n                let lines_str: String = node\n                    .lines\n                    .iter()\n                    .map(|l| l.to_string())\n                    .collect::<Vec<_>>()\n                    .join(\",\");\n                label = format!(\"{} L{}\", label, lines_str);\n            }\n\n            let url = format!(\"http://127.0.0.1:8081/run?f={}\", func_id);\n\n            writeln!(\n                dot,\n                \"    f{}_n{} [label=\\\"{}\\\", shape={}, fillcolor={}, style={}, URL=\\\"{}\\\"];\",\n                cluster_idx,\n                node.id,\n                escape_dot(&label),\n                shape,\n                color,\n                style,\n                url\n            )\n            .unwrap();\n        }\n\n        writeln!(dot, \"\").unwrap();\n\n        for edge in &cfg.edges {\n            let mut attrs = Vec::new();\n            if let Some(cond) = edge.condition {\n                let label = if cond { \"T\" } else { \"F\" };\n                let color = if cond { \"darkgreen\" } else { \"red\" };\n                attrs.push(format!(\"label=\\\"{}\\\"\", label));\n                attrs.push(format!(\"color=\\\"{}\\\"\", color));\n            }\n            let attr_str = if attrs.is_empty() {\n                \"\".to_string()\n            } else {\n                format!(\" [{}]\", attrs.join(\", \"))\n            };\n\n            writeln!(\n                dot,\n                \"    f{}_n{} -> f{}_n{}{};\",\n                cluster_idx,\n                edge.from,\n                cluster_idx,\n                edge.to,\n                attr_str\n            )\n            .unwrap();\n        }\n\n        writeln!(dot, \"  }}\").unwrap();\n        writeln!(dot, \"\").unwrap();\n    }\n\n    writeln!(dot, \"  // Inter-function calls\").unwrap();\n    writeln!(dot, \"  edge [style=dashed, color=blue, penwidth=2];\").unwrap();\n    writeln!(dot, \"\").unwrap();\n\n    for (caller, callee) in &program.call_edges {\n        if let (Some(&caller_idx), Some(&callee_idx)) =\n            (func_to_cluster.get(caller), func_to_cluster.get(callee))\n        {\n            if let (Some(caller_cfg), Some(callee_cfg)) =\n                (program.functions.get(caller), program.functions.get(callee))\n            {\n                writeln!(\n                    dot,\n                    \"  f{}_n{} -> f{}_n{} [ltail=cluster_{}, lhead=cluster_{}, label=\\\"call\\\"];\",\n                    caller_idx,\n                    caller_cfg.exit_id,\n                    callee_idx,\n                    callee_cfg.entry_id,\n                    caller_idx,\n                    callee_idx\n                )\n                .unwrap();\n            }\n        }\n    }\n\n    writeln!(dot, \"}}\").unwrap();\n\n    std::fs::write(path, dot)?;\n    println!(\"Complete program CFG exported to {}\", path);\n    Ok(())\n}\n\n// ============================================================================\n// Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_detects_cycles() {\n        detects_cycles().unwrap();\n    }\n\n    #[test]\n    fn test_generates_canonical_names_and_violations() {\n        generates_canonical_names_and_violations().unwrap();\n    }\n}\n\npub fn order_julia_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, crate::dependency::LayerGraph)> {\n    use crate::cluster_001::{collect_julia_dependencies, JuliaTarget};\n    use crate::dependency::ReferenceDetail;\n\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n    let resolver = LayerResolver::build(root)?;\n    let entry_files = crate::cluster_001::julia_entry_paths(root);\n\n    for file in files {\n        let layer = crate::cluster_001::detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let references = collect_julia_dependencies(file)\n            .with_context(|| format!(\"Failed to analyze Julia dependencies for {:?}\", file))?;\n        for dep in references {\n            match dep.target {\n                JuliaTarget::Include(include_path) => {\n                    let resolved = if include_path.is_absolute() {\n                        include_path.clone()\n                    } else {\n                        file.parent()\n                            .map(|p| p.join(&include_path))\n                            .unwrap_or(include_path.clone())\n                    };\n\n                    if resolved.exists() {\n                        let target_layer = crate::cluster_001::detect_layer(&resolved);\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n                JuliaTarget::Module(module) => {\n                    if let Some(target_layer) = resolver.resolve_module(&module) {\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_structural_cmp_to_src/010_cluster_008.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/060_layer_core.rs",
          "original_content": "//! Core layer computation functions - lowest level utilities\n//! This module must have no dependencies on other local modules to avoid circular dependencies.\n//! It provides fundamental layer prefix extraction and violation detection.\n\n#[allow(unused_imports)]\npub use crate::cluster_006::{\n    layer_prefix_value, order_directories, collect_directory_moves,\n};\n#[allow(unused_imports)]\npub use crate::cluster_008::detect_layer_violations;\n#[allow(unused_imports)]\npub use crate::cluster_001::{layer_constrained_sort, topo_sort_within};\nuse crate::cluster_008::structural_layer_value;\n\npub fn structural_cmp(a: &crate::report::PlanItem, b: &crate::report::PlanItem) -> std::cmp::Ordering {\n    let a_required = structural_layer_value(&a.required_layer, i32::MAX);\n    let b_required = structural_layer_value(&b.required_layer, i32::MAX);\n    let a_current = structural_layer_value(&a.current_layer, i32::MIN);\n    let b_current = structural_layer_value(&b.current_layer, i32::MIN);\n    let a_benefit = if a.cost == 0 {\n        0\n    } else {\n        (a.benefit.saturating_mul(1000)) / a.cost\n    };\n    let b_benefit = if b.cost == 0 {\n        0\n    } else {\n        (b.benefit.saturating_mul(1000)) / b.cost\n    };\n    a_required\n        .cmp(&b_required)\n        .then_with(|| b.is_utility.cmp(&a.is_utility))\n        .then_with(|| b_benefit.cmp(&a_benefit))\n        .then_with(|| b.impact_weight.cmp(&a.impact_weight))\n        .then_with(|| b_current.cmp(&a_current))\n        .then_with(|| a.description.cmp(&b.description))\n}\n\npub fn sort_structural_items(items: &mut Vec<crate::report::PlanItem>) {\n    use std::collections::HashMap;\n    use std::path::PathBuf;\n\n    if items.len() <= 1 {\n        return;\n    }\n\n    let count = items.len();\n    let mut edges: Vec<Vec<usize>> = vec![Vec::new(); count];\n    let mut indegree = vec![0usize; count];\n\n    let mut file_to_items: HashMap<PathBuf, Vec<usize>> = HashMap::new();\n    for (idx, item) in items.iter().enumerate() {\n        if let Some(path) = &item.current_file {\n            file_to_items.entry(path.clone()).or_default().push(idx);\n        }\n    }\n\n    for i in 0..count {\n        for j in (i + 1)..count {\n            let req_i = structural_layer_value(&items[i].required_layer, i32::MAX);\n            let req_j = structural_layer_value(&items[j].required_layer, i32::MAX);\n            let mut edge = None;\n            if req_i != req_j {\n                edge = if req_i < req_j { Some((i, j)) } else { Some((j, i)) };\n            } else if items[i].is_utility != items[j].is_utility {\n                edge = if items[i].is_utility {\n                    Some((i, j))\n                } else {\n                    Some((j, i))\n                };\n            }\n            if let Some((from, to)) = edge {\n                edges[from].push(to);\n                indegree[to] += 1;\n            }\n        }\n    }\n\n    for (idx, item) in items.iter().enumerate() {\n        for file in &item.outgoing_files {\n            if let Some(dependents) = file_to_items.get(file) {\n                for &dependent_idx in dependents {\n                    if dependent_idx == idx {\n                        continue;\n                    }\n                    edges[dependent_idx].push(idx);\n                    indegree[idx] += 1;\n                }\n            }\n        }\n    }\n\n    let mut ordered_indices = Vec::with_capacity(count);\n    let mut available: Vec<usize> = (0..count).filter(|&i| indegree[i] == 0).collect();\n    while !available.is_empty() {\n        available.sort_by(|&a, &b| structural_cmp(&items[a], &items[b]));\n        let next = available.remove(0);\n        ordered_indices.push(next);\n        for &neighbor in &edges[next] {\n            indegree[neighbor] = indegree[neighbor].saturating_sub(1);\n            if indegree[neighbor] == 0 {\n                available.push(neighbor);\n            }\n        }\n    }\n\n    if ordered_indices.len() != count {\n        items.sort_by(structural_cmp);\n        return;\n    }\n\n    let mut reordered = Vec::with_capacity(count);\n    for idx in ordered_indices {\n        reordered.push(items[idx].clone());\n    }\n    *items = reordered;\n}\n\n// Moved to layer_utilities in Batch 4 - no re-export needed\n",
          "updated_content": "//! Core layer computation functions - lowest level utilities\n//! This module must have no dependencies on other local modules to avoid circular dependencies.\n//! It provides fundamental layer prefix extraction and violation detection.\n\n#[allow(unused_imports)]\npub use crate::cluster_006::{\n    layer_prefix_value, order_directories, collect_directory_moves,\n};\n#[allow(unused_imports)]\npub use crate::cluster_008::detect_layer_violations;\n#[allow(unused_imports)]\npub use crate::cluster_001::{layer_constrained_sort, topo_sort_within};\nuse crate::cluster_008::structural_layer_value;\n\n\n\npub fn sort_structural_items(items: &mut Vec<crate::report::PlanItem>) {\n    use std::collections::HashMap;\n    use std::path::PathBuf;\n\n    if items.len() <= 1 {\n        return;\n    }\n\n    let count = items.len();\n    let mut edges: Vec<Vec<usize>> = vec![Vec::new(); count];\n    let mut indegree = vec![0usize; count];\n\n    let mut file_to_items: HashMap<PathBuf, Vec<usize>> = HashMap::new();\n    for (idx, item) in items.iter().enumerate() {\n        if let Some(path) = &item.current_file {\n            file_to_items.entry(path.clone()).or_default().push(idx);\n        }\n    }\n\n    for i in 0..count {\n        for j in (i + 1)..count {\n            let req_i = structural_layer_value(&items[i].required_layer, i32::MAX);\n            let req_j = structural_layer_value(&items[j].required_layer, i32::MAX);\n            let mut edge = None;\n            if req_i != req_j {\n                edge = if req_i < req_j { Some((i, j)) } else { Some((j, i)) };\n            } else if items[i].is_utility != items[j].is_utility {\n                edge = if items[i].is_utility {\n                    Some((i, j))\n                } else {\n                    Some((j, i))\n                };\n            }\n            if let Some((from, to)) = edge {\n                edges[from].push(to);\n                indegree[to] += 1;\n            }\n        }\n    }\n\n    for (idx, item) in items.iter().enumerate() {\n        for file in &item.outgoing_files {\n            if let Some(dependents) = file_to_items.get(file) {\n                for &dependent_idx in dependents {\n                    if dependent_idx == idx {\n                        continue;\n                    }\n                    edges[dependent_idx].push(idx);\n                    indegree[idx] += 1;\n                }\n            }\n        }\n    }\n\n    let mut ordered_indices = Vec::with_capacity(count);\n    let mut available: Vec<usize> = (0..count).filter(|&i| indegree[i] == 0).collect();\n    while !available.is_empty() {\n        available.sort_by(|&a, &b| structural_cmp(&items[a], &items[b]));\n        let next = available.remove(0);\n        ordered_indices.push(next);\n        for &neighbor in &edges[next] {\n            indegree[neighbor] = indegree[neighbor].saturating_sub(1);\n            if indegree[neighbor] == 0 {\n                available.push(neighbor);\n            }\n        }\n    }\n\n    if ordered_indices.len() != count {\n        items.sort_by(structural_cmp);\n        return;\n    }\n\n    let mut reordered = Vec::with_capacity(count);\n    for idx in ordered_indices {\n        reordered.push(items[idx].clone());\n    }\n    *items = reordered;\n}\n\n// Moved to layer_utilities in Batch 4 - no re-export needed\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/010_cluster_008.rs",
          "original_content": "//! Cluster 008: Core dependency analysis and layer utilities\n//!\n//! This module contains foundational functions for:\n//! - Rust file dependency ordering and resolution\n//! - Layer violation detection\n//! - Layer name extraction\n//! - Graph visualization node styling\n//! - Cyclomatic complexity calculation\n//!\n//! Functions moved from multiple modules as part of Phase 2 refactoring.\n\nuse anyhow::Result;\nuse std::cmp::Ordering;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, VecDeque};\nuse std::path::{Path, PathBuf};\nuse petgraph::graph::DiGraph;\nuse petgraph::visit::EdgeRef;\n\nuse crate::dependency::{\n    LayerEdge, LayerGraph, ReferenceDetail, UnresolvedDependency,\n};\nuse crate::types::{FileLayerViolation, NodeType};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\n\npub fn build_result(\n    files: &[PathBuf],\n    file_layers: HashMap<PathBuf, String>,\n    nodes: BTreeSet<String>,\n    edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n    unresolved: Vec<UnresolvedDependency>,\n    entry_files: &BTreeSet<PathBuf>,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let adjacency = adjacency_from_edges(&edges_map);\n    let (mut ordered_layers, cycles) = topo_sort(&nodes, &adjacency);\n    if let Some(pos) = ordered_layers.iter().position(|layer| layer == \"root\") {\n        let root_layer = ordered_layers.remove(pos);\n        ordered_layers.insert(0, root_layer);\n    }\n    let rank = layer_rank_map(&ordered_layers);\n\n    let mut ordered_files = files.to_vec();\n    ordered_files.sort_by(|a, b| {\n        let mmsb_a = is_mmsb_main(a);\n        let mmsb_b = is_mmsb_main(b);\n        if mmsb_a && !mmsb_b {\n            return Ordering::Less;\n        } else if mmsb_b && !mmsb_a {\n            return Ordering::Greater;\n        }\n        let entry_a = entry_files.contains(a);\n        let entry_b = entry_files.contains(b);\n        if entry_a && !entry_b {\n            return Ordering::Less;\n        } else if entry_b && !entry_a {\n            return Ordering::Greater;\n        }\n        let layer_a = file_layers\n            .get(a)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_b = file_layers\n            .get(b)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let rank_a = rank.get(&layer_a).cloned().unwrap_or(ordered_layers.len());\n        let rank_b = rank.get(&layer_b).cloned().unwrap_or(ordered_layers.len());\n        rank_a\n            .cmp(&rank_b)\n            .then_with(|| layer_a.cmp(&layer_b))\n            .then_with(|| a.cmp(b))\n    });\n\n    let edges = edges_map\n        .into_iter()\n        .map(|((from, to), references)| LayerEdge {\n            violation: is_layer_violation(&from, &to),\n            from,\n            to,\n            references: references.into_iter().collect(),\n        })\n        .collect();\n\n    let graph = LayerGraph {\n        ordered_layers,\n        edges,\n        cycles,\n        unresolved,\n    };\n\n    Ok((ordered_files, graph))\n}\n\nfn adjacency_from_edges(\n    edges_map: &BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n) -> HashMap<String, BTreeSet<String>> {\n    let mut adjacency: HashMap<String, BTreeSet<String>> = HashMap::new();\n    for ((from, to), _) in edges_map {\n        adjacency\n            .entry(from.clone())\n            .or_default()\n            .insert(to.clone());\n    }\n    adjacency\n}\n\nfn topo_sort(\n    nodes: &BTreeSet<String>,\n    adjacency: &HashMap<String, BTreeSet<String>>,\n) -> (Vec<String>, Vec<String>) {\n    let mut indegree: HashMap<String, usize> = HashMap::new();\n    for node in nodes {\n        indegree.entry(node.clone()).or_insert(0);\n    }\n    for targets in adjacency.values() {\n        for target in targets {\n            *indegree.entry(target.clone()).or_insert(0) += 1;\n        }\n    }\n\n    let mut queue: VecDeque<String> = indegree\n        .iter()\n        .filter_map(|(node, &deg)| if deg == 0 { Some(node.clone()) } else { None })\n        .collect();\n    queue.make_contiguous().sort();\n\n    let mut order = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        order.push(node.clone());\n        if let Some(targets) = adjacency.get(&node) {\n            for target in targets {\n                if let Some(entry) = indegree.get_mut(target) {\n                    *entry -= 1;\n                    if *entry == 0 {\n                        insert_sorted(&mut queue, target.clone());\n                    }\n                }\n            }\n        }\n    }\n\n    if order.len() != nodes.len() {\n        let mut remaining: Vec<_> = nodes\n            .iter()\n            .filter(|layer| !order.contains(layer))\n            .cloned()\n            .collect();\n        remaining.sort();\n        let cycles = remaining.clone();\n        order.extend(remaining);\n        return (order, cycles);\n    }\n\n    (order, Vec::new())\n}\n\nfn layer_rank_map(order: &[String]) -> HashMap<String, usize> {\n    let mut rank = HashMap::new();\n    for (idx, layer) in order.iter().enumerate() {\n        rank.insert(layer.clone(), idx);\n    }\n    rank\n}\n\nfn insert_sorted(queue: &mut VecDeque<String>, value: String) {\n    let mut inserted = false;\n    for idx in 0..queue.len() {\n        if value < queue[idx] {\n            queue.insert(idx, value.clone());\n            inserted = true;\n            break;\n        }\n    }\n    if !inserted {\n        queue.push_back(value);\n    }\n}\n\nfn is_mmsb_main(path: &Path) -> bool {\n    path.file_name()\n        .and_then(|n| n.to_str())\n        .map(|n| n == \"MMSB.jl\")\n        .unwrap_or(false)\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\n/// Checks if a dependency from one layer to another violates layer ordering\n/// Returns true if from_layer > to_layer (violation: higher depends on lower)\npub fn is_layer_violation(from: &str, to: &str) -> bool {\n    match (layer_prefix_value(from), layer_prefix_value(to)) {\n        (Some(a), Some(b)) => a > b,\n        _ => false,\n    }\n}\n\n/// Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)\nfn layer_prefix_value(layer: &str) -> Option<i32> {\n    let mut chars = layer.chars();\n    let mut digits = String::new();\n    while let Some(ch) = chars.next() {\n        if ch.is_ascii_digit() {\n            digits.push(ch);\n        } else {\n            break;\n        }\n    }\n    if digits.is_empty() {\n        None\n    } else {\n        digits.parse::<i32>().ok()\n    }\n}\n\n// ============================================================================\n// From src/010_layer_core.rs (continued)\n// ============================================================================\n\npub fn compare_dir_layers(a: &Path, b: &Path) -> Ordering {\n    let a_name = a.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let b_name = b.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let a_layer = layer_prefix_value(a_name).unwrap_or(i32::MAX);\n    let b_layer = layer_prefix_value(b_name).unwrap_or(i32::MAX);\n    a_layer.cmp(&b_layer).then_with(|| a_name.cmp(b_name))\n}\n\npub fn compare_path_components(a: &Path, b: &Path) -> Ordering {\n    let a_components: Vec<_> = a.components().collect();\n    let b_components: Vec<_> = b.components().collect();\n    let min_len = a_components.len().min(b_components.len());\n\n    for idx in 0..min_len {\n        let a_name = a_components[idx].as_os_str().to_string_lossy();\n        let b_name = b_components[idx].as_os_str().to_string_lossy();\n        let a_prefix = layer_prefix_value(&a_name);\n        let b_prefix = layer_prefix_value(&b_name);\n        let cmp = match (a_prefix, b_prefix) {\n            (Some(a_val), Some(b_val)) => a_val.cmp(&b_val),\n            _ => a_name.cmp(&b_name),\n        };\n        if cmp != Ordering::Equal {\n            return cmp;\n        }\n    }\n\n    a_components.len().cmp(&b_components.len())\n}\n\npub fn layer_adheres(current_layer: &str, target_layer: &str) -> bool {\n    match (layer_prefix_value(current_layer), layer_prefix_value(target_layer)) {\n        (Some(curr), Some(target)) => curr <= target,\n        _ => true,\n    }\n}\n\npub(crate) fn structural_layer_value(layer: &Option<String>, default: i32) -> i32 {\n    layer\n        .as_ref()\n        .and_then(|value| layer_prefix_value(value))\n        .unwrap_or(default)\n}\n\npub fn detect_layer_violations(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Vec<FileLayerViolation> {\n    let mut violations = Vec::new();\n    for edge in graph.edge_references() {\n        let from = &graph[edge.source()];\n        let to = &graph[edge.target()];\n        let from_layer = file_layers\n            .get(from)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let to_layer = file_layers\n            .get(to)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        if let (Some(from_val), Some(to_val)) =\n            (layer_prefix_value(&from_layer), layer_prefix_value(&to_layer))\n        {\n            if from_val > to_val {\n                violations.push(FileLayerViolation {\n                    from: from.clone(),\n                    to: to.clone(),\n                    from_layer,\n                    to_layer,\n                });\n            }\n        }\n    }\n    violations\n}\n\n#[derive(Clone)]\npub struct FunctionInfo {\n    pub name: String,\n    pub signature: String,\n    pub file_path: String,\n    pub layer: String,\n    pub calls: Vec<String>,\n}\n\npub fn detect_layer_violation(\n    func: &FunctionInfo,\n    functions: &[FunctionInfo],\n    outgoing: &HashMap<usize, usize>,\n    file_layers: &HashMap<String, String>,\n) -> Option<(String, String)> {\n    let current_layer = file_layers\n        .get(&func.file_path)\n        .cloned()\n        .unwrap_or_else(|| func.layer.clone());\n    let current_value = layer_prefix_value(&current_layer)?;\n\n    let mut violation: Option<(i32, String)> = None;\n    for (callee_idx, _) in outgoing {\n        let callee = &functions[*callee_idx];\n        let target_layer = file_layers\n            .get(&callee.file_path)\n            .cloned()\n            .unwrap_or_else(|| callee.layer.clone());\n        if let Some(target_value) = layer_prefix_value(&target_layer) {\n            if target_value < current_value {\n                match violation {\n                    Some((best_value, _)) if target_value >= best_value => {}\n                    _ => {\n                        violation = Some((target_value, target_layer));\n                    }\n                }\n            }\n        }\n    }\n\n    violation.map(|(_, target_layer)| (current_layer, target_layer))\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs (report planning helpers)\n// ============================================================================\n\npub fn parse_cluster_members(\n    cluster: &crate::types::FunctionCluster,\n) -> Vec<crate::report::ClusterMember> {\n    cluster\n        .members\n        .iter()\n        .filter_map(|member| {\n            let (file, name) = member.rsplit_once(\"::\")?;\n            Some(crate::report::ClusterMember {\n                file: PathBuf::from(file),\n                name: name.to_string(),\n            })\n        })\n        .collect()\n}\n\npub fn is_core_module_path(path: &Path) -> bool {\n    let Some(stem) = path.file_stem().and_then(|name| name.to_str()) else {\n        return false;\n    };\n    stem.starts_with(\"040_dependency\") || stem.starts_with(\"060_layer_core\")\n}\n\npub fn cluster_target_path(\n    target: PathBuf,\n    members: &[crate::report::ClusterMember],\n    root_path: &Path,\n    idx: usize,\n) -> PathBuf {\n    if !is_core_module_path(&target) {\n        return target;\n    }\n    let prefix = target\n        .file_stem()\n        .and_then(|name| name.to_str())\n        .and_then(|stem| layer_prefix_value(stem))\n        .unwrap_or(900);\n    let file_name = format!(\"{:03}_cluster_{:03}.rs\", prefix, idx + 1);\n    let dir = members\n        .first()\n        .and_then(|member| member.file.parent())\n        .unwrap_or(root_path);\n    dir.join(file_name)\n}\n\npub fn collect_cluster_plans(\n    clusters: &[crate::types::FunctionCluster],\n    root_path: &Path,\n) -> Vec<crate::report::ClusterPlan> {\n    let mut plans = Vec::new();\n    for (idx, cluster) in clusters.iter().enumerate() {\n        let all_members = parse_cluster_members(cluster);\n        let target = if let Some(suggested) = &cluster.suggested_file {\n            suggested.clone()\n        } else if let Some(first) = all_members.first() {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            first\n                .file\n                .parent()\n                .unwrap_or(root_path)\n                .join(file_name)\n        } else {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            root_path.join(file_name)\n        };\n        let target = cluster_target_path(target, &all_members, root_path, idx);\n        let members = all_members\n            .into_iter()\n            .filter(|member| member.file != target)\n            .collect::<Vec<_>>();\n        if members.len() < 2 {\n            continue;\n        }\n        plans.push(crate::report::ClusterPlan {\n            target,\n            cohesion: cluster.cohesion,\n            members,\n        });\n    }\n    plans.sort_by(|a, b| {\n        use std::cmp::Ordering;\n        b.cohesion\n            .partial_cmp(&a.cohesion)\n            .unwrap_or(Ordering::Equal)\n            .then_with(|| b.members.len().cmp(&a.members.len()))\n            .then_with(|| a.target.cmp(&b.target))\n    });\n    plans\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\npub fn node_style(node_type: &NodeType) -> (&str, &str, &str) {\n    match node_type {\n        NodeType::Entry => (\"ellipse\", \"lightgreen\", \"\\\"filled,bold\\\"\"),\n        NodeType::Exit => (\"doubleoctagon\", \"lightcoral\", \"\\\"filled,bold\\\"\"),\n        NodeType::BasicBlock => (\"box\", \"lightblue\", \"filled\"),\n        NodeType::Branch => (\"diamond\", \"yellow\", \"filled\"),\n        NodeType::LoopHeader => (\"box\", \"orange\", \"\\\"filled,rounded\\\"\"),\n    }\n}\n\npub fn cyclomatic_complexity(cfg: &crate::types::FunctionCfg) -> usize {\n    let edges = cfg.edges.len() as isize;\n    let nodes = cfg.nodes.len() as isize;\n    let exits = 1isize; // assume one exit\n    let cc = edges - nodes + 2 * exits;\n    if cc <= 0 {\n        1\n    } else {\n        cc as usize\n    }\n}\n",
          "updated_content": "//! Cluster 008: Core dependency analysis and layer utilities\n//!\n//! This module contains foundational functions for:\n//! - Rust file dependency ordering and resolution\n//! - Layer violation detection\n//! - Layer name extraction\n//! - Graph visualization node styling\n//! - Cyclomatic complexity calculation\n//!\n//! Functions moved from multiple modules as part of Phase 2 refactoring.\n\nuse anyhow::Result;\nuse std::cmp::Ordering;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, VecDeque};\nuse std::path::{Path, PathBuf};\nuse petgraph::graph::DiGraph;\nuse petgraph::visit::EdgeRef;\n\nuse crate::dependency::{\n    LayerEdge, LayerGraph, ReferenceDetail, UnresolvedDependency,\n};\nuse crate::types::{FileLayerViolation, NodeType};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\n\npub fn build_result(\n    files: &[PathBuf],\n    file_layers: HashMap<PathBuf, String>,\n    nodes: BTreeSet<String>,\n    edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n    unresolved: Vec<UnresolvedDependency>,\n    entry_files: &BTreeSet<PathBuf>,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let adjacency = adjacency_from_edges(&edges_map);\n    let (mut ordered_layers, cycles) = topo_sort(&nodes, &adjacency);\n    if let Some(pos) = ordered_layers.iter().position(|layer| layer == \"root\") {\n        let root_layer = ordered_layers.remove(pos);\n        ordered_layers.insert(0, root_layer);\n    }\n    let rank = layer_rank_map(&ordered_layers);\n\n    let mut ordered_files = files.to_vec();\n    ordered_files.sort_by(|a, b| {\n        let mmsb_a = is_mmsb_main(a);\n        let mmsb_b = is_mmsb_main(b);\n        if mmsb_a && !mmsb_b {\n            return Ordering::Less;\n        } else if mmsb_b && !mmsb_a {\n            return Ordering::Greater;\n        }\n        let entry_a = entry_files.contains(a);\n        let entry_b = entry_files.contains(b);\n        if entry_a && !entry_b {\n            return Ordering::Less;\n        } else if entry_b && !entry_a {\n            return Ordering::Greater;\n        }\n        let layer_a = file_layers\n            .get(a)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_b = file_layers\n            .get(b)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let rank_a = rank.get(&layer_a).cloned().unwrap_or(ordered_layers.len());\n        let rank_b = rank.get(&layer_b).cloned().unwrap_or(ordered_layers.len());\n        rank_a\n            .cmp(&rank_b)\n            .then_with(|| layer_a.cmp(&layer_b))\n            .then_with(|| a.cmp(b))\n    });\n\n    let edges = edges_map\n        .into_iter()\n        .map(|((from, to), references)| LayerEdge {\n            violation: is_layer_violation(&from, &to),\n            from,\n            to,\n            references: references.into_iter().collect(),\n        })\n        .collect();\n\n    let graph = LayerGraph {\n        ordered_layers,\n        edges,\n        cycles,\n        unresolved,\n    };\n\n    Ok((ordered_files, graph))\n}\n\nfn adjacency_from_edges(\n    edges_map: &BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n) -> HashMap<String, BTreeSet<String>> {\n    let mut adjacency: HashMap<String, BTreeSet<String>> = HashMap::new();\n    for ((from, to), _) in edges_map {\n        adjacency\n            .entry(from.clone())\n            .or_default()\n            .insert(to.clone());\n    }\n    adjacency\n}\n\nfn topo_sort(\n    nodes: &BTreeSet<String>,\n    adjacency: &HashMap<String, BTreeSet<String>>,\n) -> (Vec<String>, Vec<String>) {\n    let mut indegree: HashMap<String, usize> = HashMap::new();\n    for node in nodes {\n        indegree.entry(node.clone()).or_insert(0);\n    }\n    for targets in adjacency.values() {\n        for target in targets {\n            *indegree.entry(target.clone()).or_insert(0) += 1;\n        }\n    }\n\n    let mut queue: VecDeque<String> = indegree\n        .iter()\n        .filter_map(|(node, &deg)| if deg == 0 { Some(node.clone()) } else { None })\n        .collect();\n    queue.make_contiguous().sort();\n\n    let mut order = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        order.push(node.clone());\n        if let Some(targets) = adjacency.get(&node) {\n            for target in targets {\n                if let Some(entry) = indegree.get_mut(target) {\n                    *entry -= 1;\n                    if *entry == 0 {\n                        insert_sorted(&mut queue, target.clone());\n                    }\n                }\n            }\n        }\n    }\n\n    if order.len() != nodes.len() {\n        let mut remaining: Vec<_> = nodes\n            .iter()\n            .filter(|layer| !order.contains(layer))\n            .cloned()\n            .collect();\n        remaining.sort();\n        let cycles = remaining.clone();\n        order.extend(remaining);\n        return (order, cycles);\n    }\n\n    (order, Vec::new())\n}\n\nfn layer_rank_map(order: &[String]) -> HashMap<String, usize> {\n    let mut rank = HashMap::new();\n    for (idx, layer) in order.iter().enumerate() {\n        rank.insert(layer.clone(), idx);\n    }\n    rank\n}\n\nfn insert_sorted(queue: &mut VecDeque<String>, value: String) {\n    let mut inserted = false;\n    for idx in 0..queue.len() {\n        if value < queue[idx] {\n            queue.insert(idx, value.clone());\n            inserted = true;\n            break;\n        }\n    }\n    if !inserted {\n        queue.push_back(value);\n    }\n}\n\nfn is_mmsb_main(path: &Path) -> bool {\n    path.file_name()\n        .and_then(|n| n.to_str())\n        .map(|n| n == \"MMSB.jl\")\n        .unwrap_or(false)\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\n/// Checks if a dependency from one layer to another violates layer ordering\n/// Returns true if from_layer > to_layer (violation: higher depends on lower)\npub fn is_layer_violation(from: &str, to: &str) -> bool {\n    match (layer_prefix_value(from), layer_prefix_value(to)) {\n        (Some(a), Some(b)) => a > b,\n        _ => false,\n    }\n}\n\n/// Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)\nfn layer_prefix_value(layer: &str) -> Option<i32> {\n    let mut chars = layer.chars();\n    let mut digits = String::new();\n    while let Some(ch) = chars.next() {\n        if ch.is_ascii_digit() {\n            digits.push(ch);\n        } else {\n            break;\n        }\n    }\n    if digits.is_empty() {\n        None\n    } else {\n        digits.parse::<i32>().ok()\n    }\n}\n\n// ============================================================================\n// From src/010_layer_core.rs (continued)\n// ============================================================================\n\npub fn compare_dir_layers(a: &Path, b: &Path) -> Ordering {\n    let a_name = a.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let b_name = b.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let a_layer = layer_prefix_value(a_name).unwrap_or(i32::MAX);\n    let b_layer = layer_prefix_value(b_name).unwrap_or(i32::MAX);\n    a_layer.cmp(&b_layer).then_with(|| a_name.cmp(b_name))\n}\n\npub fn compare_path_components(a: &Path, b: &Path) -> Ordering {\n    let a_components: Vec<_> = a.components().collect();\n    let b_components: Vec<_> = b.components().collect();\n    let min_len = a_components.len().min(b_components.len());\n\n    for idx in 0..min_len {\n        let a_name = a_components[idx].as_os_str().to_string_lossy();\n        let b_name = b_components[idx].as_os_str().to_string_lossy();\n        let a_prefix = layer_prefix_value(&a_name);\n        let b_prefix = layer_prefix_value(&b_name);\n        let cmp = match (a_prefix, b_prefix) {\n            (Some(a_val), Some(b_val)) => a_val.cmp(&b_val),\n            _ => a_name.cmp(&b_name),\n        };\n        if cmp != Ordering::Equal {\n            return cmp;\n        }\n    }\n\n    a_components.len().cmp(&b_components.len())\n}\n\npub fn layer_adheres(current_layer: &str, target_layer: &str) -> bool {\n    match (layer_prefix_value(current_layer), layer_prefix_value(target_layer)) {\n        (Some(curr), Some(target)) => curr <= target,\n        _ => true,\n    }\n}\n\npub(crate) fn structural_layer_value(layer: &Option<String>, default: i32) -> i32 {\n    layer\n        .as_ref()\n        .and_then(|value| layer_prefix_value(value))\n        .unwrap_or(default)\n}\n\npub fn detect_layer_violations(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Vec<FileLayerViolation> {\n    let mut violations = Vec::new();\n    for edge in graph.edge_references() {\n        let from = &graph[edge.source()];\n        let to = &graph[edge.target()];\n        let from_layer = file_layers\n            .get(from)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let to_layer = file_layers\n            .get(to)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        if let (Some(from_val), Some(to_val)) =\n            (layer_prefix_value(&from_layer), layer_prefix_value(&to_layer))\n        {\n            if from_val > to_val {\n                violations.push(FileLayerViolation {\n                    from: from.clone(),\n                    to: to.clone(),\n                    from_layer,\n                    to_layer,\n                });\n            }\n        }\n    }\n    violations\n}\n\n#[derive(Clone)]\npub struct FunctionInfo {\n    pub name: String,\n    pub signature: String,\n    pub file_path: String,\n    pub layer: String,\n    pub calls: Vec<String>,\n}\n\npub fn detect_layer_violation(\n    func: &FunctionInfo,\n    functions: &[FunctionInfo],\n    outgoing: &HashMap<usize, usize>,\n    file_layers: &HashMap<String, String>,\n) -> Option<(String, String)> {\n    let current_layer = file_layers\n        .get(&func.file_path)\n        .cloned()\n        .unwrap_or_else(|| func.layer.clone());\n    let current_value = layer_prefix_value(&current_layer)?;\n\n    let mut violation: Option<(i32, String)> = None;\n    for (callee_idx, _) in outgoing {\n        let callee = &functions[*callee_idx];\n        let target_layer = file_layers\n            .get(&callee.file_path)\n            .cloned()\n            .unwrap_or_else(|| callee.layer.clone());\n        if let Some(target_value) = layer_prefix_value(&target_layer) {\n            if target_value < current_value {\n                match violation {\n                    Some((best_value, _)) if target_value >= best_value => {}\n                    _ => {\n                        violation = Some((target_value, target_layer));\n                    }\n                }\n            }\n        }\n    }\n\n    violation.map(|(_, target_layer)| (current_layer, target_layer))\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs (report planning helpers)\n// ============================================================================\n\npub fn parse_cluster_members(\n    cluster: &crate::types::FunctionCluster,\n) -> Vec<crate::report::ClusterMember> {\n    cluster\n        .members\n        .iter()\n        .filter_map(|member| {\n            let (file, name) = member.rsplit_once(\"::\")?;\n            Some(crate::report::ClusterMember {\n                file: PathBuf::from(file),\n                name: name.to_string(),\n            })\n        })\n        .collect()\n}\n\npub fn is_core_module_path(path: &Path) -> bool {\n    let Some(stem) = path.file_stem().and_then(|name| name.to_str()) else {\n        return false;\n    };\n    stem.starts_with(\"040_dependency\") || stem.starts_with(\"060_layer_core\")\n}\n\npub fn cluster_target_path(\n    target: PathBuf,\n    members: &[crate::report::ClusterMember],\n    root_path: &Path,\n    idx: usize,\n) -> PathBuf {\n    if !is_core_module_path(&target) {\n        return target;\n    }\n    let prefix = target\n        .file_stem()\n        .and_then(|name| name.to_str())\n        .and_then(|stem| layer_prefix_value(stem))\n        .unwrap_or(900);\n    let file_name = format!(\"{:03}_cluster_{:03}.rs\", prefix, idx + 1);\n    let dir = members\n        .first()\n        .and_then(|member| member.file.parent())\n        .unwrap_or(root_path);\n    dir.join(file_name)\n}\n\npub fn collect_cluster_plans(\n    clusters: &[crate::types::FunctionCluster],\n    root_path: &Path,\n) -> Vec<crate::report::ClusterPlan> {\n    let mut plans = Vec::new();\n    for (idx, cluster) in clusters.iter().enumerate() {\n        let all_members = parse_cluster_members(cluster);\n        let target = if let Some(suggested) = &cluster.suggested_file {\n            suggested.clone()\n        } else if let Some(first) = all_members.first() {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            first\n                .file\n                .parent()\n                .unwrap_or(root_path)\n                .join(file_name)\n        } else {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            root_path.join(file_name)\n        };\n        let target = cluster_target_path(target, &all_members, root_path, idx);\n        let members = all_members\n            .into_iter()\n            .filter(|member| member.file != target)\n            .collect::<Vec<_>>();\n        if members.len() < 2 {\n            continue;\n        }\n        plans.push(crate::report::ClusterPlan {\n            target,\n            cohesion: cluster.cohesion,\n            members,\n        });\n    }\n    plans.sort_by(|a, b| {\n        use std::cmp::Ordering;\n        b.cohesion\n            .partial_cmp(&a.cohesion)\n            .unwrap_or(Ordering::Equal)\n            .then_with(|| b.members.len().cmp(&a.members.len()))\n            .then_with(|| a.target.cmp(&b.target))\n    });\n    plans\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\npub fn node_style(node_type: &NodeType) -> (&str, &str, &str) {\n    match node_type {\n        NodeType::Entry => (\"ellipse\", \"lightgreen\", \"\\\"filled,bold\\\"\"),\n        NodeType::Exit => (\"doubleoctagon\", \"lightcoral\", \"\\\"filled,bold\\\"\"),\n        NodeType::BasicBlock => (\"box\", \"lightblue\", \"filled\"),\n        NodeType::Branch => (\"diamond\", \"yellow\", \"filled\"),\n        NodeType::LoopHeader => (\"box\", \"orange\", \"\\\"filled,rounded\\\"\"),\n    }\n}\n\npub fn cyclomatic_complexity(cfg: &crate::types::FunctionCfg) -> usize {\n    let edges = cfg.edges.len() as isize;\n    let nodes = cfg.nodes.len() as isize;\n    let exits = 1isize; // assume one exit\n    let cc = edges - nodes + 2 * exits;\n    if cc <= 0 {\n        1\n    } else {\n        cc as usize\n    }\n}\n\npub fn structural_cmp(a: &crate::report::PlanItem, b: &crate::report::PlanItem) -> std::cmp::Ordering {\n    let a_required = structural_layer_value(&a.required_layer, i32::MAX);\n    let b_required = structural_layer_value(&b.required_layer, i32::MAX);\n    let a_current = structural_layer_value(&a.current_layer, i32::MIN);\n    let b_current = structural_layer_value(&b.current_layer, i32::MIN);\n    let a_benefit = if a.cost == 0 {\n        0\n    } else {\n        (a.benefit.saturating_mul(1000)) / a.cost\n    };\n    let b_benefit = if b.cost == 0 {\n        0\n    } else {\n        (b.benefit.saturating_mul(1000)) / b.cost\n    };\n    a_required\n        .cmp(&b_required)\n        .then_with(|| b.is_utility.cmp(&a.is_utility))\n        .then_with(|| b_benefit.cmp(&a_benefit))\n        .then_with(|| b.impact_weight.cmp(&a.impact_weight))\n        .then_with(|| b_current.cmp(&a_current))\n        .then_with(|| a.description.cmp(&b.description))\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_sort_structural_items_to_src/010_cluster_008.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/060_layer_core.rs",
          "original_content": "//! Core layer computation functions - lowest level utilities\n//! This module must have no dependencies on other local modules to avoid circular dependencies.\n//! It provides fundamental layer prefix extraction and violation detection.\n\n#[allow(unused_imports)]\npub use crate::cluster_006::{\n    layer_prefix_value, order_directories, collect_directory_moves,\n};\n#[allow(unused_imports)]\npub use crate::cluster_008::detect_layer_violations;\n#[allow(unused_imports)]\npub use crate::cluster_001::{layer_constrained_sort, topo_sort_within};\nuse crate::cluster_008::structural_layer_value;\n\npub fn structural_cmp(a: &crate::report::PlanItem, b: &crate::report::PlanItem) -> std::cmp::Ordering {\n    let a_required = structural_layer_value(&a.required_layer, i32::MAX);\n    let b_required = structural_layer_value(&b.required_layer, i32::MAX);\n    let a_current = structural_layer_value(&a.current_layer, i32::MIN);\n    let b_current = structural_layer_value(&b.current_layer, i32::MIN);\n    let a_benefit = if a.cost == 0 {\n        0\n    } else {\n        (a.benefit.saturating_mul(1000)) / a.cost\n    };\n    let b_benefit = if b.cost == 0 {\n        0\n    } else {\n        (b.benefit.saturating_mul(1000)) / b.cost\n    };\n    a_required\n        .cmp(&b_required)\n        .then_with(|| b.is_utility.cmp(&a.is_utility))\n        .then_with(|| b_benefit.cmp(&a_benefit))\n        .then_with(|| b.impact_weight.cmp(&a.impact_weight))\n        .then_with(|| b_current.cmp(&a_current))\n        .then_with(|| a.description.cmp(&b.description))\n}\n\npub fn sort_structural_items(items: &mut Vec<crate::report::PlanItem>) {\n    use std::collections::HashMap;\n    use std::path::PathBuf;\n\n    if items.len() <= 1 {\n        return;\n    }\n\n    let count = items.len();\n    let mut edges: Vec<Vec<usize>> = vec![Vec::new(); count];\n    let mut indegree = vec![0usize; count];\n\n    let mut file_to_items: HashMap<PathBuf, Vec<usize>> = HashMap::new();\n    for (idx, item) in items.iter().enumerate() {\n        if let Some(path) = &item.current_file {\n            file_to_items.entry(path.clone()).or_default().push(idx);\n        }\n    }\n\n    for i in 0..count {\n        for j in (i + 1)..count {\n            let req_i = structural_layer_value(&items[i].required_layer, i32::MAX);\n            let req_j = structural_layer_value(&items[j].required_layer, i32::MAX);\n            let mut edge = None;\n            if req_i != req_j {\n                edge = if req_i < req_j { Some((i, j)) } else { Some((j, i)) };\n            } else if items[i].is_utility != items[j].is_utility {\n                edge = if items[i].is_utility {\n                    Some((i, j))\n                } else {\n                    Some((j, i))\n                };\n            }\n            if let Some((from, to)) = edge {\n                edges[from].push(to);\n                indegree[to] += 1;\n            }\n        }\n    }\n\n    for (idx, item) in items.iter().enumerate() {\n        for file in &item.outgoing_files {\n            if let Some(dependents) = file_to_items.get(file) {\n                for &dependent_idx in dependents {\n                    if dependent_idx == idx {\n                        continue;\n                    }\n                    edges[dependent_idx].push(idx);\n                    indegree[idx] += 1;\n                }\n            }\n        }\n    }\n\n    let mut ordered_indices = Vec::with_capacity(count);\n    let mut available: Vec<usize> = (0..count).filter(|&i| indegree[i] == 0).collect();\n    while !available.is_empty() {\n        available.sort_by(|&a, &b| structural_cmp(&items[a], &items[b]));\n        let next = available.remove(0);\n        ordered_indices.push(next);\n        for &neighbor in &edges[next] {\n            indegree[neighbor] = indegree[neighbor].saturating_sub(1);\n            if indegree[neighbor] == 0 {\n                available.push(neighbor);\n            }\n        }\n    }\n\n    if ordered_indices.len() != count {\n        items.sort_by(structural_cmp);\n        return;\n    }\n\n    let mut reordered = Vec::with_capacity(count);\n    for idx in ordered_indices {\n        reordered.push(items[idx].clone());\n    }\n    *items = reordered;\n}\n\n// Moved to layer_utilities in Batch 4 - no re-export needed\n",
          "updated_content": "//! Core layer computation functions - lowest level utilities\n//! This module must have no dependencies on other local modules to avoid circular dependencies.\n//! It provides fundamental layer prefix extraction and violation detection.\n\n#[allow(unused_imports)]\npub use crate::cluster_006::{\n    layer_prefix_value, order_directories, collect_directory_moves,\n};\n#[allow(unused_imports)]\npub use crate::cluster_008::detect_layer_violations;\n#[allow(unused_imports)]\npub use crate::cluster_001::{layer_constrained_sort, topo_sort_within};\nuse crate::cluster_008::structural_layer_value;\n\npub fn structural_cmp(a: &crate::report::PlanItem, b: &crate::report::PlanItem) -> std::cmp::Ordering {\n    let a_required = structural_layer_value(&a.required_layer, i32::MAX);\n    let b_required = structural_layer_value(&b.required_layer, i32::MAX);\n    let a_current = structural_layer_value(&a.current_layer, i32::MIN);\n    let b_current = structural_layer_value(&b.current_layer, i32::MIN);\n    let a_benefit = if a.cost == 0 {\n        0\n    } else {\n        (a.benefit.saturating_mul(1000)) / a.cost\n    };\n    let b_benefit = if b.cost == 0 {\n        0\n    } else {\n        (b.benefit.saturating_mul(1000)) / b.cost\n    };\n    a_required\n        .cmp(&b_required)\n        .then_with(|| b.is_utility.cmp(&a.is_utility))\n        .then_with(|| b_benefit.cmp(&a_benefit))\n        .then_with(|| b.impact_weight.cmp(&a.impact_weight))\n        .then_with(|| b_current.cmp(&a_current))\n        .then_with(|| a.description.cmp(&b.description))\n}\n\n\n\n// Moved to layer_utilities in Batch 4 - no re-export needed\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/010_cluster_008.rs",
          "original_content": "//! Cluster 008: Core dependency analysis and layer utilities\n//!\n//! This module contains foundational functions for:\n//! - Rust file dependency ordering and resolution\n//! - Layer violation detection\n//! - Layer name extraction\n//! - Graph visualization node styling\n//! - Cyclomatic complexity calculation\n//!\n//! Functions moved from multiple modules as part of Phase 2 refactoring.\n\nuse anyhow::Result;\nuse std::cmp::Ordering;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, VecDeque};\nuse std::path::{Path, PathBuf};\nuse petgraph::graph::DiGraph;\nuse petgraph::visit::EdgeRef;\n\nuse crate::dependency::{\n    LayerEdge, LayerGraph, ReferenceDetail, UnresolvedDependency,\n};\nuse crate::types::{FileLayerViolation, NodeType};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\n\npub fn build_result(\n    files: &[PathBuf],\n    file_layers: HashMap<PathBuf, String>,\n    nodes: BTreeSet<String>,\n    edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n    unresolved: Vec<UnresolvedDependency>,\n    entry_files: &BTreeSet<PathBuf>,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let adjacency = adjacency_from_edges(&edges_map);\n    let (mut ordered_layers, cycles) = topo_sort(&nodes, &adjacency);\n    if let Some(pos) = ordered_layers.iter().position(|layer| layer == \"root\") {\n        let root_layer = ordered_layers.remove(pos);\n        ordered_layers.insert(0, root_layer);\n    }\n    let rank = layer_rank_map(&ordered_layers);\n\n    let mut ordered_files = files.to_vec();\n    ordered_files.sort_by(|a, b| {\n        let mmsb_a = is_mmsb_main(a);\n        let mmsb_b = is_mmsb_main(b);\n        if mmsb_a && !mmsb_b {\n            return Ordering::Less;\n        } else if mmsb_b && !mmsb_a {\n            return Ordering::Greater;\n        }\n        let entry_a = entry_files.contains(a);\n        let entry_b = entry_files.contains(b);\n        if entry_a && !entry_b {\n            return Ordering::Less;\n        } else if entry_b && !entry_a {\n            return Ordering::Greater;\n        }\n        let layer_a = file_layers\n            .get(a)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_b = file_layers\n            .get(b)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let rank_a = rank.get(&layer_a).cloned().unwrap_or(ordered_layers.len());\n        let rank_b = rank.get(&layer_b).cloned().unwrap_or(ordered_layers.len());\n        rank_a\n            .cmp(&rank_b)\n            .then_with(|| layer_a.cmp(&layer_b))\n            .then_with(|| a.cmp(b))\n    });\n\n    let edges = edges_map\n        .into_iter()\n        .map(|((from, to), references)| LayerEdge {\n            violation: is_layer_violation(&from, &to),\n            from,\n            to,\n            references: references.into_iter().collect(),\n        })\n        .collect();\n\n    let graph = LayerGraph {\n        ordered_layers,\n        edges,\n        cycles,\n        unresolved,\n    };\n\n    Ok((ordered_files, graph))\n}\n\nfn adjacency_from_edges(\n    edges_map: &BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n) -> HashMap<String, BTreeSet<String>> {\n    let mut adjacency: HashMap<String, BTreeSet<String>> = HashMap::new();\n    for ((from, to), _) in edges_map {\n        adjacency\n            .entry(from.clone())\n            .or_default()\n            .insert(to.clone());\n    }\n    adjacency\n}\n\nfn topo_sort(\n    nodes: &BTreeSet<String>,\n    adjacency: &HashMap<String, BTreeSet<String>>,\n) -> (Vec<String>, Vec<String>) {\n    let mut indegree: HashMap<String, usize> = HashMap::new();\n    for node in nodes {\n        indegree.entry(node.clone()).or_insert(0);\n    }\n    for targets in adjacency.values() {\n        for target in targets {\n            *indegree.entry(target.clone()).or_insert(0) += 1;\n        }\n    }\n\n    let mut queue: VecDeque<String> = indegree\n        .iter()\n        .filter_map(|(node, &deg)| if deg == 0 { Some(node.clone()) } else { None })\n        .collect();\n    queue.make_contiguous().sort();\n\n    let mut order = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        order.push(node.clone());\n        if let Some(targets) = adjacency.get(&node) {\n            for target in targets {\n                if let Some(entry) = indegree.get_mut(target) {\n                    *entry -= 1;\n                    if *entry == 0 {\n                        insert_sorted(&mut queue, target.clone());\n                    }\n                }\n            }\n        }\n    }\n\n    if order.len() != nodes.len() {\n        let mut remaining: Vec<_> = nodes\n            .iter()\n            .filter(|layer| !order.contains(layer))\n            .cloned()\n            .collect();\n        remaining.sort();\n        let cycles = remaining.clone();\n        order.extend(remaining);\n        return (order, cycles);\n    }\n\n    (order, Vec::new())\n}\n\nfn layer_rank_map(order: &[String]) -> HashMap<String, usize> {\n    let mut rank = HashMap::new();\n    for (idx, layer) in order.iter().enumerate() {\n        rank.insert(layer.clone(), idx);\n    }\n    rank\n}\n\nfn insert_sorted(queue: &mut VecDeque<String>, value: String) {\n    let mut inserted = false;\n    for idx in 0..queue.len() {\n        if value < queue[idx] {\n            queue.insert(idx, value.clone());\n            inserted = true;\n            break;\n        }\n    }\n    if !inserted {\n        queue.push_back(value);\n    }\n}\n\nfn is_mmsb_main(path: &Path) -> bool {\n    path.file_name()\n        .and_then(|n| n.to_str())\n        .map(|n| n == \"MMSB.jl\")\n        .unwrap_or(false)\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\n/// Checks if a dependency from one layer to another violates layer ordering\n/// Returns true if from_layer > to_layer (violation: higher depends on lower)\npub fn is_layer_violation(from: &str, to: &str) -> bool {\n    match (layer_prefix_value(from), layer_prefix_value(to)) {\n        (Some(a), Some(b)) => a > b,\n        _ => false,\n    }\n}\n\n/// Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)\nfn layer_prefix_value(layer: &str) -> Option<i32> {\n    let mut chars = layer.chars();\n    let mut digits = String::new();\n    while let Some(ch) = chars.next() {\n        if ch.is_ascii_digit() {\n            digits.push(ch);\n        } else {\n            break;\n        }\n    }\n    if digits.is_empty() {\n        None\n    } else {\n        digits.parse::<i32>().ok()\n    }\n}\n\n// ============================================================================\n// From src/010_layer_core.rs (continued)\n// ============================================================================\n\npub fn compare_dir_layers(a: &Path, b: &Path) -> Ordering {\n    let a_name = a.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let b_name = b.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let a_layer = layer_prefix_value(a_name).unwrap_or(i32::MAX);\n    let b_layer = layer_prefix_value(b_name).unwrap_or(i32::MAX);\n    a_layer.cmp(&b_layer).then_with(|| a_name.cmp(b_name))\n}\n\npub fn compare_path_components(a: &Path, b: &Path) -> Ordering {\n    let a_components: Vec<_> = a.components().collect();\n    let b_components: Vec<_> = b.components().collect();\n    let min_len = a_components.len().min(b_components.len());\n\n    for idx in 0..min_len {\n        let a_name = a_components[idx].as_os_str().to_string_lossy();\n        let b_name = b_components[idx].as_os_str().to_string_lossy();\n        let a_prefix = layer_prefix_value(&a_name);\n        let b_prefix = layer_prefix_value(&b_name);\n        let cmp = match (a_prefix, b_prefix) {\n            (Some(a_val), Some(b_val)) => a_val.cmp(&b_val),\n            _ => a_name.cmp(&b_name),\n        };\n        if cmp != Ordering::Equal {\n            return cmp;\n        }\n    }\n\n    a_components.len().cmp(&b_components.len())\n}\n\npub fn layer_adheres(current_layer: &str, target_layer: &str) -> bool {\n    match (layer_prefix_value(current_layer), layer_prefix_value(target_layer)) {\n        (Some(curr), Some(target)) => curr <= target,\n        _ => true,\n    }\n}\n\npub(crate) fn structural_layer_value(layer: &Option<String>, default: i32) -> i32 {\n    layer\n        .as_ref()\n        .and_then(|value| layer_prefix_value(value))\n        .unwrap_or(default)\n}\n\npub fn detect_layer_violations(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Vec<FileLayerViolation> {\n    let mut violations = Vec::new();\n    for edge in graph.edge_references() {\n        let from = &graph[edge.source()];\n        let to = &graph[edge.target()];\n        let from_layer = file_layers\n            .get(from)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let to_layer = file_layers\n            .get(to)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        if let (Some(from_val), Some(to_val)) =\n            (layer_prefix_value(&from_layer), layer_prefix_value(&to_layer))\n        {\n            if from_val > to_val {\n                violations.push(FileLayerViolation {\n                    from: from.clone(),\n                    to: to.clone(),\n                    from_layer,\n                    to_layer,\n                });\n            }\n        }\n    }\n    violations\n}\n\n#[derive(Clone)]\npub struct FunctionInfo {\n    pub name: String,\n    pub signature: String,\n    pub file_path: String,\n    pub layer: String,\n    pub calls: Vec<String>,\n}\n\npub fn detect_layer_violation(\n    func: &FunctionInfo,\n    functions: &[FunctionInfo],\n    outgoing: &HashMap<usize, usize>,\n    file_layers: &HashMap<String, String>,\n) -> Option<(String, String)> {\n    let current_layer = file_layers\n        .get(&func.file_path)\n        .cloned()\n        .unwrap_or_else(|| func.layer.clone());\n    let current_value = layer_prefix_value(&current_layer)?;\n\n    let mut violation: Option<(i32, String)> = None;\n    for (callee_idx, _) in outgoing {\n        let callee = &functions[*callee_idx];\n        let target_layer = file_layers\n            .get(&callee.file_path)\n            .cloned()\n            .unwrap_or_else(|| callee.layer.clone());\n        if let Some(target_value) = layer_prefix_value(&target_layer) {\n            if target_value < current_value {\n                match violation {\n                    Some((best_value, _)) if target_value >= best_value => {}\n                    _ => {\n                        violation = Some((target_value, target_layer));\n                    }\n                }\n            }\n        }\n    }\n\n    violation.map(|(_, target_layer)| (current_layer, target_layer))\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs (report planning helpers)\n// ============================================================================\n\npub fn parse_cluster_members(\n    cluster: &crate::types::FunctionCluster,\n) -> Vec<crate::report::ClusterMember> {\n    cluster\n        .members\n        .iter()\n        .filter_map(|member| {\n            let (file, name) = member.rsplit_once(\"::\")?;\n            Some(crate::report::ClusterMember {\n                file: PathBuf::from(file),\n                name: name.to_string(),\n            })\n        })\n        .collect()\n}\n\npub fn is_core_module_path(path: &Path) -> bool {\n    let Some(stem) = path.file_stem().and_then(|name| name.to_str()) else {\n        return false;\n    };\n    stem.starts_with(\"040_dependency\") || stem.starts_with(\"060_layer_core\")\n}\n\npub fn cluster_target_path(\n    target: PathBuf,\n    members: &[crate::report::ClusterMember],\n    root_path: &Path,\n    idx: usize,\n) -> PathBuf {\n    if !is_core_module_path(&target) {\n        return target;\n    }\n    let prefix = target\n        .file_stem()\n        .and_then(|name| name.to_str())\n        .and_then(|stem| layer_prefix_value(stem))\n        .unwrap_or(900);\n    let file_name = format!(\"{:03}_cluster_{:03}.rs\", prefix, idx + 1);\n    let dir = members\n        .first()\n        .and_then(|member| member.file.parent())\n        .unwrap_or(root_path);\n    dir.join(file_name)\n}\n\npub fn collect_cluster_plans(\n    clusters: &[crate::types::FunctionCluster],\n    root_path: &Path,\n) -> Vec<crate::report::ClusterPlan> {\n    let mut plans = Vec::new();\n    for (idx, cluster) in clusters.iter().enumerate() {\n        let all_members = parse_cluster_members(cluster);\n        let target = if let Some(suggested) = &cluster.suggested_file {\n            suggested.clone()\n        } else if let Some(first) = all_members.first() {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            first\n                .file\n                .parent()\n                .unwrap_or(root_path)\n                .join(file_name)\n        } else {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            root_path.join(file_name)\n        };\n        let target = cluster_target_path(target, &all_members, root_path, idx);\n        let members = all_members\n            .into_iter()\n            .filter(|member| member.file != target)\n            .collect::<Vec<_>>();\n        if members.len() < 2 {\n            continue;\n        }\n        plans.push(crate::report::ClusterPlan {\n            target,\n            cohesion: cluster.cohesion,\n            members,\n        });\n    }\n    plans.sort_by(|a, b| {\n        use std::cmp::Ordering;\n        b.cohesion\n            .partial_cmp(&a.cohesion)\n            .unwrap_or(Ordering::Equal)\n            .then_with(|| b.members.len().cmp(&a.members.len()))\n            .then_with(|| a.target.cmp(&b.target))\n    });\n    plans\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\npub fn node_style(node_type: &NodeType) -> (&str, &str, &str) {\n    match node_type {\n        NodeType::Entry => (\"ellipse\", \"lightgreen\", \"\\\"filled,bold\\\"\"),\n        NodeType::Exit => (\"doubleoctagon\", \"lightcoral\", \"\\\"filled,bold\\\"\"),\n        NodeType::BasicBlock => (\"box\", \"lightblue\", \"filled\"),\n        NodeType::Branch => (\"diamond\", \"yellow\", \"filled\"),\n        NodeType::LoopHeader => (\"box\", \"orange\", \"\\\"filled,rounded\\\"\"),\n    }\n}\n\npub fn cyclomatic_complexity(cfg: &crate::types::FunctionCfg) -> usize {\n    let edges = cfg.edges.len() as isize;\n    let nodes = cfg.nodes.len() as isize;\n    let exits = 1isize; // assume one exit\n    let cc = edges - nodes + 2 * exits;\n    if cc <= 0 {\n        1\n    } else {\n        cc as usize\n    }\n}\n",
          "updated_content": "//! Cluster 008: Core dependency analysis and layer utilities\n//!\n//! This module contains foundational functions for:\n//! - Rust file dependency ordering and resolution\n//! - Layer violation detection\n//! - Layer name extraction\n//! - Graph visualization node styling\n//! - Cyclomatic complexity calculation\n//!\n//! Functions moved from multiple modules as part of Phase 2 refactoring.\n\nuse anyhow::Result;\nuse std::cmp::Ordering;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, VecDeque};\nuse std::path::{Path, PathBuf};\nuse petgraph::graph::DiGraph;\nuse petgraph::visit::EdgeRef;\n\nuse crate::dependency::{\n    LayerEdge, LayerGraph, ReferenceDetail, UnresolvedDependency,\n};\nuse crate::types::{FileLayerViolation, NodeType};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\n\npub fn build_result(\n    files: &[PathBuf],\n    file_layers: HashMap<PathBuf, String>,\n    nodes: BTreeSet<String>,\n    edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n    unresolved: Vec<UnresolvedDependency>,\n    entry_files: &BTreeSet<PathBuf>,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let adjacency = adjacency_from_edges(&edges_map);\n    let (mut ordered_layers, cycles) = topo_sort(&nodes, &adjacency);\n    if let Some(pos) = ordered_layers.iter().position(|layer| layer == \"root\") {\n        let root_layer = ordered_layers.remove(pos);\n        ordered_layers.insert(0, root_layer);\n    }\n    let rank = layer_rank_map(&ordered_layers);\n\n    let mut ordered_files = files.to_vec();\n    ordered_files.sort_by(|a, b| {\n        let mmsb_a = is_mmsb_main(a);\n        let mmsb_b = is_mmsb_main(b);\n        if mmsb_a && !mmsb_b {\n            return Ordering::Less;\n        } else if mmsb_b && !mmsb_a {\n            return Ordering::Greater;\n        }\n        let entry_a = entry_files.contains(a);\n        let entry_b = entry_files.contains(b);\n        if entry_a && !entry_b {\n            return Ordering::Less;\n        } else if entry_b && !entry_a {\n            return Ordering::Greater;\n        }\n        let layer_a = file_layers\n            .get(a)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_b = file_layers\n            .get(b)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let rank_a = rank.get(&layer_a).cloned().unwrap_or(ordered_layers.len());\n        let rank_b = rank.get(&layer_b).cloned().unwrap_or(ordered_layers.len());\n        rank_a\n            .cmp(&rank_b)\n            .then_with(|| layer_a.cmp(&layer_b))\n            .then_with(|| a.cmp(b))\n    });\n\n    let edges = edges_map\n        .into_iter()\n        .map(|((from, to), references)| LayerEdge {\n            violation: is_layer_violation(&from, &to),\n            from,\n            to,\n            references: references.into_iter().collect(),\n        })\n        .collect();\n\n    let graph = LayerGraph {\n        ordered_layers,\n        edges,\n        cycles,\n        unresolved,\n    };\n\n    Ok((ordered_files, graph))\n}\n\nfn adjacency_from_edges(\n    edges_map: &BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n) -> HashMap<String, BTreeSet<String>> {\n    let mut adjacency: HashMap<String, BTreeSet<String>> = HashMap::new();\n    for ((from, to), _) in edges_map {\n        adjacency\n            .entry(from.clone())\n            .or_default()\n            .insert(to.clone());\n    }\n    adjacency\n}\n\nfn topo_sort(\n    nodes: &BTreeSet<String>,\n    adjacency: &HashMap<String, BTreeSet<String>>,\n) -> (Vec<String>, Vec<String>) {\n    let mut indegree: HashMap<String, usize> = HashMap::new();\n    for node in nodes {\n        indegree.entry(node.clone()).or_insert(0);\n    }\n    for targets in adjacency.values() {\n        for target in targets {\n            *indegree.entry(target.clone()).or_insert(0) += 1;\n        }\n    }\n\n    let mut queue: VecDeque<String> = indegree\n        .iter()\n        .filter_map(|(node, &deg)| if deg == 0 { Some(node.clone()) } else { None })\n        .collect();\n    queue.make_contiguous().sort();\n\n    let mut order = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        order.push(node.clone());\n        if let Some(targets) = adjacency.get(&node) {\n            for target in targets {\n                if let Some(entry) = indegree.get_mut(target) {\n                    *entry -= 1;\n                    if *entry == 0 {\n                        insert_sorted(&mut queue, target.clone());\n                    }\n                }\n            }\n        }\n    }\n\n    if order.len() != nodes.len() {\n        let mut remaining: Vec<_> = nodes\n            .iter()\n            .filter(|layer| !order.contains(layer))\n            .cloned()\n            .collect();\n        remaining.sort();\n        let cycles = remaining.clone();\n        order.extend(remaining);\n        return (order, cycles);\n    }\n\n    (order, Vec::new())\n}\n\nfn layer_rank_map(order: &[String]) -> HashMap<String, usize> {\n    let mut rank = HashMap::new();\n    for (idx, layer) in order.iter().enumerate() {\n        rank.insert(layer.clone(), idx);\n    }\n    rank\n}\n\nfn insert_sorted(queue: &mut VecDeque<String>, value: String) {\n    let mut inserted = false;\n    for idx in 0..queue.len() {\n        if value < queue[idx] {\n            queue.insert(idx, value.clone());\n            inserted = true;\n            break;\n        }\n    }\n    if !inserted {\n        queue.push_back(value);\n    }\n}\n\nfn is_mmsb_main(path: &Path) -> bool {\n    path.file_name()\n        .and_then(|n| n.to_str())\n        .map(|n| n == \"MMSB.jl\")\n        .unwrap_or(false)\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\n/// Checks if a dependency from one layer to another violates layer ordering\n/// Returns true if from_layer > to_layer (violation: higher depends on lower)\npub fn is_layer_violation(from: &str, to: &str) -> bool {\n    match (layer_prefix_value(from), layer_prefix_value(to)) {\n        (Some(a), Some(b)) => a > b,\n        _ => false,\n    }\n}\n\n/// Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)\nfn layer_prefix_value(layer: &str) -> Option<i32> {\n    let mut chars = layer.chars();\n    let mut digits = String::new();\n    while let Some(ch) = chars.next() {\n        if ch.is_ascii_digit() {\n            digits.push(ch);\n        } else {\n            break;\n        }\n    }\n    if digits.is_empty() {\n        None\n    } else {\n        digits.parse::<i32>().ok()\n    }\n}\n\n// ============================================================================\n// From src/010_layer_core.rs (continued)\n// ============================================================================\n\npub fn compare_dir_layers(a: &Path, b: &Path) -> Ordering {\n    let a_name = a.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let b_name = b.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let a_layer = layer_prefix_value(a_name).unwrap_or(i32::MAX);\n    let b_layer = layer_prefix_value(b_name).unwrap_or(i32::MAX);\n    a_layer.cmp(&b_layer).then_with(|| a_name.cmp(b_name))\n}\n\npub fn compare_path_components(a: &Path, b: &Path) -> Ordering {\n    let a_components: Vec<_> = a.components().collect();\n    let b_components: Vec<_> = b.components().collect();\n    let min_len = a_components.len().min(b_components.len());\n\n    for idx in 0..min_len {\n        let a_name = a_components[idx].as_os_str().to_string_lossy();\n        let b_name = b_components[idx].as_os_str().to_string_lossy();\n        let a_prefix = layer_prefix_value(&a_name);\n        let b_prefix = layer_prefix_value(&b_name);\n        let cmp = match (a_prefix, b_prefix) {\n            (Some(a_val), Some(b_val)) => a_val.cmp(&b_val),\n            _ => a_name.cmp(&b_name),\n        };\n        if cmp != Ordering::Equal {\n            return cmp;\n        }\n    }\n\n    a_components.len().cmp(&b_components.len())\n}\n\npub fn layer_adheres(current_layer: &str, target_layer: &str) -> bool {\n    match (layer_prefix_value(current_layer), layer_prefix_value(target_layer)) {\n        (Some(curr), Some(target)) => curr <= target,\n        _ => true,\n    }\n}\n\npub(crate) fn structural_layer_value(layer: &Option<String>, default: i32) -> i32 {\n    layer\n        .as_ref()\n        .and_then(|value| layer_prefix_value(value))\n        .unwrap_or(default)\n}\n\npub fn detect_layer_violations(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Vec<FileLayerViolation> {\n    let mut violations = Vec::new();\n    for edge in graph.edge_references() {\n        let from = &graph[edge.source()];\n        let to = &graph[edge.target()];\n        let from_layer = file_layers\n            .get(from)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let to_layer = file_layers\n            .get(to)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        if let (Some(from_val), Some(to_val)) =\n            (layer_prefix_value(&from_layer), layer_prefix_value(&to_layer))\n        {\n            if from_val > to_val {\n                violations.push(FileLayerViolation {\n                    from: from.clone(),\n                    to: to.clone(),\n                    from_layer,\n                    to_layer,\n                });\n            }\n        }\n    }\n    violations\n}\n\n#[derive(Clone)]\npub struct FunctionInfo {\n    pub name: String,\n    pub signature: String,\n    pub file_path: String,\n    pub layer: String,\n    pub calls: Vec<String>,\n}\n\npub fn detect_layer_violation(\n    func: &FunctionInfo,\n    functions: &[FunctionInfo],\n    outgoing: &HashMap<usize, usize>,\n    file_layers: &HashMap<String, String>,\n) -> Option<(String, String)> {\n    let current_layer = file_layers\n        .get(&func.file_path)\n        .cloned()\n        .unwrap_or_else(|| func.layer.clone());\n    let current_value = layer_prefix_value(&current_layer)?;\n\n    let mut violation: Option<(i32, String)> = None;\n    for (callee_idx, _) in outgoing {\n        let callee = &functions[*callee_idx];\n        let target_layer = file_layers\n            .get(&callee.file_path)\n            .cloned()\n            .unwrap_or_else(|| callee.layer.clone());\n        if let Some(target_value) = layer_prefix_value(&target_layer) {\n            if target_value < current_value {\n                match violation {\n                    Some((best_value, _)) if target_value >= best_value => {}\n                    _ => {\n                        violation = Some((target_value, target_layer));\n                    }\n                }\n            }\n        }\n    }\n\n    violation.map(|(_, target_layer)| (current_layer, target_layer))\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs (report planning helpers)\n// ============================================================================\n\npub fn parse_cluster_members(\n    cluster: &crate::types::FunctionCluster,\n) -> Vec<crate::report::ClusterMember> {\n    cluster\n        .members\n        .iter()\n        .filter_map(|member| {\n            let (file, name) = member.rsplit_once(\"::\")?;\n            Some(crate::report::ClusterMember {\n                file: PathBuf::from(file),\n                name: name.to_string(),\n            })\n        })\n        .collect()\n}\n\npub fn is_core_module_path(path: &Path) -> bool {\n    let Some(stem) = path.file_stem().and_then(|name| name.to_str()) else {\n        return false;\n    };\n    stem.starts_with(\"040_dependency\") || stem.starts_with(\"060_layer_core\")\n}\n\npub fn cluster_target_path(\n    target: PathBuf,\n    members: &[crate::report::ClusterMember],\n    root_path: &Path,\n    idx: usize,\n) -> PathBuf {\n    if !is_core_module_path(&target) {\n        return target;\n    }\n    let prefix = target\n        .file_stem()\n        .and_then(|name| name.to_str())\n        .and_then(|stem| layer_prefix_value(stem))\n        .unwrap_or(900);\n    let file_name = format!(\"{:03}_cluster_{:03}.rs\", prefix, idx + 1);\n    let dir = members\n        .first()\n        .and_then(|member| member.file.parent())\n        .unwrap_or(root_path);\n    dir.join(file_name)\n}\n\npub fn collect_cluster_plans(\n    clusters: &[crate::types::FunctionCluster],\n    root_path: &Path,\n) -> Vec<crate::report::ClusterPlan> {\n    let mut plans = Vec::new();\n    for (idx, cluster) in clusters.iter().enumerate() {\n        let all_members = parse_cluster_members(cluster);\n        let target = if let Some(suggested) = &cluster.suggested_file {\n            suggested.clone()\n        } else if let Some(first) = all_members.first() {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            first\n                .file\n                .parent()\n                .unwrap_or(root_path)\n                .join(file_name)\n        } else {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            root_path.join(file_name)\n        };\n        let target = cluster_target_path(target, &all_members, root_path, idx);\n        let members = all_members\n            .into_iter()\n            .filter(|member| member.file != target)\n            .collect::<Vec<_>>();\n        if members.len() < 2 {\n            continue;\n        }\n        plans.push(crate::report::ClusterPlan {\n            target,\n            cohesion: cluster.cohesion,\n            members,\n        });\n    }\n    plans.sort_by(|a, b| {\n        use std::cmp::Ordering;\n        b.cohesion\n            .partial_cmp(&a.cohesion)\n            .unwrap_or(Ordering::Equal)\n            .then_with(|| b.members.len().cmp(&a.members.len()))\n            .then_with(|| a.target.cmp(&b.target))\n    });\n    plans\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\npub fn node_style(node_type: &NodeType) -> (&str, &str, &str) {\n    match node_type {\n        NodeType::Entry => (\"ellipse\", \"lightgreen\", \"\\\"filled,bold\\\"\"),\n        NodeType::Exit => (\"doubleoctagon\", \"lightcoral\", \"\\\"filled,bold\\\"\"),\n        NodeType::BasicBlock => (\"box\", \"lightblue\", \"filled\"),\n        NodeType::Branch => (\"diamond\", \"yellow\", \"filled\"),\n        NodeType::LoopHeader => (\"box\", \"orange\", \"\\\"filled,rounded\\\"\"),\n    }\n}\n\npub fn cyclomatic_complexity(cfg: &crate::types::FunctionCfg) -> usize {\n    let edges = cfg.edges.len() as isize;\n    let nodes = cfg.nodes.len() as isize;\n    let exits = 1isize; // assume one exit\n    let cc = edges - nodes + 2 * exits;\n    if cc <= 0 {\n        1\n    } else {\n        cc as usize\n    }\n}\n\npub fn sort_structural_items(items: &mut Vec<crate::report::PlanItem>) {\n    use std::collections::HashMap;\n    use std::path::PathBuf;\n\n    if items.len() <= 1 {\n        return;\n    }\n\n    let count = items.len();\n    let mut edges: Vec<Vec<usize>> = vec![Vec::new(); count];\n    let mut indegree = vec![0usize; count];\n\n    let mut file_to_items: HashMap<PathBuf, Vec<usize>> = HashMap::new();\n    for (idx, item) in items.iter().enumerate() {\n        if let Some(path) = &item.current_file {\n            file_to_items.entry(path.clone()).or_default().push(idx);\n        }\n    }\n\n    for i in 0..count {\n        for j in (i + 1)..count {\n            let req_i = structural_layer_value(&items[i].required_layer, i32::MAX);\n            let req_j = structural_layer_value(&items[j].required_layer, i32::MAX);\n            let mut edge = None;\n            if req_i != req_j {\n                edge = if req_i < req_j { Some((i, j)) } else { Some((j, i)) };\n            } else if items[i].is_utility != items[j].is_utility {\n                edge = if items[i].is_utility {\n                    Some((i, j))\n                } else {\n                    Some((j, i))\n                };\n            }\n            if let Some((from, to)) = edge {\n                edges[from].push(to);\n                indegree[to] += 1;\n            }\n        }\n    }\n\n    for (idx, item) in items.iter().enumerate() {\n        for file in &item.outgoing_files {\n            if let Some(dependents) = file_to_items.get(file) {\n                for &dependent_idx in dependents {\n                    if dependent_idx == idx {\n                        continue;\n                    }\n                    edges[dependent_idx].push(idx);\n                    indegree[idx] += 1;\n                }\n            }\n        }\n    }\n\n    let mut ordered_indices = Vec::with_capacity(count);\n    let mut available: Vec<usize> = (0..count).filter(|&i| indegree[i] == 0).collect();\n    while !available.is_empty() {\n        available.sort_by(|&a, &b| structural_cmp(&items[a], &items[b]));\n        let next = available.remove(0);\n        ordered_indices.push(next);\n        for &neighbor in &edges[next] {\n            indegree[neighbor] = indegree[neighbor].saturating_sub(1);\n            if indegree[neighbor] == 0 {\n                available.push(neighbor);\n            }\n        }\n    }\n\n    if ordered_indices.len() != count {\n        items.sort_by(structural_cmp);\n        return;\n    }\n\n    let mut reordered = Vec::with_capacity(count);\n    for idx in ordered_indices {\n        reordered.push(items[idx].clone());\n    }\n    *items = reordered;\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_gather_rust_files_to_src/020_cluster_010.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_layer_utilities.rs",
          "original_content": "//! Layer utility functions for layer-based dependency analysis\n//! This module is at layer 010 to be accessible from all higher layers\n\nuse anyhow::{Context, Result};\nuse clap::Parser;\nuse std::path::{Path, PathBuf};\n\n#[allow(unused_imports)]\npub use crate::cluster_001::{build_file_layers, detect_layer, gather_julia_files, julia_entry_paths};\n#[allow(unused_imports)]\npub use crate::cluster_010::contains_tools;\n\n/// Resolves the source root directory from a given root path\npub fn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\n/// Checks if a directory should be included in analysis\npub fn allow_analysis_dir(root: &Path, dir: &Path) -> bool {\n    let name = dir.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    \n    if name.starts_with('.') || name == \"target\" || name == \"node_modules\" {\n        return false;\n    }\n    \n    if let Ok(rel) = dir.strip_prefix(root) {\n        if rel.components().any(|c| {\n            let s = c.as_os_str().to_str().unwrap_or(\"\");\n            s.starts_with('.') || s == \"target\" || s == \"node_modules\"\n        }) {\n            return false;\n        }\n    }\n    \n    true\n}\n\npub fn gather_rust_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"rs\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n        .collect()\n}\n\n// ============================================================================\n// CLI Entrypoint (from src/000_cluster_011.rs)\n// ============================================================================\n\n#[derive(Parser, Debug)]\n#[command(name = \"mmsb-analyzer\")]\n#[command(about = \"MMSB Intelligence Substrate Analyzer\", long_about = None)]\nstruct Args {\n    /// Root directory to analyze\n    #[arg(short, long, default_value = \"../..\")]\n    root: PathBuf,\n\n    /// Output directory for reports\n    #[arg(short, long, default_value = \"../../docs/analysis\")]\n    output: PathBuf,\n\n    /// Verbose output\n    #[arg(short, long)]\n    verbose: bool,\n\n    /// Skip Julia file analysis\n    #[arg(long)]\n    skip_julia: bool,\n\n    /// Run dead code analysis\n    #[arg(long)]\n    dead_code: bool,\n\n    /// Filter dead code from downstream analysis\n    #[arg(long)]\n    dead_code_filter: bool,\n\n    /// Output JSON dead code report\n    #[arg(long)]\n    dead_code_json: Option<PathBuf>,\n\n    /// Output dead code summary markdown\n    #[arg(long)]\n    dead_code_summary: Option<PathBuf>,\n\n    /// Dead code summary limit\n    #[arg(long, default_value_t = 50)]\n    dead_code_summary_limit: usize,\n\n    /// Dead code policy file\n    #[arg(long)]\n    dead_code_policy: Option<PathBuf>,\n\n    /// Generate correction intelligence JSON\n    #[arg(long)]\n    correction_intelligence: bool,\n\n    /// Override correction intelligence JSON output path\n    #[arg(long)]\n    correction_json: Option<PathBuf>,\n\n    /// Override verification policy JSON output path\n    #[arg(long)]\n    verification_policy_json: Option<PathBuf>,\n}\n\npub fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let root_path = std::env::current_dir()?.join(&args.root).canonicalize()?;\n    let output_path = std::env::current_dir()?\n        .join(&args.output)\n        .canonicalize()\n        .unwrap_or_else(|_| {\n            let p = std::env::current_dir().unwrap().join(&args.output);\n            std::fs::create_dir_all(&p).ok();\n            p.canonicalize().unwrap_or(p)\n        });\n    run_analysis(\n        &root_path,\n        &output_path,\n        args.verbose,\n        args.skip_julia,\n        args.dead_code,\n        args.dead_code_filter,\n        args.dead_code_json,\n        args.dead_code_summary,\n        args.dead_code_summary_limit,\n        args.dead_code_policy,\n        args.correction_intelligence,\n        args.correction_json,\n        args.verification_policy_json,\n    )\n}\n\npub fn run_analysis(\n    root_path: &Path,\n    output_path: &Path,\n    verbose: bool,\n    skip_julia: bool,\n    dead_code: bool,\n    dead_code_filter: bool,\n    dead_code_json: Option<PathBuf>,\n    dead_code_summary: Option<PathBuf>,\n    dead_code_summary_limit: usize,\n    dead_code_policy: Option<PathBuf>,\n    correction_intelligence: bool,\n    correction_json: Option<PathBuf>,\n    verification_policy_json: Option<PathBuf>,\n) -> Result<()> {\n    use crate::control_flow::ControlFlowAnalyzer;\n    use crate::cohesion_analyzer::FunctionCohesionAnalyzer;\n    use crate::dependency::LayerGraph;\n    use crate::directory_analyzer::DirectoryAnalyzer;\n    use crate::dot_exporter::export_program_cfg_to_path;\n    use crate::julia_parser::JuliaAnalyzer;\n    use crate::report::ReportGenerator;\n    use crate::rust_parser::RustAnalyzer;\n    use crate::types::{AnalysisResult, FileOrderingResult};\n\n    let julia_script_path = root_path.join(\"src/000_main.jl\");\n\n    println!(\"MMSB Intelligence Substrate Analyzer\");\n    println!(\"=====================================\\n\");\n    println!(\"Root directory: {:?}\", root_path);\n    println!(\"Output directory: {:?}\", output_path);\n    println!(\"Julia script: {:?}\\n\", julia_script_path);\n\n    let rust_analyzer = RustAnalyzer::new(root_path.to_string_lossy().to_string());\n    let mut combined_result = AnalysisResult::new();\n\n    println!(\"Scanning Rust files (dependency-ordered)...\");\n    let mut rust_count = 0;\n    let rust_files = gather_rust_files(root_path);\n    let (ordered_rust_files, rust_layer_graph) =\n        crate::dependency::order_rust_files_by_dependency(&rust_files, root_path)\n            .context(\"Failed to resolve Rust dependency order\")?;\n    let rust_file_ordering =\n        crate::dependency::analyze_file_ordering(&rust_files, None)\n            .context(\"Failed to analyze Rust file ordering\")?;\n    let julia_file_ordering = FileOrderingResult {\n        ordered_files: Vec::new(),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles: Vec::new(),\n    };\n\n    for path in ordered_rust_files {\n        if verbose {\n            println!(\"  Analyzing: {:?}\", path);\n        }\n\n        match rust_analyzer.analyze_file(&path) {\n            Ok(result) => {\n                rust_count += 1;\n                combined_result.merge(result);\n            }\n            Err(e) => {\n                eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n            }\n        }\n    }\n\n    println!(\"  Analyzed {} Rust files\\n\", rust_count);\n\n    let mut julia_count = 0;\n    let mut julia_layer_graph = LayerGraph {\n        ordered_layers: Vec::new(),\n        edges: Vec::new(),\n        cycles: Vec::new(),\n        unresolved: Vec::new(),\n    };\n    if !skip_julia {\n        println!(\"Scanning Julia files (dependency-ordered)...\");\n        let julia_files = gather_julia_files(root_path);\n        let (ordered_julia_files, jlg) =\n            crate::dependency::order_julia_files_by_dependency(&julia_files, root_path)\n                .context(\"Failed to resolve Julia dependency order\")?;\n        julia_layer_graph = jlg;\n\n        if julia_script_path.exists() {\n            let julia_analyzer = JuliaAnalyzer::new(\n                root_path.to_path_buf(),\n                julia_script_path.clone(),\n                output_path.join(\"30_cfg/dots\"),\n            );\n\n            for path in ordered_julia_files {\n                if verbose {\n                    println!(\"  Analyzing: {:?}\", path);\n                }\n\n                match julia_analyzer.analyze_file(&path) {\n                    Ok(result) => {\n                        julia_count += 1;\n                        combined_result.merge(result);\n                    }\n                    Err(e) => {\n                        eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n                    }\n                }\n            }\n        } else {\n            println!(\"  Skipping Julia analysis (script not found)\");\n        }\n\n        println!(\"  Analyzed {} Julia files\\n\", julia_count);\n    }\n\n    if dead_code || dead_code_filter || dead_code_json.is_some() || dead_code_summary.is_some() {\n        let policy = if let Some(policy_path) = dead_code_policy {\n            Some(\n                crate::dead_code_policy::load_policy(&policy_path)\n                    .context(\"Failed to load dead code policy\")?,\n            )\n        } else {\n            None\n        };\n        let config = crate::dead_code_cli::DeadCodeRunConfig {\n            root: root_path.to_path_buf(),\n            output_dir: output_path.to_path_buf(),\n            policy,\n            write_json: dead_code_json,\n            write_summary: dead_code_summary,\n            summary_limit: dead_code_summary_limit,\n        };\n        let report = crate::dead_code_cli::run_dead_code_pipeline(&combined_result.elements, &config)\n            .context(\"Dead code analysis failed\")?;\n        if dead_code_filter {\n            combined_result.elements =\n                crate::dead_code_filter::filter_dead_code_elements(&combined_result.elements, &report);\n        }\n    }\n\n    println!(\"Building call graph...\");\n    let mut cf_analyzer = ControlFlowAnalyzer::new();\n    cf_analyzer.build_call_graph(&combined_result);\n\n    // NEW: Invariant detection\n    use crate::invariant_integrator::InvariantDetector;\n    println!(\"Detecting invariants...\");\n    let invariants_result = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.detect_all()\n    };\n    let constraints = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.generate_constraints(&invariants_result)\n    };\n    combined_result.invariants = invariants_result;\n    combined_result.constraints = constraints;\n\n    println!(\"Analyzing function cohesion...\");\n    let cohesion_analyzer = FunctionCohesionAnalyzer::new();\n    let placements = cohesion_analyzer.analyze(&combined_result)?;\n    let clusters = cohesion_analyzer.detect_clusters(&combined_result)?;\n\n    println!(\"Analyzing directory structure...\");\n    let dir_analyzer = DirectoryAnalyzer::new(root_path.to_path_buf());\n    let dir_analysis = dir_analyzer.analyze()?;\n\n    println!(\"\\nGenerating reports...\");\n    let report_gen = ReportGenerator::new(output_path.to_string_lossy().to_string());\n    report_gen.generate_all(\n        &combined_result,\n        &cf_analyzer,\n        &rust_layer_graph,\n        &julia_layer_graph,\n        &rust_file_ordering,\n        &julia_file_ordering,\n        &placements,\n        &clusters,\n        &dir_analysis,\n        root_path,\n        correction_intelligence,\n        correction_json,\n        verification_policy_json,\n    )\n    .context(\"Failed to generate reports\")?;\n\n    println!(\"\\nExporting program CFG...\");\n    export_program_cfg_to_path(&combined_result, &cf_analyzer.call_edges(), output_path)?;\n\n    println!(\"\\nGenerating invariant report...\");\n    use crate::invariant_reporter;\n    invariant_reporter::generate_invariant_report(&combined_result.invariants, output_path)\n        .context(\"Failed to generate invariant report\")?;\n    invariant_reporter::export_constraints_json(&combined_result.constraints, output_path)\n        .context(\"Failed to export constraints\")?;\n\n    println!(\"\\n Analysis complete!\");\n    println!(\"  Total elements: {}\", combined_result.elements.len());\n    println!(\"  Rust files: {}\", rust_count);\n    println!(\"  Julia files: {}\", julia_count);\n    println!(\"  Output: {}\\n\", output_path.display());\n\n    Ok(())\n}\n",
          "updated_content": "//! Layer utility functions for layer-based dependency analysis\n//! This module is at layer 010 to be accessible from all higher layers\n\nuse anyhow::{Context, Result};\nuse clap::Parser;\nuse std::path::{Path, PathBuf};\n\n#[allow(unused_imports)]\npub use crate::cluster_001::{build_file_layers, detect_layer, gather_julia_files, julia_entry_paths};\n#[allow(unused_imports)]\npub use crate::cluster_010::contains_tools;\n\n/// Resolves the source root directory from a given root path\npub fn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\n/// Checks if a directory should be included in analysis\npub fn allow_analysis_dir(root: &Path, dir: &Path) -> bool {\n    let name = dir.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    \n    if name.starts_with('.') || name == \"target\" || name == \"node_modules\" {\n        return false;\n    }\n    \n    if let Ok(rel) = dir.strip_prefix(root) {\n        if rel.components().any(|c| {\n            let s = c.as_os_str().to_str().unwrap_or(\"\");\n            s.starts_with('.') || s == \"target\" || s == \"node_modules\"\n        }) {\n            return false;\n        }\n    }\n    \n    true\n}\n\n\n\n// ============================================================================\n// CLI Entrypoint (from src/000_cluster_011.rs)\n// ============================================================================\n\n#[derive(Parser, Debug)]\n#[command(name = \"mmsb-analyzer\")]\n#[command(about = \"MMSB Intelligence Substrate Analyzer\", long_about = None)]\nstruct Args {\n    /// Root directory to analyze\n    #[arg(short, long, default_value = \"../..\")]\n    root: PathBuf,\n\n    /// Output directory for reports\n    #[arg(short, long, default_value = \"../../docs/analysis\")]\n    output: PathBuf,\n\n    /// Verbose output\n    #[arg(short, long)]\n    verbose: bool,\n\n    /// Skip Julia file analysis\n    #[arg(long)]\n    skip_julia: bool,\n\n    /// Run dead code analysis\n    #[arg(long)]\n    dead_code: bool,\n\n    /// Filter dead code from downstream analysis\n    #[arg(long)]\n    dead_code_filter: bool,\n\n    /// Output JSON dead code report\n    #[arg(long)]\n    dead_code_json: Option<PathBuf>,\n\n    /// Output dead code summary markdown\n    #[arg(long)]\n    dead_code_summary: Option<PathBuf>,\n\n    /// Dead code summary limit\n    #[arg(long, default_value_t = 50)]\n    dead_code_summary_limit: usize,\n\n    /// Dead code policy file\n    #[arg(long)]\n    dead_code_policy: Option<PathBuf>,\n\n    /// Generate correction intelligence JSON\n    #[arg(long)]\n    correction_intelligence: bool,\n\n    /// Override correction intelligence JSON output path\n    #[arg(long)]\n    correction_json: Option<PathBuf>,\n\n    /// Override verification policy JSON output path\n    #[arg(long)]\n    verification_policy_json: Option<PathBuf>,\n}\n\npub fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let root_path = std::env::current_dir()?.join(&args.root).canonicalize()?;\n    let output_path = std::env::current_dir()?\n        .join(&args.output)\n        .canonicalize()\n        .unwrap_or_else(|_| {\n            let p = std::env::current_dir().unwrap().join(&args.output);\n            std::fs::create_dir_all(&p).ok();\n            p.canonicalize().unwrap_or(p)\n        });\n    run_analysis(\n        &root_path,\n        &output_path,\n        args.verbose,\n        args.skip_julia,\n        args.dead_code,\n        args.dead_code_filter,\n        args.dead_code_json,\n        args.dead_code_summary,\n        args.dead_code_summary_limit,\n        args.dead_code_policy,\n        args.correction_intelligence,\n        args.correction_json,\n        args.verification_policy_json,\n    )\n}\n\npub fn run_analysis(\n    root_path: &Path,\n    output_path: &Path,\n    verbose: bool,\n    skip_julia: bool,\n    dead_code: bool,\n    dead_code_filter: bool,\n    dead_code_json: Option<PathBuf>,\n    dead_code_summary: Option<PathBuf>,\n    dead_code_summary_limit: usize,\n    dead_code_policy: Option<PathBuf>,\n    correction_intelligence: bool,\n    correction_json: Option<PathBuf>,\n    verification_policy_json: Option<PathBuf>,\n) -> Result<()> {\n    use crate::control_flow::ControlFlowAnalyzer;\n    use crate::cohesion_analyzer::FunctionCohesionAnalyzer;\n    use crate::dependency::LayerGraph;\n    use crate::directory_analyzer::DirectoryAnalyzer;\n    use crate::dot_exporter::export_program_cfg_to_path;\n    use crate::julia_parser::JuliaAnalyzer;\n    use crate::report::ReportGenerator;\n    use crate::rust_parser::RustAnalyzer;\n    use crate::types::{AnalysisResult, FileOrderingResult};\n\n    let julia_script_path = root_path.join(\"src/000_main.jl\");\n\n    println!(\"MMSB Intelligence Substrate Analyzer\");\n    println!(\"=====================================\\n\");\n    println!(\"Root directory: {:?}\", root_path);\n    println!(\"Output directory: {:?}\", output_path);\n    println!(\"Julia script: {:?}\\n\", julia_script_path);\n\n    let rust_analyzer = RustAnalyzer::new(root_path.to_string_lossy().to_string());\n    let mut combined_result = AnalysisResult::new();\n\n    println!(\"Scanning Rust files (dependency-ordered)...\");\n    let mut rust_count = 0;\n    let rust_files = gather_rust_files(root_path);\n    let (ordered_rust_files, rust_layer_graph) =\n        crate::dependency::order_rust_files_by_dependency(&rust_files, root_path)\n            .context(\"Failed to resolve Rust dependency order\")?;\n    let rust_file_ordering =\n        crate::dependency::analyze_file_ordering(&rust_files, None)\n            .context(\"Failed to analyze Rust file ordering\")?;\n    let julia_file_ordering = FileOrderingResult {\n        ordered_files: Vec::new(),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles: Vec::new(),\n    };\n\n    for path in ordered_rust_files {\n        if verbose {\n            println!(\"  Analyzing: {:?}\", path);\n        }\n\n        match rust_analyzer.analyze_file(&path) {\n            Ok(result) => {\n                rust_count += 1;\n                combined_result.merge(result);\n            }\n            Err(e) => {\n                eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n            }\n        }\n    }\n\n    println!(\"  Analyzed {} Rust files\\n\", rust_count);\n\n    let mut julia_count = 0;\n    let mut julia_layer_graph = LayerGraph {\n        ordered_layers: Vec::new(),\n        edges: Vec::new(),\n        cycles: Vec::new(),\n        unresolved: Vec::new(),\n    };\n    if !skip_julia {\n        println!(\"Scanning Julia files (dependency-ordered)...\");\n        let julia_files = gather_julia_files(root_path);\n        let (ordered_julia_files, jlg) =\n            crate::dependency::order_julia_files_by_dependency(&julia_files, root_path)\n                .context(\"Failed to resolve Julia dependency order\")?;\n        julia_layer_graph = jlg;\n\n        if julia_script_path.exists() {\n            let julia_analyzer = JuliaAnalyzer::new(\n                root_path.to_path_buf(),\n                julia_script_path.clone(),\n                output_path.join(\"30_cfg/dots\"),\n            );\n\n            for path in ordered_julia_files {\n                if verbose {\n                    println!(\"  Analyzing: {:?}\", path);\n                }\n\n                match julia_analyzer.analyze_file(&path) {\n                    Ok(result) => {\n                        julia_count += 1;\n                        combined_result.merge(result);\n                    }\n                    Err(e) => {\n                        eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n                    }\n                }\n            }\n        } else {\n            println!(\"  Skipping Julia analysis (script not found)\");\n        }\n\n        println!(\"  Analyzed {} Julia files\\n\", julia_count);\n    }\n\n    if dead_code || dead_code_filter || dead_code_json.is_some() || dead_code_summary.is_some() {\n        let policy = if let Some(policy_path) = dead_code_policy {\n            Some(\n                crate::dead_code_policy::load_policy(&policy_path)\n                    .context(\"Failed to load dead code policy\")?,\n            )\n        } else {\n            None\n        };\n        let config = crate::dead_code_cli::DeadCodeRunConfig {\n            root: root_path.to_path_buf(),\n            output_dir: output_path.to_path_buf(),\n            policy,\n            write_json: dead_code_json,\n            write_summary: dead_code_summary,\n            summary_limit: dead_code_summary_limit,\n        };\n        let report = crate::dead_code_cli::run_dead_code_pipeline(&combined_result.elements, &config)\n            .context(\"Dead code analysis failed\")?;\n        if dead_code_filter {\n            combined_result.elements =\n                crate::dead_code_filter::filter_dead_code_elements(&combined_result.elements, &report);\n        }\n    }\n\n    println!(\"Building call graph...\");\n    let mut cf_analyzer = ControlFlowAnalyzer::new();\n    cf_analyzer.build_call_graph(&combined_result);\n\n    // NEW: Invariant detection\n    use crate::invariant_integrator::InvariantDetector;\n    println!(\"Detecting invariants...\");\n    let invariants_result = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.detect_all()\n    };\n    let constraints = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.generate_constraints(&invariants_result)\n    };\n    combined_result.invariants = invariants_result;\n    combined_result.constraints = constraints;\n\n    println!(\"Analyzing function cohesion...\");\n    let cohesion_analyzer = FunctionCohesionAnalyzer::new();\n    let placements = cohesion_analyzer.analyze(&combined_result)?;\n    let clusters = cohesion_analyzer.detect_clusters(&combined_result)?;\n\n    println!(\"Analyzing directory structure...\");\n    let dir_analyzer = DirectoryAnalyzer::new(root_path.to_path_buf());\n    let dir_analysis = dir_analyzer.analyze()?;\n\n    println!(\"\\nGenerating reports...\");\n    let report_gen = ReportGenerator::new(output_path.to_string_lossy().to_string());\n    report_gen.generate_all(\n        &combined_result,\n        &cf_analyzer,\n        &rust_layer_graph,\n        &julia_layer_graph,\n        &rust_file_ordering,\n        &julia_file_ordering,\n        &placements,\n        &clusters,\n        &dir_analysis,\n        root_path,\n        correction_intelligence,\n        correction_json,\n        verification_policy_json,\n    )\n    .context(\"Failed to generate reports\")?;\n\n    println!(\"\\nExporting program CFG...\");\n    export_program_cfg_to_path(&combined_result, &cf_analyzer.call_edges(), output_path)?;\n\n    println!(\"\\nGenerating invariant report...\");\n    use crate::invariant_reporter;\n    invariant_reporter::generate_invariant_report(&combined_result.invariants, output_path)\n        .context(\"Failed to generate invariant report\")?;\n    invariant_reporter::export_constraints_json(&combined_result.constraints, output_path)\n        .context(\"Failed to export constraints\")?;\n\n    println!(\"\\n Analysis complete!\");\n    println!(\"  Total elements: {}\", combined_result.elements.len());\n    println!(\"  Rust files: {}\", rust_count);\n    println!(\"  Julia files: {}\", julia_count);\n    println!(\"  Output: {}\\n\", output_path.display());\n\n    Ok(())\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/020_cluster_010.rs",
          "original_content": "//! Cluster 010: Module resolution and dependency extraction utilities\n//!\n//! This module contains foundational functions for:\n//! - Module name resolution and path mapping\n//! - Rust dependency extraction from source files\n//! - Julia dependency extraction from source files\n//!\n//! Functions moved from src/000_dependency.rs as part of Phase 2, Batch 6 refactoring.\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::ItemUse;\nuse walkdir::WalkDir;\n\nuse crate::dependency::RootState;\n\n// ============================================================================\n// Module Resolution (from src/000_dependency.rs)\n// ============================================================================\n\npub fn normalize_module_name(name: &str) -> String {\n    if let Some(pos) = name.find('_') {\n        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n            return name[pos + 1..].to_string();\n        }\n    }\n    name.to_string()\n}\n\npub fn resolve_module(\n    root: &str,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Option<PathBuf> {\n    let key = normalize_module_name(root);\n    if let Some(path) = module_map.get(&key) {\n        return Some(path.clone());\n    }\n    module_map\n        .iter()\n        .find(|(name, _)| name == &&key)\n        .map(|(_, path)| path.clone())\n        .or_else(|| {\n            module_map\n                .iter()\n                .find(|(name, _)| key.starts_with(name.as_str()))\n                .map(|(_, path)| path.clone())\n        })\n        .or_else(|| crate::cluster_011::resolve_path(&PathBuf::from(root), file_set, module_map))\n}\n\npub fn contains_tools(path: &Path) -> bool {\n    path.components().any(|c| c.as_os_str() == \"tools\")\n}\n\n#[derive(Clone)]\npub struct ModuleRoot {\n    pub layer: String,\n}\n\npub fn build_module_root_map(root: &Path) -> Result<HashMap<String, ModuleRoot>, std::io::Error> {\n    let src_dir = root.join(\"src\");\n    let mut map = HashMap::new();\n    if src_dir.is_dir() {\n        for entry in fs::read_dir(&src_dir)? {\n            let entry = entry?;\n            let path = entry.path();\n            if contains_tools(&path) {\n                continue;\n            }\n            let name = entry\n                .file_name()\n                .to_string_lossy()\n                .to_string()\n                .trim_end_matches(\".rs\")\n                .to_string();\n            if path.is_dir() {\n                let normalized = normalize_module_name(&name);\n                map.insert(\n                    normalized,\n                    ModuleRoot {\n                        layer: name.clone(),\n                    },\n                );\n            } else if path.extension().map(|ext| ext == \"rs\").unwrap_or(false) {\n                map.insert(\n                    name.clone(),\n                    ModuleRoot {\n                        layer: crate::cluster_001::detect_layer(&path),\n                    },\n                );\n            }\n        }\n    }\n    Ok(map)\n}\n\n// ============================================================================\n// Julia Dependency Ordering (from src/020_layer_utilities.rs)\n// ============================================================================\n\nstruct LayerResolver {\n    aliases: HashMap<String, String>,\n}\n\nfn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\nimpl LayerResolver {\n    fn build(root: &Path) -> Result<Self> {\n        let mut resolver = LayerResolver {\n            aliases: HashMap::new(),\n        };\n        let src_dir = resolve_source_root(root);\n        if src_dir.is_dir() {\n            for entry in WalkDir::new(&src_dir).into_iter().filter_map(|e| e.ok()) {\n                let path = entry.path();\n                if contains_tools(path) {\n                    continue;\n                }\n                let layer = crate::cluster_001::detect_layer(path);\n                if layer == \"root\" {\n                    continue;\n                }\n                if path.is_dir() {\n                    if let Some(name) = path.file_name().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(name, &layer);\n                    }\n                } else if path.extension().map_or(false, |ext| ext == \"jl\") {\n                    if let Some(stem) = path.file_stem().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(stem, &layer);\n                    }\n                }\n            }\n        }\n        Ok(resolver)\n    }\n\n    fn add_aliases(&mut self, name: &str, layer: &str) {\n        let lower = name.to_lowercase();\n        self.aliases\n            .entry(lower.clone())\n            .or_insert_with(|| layer.to_string());\n        let condensed = lower.replace('_', \"\");\n        self.aliases\n            .entry(condensed)\n            .or_insert_with(|| layer.to_string());\n    }\n\n    fn resolve_module(&self, module: &str) -> Option<String> {\n        let key = module.to_lowercase();\n        if let Some(layer) = self.aliases.get(&key) {\n            return Some(layer.clone());\n        }\n        let condensed = key.replace('_', \"\");\n        if let Some(layer) = self.aliases.get(&condensed) {\n            return Some(layer.clone());\n        }\n        self.aliases\n            .iter()\n            .filter(|(alias, _)| !alias.is_empty())\n            .find(|(alias, _)| key.starts_with(alias.as_str()))\n            .map(|(_, layer)| layer.clone())\n    }\n}\n\npub fn order_julia_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, crate::dependency::LayerGraph)> {\n    use crate::cluster_001::{collect_julia_dependencies, JuliaTarget};\n    use crate::dependency::ReferenceDetail;\n\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n    let resolver = LayerResolver::build(root)?;\n    let entry_files = crate::cluster_001::julia_entry_paths(root);\n\n    for file in files {\n        let layer = crate::cluster_001::detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let references = collect_julia_dependencies(file)\n            .with_context(|| format!(\"Failed to analyze Julia dependencies for {:?}\", file))?;\n        for dep in references {\n            match dep.target {\n                JuliaTarget::Include(include_path) => {\n                    let resolved = if include_path.is_absolute() {\n                        include_path.clone()\n                    } else {\n                        file.parent()\n                            .map(|p| p.join(&include_path))\n                            .unwrap_or(include_path.clone())\n                    };\n\n                    if resolved.exists() {\n                        let target_layer = crate::cluster_001::detect_layer(&resolved);\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n                JuliaTarget::Module(module) => {\n                    if let Some(target_layer) = resolver.resolve_module(&module) {\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n\n// ============================================================================\n// Rust Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_rust_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    #[derive(Default)]\n    struct UseCollector {\n        roots: BTreeSet<String>,\n        mods: BTreeSet<String>,\n    }\n\n    impl<'ast> Visit<'ast> for UseCollector {\n        fn visit_item_use(&mut self, node: &'ast ItemUse) {\n            crate::dependency::collect_roots(&node.tree, RootState::Start, &mut self.roots);\n        }\n\n        fn visit_item_mod(&mut self, node: &'ast syn::ItemMod) {\n            if node.content.is_none() {\n                self.mods.insert(node.ident.to_string());\n            }\n        }\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", file))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    let mut deps = Vec::new();\n    for root in collector.roots {\n        if let Some(path) = resolve_module(&root, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    for module in collector.mods {\n        if let Some(path) = resolve_module(&module, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    Ok(deps)\n}\n\n// ============================================================================\n// Julia Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_julia_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    static INCLUDE_RE: Lazy<Regex> =\n        Lazy::new(|| Regex::new(r#\"include\\s*\\(\\s*[\"']([^\"']+)[\"']\"#).unwrap());\n    static MMSB_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static MMSB_SYMBOL_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#).unwrap()\n    });\n    static LOCAL_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+\\.\\s*([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static PLAIN_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+([A-Za-z_][A-Za-z0-9_\\.]*)\"#).unwrap()\n    });\n\n    fn resolve_module_name(\n        module: &str,\n        file_set: &HashSet<PathBuf>,\n        module_map: &HashMap<String, PathBuf>,\n    ) -> Option<PathBuf> {\n        let primary = module.split('.').next().unwrap_or(module);\n        resolve_module(primary, file_set, module_map)\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_RE.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let raw = path_match.as_str();\n            let mut candidate = PathBuf::from(raw);\n            if candidate.extension().is_none() {\n                candidate.set_extension(\"jl\");\n            }\n            let resolved = if candidate.is_absolute() {\n                candidate\n            } else {\n                file.parent()\n                    .map(|p| p.join(&candidate))\n                    .unwrap_or(candidate)\n            };\n            if let Some(path) = crate::cluster_011::resolve_path(&resolved, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_SYMBOL_RE.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                if let Some(path) = resolve_module_name(symbol, file_set, module_map) {\n                    deps.push(path);\n                }\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in PLAIN_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            if module.starts_with(\"MMSB\") {\n                continue;\n            }\n            if let Some(path) = resolve_module_name(module, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    Ok(deps)\n}\n\n// ============================================================================\n// File Dependency Mapping (from src/090_file_ordering.rs)\n// ============================================================================\n\npub fn build_dependency_map(\n    files: &[PathBuf],\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<HashMap<PathBuf, Vec<PathBuf>>> {\n    let mut dep_map: HashMap<PathBuf, Vec<PathBuf>> = HashMap::new();\n    for file in files {\n        let deps = extract_dependencies(file, file_set, module_map)\n            .with_context(|| format!(\"Failed to extract dependencies for {:?}\", file))?;\n        dep_map.insert(file.clone(), deps);\n    }\n    Ok(dep_map)\n}\n\npub(crate) fn extract_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    let ext = file.extension().and_then(|s| s.to_str()).unwrap_or(\"\");\n    match ext {\n        \"rs\" => extract_rust_dependencies(file, file_set, module_map),\n        \"jl\" => extract_julia_dependencies(file, file_set, module_map),\n        _ => Ok(Vec::new()),\n    }\n}\n",
          "updated_content": "//! Cluster 010: Module resolution and dependency extraction utilities\n//!\n//! This module contains foundational functions for:\n//! - Module name resolution and path mapping\n//! - Rust dependency extraction from source files\n//! - Julia dependency extraction from source files\n//!\n//! Functions moved from src/000_dependency.rs as part of Phase 2, Batch 6 refactoring.\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::ItemUse;\nuse walkdir::WalkDir;\n\nuse crate::dependency::RootState;\n\n// ============================================================================\n// Module Resolution (from src/000_dependency.rs)\n// ============================================================================\n\npub fn normalize_module_name(name: &str) -> String {\n    if let Some(pos) = name.find('_') {\n        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n            return name[pos + 1..].to_string();\n        }\n    }\n    name.to_string()\n}\n\npub fn resolve_module(\n    root: &str,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Option<PathBuf> {\n    let key = normalize_module_name(root);\n    if let Some(path) = module_map.get(&key) {\n        return Some(path.clone());\n    }\n    module_map\n        .iter()\n        .find(|(name, _)| name == &&key)\n        .map(|(_, path)| path.clone())\n        .or_else(|| {\n            module_map\n                .iter()\n                .find(|(name, _)| key.starts_with(name.as_str()))\n                .map(|(_, path)| path.clone())\n        })\n        .or_else(|| crate::cluster_011::resolve_path(&PathBuf::from(root), file_set, module_map))\n}\n\npub fn contains_tools(path: &Path) -> bool {\n    path.components().any(|c| c.as_os_str() == \"tools\")\n}\n\n#[derive(Clone)]\npub struct ModuleRoot {\n    pub layer: String,\n}\n\npub fn build_module_root_map(root: &Path) -> Result<HashMap<String, ModuleRoot>, std::io::Error> {\n    let src_dir = root.join(\"src\");\n    let mut map = HashMap::new();\n    if src_dir.is_dir() {\n        for entry in fs::read_dir(&src_dir)? {\n            let entry = entry?;\n            let path = entry.path();\n            if contains_tools(&path) {\n                continue;\n            }\n            let name = entry\n                .file_name()\n                .to_string_lossy()\n                .to_string()\n                .trim_end_matches(\".rs\")\n                .to_string();\n            if path.is_dir() {\n                let normalized = normalize_module_name(&name);\n                map.insert(\n                    normalized,\n                    ModuleRoot {\n                        layer: name.clone(),\n                    },\n                );\n            } else if path.extension().map(|ext| ext == \"rs\").unwrap_or(false) {\n                map.insert(\n                    name.clone(),\n                    ModuleRoot {\n                        layer: crate::cluster_001::detect_layer(&path),\n                    },\n                );\n            }\n        }\n    }\n    Ok(map)\n}\n\n// ============================================================================\n// Julia Dependency Ordering (from src/020_layer_utilities.rs)\n// ============================================================================\n\nstruct LayerResolver {\n    aliases: HashMap<String, String>,\n}\n\nfn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\nimpl LayerResolver {\n    fn build(root: &Path) -> Result<Self> {\n        let mut resolver = LayerResolver {\n            aliases: HashMap::new(),\n        };\n        let src_dir = resolve_source_root(root);\n        if src_dir.is_dir() {\n            for entry in WalkDir::new(&src_dir).into_iter().filter_map(|e| e.ok()) {\n                let path = entry.path();\n                if contains_tools(path) {\n                    continue;\n                }\n                let layer = crate::cluster_001::detect_layer(path);\n                if layer == \"root\" {\n                    continue;\n                }\n                if path.is_dir() {\n                    if let Some(name) = path.file_name().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(name, &layer);\n                    }\n                } else if path.extension().map_or(false, |ext| ext == \"jl\") {\n                    if let Some(stem) = path.file_stem().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(stem, &layer);\n                    }\n                }\n            }\n        }\n        Ok(resolver)\n    }\n\n    fn add_aliases(&mut self, name: &str, layer: &str) {\n        let lower = name.to_lowercase();\n        self.aliases\n            .entry(lower.clone())\n            .or_insert_with(|| layer.to_string());\n        let condensed = lower.replace('_', \"\");\n        self.aliases\n            .entry(condensed)\n            .or_insert_with(|| layer.to_string());\n    }\n\n    fn resolve_module(&self, module: &str) -> Option<String> {\n        let key = module.to_lowercase();\n        if let Some(layer) = self.aliases.get(&key) {\n            return Some(layer.clone());\n        }\n        let condensed = key.replace('_', \"\");\n        if let Some(layer) = self.aliases.get(&condensed) {\n            return Some(layer.clone());\n        }\n        self.aliases\n            .iter()\n            .filter(|(alias, _)| !alias.is_empty())\n            .find(|(alias, _)| key.starts_with(alias.as_str()))\n            .map(|(_, layer)| layer.clone())\n    }\n}\n\npub fn order_julia_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, crate::dependency::LayerGraph)> {\n    use crate::cluster_001::{collect_julia_dependencies, JuliaTarget};\n    use crate::dependency::ReferenceDetail;\n\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n    let resolver = LayerResolver::build(root)?;\n    let entry_files = crate::cluster_001::julia_entry_paths(root);\n\n    for file in files {\n        let layer = crate::cluster_001::detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let references = collect_julia_dependencies(file)\n            .with_context(|| format!(\"Failed to analyze Julia dependencies for {:?}\", file))?;\n        for dep in references {\n            match dep.target {\n                JuliaTarget::Include(include_path) => {\n                    let resolved = if include_path.is_absolute() {\n                        include_path.clone()\n                    } else {\n                        file.parent()\n                            .map(|p| p.join(&include_path))\n                            .unwrap_or(include_path.clone())\n                    };\n\n                    if resolved.exists() {\n                        let target_layer = crate::cluster_001::detect_layer(&resolved);\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n                JuliaTarget::Module(module) => {\n                    if let Some(target_layer) = resolver.resolve_module(&module) {\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n\n// ============================================================================\n// Rust Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_rust_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    #[derive(Default)]\n    struct UseCollector {\n        roots: BTreeSet<String>,\n        mods: BTreeSet<String>,\n    }\n\n    impl<'ast> Visit<'ast> for UseCollector {\n        fn visit_item_use(&mut self, node: &'ast ItemUse) {\n            crate::dependency::collect_roots(&node.tree, RootState::Start, &mut self.roots);\n        }\n\n        fn visit_item_mod(&mut self, node: &'ast syn::ItemMod) {\n            if node.content.is_none() {\n                self.mods.insert(node.ident.to_string());\n            }\n        }\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", file))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    let mut deps = Vec::new();\n    for root in collector.roots {\n        if let Some(path) = resolve_module(&root, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    for module in collector.mods {\n        if let Some(path) = resolve_module(&module, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    Ok(deps)\n}\n\n// ============================================================================\n// Julia Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_julia_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    static INCLUDE_RE: Lazy<Regex> =\n        Lazy::new(|| Regex::new(r#\"include\\s*\\(\\s*[\"']([^\"']+)[\"']\"#).unwrap());\n    static MMSB_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static MMSB_SYMBOL_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#).unwrap()\n    });\n    static LOCAL_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+\\.\\s*([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static PLAIN_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+([A-Za-z_][A-Za-z0-9_\\.]*)\"#).unwrap()\n    });\n\n    fn resolve_module_name(\n        module: &str,\n        file_set: &HashSet<PathBuf>,\n        module_map: &HashMap<String, PathBuf>,\n    ) -> Option<PathBuf> {\n        let primary = module.split('.').next().unwrap_or(module);\n        resolve_module(primary, file_set, module_map)\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_RE.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let raw = path_match.as_str();\n            let mut candidate = PathBuf::from(raw);\n            if candidate.extension().is_none() {\n                candidate.set_extension(\"jl\");\n            }\n            let resolved = if candidate.is_absolute() {\n                candidate\n            } else {\n                file.parent()\n                    .map(|p| p.join(&candidate))\n                    .unwrap_or(candidate)\n            };\n            if let Some(path) = crate::cluster_011::resolve_path(&resolved, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_SYMBOL_RE.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                if let Some(path) = resolve_module_name(symbol, file_set, module_map) {\n                    deps.push(path);\n                }\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in PLAIN_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            if module.starts_with(\"MMSB\") {\n                continue;\n            }\n            if let Some(path) = resolve_module_name(module, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    Ok(deps)\n}\n\n// ============================================================================\n// File Dependency Mapping (from src/090_file_ordering.rs)\n// ============================================================================\n\npub fn build_dependency_map(\n    files: &[PathBuf],\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<HashMap<PathBuf, Vec<PathBuf>>> {\n    let mut dep_map: HashMap<PathBuf, Vec<PathBuf>> = HashMap::new();\n    for file in files {\n        let deps = extract_dependencies(file, file_set, module_map)\n            .with_context(|| format!(\"Failed to extract dependencies for {:?}\", file))?;\n        dep_map.insert(file.clone(), deps);\n    }\n    Ok(dep_map)\n}\n\npub(crate) fn extract_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    let ext = file.extension().and_then(|s| s.to_str()).unwrap_or(\"\");\n    match ext {\n        \"rs\" => extract_rust_dependencies(file, file_set, module_map),\n        \"jl\" => extract_julia_dependencies(file, file_set, module_map),\n        _ => Ok(Vec::new()),\n    }\n}\n\npub fn gather_rust_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"rs\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n        .collect()\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_run_analysis_to_src/000_cluster_001.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_layer_utilities.rs",
          "original_content": "//! Layer utility functions for layer-based dependency analysis\n//! This module is at layer 010 to be accessible from all higher layers\n\nuse anyhow::{Context, Result};\nuse clap::Parser;\nuse std::path::{Path, PathBuf};\n\n#[allow(unused_imports)]\npub use crate::cluster_001::{build_file_layers, detect_layer, gather_julia_files, julia_entry_paths};\n#[allow(unused_imports)]\npub use crate::cluster_010::contains_tools;\n\n/// Resolves the source root directory from a given root path\npub fn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\n/// Checks if a directory should be included in analysis\npub fn allow_analysis_dir(root: &Path, dir: &Path) -> bool {\n    let name = dir.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    \n    if name.starts_with('.') || name == \"target\" || name == \"node_modules\" {\n        return false;\n    }\n    \n    if let Ok(rel) = dir.strip_prefix(root) {\n        if rel.components().any(|c| {\n            let s = c.as_os_str().to_str().unwrap_or(\"\");\n            s.starts_with('.') || s == \"target\" || s == \"node_modules\"\n        }) {\n            return false;\n        }\n    }\n    \n    true\n}\n\npub fn gather_rust_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"rs\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n        .collect()\n}\n\n// ============================================================================\n// CLI Entrypoint (from src/000_cluster_011.rs)\n// ============================================================================\n\n#[derive(Parser, Debug)]\n#[command(name = \"mmsb-analyzer\")]\n#[command(about = \"MMSB Intelligence Substrate Analyzer\", long_about = None)]\nstruct Args {\n    /// Root directory to analyze\n    #[arg(short, long, default_value = \"../..\")]\n    root: PathBuf,\n\n    /// Output directory for reports\n    #[arg(short, long, default_value = \"../../docs/analysis\")]\n    output: PathBuf,\n\n    /// Verbose output\n    #[arg(short, long)]\n    verbose: bool,\n\n    /// Skip Julia file analysis\n    #[arg(long)]\n    skip_julia: bool,\n\n    /// Run dead code analysis\n    #[arg(long)]\n    dead_code: bool,\n\n    /// Filter dead code from downstream analysis\n    #[arg(long)]\n    dead_code_filter: bool,\n\n    /// Output JSON dead code report\n    #[arg(long)]\n    dead_code_json: Option<PathBuf>,\n\n    /// Output dead code summary markdown\n    #[arg(long)]\n    dead_code_summary: Option<PathBuf>,\n\n    /// Dead code summary limit\n    #[arg(long, default_value_t = 50)]\n    dead_code_summary_limit: usize,\n\n    /// Dead code policy file\n    #[arg(long)]\n    dead_code_policy: Option<PathBuf>,\n\n    /// Generate correction intelligence JSON\n    #[arg(long)]\n    correction_intelligence: bool,\n\n    /// Override correction intelligence JSON output path\n    #[arg(long)]\n    correction_json: Option<PathBuf>,\n\n    /// Override verification policy JSON output path\n    #[arg(long)]\n    verification_policy_json: Option<PathBuf>,\n}\n\npub fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let root_path = std::env::current_dir()?.join(&args.root).canonicalize()?;\n    let output_path = std::env::current_dir()?\n        .join(&args.output)\n        .canonicalize()\n        .unwrap_or_else(|_| {\n            let p = std::env::current_dir().unwrap().join(&args.output);\n            std::fs::create_dir_all(&p).ok();\n            p.canonicalize().unwrap_or(p)\n        });\n    run_analysis(\n        &root_path,\n        &output_path,\n        args.verbose,\n        args.skip_julia,\n        args.dead_code,\n        args.dead_code_filter,\n        args.dead_code_json,\n        args.dead_code_summary,\n        args.dead_code_summary_limit,\n        args.dead_code_policy,\n        args.correction_intelligence,\n        args.correction_json,\n        args.verification_policy_json,\n    )\n}\n\npub fn run_analysis(\n    root_path: &Path,\n    output_path: &Path,\n    verbose: bool,\n    skip_julia: bool,\n    dead_code: bool,\n    dead_code_filter: bool,\n    dead_code_json: Option<PathBuf>,\n    dead_code_summary: Option<PathBuf>,\n    dead_code_summary_limit: usize,\n    dead_code_policy: Option<PathBuf>,\n    correction_intelligence: bool,\n    correction_json: Option<PathBuf>,\n    verification_policy_json: Option<PathBuf>,\n) -> Result<()> {\n    use crate::control_flow::ControlFlowAnalyzer;\n    use crate::cohesion_analyzer::FunctionCohesionAnalyzer;\n    use crate::dependency::LayerGraph;\n    use crate::directory_analyzer::DirectoryAnalyzer;\n    use crate::dot_exporter::export_program_cfg_to_path;\n    use crate::julia_parser::JuliaAnalyzer;\n    use crate::report::ReportGenerator;\n    use crate::rust_parser::RustAnalyzer;\n    use crate::types::{AnalysisResult, FileOrderingResult};\n\n    let julia_script_path = root_path.join(\"src/000_main.jl\");\n\n    println!(\"MMSB Intelligence Substrate Analyzer\");\n    println!(\"=====================================\\n\");\n    println!(\"Root directory: {:?}\", root_path);\n    println!(\"Output directory: {:?}\", output_path);\n    println!(\"Julia script: {:?}\\n\", julia_script_path);\n\n    let rust_analyzer = RustAnalyzer::new(root_path.to_string_lossy().to_string());\n    let mut combined_result = AnalysisResult::new();\n\n    println!(\"Scanning Rust files (dependency-ordered)...\");\n    let mut rust_count = 0;\n    let rust_files = gather_rust_files(root_path);\n    let (ordered_rust_files, rust_layer_graph) =\n        crate::dependency::order_rust_files_by_dependency(&rust_files, root_path)\n            .context(\"Failed to resolve Rust dependency order\")?;\n    let rust_file_ordering =\n        crate::dependency::analyze_file_ordering(&rust_files, None)\n            .context(\"Failed to analyze Rust file ordering\")?;\n    let julia_file_ordering = FileOrderingResult {\n        ordered_files: Vec::new(),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles: Vec::new(),\n    };\n\n    for path in ordered_rust_files {\n        if verbose {\n            println!(\"  Analyzing: {:?}\", path);\n        }\n\n        match rust_analyzer.analyze_file(&path) {\n            Ok(result) => {\n                rust_count += 1;\n                combined_result.merge(result);\n            }\n            Err(e) => {\n                eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n            }\n        }\n    }\n\n    println!(\"  Analyzed {} Rust files\\n\", rust_count);\n\n    let mut julia_count = 0;\n    let mut julia_layer_graph = LayerGraph {\n        ordered_layers: Vec::new(),\n        edges: Vec::new(),\n        cycles: Vec::new(),\n        unresolved: Vec::new(),\n    };\n    if !skip_julia {\n        println!(\"Scanning Julia files (dependency-ordered)...\");\n        let julia_files = gather_julia_files(root_path);\n        let (ordered_julia_files, jlg) =\n            crate::dependency::order_julia_files_by_dependency(&julia_files, root_path)\n                .context(\"Failed to resolve Julia dependency order\")?;\n        julia_layer_graph = jlg;\n\n        if julia_script_path.exists() {\n            let julia_analyzer = JuliaAnalyzer::new(\n                root_path.to_path_buf(),\n                julia_script_path.clone(),\n                output_path.join(\"30_cfg/dots\"),\n            );\n\n            for path in ordered_julia_files {\n                if verbose {\n                    println!(\"  Analyzing: {:?}\", path);\n                }\n\n                match julia_analyzer.analyze_file(&path) {\n                    Ok(result) => {\n                        julia_count += 1;\n                        combined_result.merge(result);\n                    }\n                    Err(e) => {\n                        eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n                    }\n                }\n            }\n        } else {\n            println!(\"  Skipping Julia analysis (script not found)\");\n        }\n\n        println!(\"  Analyzed {} Julia files\\n\", julia_count);\n    }\n\n    if dead_code || dead_code_filter || dead_code_json.is_some() || dead_code_summary.is_some() {\n        let policy = if let Some(policy_path) = dead_code_policy {\n            Some(\n                crate::dead_code_policy::load_policy(&policy_path)\n                    .context(\"Failed to load dead code policy\")?,\n            )\n        } else {\n            None\n        };\n        let config = crate::dead_code_cli::DeadCodeRunConfig {\n            root: root_path.to_path_buf(),\n            output_dir: output_path.to_path_buf(),\n            policy,\n            write_json: dead_code_json,\n            write_summary: dead_code_summary,\n            summary_limit: dead_code_summary_limit,\n        };\n        let report = crate::dead_code_cli::run_dead_code_pipeline(&combined_result.elements, &config)\n            .context(\"Dead code analysis failed\")?;\n        if dead_code_filter {\n            combined_result.elements =\n                crate::dead_code_filter::filter_dead_code_elements(&combined_result.elements, &report);\n        }\n    }\n\n    println!(\"Building call graph...\");\n    let mut cf_analyzer = ControlFlowAnalyzer::new();\n    cf_analyzer.build_call_graph(&combined_result);\n\n    // NEW: Invariant detection\n    use crate::invariant_integrator::InvariantDetector;\n    println!(\"Detecting invariants...\");\n    let invariants_result = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.detect_all()\n    };\n    let constraints = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.generate_constraints(&invariants_result)\n    };\n    combined_result.invariants = invariants_result;\n    combined_result.constraints = constraints;\n\n    println!(\"Analyzing function cohesion...\");\n    let cohesion_analyzer = FunctionCohesionAnalyzer::new();\n    let placements = cohesion_analyzer.analyze(&combined_result)?;\n    let clusters = cohesion_analyzer.detect_clusters(&combined_result)?;\n\n    println!(\"Analyzing directory structure...\");\n    let dir_analyzer = DirectoryAnalyzer::new(root_path.to_path_buf());\n    let dir_analysis = dir_analyzer.analyze()?;\n\n    println!(\"\\nGenerating reports...\");\n    let report_gen = ReportGenerator::new(output_path.to_string_lossy().to_string());\n    report_gen.generate_all(\n        &combined_result,\n        &cf_analyzer,\n        &rust_layer_graph,\n        &julia_layer_graph,\n        &rust_file_ordering,\n        &julia_file_ordering,\n        &placements,\n        &clusters,\n        &dir_analysis,\n        root_path,\n        correction_intelligence,\n        correction_json,\n        verification_policy_json,\n    )\n    .context(\"Failed to generate reports\")?;\n\n    println!(\"\\nExporting program CFG...\");\n    export_program_cfg_to_path(&combined_result, &cf_analyzer.call_edges(), output_path)?;\n\n    println!(\"\\nGenerating invariant report...\");\n    use crate::invariant_reporter;\n    invariant_reporter::generate_invariant_report(&combined_result.invariants, output_path)\n        .context(\"Failed to generate invariant report\")?;\n    invariant_reporter::export_constraints_json(&combined_result.constraints, output_path)\n        .context(\"Failed to export constraints\")?;\n\n    println!(\"\\n Analysis complete!\");\n    println!(\"  Total elements: {}\", combined_result.elements.len());\n    println!(\"  Rust files: {}\", rust_count);\n    println!(\"  Julia files: {}\", julia_count);\n    println!(\"  Output: {}\\n\", output_path.display());\n\n    Ok(())\n}\n",
          "updated_content": "//! Layer utility functions for layer-based dependency analysis\n//! This module is at layer 010 to be accessible from all higher layers\n\nuse anyhow::{Context, Result};\nuse clap::Parser;\nuse std::path::{Path, PathBuf};\n\n#[allow(unused_imports)]\npub use crate::cluster_001::{build_file_layers, detect_layer, gather_julia_files, julia_entry_paths};\n#[allow(unused_imports)]\npub use crate::cluster_010::contains_tools;\n\n/// Resolves the source root directory from a given root path\npub fn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\n/// Checks if a directory should be included in analysis\npub fn allow_analysis_dir(root: &Path, dir: &Path) -> bool {\n    let name = dir.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    \n    if name.starts_with('.') || name == \"target\" || name == \"node_modules\" {\n        return false;\n    }\n    \n    if let Ok(rel) = dir.strip_prefix(root) {\n        if rel.components().any(|c| {\n            let s = c.as_os_str().to_str().unwrap_or(\"\");\n            s.starts_with('.') || s == \"target\" || s == \"node_modules\"\n        }) {\n            return false;\n        }\n    }\n    \n    true\n}\n\npub fn gather_rust_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"rs\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n        .collect()\n}\n\n// ============================================================================\n// CLI Entrypoint (from src/000_cluster_011.rs)\n// ============================================================================\n\n#[derive(Parser, Debug)]\n#[command(name = \"mmsb-analyzer\")]\n#[command(about = \"MMSB Intelligence Substrate Analyzer\", long_about = None)]\nstruct Args {\n    /// Root directory to analyze\n    #[arg(short, long, default_value = \"../..\")]\n    root: PathBuf,\n\n    /// Output directory for reports\n    #[arg(short, long, default_value = \"../../docs/analysis\")]\n    output: PathBuf,\n\n    /// Verbose output\n    #[arg(short, long)]\n    verbose: bool,\n\n    /// Skip Julia file analysis\n    #[arg(long)]\n    skip_julia: bool,\n\n    /// Run dead code analysis\n    #[arg(long)]\n    dead_code: bool,\n\n    /// Filter dead code from downstream analysis\n    #[arg(long)]\n    dead_code_filter: bool,\n\n    /// Output JSON dead code report\n    #[arg(long)]\n    dead_code_json: Option<PathBuf>,\n\n    /// Output dead code summary markdown\n    #[arg(long)]\n    dead_code_summary: Option<PathBuf>,\n\n    /// Dead code summary limit\n    #[arg(long, default_value_t = 50)]\n    dead_code_summary_limit: usize,\n\n    /// Dead code policy file\n    #[arg(long)]\n    dead_code_policy: Option<PathBuf>,\n\n    /// Generate correction intelligence JSON\n    #[arg(long)]\n    correction_intelligence: bool,\n\n    /// Override correction intelligence JSON output path\n    #[arg(long)]\n    correction_json: Option<PathBuf>,\n\n    /// Override verification policy JSON output path\n    #[arg(long)]\n    verification_policy_json: Option<PathBuf>,\n}\n\npub fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let root_path = std::env::current_dir()?.join(&args.root).canonicalize()?;\n    let output_path = std::env::current_dir()?\n        .join(&args.output)\n        .canonicalize()\n        .unwrap_or_else(|_| {\n            let p = std::env::current_dir().unwrap().join(&args.output);\n            std::fs::create_dir_all(&p).ok();\n            p.canonicalize().unwrap_or(p)\n        });\n    run_analysis(\n        &root_path,\n        &output_path,\n        args.verbose,\n        args.skip_julia,\n        args.dead_code,\n        args.dead_code_filter,\n        args.dead_code_json,\n        args.dead_code_summary,\n        args.dead_code_summary_limit,\n        args.dead_code_policy,\n        args.correction_intelligence,\n        args.correction_json,\n        args.verification_policy_json,\n    )\n}\n\n\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/000_cluster_001.rs",
          "original_content": "//! Cluster 001: Core dependency analysis and file ordering utilities\n//!\n//! This module contains fundamental functions for:\n//! - Module mapping and dependency resolution\n//! - Topological sorting and layer-constrained ordering\n//! - File gathering and layer construction\n//! - Dependency graph building and cycle detection\n//! - DOT export for program CFGs\n//! - Naming validation and warnings\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse petgraph::algo::tarjan_scc;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::{ItemUse, UseTree};\n\nuse crate::dependency::{LayerGraph, ReferenceDetail, UnresolvedDependency};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\npub fn build_directory_entry_map(\n    files: &[PathBuf],\n) -> Result<HashMap<PathBuf, crate::types::FileOrderEntry>> {\n    use crate::file_ordering::{\n        build_dependency_map, build_entries, build_file_dag, detect_cycles, ordered_by_name,\n        topological_sort,\n    };\n    use crate::layer_core::layer_constrained_sort;\n    use crate::layer_utilities::build_file_layers;\n    use crate::types::FileOrderingResult;\n    use std::collections::HashSet;\n\n    const DEFAULT_STEP: usize = 10;\n\n    if files.is_empty() {\n        return Ok(HashMap::new());\n    }\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = crate::cluster_011::build_module_map(files);\n    let dep_map = build_dependency_map(files, &file_set, &module_map)?;\n    let file_layers = build_file_layers(files);\n    let (graph, node_map) = build_file_dag(files, &dep_map);\n    let cycles = detect_cycles(&graph, files);\n\n    let ordered_nodes = if cycles.is_empty() {\n        layer_constrained_sort(&graph, &file_layers).unwrap_or_else(|_| {\n            topological_sort(&graph).unwrap_or_else(|_| ordered_by_name(files, &node_map))\n        })\n    } else {\n        ordered_by_name(files, &node_map)\n    };\n\n    let ordered_files = ordered_nodes\n        .into_iter()\n        .map(|idx| graph[idx].clone())\n        .collect::<Vec<_>>();\n\n    let ordering = FileOrderingResult {\n        ordered_files: build_entries(&ordered_files, DEFAULT_STEP),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles,\n    };\n    let mut map = HashMap::new();\n    for entry in ordering.ordered_files {\n        map.insert(entry.current_path.clone(), entry);\n    }\n    Ok(map)\n}\n\npub fn collect_naming_warnings(\n    directory: &crate::types::DirectoryAnalysis,\n    config: &crate::report::ReportConfig,\n    warnings: &mut Vec<String>,\n) -> Result<()> {\n    use crate::utilities::compress_path;\n    use crate::dependency::naming_score_for_file;\n    if directory\n        .path\n        .components()\n        .any(|comp| comp.as_os_str() == \"_old\")\n    {\n        return Ok(());\n    }\n    let file_map = build_directory_entry_map(&directory.files)?;\n    for file in &directory.files {\n        if file.components().any(|comp| comp.as_os_str() == \"_old\") {\n            continue;\n        }\n        let entry = file_map.get(file);\n        if let Some(score) = naming_score_for_file(file, entry) {\n            if score < config.naming_score_warning {\n                let suggested = entry\n                    .map(|e| e.suggested_name.as_str())\n                    .unwrap_or(\"suggested name unavailable\");\n                warnings.push(format!(\n                    \"File `{}` has naming score {:.0}; consider renaming to `{}`.\",\n                    compress_path(file.to_string_lossy().as_ref()),\n                    score,\n                    suggested,\n                ));\n            }\n        }\n    }\n    for child in &directory.subdirectories {\n        collect_naming_warnings(child, config, warnings)?;\n    }\n    Ok(())\n}\n\n#[cfg(test)]\nfn temp_dir(name: &str) -> PathBuf {\n    let mut dir = std::env::temp_dir();\n    dir.push(format!(\n        \"mmsb_analyzer_{}_{}\",\n        name,\n        std::time::SystemTime::now()\n            .duration_since(std::time::UNIX_EPOCH)\n            .unwrap()\n            .as_nanos()\n    ));\n    dir\n}\n\n#[cfg(test)]\nfn detects_cycles() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"cycle\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    write(&a, \"use crate::b; pub fn a() {}\")?;\n    write(&b, \"use crate::a; pub fn b() {}\")?;\n\n    let result = analyze_file_ordering(&[a.clone(), b.clone()], Some(10))?;\n    assert!(!result.cycles.is_empty());\n    Ok(())\n}\n\n#[cfg(test)]\nfn generates_canonical_names_and_violations() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"names\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    write(&a, \"use crate::b; pub fn a() {}\")?;\n    write(&b, \"pub fn b() {}\")?;\n\n    let result = analyze_file_ordering(&[a.clone(), b.clone()], Some(10))?;\n    let entries = &result.ordered_files;\n    assert_eq!(entries[0].suggested_name, \"000_b.rs\");\n    assert_eq!(entries[1].suggested_name, \"010_a.rs\");\n    assert!(!result.violations.is_empty());\n    Ok(())\n}\n\n#[cfg(test)]\n#[allow(dead_code)]\nfn topo_sort_orders_dependencies() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"topo\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    let c = dir.join(\"c.rs\");\n    write(&a, \"pub fn a() {}\")?;\n    write(&b, \"use crate::a; pub fn b() {}\")?;\n    write(&c, \"use crate::b; pub fn c() {}\")?;\n\n    let result = analyze_file_ordering(&[c.clone(), b.clone(), a.clone()], Some(10))?;\n    let ordered: Vec<_> = result\n        .ordered_files\n        .iter()\n        .map(|entry| entry.current_path.clone())\n        .collect();\n    assert_eq!(ordered, vec![a, b, c]);\n    assert!(result.cycles.is_empty());\n    Ok(())\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\npub fn layer_constrained_sort(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Result<Vec<NodeIndex>> {\n    use crate::cluster_006::layer_prefix_value;\n\n    let mut layer_nodes: BTreeMap<i32, Vec<NodeIndex>> = BTreeMap::new();\n    for node in graph.node_indices() {\n        let file = &graph[node];\n        let layer_name = file_layers\n            .get(file)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_value = layer_prefix_value(&layer_name).unwrap_or(0);\n        layer_nodes.entry(layer_value).or_default().push(node);\n    }\n\n    let mut ordered = Vec::new();\n    for (_layer, nodes) in layer_nodes {\n        let sorted = topo_sort_within(graph, &nodes)?;\n        ordered.extend(sorted);\n    }\n    Ok(ordered)\n}\n\npub fn topo_sort_within(\n    graph: &DiGraph<PathBuf, ()>,\n    nodes: &[NodeIndex],\n) -> Result<Vec<NodeIndex>> {\n    let node_set: HashSet<NodeIndex> = nodes.iter().copied().collect();\n    let mut indegree: HashMap<NodeIndex, usize> = HashMap::new();\n    for &node in nodes {\n        indegree.insert(node, 0);\n    }\n    for &node in nodes {\n        let incoming = graph\n            .neighbors_directed(node, petgraph::Direction::Incoming)\n            .filter(|n| node_set.contains(n))\n            .count();\n        indegree.insert(node, incoming);\n    }\n    let mut queue = std::collections::VecDeque::new();\n    for &node in nodes {\n        if indegree.get(&node).copied().unwrap_or(0) == 0 {\n            queue.push_back(node);\n        }\n    }\n    let mut ordered = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        ordered.push(node);\n        for neighbor in graph.neighbors_directed(node, petgraph::Direction::Outgoing) {\n            if !node_set.contains(&neighbor) {\n                continue;\n            }\n            if let Some(entry) = indegree.get_mut(&neighbor) {\n                *entry = entry.saturating_sub(1);\n                if *entry == 0 {\n                    queue.push_back(neighbor);\n                }\n            }\n        }\n    }\n    if ordered.len() != nodes.len() {\n        return Err(anyhow::anyhow!(\"Cycle detected within layer group\"));\n    }\n    Ok(ordered)\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs\n// ============================================================================\n\n/// Detects the layer identifier from a path by finding first digit-prefixed component\npub fn detect_layer(path: &Path) -> String {\n    for component in path.components() {\n        if let Some(name) = component.as_os_str().to_str() {\n            if let Some(first) = name.chars().next() {\n                if first.is_ascii_digit() {\n                    if let Some(pos) = name.find('_') {\n                        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n                            return name.to_string();\n                        }\n                    }\n                }\n            }\n        }\n    }\n    \"root\".to_string()\n}\n\npub fn rust_entry_paths(root: &Path) -> BTreeSet<PathBuf> {\n    let src_dir = crate::layer_utilities::resolve_source_root(root);\n    [\"lib.rs\", \"main.rs\"]\n        .iter()\n        .map(|rel| src_dir.join(rel))\n        .filter(|p| p.exists())\n        .collect()\n}\n\n#[derive(Clone)]\nstruct RustDependency {\n    root: String,\n    detail: String,\n}\n\nfn collect_rust_dependencies(path: &Path) -> Result<Vec<RustDependency>> {\n    let content =\n        fs::read_to_string(path).with_context(|| format!(\"Unable to read Rust file {:?}\", path))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", path))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    Ok(collector.deps)\n}\n\n#[derive(Default)]\nstruct UseCollector {\n    deps: Vec<RustDependency>,\n}\n\nimpl<'ast> Visit<'ast> for UseCollector {\n    fn visit_item_use(&mut self, node: &'ast ItemUse) {\n        let mut roots = BTreeSet::new();\n        collect_roots_from_crate(&node.tree, CrateRootState::Start, &mut roots);\n        let stmt = quote::quote!(#node).to_string();\n        for root in roots {\n            self.deps.push(RustDependency {\n                root,\n                detail: stmt.clone(),\n            });\n        }\n    }\n}\n\n#[derive(Copy, Clone, Eq, PartialEq)]\nenum CrateRootState {\n    Start,\n    AfterCrate,\n}\n\nfn collect_roots_from_crate(tree: &UseTree, state: CrateRootState, acc: &mut BTreeSet<String>) {\n    match tree {\n        UseTree::Path(path) => {\n            let ident = path.ident.to_string();\n            if state == CrateRootState::Start && ident == \"crate\" {\n                collect_roots_from_crate(&path.tree, CrateRootState::AfterCrate, acc);\n            } else if state == CrateRootState::AfterCrate {\n                acc.insert(ident);\n            } else {\n                collect_roots_from_crate(&path.tree, state, acc);\n            }\n        }\n        UseTree::Group(group) => {\n            for tree in &group.items {\n                collect_roots_from_crate(tree, state, acc);\n            }\n        }\n        UseTree::Name(name) => {\n            if state == CrateRootState::AfterCrate {\n                acc.insert(name.ident.to_string());\n            }\n        }\n        UseTree::Rename(rename) => {\n            if state == CrateRootState::AfterCrate {\n                acc.insert(rename.ident.to_string());\n            }\n        }\n        UseTree::Glob(_) => {}\n    }\n}\n\n/// Order Rust files by dependency and capture layer graph details.\npub fn order_rust_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let module_map = crate::cluster_010::build_module_root_map(root)?;\n    let entry_files = rust_entry_paths(root);\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n\n    for file in files {\n        let layer = detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let deps = collect_rust_dependencies(file)\n            .with_context(|| format!(\"Failed to collect dependencies for {:?}\", file))?;\n        for dep in deps {\n            if let Some(info) = module_map.get(&dep.root) {\n                nodes.insert(info.layer.clone());\n                if info.layer != layer {\n                    edges_map\n                        .entry((info.layer.clone(), layer.clone()))\n                        .or_default()\n                        .insert(ReferenceDetail {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                }\n            } else {\n                unresolved.push(UnresolvedDependency {\n                    file: file.clone(),\n                    reference: dep.detail.clone(),\n                });\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n\n// ============================================================================\n// Julia Dependency Analysis (from src/000_dependency.rs)\n// ============================================================================\n\n#[derive(Clone)]\npub(crate) struct JuliaDependency {\n    pub(crate) target: JuliaTarget,\n    pub(crate) detail: String,\n}\n\n#[derive(Clone)]\npub(crate) enum JuliaTarget {\n    Include(PathBuf),\n    Module(String),\n}\n\nstatic INCLUDE_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)include\\s*\\(\\s*[\"']([^\"'\\n]+)[\"']\"#).expect(\"failed to compile include regex\")\n});\nstatic USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#)\n        .expect(\"failed to compile using regex\")\n});\nstatic ROOT_USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#)\n        .expect(\"failed to compile root using regex\")\n});\nstatic LOCAL_USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+\\.\\s*([A-Za-z0-9_]+)\"#)\n        .expect(\"failed to compile local using regex\")\n});\n\npub(crate) fn collect_julia_dependencies(path: &Path) -> Result<Vec<JuliaDependency>> {\n    let content = fs::read_to_string(path)\n        .with_context(|| format!(\"Unable to read Julia file {:?}\", path))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_REGEX.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let relative = PathBuf::from(path_match.as_str());\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Include(relative),\n                detail,\n            });\n        }\n    }\n\n    for cap in USING_REGEX.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            let primary = module.split('.').next().unwrap_or(module).to_string();\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Module(primary),\n                detail,\n            });\n        }\n    }\n\n    for cap in ROOT_USING_REGEX.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                let primary = symbol.split('.').next().unwrap_or(symbol).to_string();\n                deps.push(JuliaDependency {\n                    target: JuliaTarget::Module(primary),\n                    detail: detail.clone(),\n                });\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_REGEX.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Module(module.to_string()),\n                detail,\n            });\n        }\n    }\n\n    Ok(deps)\n}\n\npub fn julia_entry_paths(root: &Path) -> BTreeSet<PathBuf> {\n    let src_dir = crate::layer_utilities::resolve_source_root(root);\n    [\"MMSB.jl\", \"API.jl\", \"MMSB/API.jl\"]\n        .iter()\n        .map(|rel| src_dir.join(rel))\n        .filter(|p| p.exists())\n        .collect()\n}\n\npub fn build_file_layers(files: &[PathBuf]) -> HashMap<PathBuf, String> {\n    let mut layers = HashMap::new();\n    for file in files {\n        layers.insert(file.clone(), detect_layer(file));\n    }\n    layers\n}\n\npub fn gather_julia_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = crate::layer_utilities::resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            crate::layer_utilities::allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"jl\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n    .collect()\n}\n\n// ============================================================================\n// From src/090_file_ordering.rs\n// ============================================================================\n\npub fn topological_sort(graph: &DiGraph<PathBuf, ()>) -> Result<Vec<NodeIndex>> {\n    use petgraph::Direction;\n    use std::collections::VecDeque;\n\n    let mut indegree = vec![0usize; graph.node_count()];\n    for node in graph.node_indices() {\n        indegree[node.index()] = graph\n            .neighbors_directed(node, Direction::Incoming)\n            .count();\n    }\n\n    let mut queue = VecDeque::new();\n    for node in graph.node_indices() {\n        if indegree[node.index()] == 0 {\n            queue.push_back(node);\n        }\n    }\n\n    let mut ordered = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        ordered.push(node);\n        for neighbor in graph.neighbors_directed(node, Direction::Outgoing) {\n            let entry = &mut indegree[neighbor.index()];\n            *entry = entry.saturating_sub(1);\n            if *entry == 0 {\n                queue.push_back(neighbor);\n            }\n        }\n    }\n\n    if ordered.len() != graph.node_count() {\n        return Err(anyhow::anyhow!(\"Cycle detected in dependency graph\"));\n    }\n\n    Ok(ordered)\n}\n\npub fn ordered_by_name(\n    files: &[PathBuf],\n    node_map: &HashMap<PathBuf, NodeIndex>,\n) -> Vec<NodeIndex> {\n    let mut sorted = files.to_vec();\n    sorted.sort();\n    sorted\n        .into_iter()\n        .filter_map(|path| node_map.get(&path).copied())\n        .collect()\n}\n\n/// Builds file ordering entries with canonical names and rename flags\npub fn build_entries(ordered: &[PathBuf], step: usize) -> Vec<crate::types::FileOrderEntry> {\n    ordered\n        .iter()\n        .enumerate()\n        .map(|(idx, path)| {\n            let canonical_order = idx * step;\n            let suggested_name =\n                crate::cluster_006::generate_canonical_name(path, canonical_order);\n            let needs_rename = path\n                .file_name()\n                .and_then(|n| n.to_str())\n                .map(|name| name != suggested_name)\n                .unwrap_or(false);\n            crate::types::FileOrderEntry {\n                current_path: path.clone(),\n                canonical_order,\n                suggested_name,\n                needs_rename,\n            }\n        })\n        .collect()\n}\n\npub fn analyze_file_ordering(\n    files: &[PathBuf],\n    step: Option<usize>,\n) -> Result<crate::types::FileOrderingResult> {\n    let step = step.unwrap_or(10);\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = crate::cluster_011::build_module_map(files);\n    let dep_map = crate::cluster_010::build_dependency_map(files, &file_set, &module_map)?;\n    let file_layers = build_file_layers(files);\n    let ordered_directories = crate::layer_core::order_directories(files, &dep_map);\n\n    let (graph, node_map) = crate::cluster_011::build_file_dag(files, &dep_map);\n    let layer_violations = crate::cluster_008::detect_layer_violations(&graph, &file_layers);\n    let cycles = detect_cycles(&graph, files);\n\n    let ordered_nodes = if cycles.is_empty() {\n        crate::layer_core::layer_constrained_sort(&graph, &file_layers).unwrap_or_else(|_| {\n            topological_sort(&graph).unwrap_or_else(|_| ordered_by_name(files, &node_map))\n        })\n    } else {\n        ordered_by_name(files, &node_map)\n    };\n\n    let ordered_files = ordered_nodes\n        .into_iter()\n        .map(|idx| graph[idx].clone())\n        .collect::<Vec<_>>();\n\n    let file_entries = build_entries(&ordered_files, step);\n    let violations = detect_violations(&file_entries, &dep_map);\n\n    Ok(crate::types::FileOrderingResult {\n        ordered_files: file_entries,\n        violations,\n        layer_violations,\n        ordered_directories,\n        cycles,\n    })\n}\n\npub fn naming_score_for_file(\n    file: &Path,\n    order_entry: Option<&crate::types::FileOrderEntry>,\n) -> Option<f64> {\n    let name = file.file_name()?.to_string_lossy();\n    let stem = file.file_stem()?.to_string_lossy();\n    let mut score = 1.0f64;\n\n    if stem.len() < 3 {\n        score -= 0.2;\n    }\n    if stem.len() > 40 {\n        score -= 0.1;\n    }\n    if stem.chars().any(|c| c.is_uppercase()) {\n        score -= 0.1;\n    }\n    if !stem\n        .chars()\n        .all(|c| c.is_ascii_lowercase() || c.is_ascii_digit() || c == '_')\n    {\n        score -= 0.1;\n    }\n    if name.contains(\"__\") {\n        score -= 0.1;\n    }\n\n    if let Some(entry) = order_entry {\n        let expected = entry.suggested_name.as_str();\n        let actual = name.as_ref();\n        if expected != actual {\n            score -= 0.3;\n        } else {\n            score += 0.1;\n        }\n    }\n\n    if let Ok(contents) = fs::read_to_string(file) {\n        let mut ident_counts: HashMap<String, usize> = HashMap::new();\n        let ident_re = match Regex::new(r\"[A-Za-z_][A-Za-z0-9_]*\") {\n            Ok(regex) => regex,\n            Err(_) => return None,\n        };\n        for cap in ident_re.captures_iter(&contents) {\n            let Some(m) = cap.get(0) else {\n                continue;\n            };\n            let ident = m.as_str().to_lowercase();\n            if matches!(\n                ident.as_str(),\n                \"fn\"\n                    | \"pub\"\n                    | \"use\"\n                    | \"struct\"\n                    | \"enum\"\n                    | \"impl\"\n                    | \"mod\"\n                    | \"let\"\n                    | \"mut\"\n                    | \"ref\"\n                    | \"self\"\n                    | \"crate\"\n                    | \"super\"\n                    | \"where\"\n                    | \"trait\"\n                    | \"type\"\n                    | \"const\"\n                    | \"static\"\n                    | \"match\"\n                    | \"if\"\n                    | \"else\"\n                    | \"for\"\n                    | \"while\"\n                    | \"loop\"\n                    | \"return\"\n                    | \"async\"\n                    | \"await\"\n                    | \"move\"\n                    | \"dyn\"\n                    | \"as\"\n            ) {\n                continue;\n            }\n            *ident_counts.entry(ident).or_insert(0) += 1;\n        }\n\n        let mut idents = ident_counts.into_iter().collect::<Vec<_>>();\n        idents.sort_by(|a, b| b.1.cmp(&a.1));\n        let top_idents = idents.into_iter().take(8).map(|(k, _)| k).collect::<Vec<_>>();\n        let name_tokens = stem\n            .split('_')\n            .map(|s| s.to_lowercase())\n            .filter(|s| !s.is_empty() && !s.chars().all(|c| c.is_ascii_digit()))\n            .collect::<Vec<_>>();\n        let overlap = top_idents\n            .iter()\n            .filter(|ident| name_tokens.iter().any(|t| t == *ident))\n            .count();\n\n        if overlap == 0 {\n            score -= 0.1;\n        } else if overlap >= 2 {\n            score += 0.1;\n        }\n    }\n\n    if score < 0.0 {\n        score = 0.0;\n    }\n    if score > 1.0 {\n        score = 1.0;\n    }\n    Some(score * 100.0)\n}\n\npub(crate) fn detect_cycles(\n    graph: &DiGraph<PathBuf, ()>,\n    files: &[PathBuf],\n) -> Vec<Vec<PathBuf>> {\n    let sccs = tarjan_scc(graph);\n    let mut cycles = Vec::new();\n    for scc in sccs {\n        if scc.len() > 1 {\n            cycles.push(scc.into_iter().map(|idx| graph[idx].clone()).collect());\n        }\n    }\n    if cycles.is_empty() {\n        return cycles;\n    }\n    if cycles.iter().all(|cycle| cycle.is_empty()) {\n        let mut fallback = files.to_vec();\n        fallback.sort();\n        cycles.push(fallback);\n    }\n    cycles\n}\n\npub(crate) fn detect_violations(\n    ordered_files: &[crate::types::FileOrderEntry],\n    dep_map: &HashMap<PathBuf, Vec<PathBuf>>,\n) -> Vec<crate::types::OrderViolation> {\n    let mut alpha = ordered_files.to_vec();\n    alpha.sort_by(|a, b| a.current_path.cmp(&b.current_path));\n    let alpha_positions: HashMap<PathBuf, usize> = alpha\n        .iter()\n        .enumerate()\n        .map(|(idx, entry)| (entry.current_path.clone(), idx))\n        .collect();\n\n    let canonical_positions: HashMap<PathBuf, usize> = ordered_files\n        .iter()\n        .enumerate()\n        .map(|(idx, entry)| (entry.current_path.clone(), idx))\n        .collect();\n\n    let mut violations = Vec::new();\n    for entry in ordered_files {\n        let Some(&alpha_pos) = alpha_positions.get(&entry.current_path) else {\n            continue;\n        };\n        let Some(&required_pos) = canonical_positions.get(&entry.current_path) else {\n            continue;\n        };\n        if alpha_pos != required_pos {\n            let blocking_dependencies = dep_map\n                .get(&entry.current_path)\n                .map(|deps| {\n                    deps.iter()\n                        .filter(|dep| {\n                            let dep_alpha = alpha_positions.get(*dep).copied().unwrap_or(0);\n                            dep_alpha > alpha_pos\n                        })\n                        .cloned()\n                        .collect::<Vec<_>>()\n                })\n                .unwrap_or_default();\n            violations.push(crate::types::OrderViolation {\n                file: entry.current_path.clone(),\n                current_position: alpha_pos,\n                required_position: required_pos,\n                blocking_dependencies,\n            });\n        }\n    }\n\n    violations\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\n/// Exports a complete program CFG to DOT format\npub fn export_complete_program_dot(\n    program: &crate::types::ProgramCFG,\n    path: &str,\n) -> std::io::Result<()> {\n    use std::collections::HashMap;\n    use std::fmt::Write;\n\n    fn escape_dot(s: &str) -> String {\n        s.replace('\\\\', \"\\\\\\\\\").replace('\"', \"\\\\\\\"\").replace('\\n', \"\\\\n\")\n    }\n\n    let mut dot = String::new();\n\n    writeln!(dot, \"digraph ProgramCFG {{\").unwrap();\n    writeln!(dot, \"  rankdir=TB;\").unwrap();\n    writeln!(dot, \"  compound=true;\").unwrap();\n    writeln!(dot, \"  newrank=true;\").unwrap();\n    writeln!(\n        dot,\n        \"  label=\\\"Complete Program CFG - {} functions\\\";\",\n        program.functions.len()\n    )\n    .unwrap();\n    writeln!(dot, \"  labelloc=t;\").unwrap();\n    writeln!(dot, \"  fontsize=16;\").unwrap();\n    writeln!(dot, \"\").unwrap();\n\n    let mut funcs: Vec<_> = program.functions.iter().collect();\n    funcs.sort_by_key(|(fid, _)| fid.as_str());\n\n    let mut func_to_cluster: HashMap<&String, usize> = HashMap::new();\n\n    for (cluster_idx, (func_id, cfg)) in funcs.iter().enumerate() {\n        let safe_name = func_id.replace(['!', '?', '*'], \"_\");\n        let cc = crate::cluster_008::cyclomatic_complexity(cfg);\n        func_to_cluster.insert(func_id, cluster_idx);\n\n        writeln!(dot, \"  subgraph cluster_{} {{\", cluster_idx).unwrap();\n        writeln!(dot, \"    label=\\\"{} (CC={})\\\";\", safe_name, cc).unwrap();\n        writeln!(dot, \"    style=filled;\").unwrap();\n        writeln!(dot, \"    fillcolor=lightgray;\").unwrap();\n        writeln!(dot, \"    color=black;\").unwrap();\n        writeln!(dot, \"\").unwrap();\n\n        for node in &cfg.nodes {\n            let (shape, color, style) = crate::cluster_008::node_style(&node.node_type);\n\n            let mut label = node.label.clone();\n            if !node.lines.is_empty() {\n                let lines_str: String = node\n                    .lines\n                    .iter()\n                    .map(|l| l.to_string())\n                    .collect::<Vec<_>>()\n                    .join(\",\");\n                label = format!(\"{} L{}\", label, lines_str);\n            }\n\n            let url = format!(\"http://127.0.0.1:8081/run?f={}\", func_id);\n\n            writeln!(\n                dot,\n                \"    f{}_n{} [label=\\\"{}\\\", shape={}, fillcolor={}, style={}, URL=\\\"{}\\\"];\",\n                cluster_idx,\n                node.id,\n                escape_dot(&label),\n                shape,\n                color,\n                style,\n                url\n            )\n            .unwrap();\n        }\n\n        writeln!(dot, \"\").unwrap();\n\n        for edge in &cfg.edges {\n            let mut attrs = Vec::new();\n            if let Some(cond) = edge.condition {\n                let label = if cond { \"T\" } else { \"F\" };\n                let color = if cond { \"darkgreen\" } else { \"red\" };\n                attrs.push(format!(\"label=\\\"{}\\\"\", label));\n                attrs.push(format!(\"color=\\\"{}\\\"\", color));\n            }\n            let attr_str = if attrs.is_empty() {\n                \"\".to_string()\n            } else {\n                format!(\" [{}]\", attrs.join(\", \"))\n            };\n\n            writeln!(\n                dot,\n                \"    f{}_n{} -> f{}_n{}{};\",\n                cluster_idx,\n                edge.from,\n                cluster_idx,\n                edge.to,\n                attr_str\n            )\n            .unwrap();\n        }\n\n        writeln!(dot, \"  }}\").unwrap();\n        writeln!(dot, \"\").unwrap();\n    }\n\n    writeln!(dot, \"  // Inter-function calls\").unwrap();\n    writeln!(dot, \"  edge [style=dashed, color=blue, penwidth=2];\").unwrap();\n    writeln!(dot, \"\").unwrap();\n\n    for (caller, callee) in &program.call_edges {\n        if let (Some(&caller_idx), Some(&callee_idx)) =\n            (func_to_cluster.get(caller), func_to_cluster.get(callee))\n        {\n            if let (Some(caller_cfg), Some(callee_cfg)) =\n                (program.functions.get(caller), program.functions.get(callee))\n            {\n                writeln!(\n                    dot,\n                    \"  f{}_n{} -> f{}_n{} [ltail=cluster_{}, lhead=cluster_{}, label=\\\"call\\\"];\",\n                    caller_idx,\n                    caller_cfg.exit_id,\n                    callee_idx,\n                    callee_cfg.entry_id,\n                    caller_idx,\n                    callee_idx\n                )\n                .unwrap();\n            }\n        }\n    }\n\n    writeln!(dot, \"}}\").unwrap();\n\n    std::fs::write(path, dot)?;\n    println!(\"Complete program CFG exported to {}\", path);\n    Ok(())\n}\n\n// ============================================================================\n// Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_detects_cycles() {\n        detects_cycles().unwrap();\n    }\n\n    #[test]\n    fn test_generates_canonical_names_and_violations() {\n        generates_canonical_names_and_violations().unwrap();\n    }\n}\n",
          "updated_content": "//! Cluster 001: Core dependency analysis and file ordering utilities\n//!\n//! This module contains fundamental functions for:\n//! - Module mapping and dependency resolution\n//! - Topological sorting and layer-constrained ordering\n//! - File gathering and layer construction\n//! - Dependency graph building and cycle detection\n//! - DOT export for program CFGs\n//! - Naming validation and warnings\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse petgraph::algo::tarjan_scc;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::{ItemUse, UseTree};\n\nuse crate::dependency::{LayerGraph, ReferenceDetail, UnresolvedDependency};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\npub fn build_directory_entry_map(\n    files: &[PathBuf],\n) -> Result<HashMap<PathBuf, crate::types::FileOrderEntry>> {\n    use crate::file_ordering::{\n        build_dependency_map, build_entries, build_file_dag, detect_cycles, ordered_by_name,\n        topological_sort,\n    };\n    use crate::layer_core::layer_constrained_sort;\n    use crate::layer_utilities::build_file_layers;\n    use crate::types::FileOrderingResult;\n    use std::collections::HashSet;\n\n    const DEFAULT_STEP: usize = 10;\n\n    if files.is_empty() {\n        return Ok(HashMap::new());\n    }\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = crate::cluster_011::build_module_map(files);\n    let dep_map = build_dependency_map(files, &file_set, &module_map)?;\n    let file_layers = build_file_layers(files);\n    let (graph, node_map) = build_file_dag(files, &dep_map);\n    let cycles = detect_cycles(&graph, files);\n\n    let ordered_nodes = if cycles.is_empty() {\n        layer_constrained_sort(&graph, &file_layers).unwrap_or_else(|_| {\n            topological_sort(&graph).unwrap_or_else(|_| ordered_by_name(files, &node_map))\n        })\n    } else {\n        ordered_by_name(files, &node_map)\n    };\n\n    let ordered_files = ordered_nodes\n        .into_iter()\n        .map(|idx| graph[idx].clone())\n        .collect::<Vec<_>>();\n\n    let ordering = FileOrderingResult {\n        ordered_files: build_entries(&ordered_files, DEFAULT_STEP),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles,\n    };\n    let mut map = HashMap::new();\n    for entry in ordering.ordered_files {\n        map.insert(entry.current_path.clone(), entry);\n    }\n    Ok(map)\n}\n\npub fn collect_naming_warnings(\n    directory: &crate::types::DirectoryAnalysis,\n    config: &crate::report::ReportConfig,\n    warnings: &mut Vec<String>,\n) -> Result<()> {\n    use crate::utilities::compress_path;\n    use crate::dependency::naming_score_for_file;\n    if directory\n        .path\n        .components()\n        .any(|comp| comp.as_os_str() == \"_old\")\n    {\n        return Ok(());\n    }\n    let file_map = build_directory_entry_map(&directory.files)?;\n    for file in &directory.files {\n        if file.components().any(|comp| comp.as_os_str() == \"_old\") {\n            continue;\n        }\n        let entry = file_map.get(file);\n        if let Some(score) = naming_score_for_file(file, entry) {\n            if score < config.naming_score_warning {\n                let suggested = entry\n                    .map(|e| e.suggested_name.as_str())\n                    .unwrap_or(\"suggested name unavailable\");\n                warnings.push(format!(\n                    \"File `{}` has naming score {:.0}; consider renaming to `{}`.\",\n                    compress_path(file.to_string_lossy().as_ref()),\n                    score,\n                    suggested,\n                ));\n            }\n        }\n    }\n    for child in &directory.subdirectories {\n        collect_naming_warnings(child, config, warnings)?;\n    }\n    Ok(())\n}\n\n#[cfg(test)]\nfn temp_dir(name: &str) -> PathBuf {\n    let mut dir = std::env::temp_dir();\n    dir.push(format!(\n        \"mmsb_analyzer_{}_{}\",\n        name,\n        std::time::SystemTime::now()\n            .duration_since(std::time::UNIX_EPOCH)\n            .unwrap()\n            .as_nanos()\n    ));\n    dir\n}\n\n#[cfg(test)]\nfn detects_cycles() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"cycle\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    write(&a, \"use crate::b; pub fn a() {}\")?;\n    write(&b, \"use crate::a; pub fn b() {}\")?;\n\n    let result = analyze_file_ordering(&[a.clone(), b.clone()], Some(10))?;\n    assert!(!result.cycles.is_empty());\n    Ok(())\n}\n\n#[cfg(test)]\nfn generates_canonical_names_and_violations() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"names\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    write(&a, \"use crate::b; pub fn a() {}\")?;\n    write(&b, \"pub fn b() {}\")?;\n\n    let result = analyze_file_ordering(&[a.clone(), b.clone()], Some(10))?;\n    let entries = &result.ordered_files;\n    assert_eq!(entries[0].suggested_name, \"000_b.rs\");\n    assert_eq!(entries[1].suggested_name, \"010_a.rs\");\n    assert!(!result.violations.is_empty());\n    Ok(())\n}\n\n#[cfg(test)]\n#[allow(dead_code)]\nfn topo_sort_orders_dependencies() -> Result<()> {\n    use crate::dependency::analyze_file_ordering;\n    use std::fs::{create_dir_all, write};\n\n    let dir = temp_dir(\"topo\");\n    create_dir_all(&dir)?;\n    let a = dir.join(\"a.rs\");\n    let b = dir.join(\"b.rs\");\n    let c = dir.join(\"c.rs\");\n    write(&a, \"pub fn a() {}\")?;\n    write(&b, \"use crate::a; pub fn b() {}\")?;\n    write(&c, \"use crate::b; pub fn c() {}\")?;\n\n    let result = analyze_file_ordering(&[c.clone(), b.clone(), a.clone()], Some(10))?;\n    let ordered: Vec<_> = result\n        .ordered_files\n        .iter()\n        .map(|entry| entry.current_path.clone())\n        .collect();\n    assert_eq!(ordered, vec![a, b, c]);\n    assert!(result.cycles.is_empty());\n    Ok(())\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\npub fn layer_constrained_sort(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Result<Vec<NodeIndex>> {\n    use crate::cluster_006::layer_prefix_value;\n\n    let mut layer_nodes: BTreeMap<i32, Vec<NodeIndex>> = BTreeMap::new();\n    for node in graph.node_indices() {\n        let file = &graph[node];\n        let layer_name = file_layers\n            .get(file)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_value = layer_prefix_value(&layer_name).unwrap_or(0);\n        layer_nodes.entry(layer_value).or_default().push(node);\n    }\n\n    let mut ordered = Vec::new();\n    for (_layer, nodes) in layer_nodes {\n        let sorted = topo_sort_within(graph, &nodes)?;\n        ordered.extend(sorted);\n    }\n    Ok(ordered)\n}\n\npub fn topo_sort_within(\n    graph: &DiGraph<PathBuf, ()>,\n    nodes: &[NodeIndex],\n) -> Result<Vec<NodeIndex>> {\n    let node_set: HashSet<NodeIndex> = nodes.iter().copied().collect();\n    let mut indegree: HashMap<NodeIndex, usize> = HashMap::new();\n    for &node in nodes {\n        indegree.insert(node, 0);\n    }\n    for &node in nodes {\n        let incoming = graph\n            .neighbors_directed(node, petgraph::Direction::Incoming)\n            .filter(|n| node_set.contains(n))\n            .count();\n        indegree.insert(node, incoming);\n    }\n    let mut queue = std::collections::VecDeque::new();\n    for &node in nodes {\n        if indegree.get(&node).copied().unwrap_or(0) == 0 {\n            queue.push_back(node);\n        }\n    }\n    let mut ordered = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        ordered.push(node);\n        for neighbor in graph.neighbors_directed(node, petgraph::Direction::Outgoing) {\n            if !node_set.contains(&neighbor) {\n                continue;\n            }\n            if let Some(entry) = indegree.get_mut(&neighbor) {\n                *entry = entry.saturating_sub(1);\n                if *entry == 0 {\n                    queue.push_back(neighbor);\n                }\n            }\n        }\n    }\n    if ordered.len() != nodes.len() {\n        return Err(anyhow::anyhow!(\"Cycle detected within layer group\"));\n    }\n    Ok(ordered)\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs\n// ============================================================================\n\n/// Detects the layer identifier from a path by finding first digit-prefixed component\npub fn detect_layer(path: &Path) -> String {\n    for component in path.components() {\n        if let Some(name) = component.as_os_str().to_str() {\n            if let Some(first) = name.chars().next() {\n                if first.is_ascii_digit() {\n                    if let Some(pos) = name.find('_') {\n                        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n                            return name.to_string();\n                        }\n                    }\n                }\n            }\n        }\n    }\n    \"root\".to_string()\n}\n\npub fn rust_entry_paths(root: &Path) -> BTreeSet<PathBuf> {\n    let src_dir = crate::layer_utilities::resolve_source_root(root);\n    [\"lib.rs\", \"main.rs\"]\n        .iter()\n        .map(|rel| src_dir.join(rel))\n        .filter(|p| p.exists())\n        .collect()\n}\n\n#[derive(Clone)]\nstruct RustDependency {\n    root: String,\n    detail: String,\n}\n\nfn collect_rust_dependencies(path: &Path) -> Result<Vec<RustDependency>> {\n    let content =\n        fs::read_to_string(path).with_context(|| format!(\"Unable to read Rust file {:?}\", path))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", path))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    Ok(collector.deps)\n}\n\n#[derive(Default)]\nstruct UseCollector {\n    deps: Vec<RustDependency>,\n}\n\nimpl<'ast> Visit<'ast> for UseCollector {\n    fn visit_item_use(&mut self, node: &'ast ItemUse) {\n        let mut roots = BTreeSet::new();\n        collect_roots_from_crate(&node.tree, CrateRootState::Start, &mut roots);\n        let stmt = quote::quote!(#node).to_string();\n        for root in roots {\n            self.deps.push(RustDependency {\n                root,\n                detail: stmt.clone(),\n            });\n        }\n    }\n}\n\n#[derive(Copy, Clone, Eq, PartialEq)]\nenum CrateRootState {\n    Start,\n    AfterCrate,\n}\n\nfn collect_roots_from_crate(tree: &UseTree, state: CrateRootState, acc: &mut BTreeSet<String>) {\n    match tree {\n        UseTree::Path(path) => {\n            let ident = path.ident.to_string();\n            if state == CrateRootState::Start && ident == \"crate\" {\n                collect_roots_from_crate(&path.tree, CrateRootState::AfterCrate, acc);\n            } else if state == CrateRootState::AfterCrate {\n                acc.insert(ident);\n            } else {\n                collect_roots_from_crate(&path.tree, state, acc);\n            }\n        }\n        UseTree::Group(group) => {\n            for tree in &group.items {\n                collect_roots_from_crate(tree, state, acc);\n            }\n        }\n        UseTree::Name(name) => {\n            if state == CrateRootState::AfterCrate {\n                acc.insert(name.ident.to_string());\n            }\n        }\n        UseTree::Rename(rename) => {\n            if state == CrateRootState::AfterCrate {\n                acc.insert(rename.ident.to_string());\n            }\n        }\n        UseTree::Glob(_) => {}\n    }\n}\n\n/// Order Rust files by dependency and capture layer graph details.\npub fn order_rust_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let module_map = crate::cluster_010::build_module_root_map(root)?;\n    let entry_files = rust_entry_paths(root);\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n\n    for file in files {\n        let layer = detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let deps = collect_rust_dependencies(file)\n            .with_context(|| format!(\"Failed to collect dependencies for {:?}\", file))?;\n        for dep in deps {\n            if let Some(info) = module_map.get(&dep.root) {\n                nodes.insert(info.layer.clone());\n                if info.layer != layer {\n                    edges_map\n                        .entry((info.layer.clone(), layer.clone()))\n                        .or_default()\n                        .insert(ReferenceDetail {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                }\n            } else {\n                unresolved.push(UnresolvedDependency {\n                    file: file.clone(),\n                    reference: dep.detail.clone(),\n                });\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n\n// ============================================================================\n// Julia Dependency Analysis (from src/000_dependency.rs)\n// ============================================================================\n\n#[derive(Clone)]\npub(crate) struct JuliaDependency {\n    pub(crate) target: JuliaTarget,\n    pub(crate) detail: String,\n}\n\n#[derive(Clone)]\npub(crate) enum JuliaTarget {\n    Include(PathBuf),\n    Module(String),\n}\n\nstatic INCLUDE_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)include\\s*\\(\\s*[\"']([^\"'\\n]+)[\"']\"#).expect(\"failed to compile include regex\")\n});\nstatic USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#)\n        .expect(\"failed to compile using regex\")\n});\nstatic ROOT_USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#)\n        .expect(\"failed to compile root using regex\")\n});\nstatic LOCAL_USING_REGEX: Lazy<Regex> = Lazy::new(|| {\n    Regex::new(r#\"(?m)(?:using|import)\\s+\\.\\s*([A-Za-z0-9_]+)\"#)\n        .expect(\"failed to compile local using regex\")\n});\n\npub(crate) fn collect_julia_dependencies(path: &Path) -> Result<Vec<JuliaDependency>> {\n    let content = fs::read_to_string(path)\n        .with_context(|| format!(\"Unable to read Julia file {:?}\", path))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_REGEX.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let relative = PathBuf::from(path_match.as_str());\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Include(relative),\n                detail,\n            });\n        }\n    }\n\n    for cap in USING_REGEX.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            let primary = module.split('.').next().unwrap_or(module).to_string();\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Module(primary),\n                detail,\n            });\n        }\n    }\n\n    for cap in ROOT_USING_REGEX.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                let primary = symbol.split('.').next().unwrap_or(symbol).to_string();\n                deps.push(JuliaDependency {\n                    target: JuliaTarget::Module(primary),\n                    detail: detail.clone(),\n                });\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_REGEX.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            let detail = cap\n                .get(0)\n                .map(|m| m.as_str().trim().to_string())\n                .unwrap_or_default();\n            deps.push(JuliaDependency {\n                target: JuliaTarget::Module(module.to_string()),\n                detail,\n            });\n        }\n    }\n\n    Ok(deps)\n}\n\npub fn julia_entry_paths(root: &Path) -> BTreeSet<PathBuf> {\n    let src_dir = crate::layer_utilities::resolve_source_root(root);\n    [\"MMSB.jl\", \"API.jl\", \"MMSB/API.jl\"]\n        .iter()\n        .map(|rel| src_dir.join(rel))\n        .filter(|p| p.exists())\n        .collect()\n}\n\npub fn build_file_layers(files: &[PathBuf]) -> HashMap<PathBuf, String> {\n    let mut layers = HashMap::new();\n    for file in files {\n        layers.insert(file.clone(), detect_layer(file));\n    }\n    layers\n}\n\npub fn gather_julia_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = crate::layer_utilities::resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            crate::layer_utilities::allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"jl\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n    .collect()\n}\n\n// ============================================================================\n// From src/090_file_ordering.rs\n// ============================================================================\n\npub fn topological_sort(graph: &DiGraph<PathBuf, ()>) -> Result<Vec<NodeIndex>> {\n    use petgraph::Direction;\n    use std::collections::VecDeque;\n\n    let mut indegree = vec![0usize; graph.node_count()];\n    for node in graph.node_indices() {\n        indegree[node.index()] = graph\n            .neighbors_directed(node, Direction::Incoming)\n            .count();\n    }\n\n    let mut queue = VecDeque::new();\n    for node in graph.node_indices() {\n        if indegree[node.index()] == 0 {\n            queue.push_back(node);\n        }\n    }\n\n    let mut ordered = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        ordered.push(node);\n        for neighbor in graph.neighbors_directed(node, Direction::Outgoing) {\n            let entry = &mut indegree[neighbor.index()];\n            *entry = entry.saturating_sub(1);\n            if *entry == 0 {\n                queue.push_back(neighbor);\n            }\n        }\n    }\n\n    if ordered.len() != graph.node_count() {\n        return Err(anyhow::anyhow!(\"Cycle detected in dependency graph\"));\n    }\n\n    Ok(ordered)\n}\n\npub fn ordered_by_name(\n    files: &[PathBuf],\n    node_map: &HashMap<PathBuf, NodeIndex>,\n) -> Vec<NodeIndex> {\n    let mut sorted = files.to_vec();\n    sorted.sort();\n    sorted\n        .into_iter()\n        .filter_map(|path| node_map.get(&path).copied())\n        .collect()\n}\n\n/// Builds file ordering entries with canonical names and rename flags\npub fn build_entries(ordered: &[PathBuf], step: usize) -> Vec<crate::types::FileOrderEntry> {\n    ordered\n        .iter()\n        .enumerate()\n        .map(|(idx, path)| {\n            let canonical_order = idx * step;\n            let suggested_name =\n                crate::cluster_006::generate_canonical_name(path, canonical_order);\n            let needs_rename = path\n                .file_name()\n                .and_then(|n| n.to_str())\n                .map(|name| name != suggested_name)\n                .unwrap_or(false);\n            crate::types::FileOrderEntry {\n                current_path: path.clone(),\n                canonical_order,\n                suggested_name,\n                needs_rename,\n            }\n        })\n        .collect()\n}\n\npub fn analyze_file_ordering(\n    files: &[PathBuf],\n    step: Option<usize>,\n) -> Result<crate::types::FileOrderingResult> {\n    let step = step.unwrap_or(10);\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = crate::cluster_011::build_module_map(files);\n    let dep_map = crate::cluster_010::build_dependency_map(files, &file_set, &module_map)?;\n    let file_layers = build_file_layers(files);\n    let ordered_directories = crate::layer_core::order_directories(files, &dep_map);\n\n    let (graph, node_map) = crate::cluster_011::build_file_dag(files, &dep_map);\n    let layer_violations = crate::cluster_008::detect_layer_violations(&graph, &file_layers);\n    let cycles = detect_cycles(&graph, files);\n\n    let ordered_nodes = if cycles.is_empty() {\n        crate::layer_core::layer_constrained_sort(&graph, &file_layers).unwrap_or_else(|_| {\n            topological_sort(&graph).unwrap_or_else(|_| ordered_by_name(files, &node_map))\n        })\n    } else {\n        ordered_by_name(files, &node_map)\n    };\n\n    let ordered_files = ordered_nodes\n        .into_iter()\n        .map(|idx| graph[idx].clone())\n        .collect::<Vec<_>>();\n\n    let file_entries = build_entries(&ordered_files, step);\n    let violations = detect_violations(&file_entries, &dep_map);\n\n    Ok(crate::types::FileOrderingResult {\n        ordered_files: file_entries,\n        violations,\n        layer_violations,\n        ordered_directories,\n        cycles,\n    })\n}\n\npub fn naming_score_for_file(\n    file: &Path,\n    order_entry: Option<&crate::types::FileOrderEntry>,\n) -> Option<f64> {\n    let name = file.file_name()?.to_string_lossy();\n    let stem = file.file_stem()?.to_string_lossy();\n    let mut score = 1.0f64;\n\n    if stem.len() < 3 {\n        score -= 0.2;\n    }\n    if stem.len() > 40 {\n        score -= 0.1;\n    }\n    if stem.chars().any(|c| c.is_uppercase()) {\n        score -= 0.1;\n    }\n    if !stem\n        .chars()\n        .all(|c| c.is_ascii_lowercase() || c.is_ascii_digit() || c == '_')\n    {\n        score -= 0.1;\n    }\n    if name.contains(\"__\") {\n        score -= 0.1;\n    }\n\n    if let Some(entry) = order_entry {\n        let expected = entry.suggested_name.as_str();\n        let actual = name.as_ref();\n        if expected != actual {\n            score -= 0.3;\n        } else {\n            score += 0.1;\n        }\n    }\n\n    if let Ok(contents) = fs::read_to_string(file) {\n        let mut ident_counts: HashMap<String, usize> = HashMap::new();\n        let ident_re = match Regex::new(r\"[A-Za-z_][A-Za-z0-9_]*\") {\n            Ok(regex) => regex,\n            Err(_) => return None,\n        };\n        for cap in ident_re.captures_iter(&contents) {\n            let Some(m) = cap.get(0) else {\n                continue;\n            };\n            let ident = m.as_str().to_lowercase();\n            if matches!(\n                ident.as_str(),\n                \"fn\"\n                    | \"pub\"\n                    | \"use\"\n                    | \"struct\"\n                    | \"enum\"\n                    | \"impl\"\n                    | \"mod\"\n                    | \"let\"\n                    | \"mut\"\n                    | \"ref\"\n                    | \"self\"\n                    | \"crate\"\n                    | \"super\"\n                    | \"where\"\n                    | \"trait\"\n                    | \"type\"\n                    | \"const\"\n                    | \"static\"\n                    | \"match\"\n                    | \"if\"\n                    | \"else\"\n                    | \"for\"\n                    | \"while\"\n                    | \"loop\"\n                    | \"return\"\n                    | \"async\"\n                    | \"await\"\n                    | \"move\"\n                    | \"dyn\"\n                    | \"as\"\n            ) {\n                continue;\n            }\n            *ident_counts.entry(ident).or_insert(0) += 1;\n        }\n\n        let mut idents = ident_counts.into_iter().collect::<Vec<_>>();\n        idents.sort_by(|a, b| b.1.cmp(&a.1));\n        let top_idents = idents.into_iter().take(8).map(|(k, _)| k).collect::<Vec<_>>();\n        let name_tokens = stem\n            .split('_')\n            .map(|s| s.to_lowercase())\n            .filter(|s| !s.is_empty() && !s.chars().all(|c| c.is_ascii_digit()))\n            .collect::<Vec<_>>();\n        let overlap = top_idents\n            .iter()\n            .filter(|ident| name_tokens.iter().any(|t| t == *ident))\n            .count();\n\n        if overlap == 0 {\n            score -= 0.1;\n        } else if overlap >= 2 {\n            score += 0.1;\n        }\n    }\n\n    if score < 0.0 {\n        score = 0.0;\n    }\n    if score > 1.0 {\n        score = 1.0;\n    }\n    Some(score * 100.0)\n}\n\npub(crate) fn detect_cycles(\n    graph: &DiGraph<PathBuf, ()>,\n    files: &[PathBuf],\n) -> Vec<Vec<PathBuf>> {\n    let sccs = tarjan_scc(graph);\n    let mut cycles = Vec::new();\n    for scc in sccs {\n        if scc.len() > 1 {\n            cycles.push(scc.into_iter().map(|idx| graph[idx].clone()).collect());\n        }\n    }\n    if cycles.is_empty() {\n        return cycles;\n    }\n    if cycles.iter().all(|cycle| cycle.is_empty()) {\n        let mut fallback = files.to_vec();\n        fallback.sort();\n        cycles.push(fallback);\n    }\n    cycles\n}\n\npub(crate) fn detect_violations(\n    ordered_files: &[crate::types::FileOrderEntry],\n    dep_map: &HashMap<PathBuf, Vec<PathBuf>>,\n) -> Vec<crate::types::OrderViolation> {\n    let mut alpha = ordered_files.to_vec();\n    alpha.sort_by(|a, b| a.current_path.cmp(&b.current_path));\n    let alpha_positions: HashMap<PathBuf, usize> = alpha\n        .iter()\n        .enumerate()\n        .map(|(idx, entry)| (entry.current_path.clone(), idx))\n        .collect();\n\n    let canonical_positions: HashMap<PathBuf, usize> = ordered_files\n        .iter()\n        .enumerate()\n        .map(|(idx, entry)| (entry.current_path.clone(), idx))\n        .collect();\n\n    let mut violations = Vec::new();\n    for entry in ordered_files {\n        let Some(&alpha_pos) = alpha_positions.get(&entry.current_path) else {\n            continue;\n        };\n        let Some(&required_pos) = canonical_positions.get(&entry.current_path) else {\n            continue;\n        };\n        if alpha_pos != required_pos {\n            let blocking_dependencies = dep_map\n                .get(&entry.current_path)\n                .map(|deps| {\n                    deps.iter()\n                        .filter(|dep| {\n                            let dep_alpha = alpha_positions.get(*dep).copied().unwrap_or(0);\n                            dep_alpha > alpha_pos\n                        })\n                        .cloned()\n                        .collect::<Vec<_>>()\n                })\n                .unwrap_or_default();\n            violations.push(crate::types::OrderViolation {\n                file: entry.current_path.clone(),\n                current_position: alpha_pos,\n                required_position: required_pos,\n                blocking_dependencies,\n            });\n        }\n    }\n\n    violations\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\n/// Exports a complete program CFG to DOT format\npub fn export_complete_program_dot(\n    program: &crate::types::ProgramCFG,\n    path: &str,\n) -> std::io::Result<()> {\n    use std::collections::HashMap;\n    use std::fmt::Write;\n\n    fn escape_dot(s: &str) -> String {\n        s.replace('\\\\', \"\\\\\\\\\").replace('\"', \"\\\\\\\"\").replace('\\n', \"\\\\n\")\n    }\n\n    let mut dot = String::new();\n\n    writeln!(dot, \"digraph ProgramCFG {{\").unwrap();\n    writeln!(dot, \"  rankdir=TB;\").unwrap();\n    writeln!(dot, \"  compound=true;\").unwrap();\n    writeln!(dot, \"  newrank=true;\").unwrap();\n    writeln!(\n        dot,\n        \"  label=\\\"Complete Program CFG - {} functions\\\";\",\n        program.functions.len()\n    )\n    .unwrap();\n    writeln!(dot, \"  labelloc=t;\").unwrap();\n    writeln!(dot, \"  fontsize=16;\").unwrap();\n    writeln!(dot, \"\").unwrap();\n\n    let mut funcs: Vec<_> = program.functions.iter().collect();\n    funcs.sort_by_key(|(fid, _)| fid.as_str());\n\n    let mut func_to_cluster: HashMap<&String, usize> = HashMap::new();\n\n    for (cluster_idx, (func_id, cfg)) in funcs.iter().enumerate() {\n        let safe_name = func_id.replace(['!', '?', '*'], \"_\");\n        let cc = crate::cluster_008::cyclomatic_complexity(cfg);\n        func_to_cluster.insert(func_id, cluster_idx);\n\n        writeln!(dot, \"  subgraph cluster_{} {{\", cluster_idx).unwrap();\n        writeln!(dot, \"    label=\\\"{} (CC={})\\\";\", safe_name, cc).unwrap();\n        writeln!(dot, \"    style=filled;\").unwrap();\n        writeln!(dot, \"    fillcolor=lightgray;\").unwrap();\n        writeln!(dot, \"    color=black;\").unwrap();\n        writeln!(dot, \"\").unwrap();\n\n        for node in &cfg.nodes {\n            let (shape, color, style) = crate::cluster_008::node_style(&node.node_type);\n\n            let mut label = node.label.clone();\n            if !node.lines.is_empty() {\n                let lines_str: String = node\n                    .lines\n                    .iter()\n                    .map(|l| l.to_string())\n                    .collect::<Vec<_>>()\n                    .join(\",\");\n                label = format!(\"{} L{}\", label, lines_str);\n            }\n\n            let url = format!(\"http://127.0.0.1:8081/run?f={}\", func_id);\n\n            writeln!(\n                dot,\n                \"    f{}_n{} [label=\\\"{}\\\", shape={}, fillcolor={}, style={}, URL=\\\"{}\\\"];\",\n                cluster_idx,\n                node.id,\n                escape_dot(&label),\n                shape,\n                color,\n                style,\n                url\n            )\n            .unwrap();\n        }\n\n        writeln!(dot, \"\").unwrap();\n\n        for edge in &cfg.edges {\n            let mut attrs = Vec::new();\n            if let Some(cond) = edge.condition {\n                let label = if cond { \"T\" } else { \"F\" };\n                let color = if cond { \"darkgreen\" } else { \"red\" };\n                attrs.push(format!(\"label=\\\"{}\\\"\", label));\n                attrs.push(format!(\"color=\\\"{}\\\"\", color));\n            }\n            let attr_str = if attrs.is_empty() {\n                \"\".to_string()\n            } else {\n                format!(\" [{}]\", attrs.join(\", \"))\n            };\n\n            writeln!(\n                dot,\n                \"    f{}_n{} -> f{}_n{}{};\",\n                cluster_idx,\n                edge.from,\n                cluster_idx,\n                edge.to,\n                attr_str\n            )\n            .unwrap();\n        }\n\n        writeln!(dot, \"  }}\").unwrap();\n        writeln!(dot, \"\").unwrap();\n    }\n\n    writeln!(dot, \"  // Inter-function calls\").unwrap();\n    writeln!(dot, \"  edge [style=dashed, color=blue, penwidth=2];\").unwrap();\n    writeln!(dot, \"\").unwrap();\n\n    for (caller, callee) in &program.call_edges {\n        if let (Some(&caller_idx), Some(&callee_idx)) =\n            (func_to_cluster.get(caller), func_to_cluster.get(callee))\n        {\n            if let (Some(caller_cfg), Some(callee_cfg)) =\n                (program.functions.get(caller), program.functions.get(callee))\n            {\n                writeln!(\n                    dot,\n                    \"  f{}_n{} -> f{}_n{} [ltail=cluster_{}, lhead=cluster_{}, label=\\\"call\\\"];\",\n                    caller_idx,\n                    caller_cfg.exit_id,\n                    callee_idx,\n                    callee_cfg.entry_id,\n                    caller_idx,\n                    callee_idx\n                )\n                .unwrap();\n            }\n        }\n    }\n\n    writeln!(dot, \"}}\").unwrap();\n\n    std::fs::write(path, dot)?;\n    println!(\"Complete program CFG exported to {}\", path);\n    Ok(())\n}\n\n// ============================================================================\n// Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_detects_cycles() {\n        detects_cycles().unwrap();\n    }\n\n    #[test]\n    fn test_generates_canonical_names_and_violations() {\n        generates_canonical_names_and_violations().unwrap();\n    }\n}\n\npub fn run_analysis(\n    root_path: &Path,\n    output_path: &Path,\n    verbose: bool,\n    skip_julia: bool,\n    dead_code: bool,\n    dead_code_filter: bool,\n    dead_code_json: Option<PathBuf>,\n    dead_code_summary: Option<PathBuf>,\n    dead_code_summary_limit: usize,\n    dead_code_policy: Option<PathBuf>,\n    correction_intelligence: bool,\n    correction_json: Option<PathBuf>,\n    verification_policy_json: Option<PathBuf>,\n) -> Result<()> {\n    use crate::control_flow::ControlFlowAnalyzer;\n    use crate::cohesion_analyzer::FunctionCohesionAnalyzer;\n    use crate::dependency::LayerGraph;\n    use crate::directory_analyzer::DirectoryAnalyzer;\n    use crate::dot_exporter::export_program_cfg_to_path;\n    use crate::julia_parser::JuliaAnalyzer;\n    use crate::report::ReportGenerator;\n    use crate::rust_parser::RustAnalyzer;\n    use crate::types::{AnalysisResult, FileOrderingResult};\n\n    let julia_script_path = root_path.join(\"src/000_main.jl\");\n\n    println!(\"MMSB Intelligence Substrate Analyzer\");\n    println!(\"=====================================\\n\");\n    println!(\"Root directory: {:?}\", root_path);\n    println!(\"Output directory: {:?}\", output_path);\n    println!(\"Julia script: {:?}\\n\", julia_script_path);\n\n    let rust_analyzer = RustAnalyzer::new(root_path.to_string_lossy().to_string());\n    let mut combined_result = AnalysisResult::new();\n\n    println!(\"Scanning Rust files (dependency-ordered)...\");\n    let mut rust_count = 0;\n    let rust_files = gather_rust_files(root_path);\n    let (ordered_rust_files, rust_layer_graph) =\n        crate::dependency::order_rust_files_by_dependency(&rust_files, root_path)\n            .context(\"Failed to resolve Rust dependency order\")?;\n    let rust_file_ordering =\n        crate::dependency::analyze_file_ordering(&rust_files, None)\n            .context(\"Failed to analyze Rust file ordering\")?;\n    let julia_file_ordering = FileOrderingResult {\n        ordered_files: Vec::new(),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles: Vec::new(),\n    };\n\n    for path in ordered_rust_files {\n        if verbose {\n            println!(\"  Analyzing: {:?}\", path);\n        }\n\n        match rust_analyzer.analyze_file(&path) {\n            Ok(result) => {\n                rust_count += 1;\n                combined_result.merge(result);\n            }\n            Err(e) => {\n                eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n            }\n        }\n    }\n\n    println!(\"  Analyzed {} Rust files\\n\", rust_count);\n\n    let mut julia_count = 0;\n    let mut julia_layer_graph = LayerGraph {\n        ordered_layers: Vec::new(),\n        edges: Vec::new(),\n        cycles: Vec::new(),\n        unresolved: Vec::new(),\n    };\n    if !skip_julia {\n        println!(\"Scanning Julia files (dependency-ordered)...\");\n        let julia_files = gather_julia_files(root_path);\n        let (ordered_julia_files, jlg) =\n            crate::dependency::order_julia_files_by_dependency(&julia_files, root_path)\n                .context(\"Failed to resolve Julia dependency order\")?;\n        julia_layer_graph = jlg;\n\n        if julia_script_path.exists() {\n            let julia_analyzer = JuliaAnalyzer::new(\n                root_path.to_path_buf(),\n                julia_script_path.clone(),\n                output_path.join(\"30_cfg/dots\"),\n            );\n\n            for path in ordered_julia_files {\n                if verbose {\n                    println!(\"  Analyzing: {:?}\", path);\n                }\n\n                match julia_analyzer.analyze_file(&path) {\n                    Ok(result) => {\n                        julia_count += 1;\n                        combined_result.merge(result);\n                    }\n                    Err(e) => {\n                        eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n                    }\n                }\n            }\n        } else {\n            println!(\"  Skipping Julia analysis (script not found)\");\n        }\n\n        println!(\"  Analyzed {} Julia files\\n\", julia_count);\n    }\n\n    if dead_code || dead_code_filter || dead_code_json.is_some() || dead_code_summary.is_some() {\n        let policy = if let Some(policy_path) = dead_code_policy {\n            Some(\n                crate::dead_code_policy::load_policy(&policy_path)\n                    .context(\"Failed to load dead code policy\")?,\n            )\n        } else {\n            None\n        };\n        let config = crate::dead_code_cli::DeadCodeRunConfig {\n            root: root_path.to_path_buf(),\n            output_dir: output_path.to_path_buf(),\n            policy,\n            write_json: dead_code_json,\n            write_summary: dead_code_summary,\n            summary_limit: dead_code_summary_limit,\n        };\n        let report = crate::dead_code_cli::run_dead_code_pipeline(&combined_result.elements, &config)\n            .context(\"Dead code analysis failed\")?;\n        if dead_code_filter {\n            combined_result.elements =\n                crate::dead_code_filter::filter_dead_code_elements(&combined_result.elements, &report);\n        }\n    }\n\n    println!(\"Building call graph...\");\n    let mut cf_analyzer = ControlFlowAnalyzer::new();\n    cf_analyzer.build_call_graph(&combined_result);\n\n    // NEW: Invariant detection\n    use crate::invariant_integrator::InvariantDetector;\n    println!(\"Detecting invariants...\");\n    let invariants_result = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.detect_all()\n    };\n    let constraints = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.generate_constraints(&invariants_result)\n    };\n    combined_result.invariants = invariants_result;\n    combined_result.constraints = constraints;\n\n    println!(\"Analyzing function cohesion...\");\n    let cohesion_analyzer = FunctionCohesionAnalyzer::new();\n    let placements = cohesion_analyzer.analyze(&combined_result)?;\n    let clusters = cohesion_analyzer.detect_clusters(&combined_result)?;\n\n    println!(\"Analyzing directory structure...\");\n    let dir_analyzer = DirectoryAnalyzer::new(root_path.to_path_buf());\n    let dir_analysis = dir_analyzer.analyze()?;\n\n    println!(\"\\nGenerating reports...\");\n    let report_gen = ReportGenerator::new(output_path.to_string_lossy().to_string());\n    report_gen.generate_all(\n        &combined_result,\n        &cf_analyzer,\n        &rust_layer_graph,\n        &julia_layer_graph,\n        &rust_file_ordering,\n        &julia_file_ordering,\n        &placements,\n        &clusters,\n        &dir_analysis,\n        root_path,\n        correction_intelligence,\n        correction_json,\n        verification_policy_json,\n    )\n    .context(\"Failed to generate reports\")?;\n\n    println!(\"\\nExporting program CFG...\");\n    export_program_cfg_to_path(&combined_result, &cf_analyzer.call_edges(), output_path)?;\n\n    println!(\"\\nGenerating invariant report...\");\n    use crate::invariant_reporter;\n    invariant_reporter::generate_invariant_report(&combined_result.invariants, output_path)\n        .context(\"Failed to generate invariant report\")?;\n    invariant_reporter::export_constraints_json(&combined_result.constraints, output_path)\n        .context(\"Failed to export constraints\")?;\n\n    println!(\"\\n Analysis complete!\");\n    println!(\"  Total elements: {}\", combined_result.elements.len());\n    println!(\"  Rust files: {}\", rust_count);\n    println!(\"  Julia files: {}\", julia_count);\n    println!(\"  Output: {}\\n\", output_path.display());\n\n    Ok(())\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_parse_mmsb_latent_attr_to_src/214_dead_code_intent.rs",
      "mutations": [],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_scan_doc_comments_to_src/211_dead_code_attribute_parser.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/212_dead_code_doc_comment_parser.rs",
          "original_content": "#![allow(dead_code)]\n//! Doc comment parsing for intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMap};\nuse std::collections::{HashMap, HashSet};\nuse std::path::Path;\nuse syn::{Attribute, Item, Meta, MetaNameValue};\n\npub fn scan_doc_comments(file: &Path) -> HashMap<String, Vec<IntentMarker>> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMarker>> = HashMap::new();\n    for item in &parsed.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        let markers = extract_doc_markers(item_attrs(item));\n        if markers.is_empty() {\n            continue;\n        }\n        map.entry(symbol).or_default().extend(markers);\n    }\n    map\n}\n\npub fn detect_latent_markers(comment: &str) -> Option<IntentMarker> {\n    IntentMarker::from_comment(comment)\n}\n\npub fn merge_doc_intent(map: HashMap<String, Vec<IntentMarker>>) -> IntentMap {\n    let mut merged = IntentMap::new();\n    for (symbol, markers) in map {\n        let mut uniques = HashSet::new();\n        for marker in markers {\n            if !uniques.insert(marker) {\n                continue;\n            }\n            merged\n                .entry(symbol.clone())\n                .or_default()\n                .push(crate::dead_code_types::IntentMetadata {\n                    marker,\n                    source: crate::dead_code_types::IntentSource::DocComment,\n                    value: None,\n                });\n        }\n    }\n    merged\n}\n\nfn extract_doc_markers(attrs: &[Attribute]) -> Vec<IntentMarker> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"doc\") {\n            continue;\n        }\n        let Meta::NameValue(MetaNameValue { value, .. }) = &attr.meta else {\n            continue;\n        };\n        let syn::Expr::Lit(expr_lit) = value else {\n            continue;\n        };\n        let syn::Lit::Str(lit) = &expr_lit.lit else {\n            continue;\n        };\n        if let Some(marker) = detect_latent_markers(&lit.value()) {\n            markers.push(marker);\n        }\n    }\n    markers\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Doc comment parsing for intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMap};\nuse std::collections::{HashMap, HashSet};\nuse std::path::Path;\nuse syn::{Attribute, Item, Meta, MetaNameValue};\n\n\n\npub fn detect_latent_markers(comment: &str) -> Option<IntentMarker> {\n    IntentMarker::from_comment(comment)\n}\n\npub fn merge_doc_intent(map: HashMap<String, Vec<IntentMarker>>) -> IntentMap {\n    let mut merged = IntentMap::new();\n    for (symbol, markers) in map {\n        let mut uniques = HashSet::new();\n        for marker in markers {\n            if !uniques.insert(marker) {\n                continue;\n            }\n            merged\n                .entry(symbol.clone())\n                .or_default()\n                .push(crate::dead_code_types::IntentMetadata {\n                    marker,\n                    source: crate::dead_code_types::IntentSource::DocComment,\n                    value: None,\n                });\n        }\n    }\n    merged\n}\n\nfn extract_doc_markers(attrs: &[Attribute]) -> Vec<IntentMarker> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"doc\") {\n            continue;\n        }\n        let Meta::NameValue(MetaNameValue { value, .. }) = &attr.meta else {\n            continue;\n        };\n        let syn::Expr::Lit(expr_lit) = value else {\n            continue;\n        };\n        let syn::Lit::Str(lit) = &expr_lit.lit else {\n            continue;\n        };\n        if let Some(marker) = detect_latent_markers(&lit.value()) {\n            markers.push(marker);\n        }\n    }\n    markers\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/211_dead_code_attribute_parser.rs",
          "original_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n\npub fn scan_doc_comments(file: &Path) -> HashMap<String, Vec<IntentMarker>> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMarker>> = HashMap::new();\n    for item in &parsed.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        let markers = extract_doc_markers(item_attrs(item));\n        if markers.is_empty() {\n            continue;\n        }\n        map.entry(symbol).or_default().extend(markers);\n    }\n    map\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_detect_intent_signals_to_src/211_dead_code_attribute_parser.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/214_dead_code_intent.rs",
          "original_content": "#![allow(dead_code)]\n//! Intent detection aggregation for dead code classification.\n\nuse crate::dead_code_attribute_parser::parse_mmsb_latent_attr;\nuse crate::dead_code_doc_comment_parser::{merge_doc_intent, scan_doc_comments};\nuse crate::dead_code_types::{\n    IntentMap, IntentMarker, IntentMetadata, IntentSource, IntentTag,\n};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\nuse syn::Item;\n\n#[derive(Debug, Clone, Default)]\npub struct DeadCodePolicy {\n    pub planned_directories: Vec<PathBuf>,\n    pub public_api_roots: Vec<PathBuf>,\n    pub entrypoint_symbols: Vec<String>,\n    pub treat_public_as_entrypoint: bool,\n}\n\npub fn detect_intent_signals(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    let attrs = parse_mmsb_latent_attr(file);\n    let doc_map = scan_doc_comments(file);\n    let docs = merge_doc_intent(doc_map);\n    let dir_map = planned_directory_intent(file, policy);\n    merge_intent_sources(attrs, docs, dir_map)\n}\n\npub fn check_planned_directory(path: &Path, policy: Option<&DeadCodePolicy>) -> bool {\n    let Some(policy) = policy else {\n        return false;\n    };\n    for dir in &policy.planned_directories {\n        if path.starts_with(dir) {\n            return true;\n        }\n    }\n    false\n}\n\npub fn merge_intent_sources(\n    attrs: IntentMap,\n    docs: IntentMap,\n    dir: IntentMap,\n) -> IntentMap {\n    let mut merged = IntentMap::new();\n    for (symbol, items) in attrs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in docs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in dir {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    merged\n}\n\npub fn scan_intent_tags(file: &Path, policy: Option<&DeadCodePolicy>) -> Vec<IntentTag> {\n    let mut tags = Vec::new();\n    let attrs = parse_mmsb_latent_attr(file);\n    for (symbol, items) in attrs {\n        for meta in items {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n\n    let doc_map = scan_doc_comments(file);\n    for (symbol, markers) in doc_map {\n        for marker in markers {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker,\n                source: IntentSource::DocComment,\n                value: None,\n            });\n        }\n    }\n\n    if check_planned_directory(file, policy) {\n        for symbol in collect_symbols(file) {\n            tags.push(IntentTag {\n                symbol,\n                file: file.to_path_buf(),\n                line: None,\n                marker: IntentMarker::Planned,\n                source: IntentSource::Directory,\n                value: None,\n            });\n        }\n    }\n\n    tags\n}\n\nfn planned_directory_intent(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    if !check_planned_directory(file, policy) {\n        return IntentMap::new();\n    }\n    let mut map: IntentMap = HashMap::new();\n    for symbol in collect_symbols(file) {\n        map.entry(symbol).or_default().push(IntentMetadata {\n            marker: IntentMarker::Planned,\n            source: IntentSource::Directory,\n            value: None,\n        });\n    }\n    map\n}\n\nfn collect_symbols(file: &Path) -> Vec<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    parsed\n        .items\n        .iter()\n        .filter_map(item_name)\n        .collect()\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Intent detection aggregation for dead code classification.\n\nuse crate::dead_code_attribute_parser::parse_mmsb_latent_attr;\nuse crate::dead_code_doc_comment_parser::{merge_doc_intent, scan_doc_comments};\nuse crate::dead_code_types::{\n    IntentMap, IntentMarker, IntentMetadata, IntentSource, IntentTag,\n};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\nuse syn::Item;\n\n#[derive(Debug, Clone, Default)]\npub struct DeadCodePolicy {\n    pub planned_directories: Vec<PathBuf>,\n    pub public_api_roots: Vec<PathBuf>,\n    pub entrypoint_symbols: Vec<String>,\n    pub treat_public_as_entrypoint: bool,\n}\n\n\n\npub fn check_planned_directory(path: &Path, policy: Option<&DeadCodePolicy>) -> bool {\n    let Some(policy) = policy else {\n        return false;\n    };\n    for dir in &policy.planned_directories {\n        if path.starts_with(dir) {\n            return true;\n        }\n    }\n    false\n}\n\npub fn merge_intent_sources(\n    attrs: IntentMap,\n    docs: IntentMap,\n    dir: IntentMap,\n) -> IntentMap {\n    let mut merged = IntentMap::new();\n    for (symbol, items) in attrs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in docs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in dir {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    merged\n}\n\npub fn scan_intent_tags(file: &Path, policy: Option<&DeadCodePolicy>) -> Vec<IntentTag> {\n    let mut tags = Vec::new();\n    let attrs = parse_mmsb_latent_attr(file);\n    for (symbol, items) in attrs {\n        for meta in items {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n\n    let doc_map = scan_doc_comments(file);\n    for (symbol, markers) in doc_map {\n        for marker in markers {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker,\n                source: IntentSource::DocComment,\n                value: None,\n            });\n        }\n    }\n\n    if check_planned_directory(file, policy) {\n        for symbol in collect_symbols(file) {\n            tags.push(IntentTag {\n                symbol,\n                file: file.to_path_buf(),\n                line: None,\n                marker: IntentMarker::Planned,\n                source: IntentSource::Directory,\n                value: None,\n            });\n        }\n    }\n\n    tags\n}\n\nfn planned_directory_intent(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    if !check_planned_directory(file, policy) {\n        return IntentMap::new();\n    }\n    let mut map: IntentMap = HashMap::new();\n    for symbol in collect_symbols(file) {\n        map.entry(symbol).or_default().push(IntentMetadata {\n            marker: IntentMarker::Planned,\n            source: IntentSource::Directory,\n            value: None,\n        });\n    }\n    map\n}\n\nfn collect_symbols(file: &Path) -> Vec<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    parsed\n        .items\n        .iter()\n        .filter_map(item_name)\n        .collect()\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/211_dead_code_attribute_parser.rs",
          "original_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n\npub fn detect_intent_signals(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    let attrs = parse_mmsb_latent_attr(file);\n    let doc_map = scan_doc_comments(file);\n    let docs = merge_doc_intent(doc_map);\n    let dir_map = planned_directory_intent(file, policy);\n    merge_intent_sources(attrs, docs, dir_map)\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_scan_intent_tags_to_src/211_dead_code_attribute_parser.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/214_dead_code_intent.rs",
          "original_content": "#![allow(dead_code)]\n//! Intent detection aggregation for dead code classification.\n\nuse crate::dead_code_attribute_parser::parse_mmsb_latent_attr;\nuse crate::dead_code_doc_comment_parser::{merge_doc_intent, scan_doc_comments};\nuse crate::dead_code_types::{\n    IntentMap, IntentMarker, IntentMetadata, IntentSource, IntentTag,\n};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\nuse syn::Item;\n\n#[derive(Debug, Clone, Default)]\npub struct DeadCodePolicy {\n    pub planned_directories: Vec<PathBuf>,\n    pub public_api_roots: Vec<PathBuf>,\n    pub entrypoint_symbols: Vec<String>,\n    pub treat_public_as_entrypoint: bool,\n}\n\npub fn detect_intent_signals(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    let attrs = parse_mmsb_latent_attr(file);\n    let doc_map = scan_doc_comments(file);\n    let docs = merge_doc_intent(doc_map);\n    let dir_map = planned_directory_intent(file, policy);\n    merge_intent_sources(attrs, docs, dir_map)\n}\n\npub fn check_planned_directory(path: &Path, policy: Option<&DeadCodePolicy>) -> bool {\n    let Some(policy) = policy else {\n        return false;\n    };\n    for dir in &policy.planned_directories {\n        if path.starts_with(dir) {\n            return true;\n        }\n    }\n    false\n}\n\npub fn merge_intent_sources(\n    attrs: IntentMap,\n    docs: IntentMap,\n    dir: IntentMap,\n) -> IntentMap {\n    let mut merged = IntentMap::new();\n    for (symbol, items) in attrs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in docs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in dir {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    merged\n}\n\npub fn scan_intent_tags(file: &Path, policy: Option<&DeadCodePolicy>) -> Vec<IntentTag> {\n    let mut tags = Vec::new();\n    let attrs = parse_mmsb_latent_attr(file);\n    for (symbol, items) in attrs {\n        for meta in items {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n\n    let doc_map = scan_doc_comments(file);\n    for (symbol, markers) in doc_map {\n        for marker in markers {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker,\n                source: IntentSource::DocComment,\n                value: None,\n            });\n        }\n    }\n\n    if check_planned_directory(file, policy) {\n        for symbol in collect_symbols(file) {\n            tags.push(IntentTag {\n                symbol,\n                file: file.to_path_buf(),\n                line: None,\n                marker: IntentMarker::Planned,\n                source: IntentSource::Directory,\n                value: None,\n            });\n        }\n    }\n\n    tags\n}\n\nfn planned_directory_intent(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    if !check_planned_directory(file, policy) {\n        return IntentMap::new();\n    }\n    let mut map: IntentMap = HashMap::new();\n    for symbol in collect_symbols(file) {\n        map.entry(symbol).or_default().push(IntentMetadata {\n            marker: IntentMarker::Planned,\n            source: IntentSource::Directory,\n            value: None,\n        });\n    }\n    map\n}\n\nfn collect_symbols(file: &Path) -> Vec<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    parsed\n        .items\n        .iter()\n        .filter_map(item_name)\n        .collect()\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Intent detection aggregation for dead code classification.\n\nuse crate::dead_code_attribute_parser::parse_mmsb_latent_attr;\nuse crate::dead_code_doc_comment_parser::{merge_doc_intent, scan_doc_comments};\nuse crate::dead_code_types::{\n    IntentMap, IntentMarker, IntentMetadata, IntentSource, IntentTag,\n};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\nuse syn::Item;\n\n#[derive(Debug, Clone, Default)]\npub struct DeadCodePolicy {\n    pub planned_directories: Vec<PathBuf>,\n    pub public_api_roots: Vec<PathBuf>,\n    pub entrypoint_symbols: Vec<String>,\n    pub treat_public_as_entrypoint: bool,\n}\n\npub fn detect_intent_signals(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    let attrs = parse_mmsb_latent_attr(file);\n    let doc_map = scan_doc_comments(file);\n    let docs = merge_doc_intent(doc_map);\n    let dir_map = planned_directory_intent(file, policy);\n    merge_intent_sources(attrs, docs, dir_map)\n}\n\npub fn check_planned_directory(path: &Path, policy: Option<&DeadCodePolicy>) -> bool {\n    let Some(policy) = policy else {\n        return false;\n    };\n    for dir in &policy.planned_directories {\n        if path.starts_with(dir) {\n            return true;\n        }\n    }\n    false\n}\n\npub fn merge_intent_sources(\n    attrs: IntentMap,\n    docs: IntentMap,\n    dir: IntentMap,\n) -> IntentMap {\n    let mut merged = IntentMap::new();\n    for (symbol, items) in attrs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in docs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in dir {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    merged\n}\n\n\n\nfn planned_directory_intent(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    if !check_planned_directory(file, policy) {\n        return IntentMap::new();\n    }\n    let mut map: IntentMap = HashMap::new();\n    for symbol in collect_symbols(file) {\n        map.entry(symbol).or_default().push(IntentMetadata {\n            marker: IntentMarker::Planned,\n            source: IntentSource::Directory,\n            value: None,\n        });\n    }\n    map\n}\n\nfn collect_symbols(file: &Path) -> Vec<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    parsed\n        .items\n        .iter()\n        .filter_map(item_name)\n        .collect()\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/211_dead_code_attribute_parser.rs",
          "original_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n\npub fn scan_intent_tags(file: &Path, policy: Option<&DeadCodePolicy>) -> Vec<IntentTag> {\n    let mut tags = Vec::new();\n    let attrs = parse_mmsb_latent_attr(file);\n    for (symbol, items) in attrs {\n        for meta in items {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n\n    let doc_map = scan_doc_comments(file);\n    for (symbol, markers) in doc_map {\n        for marker in markers {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker,\n                source: IntentSource::DocComment,\n                value: None,\n            });\n        }\n    }\n\n    if check_planned_directory(file, policy) {\n        for symbol in collect_symbols(file) {\n            tags.push(IntentTag {\n                symbol,\n                file: file.to_path_buf(),\n                line: None,\n                marker: IntentMarker::Planned,\n                source: IntentSource::Directory,\n                value: None,\n            });\n        }\n    }\n\n    tags\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_is_cfg_test_item_to_src/211_dead_code_attribute_parser.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/215_dead_code_test_boundaries.rs",
          "original_content": "#![allow(dead_code)]\n//! Test boundary detection for dead code classification.\n\nuse crate::dead_code_call_graph::{build_reverse_call_graph, CallGraph};\nuse std::collections::{HashSet, VecDeque};\nuse std::path::{Path, PathBuf};\nuse syn::{Attribute, Item};\n\n#[derive(Debug, Clone, Default)]\npub struct TestBoundaries {\n    pub test_modules: HashSet<String>,\n    pub test_symbols: HashSet<String>,\n    pub test_files: HashSet<PathBuf>,\n}\n\npub fn detect_test_modules(file: &Path) -> HashSet<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashSet::new(),\n    };\n    let mut modules = HashSet::new();\n    for item in &parsed.items {\n        if let Item::Mod(item_mod) = item {\n            if is_cfg_test_item(item) {\n                modules.insert(item_mod.ident.to_string());\n            }\n        }\n    }\n    modules\n}\n\npub fn detect_test_symbols(file: &Path) -> HashSet<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashSet::new(),\n    };\n    let mut symbols = HashSet::new();\n    for item in &parsed.items {\n        if let Item::Fn(item_fn) = item {\n            if has_test_attr(&item_fn.attrs) {\n                symbols.insert(item_fn.sig.ident.to_string());\n            }\n        }\n        if let Item::Mod(item_mod) = item {\n            if is_cfg_test_item(item) {\n                symbols.insert(item_mod.ident.to_string());\n                if let Some((_, items)) = &item_mod.content {\n                    for nested in items {\n                        if let Item::Fn(nested_fn) = nested {\n                            symbols.insert(nested_fn.sig.ident.to_string());\n                        }\n                    }\n                }\n            }\n        }\n    }\n    symbols\n}\n\npub fn is_cfg_test_item(item: &Item) -> bool {\n    item_attrs(item).iter().any(|attr| {\n        if !attr.path().is_ident(\"cfg\") {\n            return false;\n        }\n        let mut found = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            if meta.path.is_ident(\"test\") {\n                found = true;\n                return Ok(());\n            }\n            if meta.path.is_ident(\"any\") {\n                meta.parse_nested_meta(|nested| {\n                    if nested.path.is_ident(\"test\") {\n                        found = true;\n                    }\n                    Ok(())\n                })?;\n            }\n            Ok(())\n        });\n        found\n    })\n}\n\npub fn find_test_callers(\n    symbol: &str,\n    call_graph: &CallGraph,\n    test_symbols: &HashSet<String>,\n) -> Vec<String> {\n    if test_symbols.is_empty() {\n        return Vec::new();\n    }\n    let reverse = build_reverse_call_graph(call_graph);\n    let mut callers = Vec::new();\n    let mut visited = HashSet::new();\n    let mut queue: VecDeque<String> = reverse\n        .get(symbol)\n        .cloned()\n        .unwrap_or_default()\n        .into_iter()\n        .collect();\n\n    while let Some(caller) = queue.pop_front() {\n        if !visited.insert(caller.clone()) {\n            continue;\n        }\n        if test_symbols.contains(&caller) {\n            callers.push(caller.clone());\n        }\n        if let Some(next) = reverse.get(&caller) {\n            for parent in next {\n                if !visited.contains(parent) {\n                    queue.push_back(parent.clone());\n                }\n            }\n        }\n    }\n\n    callers\n}\n\nfn has_test_attr(attrs: &[Attribute]) -> bool {\n    attrs.iter().any(|attr| {\n        let path = attr.path();\n        if path.is_ident(\"test\") {\n            return true;\n        }\n        let last = path.segments.last().map(|seg| seg.ident.to_string());\n        matches!(last.as_deref(), Some(\"test\"))\n    })\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Test boundary detection for dead code classification.\n\nuse crate::dead_code_call_graph::{build_reverse_call_graph, CallGraph};\nuse std::collections::{HashSet, VecDeque};\nuse std::path::{Path, PathBuf};\nuse syn::{Attribute, Item};\n\n#[derive(Debug, Clone, Default)]\npub struct TestBoundaries {\n    pub test_modules: HashSet<String>,\n    pub test_symbols: HashSet<String>,\n    pub test_files: HashSet<PathBuf>,\n}\n\npub fn detect_test_modules(file: &Path) -> HashSet<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashSet::new(),\n    };\n    let mut modules = HashSet::new();\n    for item in &parsed.items {\n        if let Item::Mod(item_mod) = item {\n            if is_cfg_test_item(item) {\n                modules.insert(item_mod.ident.to_string());\n            }\n        }\n    }\n    modules\n}\n\npub fn detect_test_symbols(file: &Path) -> HashSet<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashSet::new(),\n    };\n    let mut symbols = HashSet::new();\n    for item in &parsed.items {\n        if let Item::Fn(item_fn) = item {\n            if has_test_attr(&item_fn.attrs) {\n                symbols.insert(item_fn.sig.ident.to_string());\n            }\n        }\n        if let Item::Mod(item_mod) = item {\n            if is_cfg_test_item(item) {\n                symbols.insert(item_mod.ident.to_string());\n                if let Some((_, items)) = &item_mod.content {\n                    for nested in items {\n                        if let Item::Fn(nested_fn) = nested {\n                            symbols.insert(nested_fn.sig.ident.to_string());\n                        }\n                    }\n                }\n            }\n        }\n    }\n    symbols\n}\n\n\n\npub fn find_test_callers(\n    symbol: &str,\n    call_graph: &CallGraph,\n    test_symbols: &HashSet<String>,\n) -> Vec<String> {\n    if test_symbols.is_empty() {\n        return Vec::new();\n    }\n    let reverse = build_reverse_call_graph(call_graph);\n    let mut callers = Vec::new();\n    let mut visited = HashSet::new();\n    let mut queue: VecDeque<String> = reverse\n        .get(symbol)\n        .cloned()\n        .unwrap_or_default()\n        .into_iter()\n        .collect();\n\n    while let Some(caller) = queue.pop_front() {\n        if !visited.insert(caller.clone()) {\n            continue;\n        }\n        if test_symbols.contains(&caller) {\n            callers.push(caller.clone());\n        }\n        if let Some(next) = reverse.get(&caller) {\n            for parent in next {\n                if !visited.contains(parent) {\n                    queue.push_back(parent.clone());\n                }\n            }\n        }\n    }\n\n    callers\n}\n\nfn has_test_attr(attrs: &[Attribute]) -> bool {\n    attrs.iter().any(|attr| {\n        let path = attr.path();\n        if path.is_ident(\"test\") {\n            return true;\n        }\n        let last = path.segments.last().map(|seg| seg.ident.to_string());\n        matches!(last.as_deref(), Some(\"test\"))\n    })\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/211_dead_code_attribute_parser.rs",
          "original_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n\npub fn is_cfg_test_item(item: &Item) -> bool {\n    item_attrs(item).iter().any(|attr| {\n        if !attr.path().is_ident(\"cfg\") {\n            return false;\n        }\n        let mut found = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            if meta.path.is_ident(\"test\") {\n                found = true;\n                return Ok(());\n            }\n            if meta.path.is_ident(\"any\") {\n                meta.parse_nested_meta(|nested| {\n                    if nested.path.is_ident(\"test\") {\n                        found = true;\n                    }\n                    Ok(())\n                })?;\n            }\n            Ok(())\n        });\n        found\n    })\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_classify_symbol_to_src/213_dead_code_call_graph.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/217_dead_code_classifier.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code category classification.\n\nuse crate::dead_code_call_graph::{build_reverse_call_graph, CallGraph};\nuse crate::dead_code_intent::DeadCodePolicy;\nuse crate::dead_code_test_boundaries::TestBoundaries;\nuse crate::dead_code_types::{DeadCodeCategory, IntentMap};\nuse std::collections::HashSet;\n\npub fn classify_symbol(\n    symbol: &str,\n    call_graph: &CallGraph,\n    intent_map: &IntentMap,\n    test_boundaries: &TestBoundaries,\n    entrypoints: &HashSet<String>,\n    _policy: Option<&DeadCodePolicy>,\n) -> DeadCodeCategory {\n    if intent_map.contains_key(symbol) {\n        return DeadCodeCategory::LatentPlanned;\n    }\n\n    if is_test_only(symbol, call_graph, test_boundaries) {\n        return DeadCodeCategory::TestOnly;\n    }\n\n    if !is_reachable(symbol, call_graph, entrypoints) {\n        return DeadCodeCategory::Unreachable;\n    }\n\n    DeadCodeCategory::ReachableUnused\n}\n\npub fn is_reachable(symbol: &str, call_graph: &CallGraph, entrypoints: &HashSet<String>) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    let reachable = crate::dead_code_call_graph::compute_reachability(call_graph, entrypoints);\n    reachable.contains(symbol)\n}\n\npub fn is_test_only(\n    symbol: &str,\n    call_graph: &CallGraph,\n    test_boundaries: &TestBoundaries,\n) -> bool {\n    if test_boundaries.test_symbols.contains(symbol) {\n        return true;\n    }\n    let reverse = build_reverse_call_graph(call_graph);\n    let callers = reverse.get(symbol);\n    let Some(callers) = callers else {\n        return false;\n    };\n    if callers.is_empty() {\n        return false;\n    }\n    callers\n        .iter()\n        .all(|caller| test_boundaries.test_symbols.contains(caller))\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Dead code category classification.\n\nuse crate::dead_code_call_graph::{build_reverse_call_graph, CallGraph};\nuse crate::dead_code_intent::DeadCodePolicy;\nuse crate::dead_code_test_boundaries::TestBoundaries;\nuse crate::dead_code_types::{DeadCodeCategory, IntentMap};\nuse std::collections::HashSet;\n\n\n\npub fn is_reachable(symbol: &str, call_graph: &CallGraph, entrypoints: &HashSet<String>) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    let reachable = crate::dead_code_call_graph::compute_reachability(call_graph, entrypoints);\n    reachable.contains(symbol)\n}\n\npub fn is_test_only(\n    symbol: &str,\n    call_graph: &CallGraph,\n    test_boundaries: &TestBoundaries,\n) -> bool {\n    if test_boundaries.test_symbols.contains(symbol) {\n        return true;\n    }\n    let reverse = build_reverse_call_graph(call_graph);\n    let callers = reverse.get(symbol);\n    let Some(callers) = callers else {\n        return false;\n    };\n    if callers.is_empty() {\n        return false;\n    }\n    callers\n        .iter()\n        .all(|caller| test_boundaries.test_symbols.contains(caller))\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/213_dead_code_call_graph.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code call graph utilities.\n\nuse crate::types::{CodeElement, ElementType, Language};\nuse std::collections::{HashMap, HashSet, VecDeque};\n\npub type CallGraph = HashMap<String, Vec<String>>;\n\npub fn build_call_graph(elements: &[CodeElement]) -> CallGraph {\n    let mut graph: CallGraph = HashMap::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let entry = graph.entry(element.name.clone()).or_default();\n        entry.extend(element.calls.iter().cloned());\n    }\n    graph\n}\n\npub fn build_reverse_call_graph(graph: &CallGraph) -> CallGraph {\n    let mut reverse: CallGraph = HashMap::new();\n    for (caller, callees) in graph {\n        for callee in callees {\n            reverse.entry(callee.clone()).or_default().push(caller.clone());\n        }\n    }\n    reverse\n}\n\npub fn compute_reachability(graph: &CallGraph, entrypoints: &HashSet<String>) -> HashSet<String> {\n    let mut reachable = HashSet::new();\n    let mut queue: VecDeque<String> = entrypoints.iter().cloned().collect();\n\n    while let Some(node) = queue.pop_front() {\n        if !reachable.insert(node.clone()) {\n            continue;\n        }\n        if let Some(callees) = graph.get(&node) {\n            for callee in callees {\n                if !reachable.contains(callee) {\n                    queue.push_back(callee.clone());\n                }\n            }\n        }\n    }\n\n    reachable\n}\n\npub fn is_reachable(\n    symbol: &str,\n    graph: &CallGraph,\n    entrypoints: &HashSet<String>,\n) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    compute_reachability(graph, entrypoints).contains(symbol)\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Dead code call graph utilities.\n\nuse crate::types::{CodeElement, ElementType, Language};\nuse std::collections::{HashMap, HashSet, VecDeque};\n\npub type CallGraph = HashMap<String, Vec<String>>;\n\npub fn build_call_graph(elements: &[CodeElement]) -> CallGraph {\n    let mut graph: CallGraph = HashMap::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let entry = graph.entry(element.name.clone()).or_default();\n        entry.extend(element.calls.iter().cloned());\n    }\n    graph\n}\n\npub fn build_reverse_call_graph(graph: &CallGraph) -> CallGraph {\n    let mut reverse: CallGraph = HashMap::new();\n    for (caller, callees) in graph {\n        for callee in callees {\n            reverse.entry(callee.clone()).or_default().push(caller.clone());\n        }\n    }\n    reverse\n}\n\npub fn compute_reachability(graph: &CallGraph, entrypoints: &HashSet<String>) -> HashSet<String> {\n    let mut reachable = HashSet::new();\n    let mut queue: VecDeque<String> = entrypoints.iter().cloned().collect();\n\n    while let Some(node) = queue.pop_front() {\n        if !reachable.insert(node.clone()) {\n            continue;\n        }\n        if let Some(callees) = graph.get(&node) {\n            for callee in callees {\n                if !reachable.contains(callee) {\n                    queue.push_back(callee.clone());\n                }\n            }\n        }\n    }\n\n    reachable\n}\n\npub fn is_reachable(\n    symbol: &str,\n    graph: &CallGraph,\n    entrypoints: &HashSet<String>,\n) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    compute_reachability(graph, entrypoints).contains(symbol)\n}\n\npub fn classify_symbol(\n    symbol: &str,\n    call_graph: &CallGraph,\n    intent_map: &IntentMap,\n    test_boundaries: &TestBoundaries,\n    entrypoints: &HashSet<String>,\n    _policy: Option<&DeadCodePolicy>,\n) -> DeadCodeCategory {\n    if intent_map.contains_key(symbol) {\n        return DeadCodeCategory::LatentPlanned;\n    }\n\n    if is_test_only(symbol, call_graph, test_boundaries) {\n        return DeadCodeCategory::TestOnly;\n    }\n\n    if !is_reachable(symbol, call_graph, entrypoints) {\n        return DeadCodeCategory::Unreachable;\n    }\n\n    DeadCodeCategory::ReachableUnused\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_is_test_only_to_src/213_dead_code_call_graph.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/217_dead_code_classifier.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code category classification.\n\nuse crate::dead_code_call_graph::{build_reverse_call_graph, CallGraph};\nuse crate::dead_code_intent::DeadCodePolicy;\nuse crate::dead_code_test_boundaries::TestBoundaries;\nuse crate::dead_code_types::{DeadCodeCategory, IntentMap};\nuse std::collections::HashSet;\n\npub fn classify_symbol(\n    symbol: &str,\n    call_graph: &CallGraph,\n    intent_map: &IntentMap,\n    test_boundaries: &TestBoundaries,\n    entrypoints: &HashSet<String>,\n    _policy: Option<&DeadCodePolicy>,\n) -> DeadCodeCategory {\n    if intent_map.contains_key(symbol) {\n        return DeadCodeCategory::LatentPlanned;\n    }\n\n    if is_test_only(symbol, call_graph, test_boundaries) {\n        return DeadCodeCategory::TestOnly;\n    }\n\n    if !is_reachable(symbol, call_graph, entrypoints) {\n        return DeadCodeCategory::Unreachable;\n    }\n\n    DeadCodeCategory::ReachableUnused\n}\n\npub fn is_reachable(symbol: &str, call_graph: &CallGraph, entrypoints: &HashSet<String>) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    let reachable = crate::dead_code_call_graph::compute_reachability(call_graph, entrypoints);\n    reachable.contains(symbol)\n}\n\npub fn is_test_only(\n    symbol: &str,\n    call_graph: &CallGraph,\n    test_boundaries: &TestBoundaries,\n) -> bool {\n    if test_boundaries.test_symbols.contains(symbol) {\n        return true;\n    }\n    let reverse = build_reverse_call_graph(call_graph);\n    let callers = reverse.get(symbol);\n    let Some(callers) = callers else {\n        return false;\n    };\n    if callers.is_empty() {\n        return false;\n    }\n    callers\n        .iter()\n        .all(|caller| test_boundaries.test_symbols.contains(caller))\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Dead code category classification.\n\nuse crate::dead_code_call_graph::{build_reverse_call_graph, CallGraph};\nuse crate::dead_code_intent::DeadCodePolicy;\nuse crate::dead_code_test_boundaries::TestBoundaries;\nuse crate::dead_code_types::{DeadCodeCategory, IntentMap};\nuse std::collections::HashSet;\n\npub fn classify_symbol(\n    symbol: &str,\n    call_graph: &CallGraph,\n    intent_map: &IntentMap,\n    test_boundaries: &TestBoundaries,\n    entrypoints: &HashSet<String>,\n    _policy: Option<&DeadCodePolicy>,\n) -> DeadCodeCategory {\n    if intent_map.contains_key(symbol) {\n        return DeadCodeCategory::LatentPlanned;\n    }\n\n    if is_test_only(symbol, call_graph, test_boundaries) {\n        return DeadCodeCategory::TestOnly;\n    }\n\n    if !is_reachable(symbol, call_graph, entrypoints) {\n        return DeadCodeCategory::Unreachable;\n    }\n\n    DeadCodeCategory::ReachableUnused\n}\n\npub fn is_reachable(symbol: &str, call_graph: &CallGraph, entrypoints: &HashSet<String>) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    let reachable = crate::dead_code_call_graph::compute_reachability(call_graph, entrypoints);\n    reachable.contains(symbol)\n}\n\n\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/213_dead_code_call_graph.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code call graph utilities.\n\nuse crate::types::{CodeElement, ElementType, Language};\nuse std::collections::{HashMap, HashSet, VecDeque};\n\npub type CallGraph = HashMap<String, Vec<String>>;\n\npub fn build_call_graph(elements: &[CodeElement]) -> CallGraph {\n    let mut graph: CallGraph = HashMap::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let entry = graph.entry(element.name.clone()).or_default();\n        entry.extend(element.calls.iter().cloned());\n    }\n    graph\n}\n\npub fn build_reverse_call_graph(graph: &CallGraph) -> CallGraph {\n    let mut reverse: CallGraph = HashMap::new();\n    for (caller, callees) in graph {\n        for callee in callees {\n            reverse.entry(callee.clone()).or_default().push(caller.clone());\n        }\n    }\n    reverse\n}\n\npub fn compute_reachability(graph: &CallGraph, entrypoints: &HashSet<String>) -> HashSet<String> {\n    let mut reachable = HashSet::new();\n    let mut queue: VecDeque<String> = entrypoints.iter().cloned().collect();\n\n    while let Some(node) = queue.pop_front() {\n        if !reachable.insert(node.clone()) {\n            continue;\n        }\n        if let Some(callees) = graph.get(&node) {\n            for callee in callees {\n                if !reachable.contains(callee) {\n                    queue.push_back(callee.clone());\n                }\n            }\n        }\n    }\n\n    reachable\n}\n\npub fn is_reachable(\n    symbol: &str,\n    graph: &CallGraph,\n    entrypoints: &HashSet<String>,\n) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    compute_reachability(graph, entrypoints).contains(symbol)\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Dead code call graph utilities.\n\nuse crate::types::{CodeElement, ElementType, Language};\nuse std::collections::{HashMap, HashSet, VecDeque};\n\npub type CallGraph = HashMap<String, Vec<String>>;\n\npub fn build_call_graph(elements: &[CodeElement]) -> CallGraph {\n    let mut graph: CallGraph = HashMap::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let entry = graph.entry(element.name.clone()).or_default();\n        entry.extend(element.calls.iter().cloned());\n    }\n    graph\n}\n\npub fn build_reverse_call_graph(graph: &CallGraph) -> CallGraph {\n    let mut reverse: CallGraph = HashMap::new();\n    for (caller, callees) in graph {\n        for callee in callees {\n            reverse.entry(callee.clone()).or_default().push(caller.clone());\n        }\n    }\n    reverse\n}\n\npub fn compute_reachability(graph: &CallGraph, entrypoints: &HashSet<String>) -> HashSet<String> {\n    let mut reachable = HashSet::new();\n    let mut queue: VecDeque<String> = entrypoints.iter().cloned().collect();\n\n    while let Some(node) = queue.pop_front() {\n        if !reachable.insert(node.clone()) {\n            continue;\n        }\n        if let Some(callees) = graph.get(&node) {\n            for callee in callees {\n                if !reachable.contains(callee) {\n                    queue.push_back(callee.clone());\n                }\n            }\n        }\n    }\n\n    reachable\n}\n\npub fn is_reachable(\n    symbol: &str,\n    graph: &CallGraph,\n    entrypoints: &HashSet<String>,\n) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    compute_reachability(graph, entrypoints).contains(symbol)\n}\n\npub fn is_test_only(\n    symbol: &str,\n    call_graph: &CallGraph,\n    test_boundaries: &TestBoundaries,\n) -> bool {\n    if test_boundaries.test_symbols.contains(symbol) {\n        return true;\n    }\n    let reverse = build_reverse_call_graph(call_graph);\n    let callers = reverse.get(symbol);\n    let Some(callers) = callers else {\n        return false;\n    };\n    if callers.is_empty() {\n        return false;\n    }\n    callers\n        .iter()\n        .all(|caller| test_boundaries.test_symbols.contains(caller))\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_run_dead_code_pipeline_to_src/070_layer_utilities.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/222_dead_code_cli.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code analysis pipeline runner.\n\nuse anyhow::Result;\nuse crate::dead_code_actions::recommend_action;\nuse crate::dead_code_call_graph::build_call_graph;\nuse crate::dead_code_classifier::{classify_symbol, is_reachable};\nuse crate::dead_code_confidence::{assign_confidence, Evidence};\nuse crate::dead_code_entrypoints::{collect_entrypoints, collect_exports, is_public_api};\nuse crate::dead_code_intent::{detect_intent_signals, DeadCodePolicy};\nuse crate::dead_code_report::{build_report, write_report, DeadCodeReportMetadata, DeadCodeReportWithMeta};\nuse crate::dead_code_report_split::write_summary_markdown;\nuse crate::dead_code_test_boundaries::{detect_test_modules, detect_test_symbols, TestBoundaries};\nuse crate::dead_code_types::{DeadCodeCategory, DeadCodeItem};\nuse crate::layer_utilities::gather_rust_files;\nuse crate::types::{CodeElement, ElementType, Language, Visibility};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\n#[derive(Debug, Clone)]\npub struct DeadCodeRunConfig {\n    pub root: PathBuf,\n    pub output_dir: PathBuf,\n    pub policy: Option<DeadCodePolicy>,\n    pub write_json: Option<PathBuf>,\n    pub write_summary: Option<PathBuf>,\n    pub summary_limit: usize,\n}\n\npub fn run_dead_code_pipeline(\n    elements: &[CodeElement],\n    config: &DeadCodeRunConfig,\n) -> Result<DeadCodeReportWithMeta> {\n    let rust_files = gather_rust_files(&config.root);\n    let mut intent_map = HashMap::new();\n    let mut test_boundaries = TestBoundaries::default();\n\n    for file in &rust_files {\n        let intents = detect_intent_signals(file, config.policy.as_ref());\n        merge_intent_map(&mut intent_map, intents);\n        let test_modules = detect_test_modules(file);\n        test_boundaries.test_modules.extend(test_modules);\n        let test_symbols = detect_test_symbols(file);\n        test_boundaries.test_symbols.extend(test_symbols);\n        if is_test_path(file) {\n            test_boundaries.test_files.insert(file.clone());\n        }\n    }\n\n    let call_graph = build_call_graph(elements);\n    let entrypoints = collect_entrypoints(elements, config.policy.as_ref());\n    let exports = collect_exports(&config.root);\n\n    let mut items = Vec::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let category = classify_symbol(\n            &element.name,\n            &call_graph,\n            &intent_map,\n            &test_boundaries,\n            &entrypoints,\n            config.policy.as_ref(),\n        );\n        let intent_tag = intent_map.contains_key(&element.name);\n        let test_reference = test_boundaries.test_symbols.contains(&element.name);\n        let call_graph_proven =\n            category == DeadCodeCategory::Unreachable && is_reachable(&element.name, &call_graph, &entrypoints) == false;\n\n        let mut item = DeadCodeItem {\n            symbol: element.name.clone(),\n            file: PathBuf::from(&element.file_path),\n            line: element.line_number,\n            category,\n            confidence: crate::dead_code_types::ConfidenceLevel::Heuristic,\n            action: crate::dead_code_types::RecommendedAction::ManualReview,\n            reason: reason_for_category(category, intent_tag, test_reference),\n        };\n\n        let confidence = assign_confidence(\n            &item,\n            &Evidence {\n                intent_tag,\n                test_reference,\n                call_graph_proven,\n            },\n        );\n        item.confidence = confidence;\n\n        let public_api = is_public_api(&element.name, &exports)\n            || matches!(element.visibility, Visibility::Public);\n        item.action = recommend_action(category, confidence, public_api);\n\n        items.push(item);\n    }\n\n    let metadata = DeadCodeReportMetadata {\n        analyzer_version: env!(\"CARGO_PKG_VERSION\").to_string(),\n        project_root: config.root.display().to_string(),\n        entrypoints_found: entrypoints.len(),\n    };\n    let report = build_report(\n        chrono::Local::now().to_rfc3339(),\n        items,\n        metadata,\n    );\n\n    write_outputs(&report, config)?;\n    Ok(report)\n}\n\nfn write_outputs(report: &DeadCodeReportWithMeta, config: &DeadCodeRunConfig) -> Result<()> {\n    let json_path = config\n        .write_json\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_full.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_report(&json_path, report)?;\n\n    let summary_path = config\n        .write_summary\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_summary.md\"));\n    if let Some(parent) = summary_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_summary_markdown(&summary_path, report, config.summary_limit)?;\n    Ok(())\n}\n\nfn merge_intent_map(base: &mut HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>, next: HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>) {\n    for (symbol, items) in next {\n        base.entry(symbol).or_default().extend(items);\n    }\n}\n\nfn reason_for_category(category: DeadCodeCategory, intent_tag: bool, test_reference: bool) -> String {\n    match category {\n        DeadCodeCategory::LatentPlanned => {\n            if intent_tag {\n                \"Intent tag present\".to_string()\n            } else {\n                \"Intent directory policy\".to_string()\n            }\n        }\n        DeadCodeCategory::TestOnly => {\n            if test_reference {\n                \"Called only by test symbols\".to_string()\n            } else {\n                \"Defined in test-only module\".to_string()\n            }\n        }\n        DeadCodeCategory::Unreachable => \"No callers reachable from entrypoints\".to_string(),\n        DeadCodeCategory::ReachableUnused => \"Reachable but unused in execution\".to_string(),\n    }\n}\n\nfn is_test_path(path: &Path) -> bool {\n    path.components().any(|c| {\n        let name = c.as_os_str().to_str().unwrap_or(\"\");\n        name == \"tests\" || name == \"test\" || name == \"benches\"\n    })\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Dead code analysis pipeline runner.\n\nuse anyhow::Result;\nuse crate::dead_code_actions::recommend_action;\nuse crate::dead_code_call_graph::build_call_graph;\nuse crate::dead_code_classifier::{classify_symbol, is_reachable};\nuse crate::dead_code_confidence::{assign_confidence, Evidence};\nuse crate::dead_code_entrypoints::{collect_entrypoints, collect_exports, is_public_api};\nuse crate::dead_code_intent::{detect_intent_signals, DeadCodePolicy};\nuse crate::dead_code_report::{build_report, write_report, DeadCodeReportMetadata, DeadCodeReportWithMeta};\nuse crate::dead_code_report_split::write_summary_markdown;\nuse crate::dead_code_test_boundaries::{detect_test_modules, detect_test_symbols, TestBoundaries};\nuse crate::dead_code_types::{DeadCodeCategory, DeadCodeItem};\nuse crate::layer_utilities::gather_rust_files;\nuse crate::types::{CodeElement, ElementType, Language, Visibility};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\n#[derive(Debug, Clone)]\npub struct DeadCodeRunConfig {\n    pub root: PathBuf,\n    pub output_dir: PathBuf,\n    pub policy: Option<DeadCodePolicy>,\n    pub write_json: Option<PathBuf>,\n    pub write_summary: Option<PathBuf>,\n    pub summary_limit: usize,\n}\n\n\n\nfn write_outputs(report: &DeadCodeReportWithMeta, config: &DeadCodeRunConfig) -> Result<()> {\n    let json_path = config\n        .write_json\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_full.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_report(&json_path, report)?;\n\n    let summary_path = config\n        .write_summary\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_summary.md\"));\n    if let Some(parent) = summary_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_summary_markdown(&summary_path, report, config.summary_limit)?;\n    Ok(())\n}\n\nfn merge_intent_map(base: &mut HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>, next: HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>) {\n    for (symbol, items) in next {\n        base.entry(symbol).or_default().extend(items);\n    }\n}\n\nfn reason_for_category(category: DeadCodeCategory, intent_tag: bool, test_reference: bool) -> String {\n    match category {\n        DeadCodeCategory::LatentPlanned => {\n            if intent_tag {\n                \"Intent tag present\".to_string()\n            } else {\n                \"Intent directory policy\".to_string()\n            }\n        }\n        DeadCodeCategory::TestOnly => {\n            if test_reference {\n                \"Called only by test symbols\".to_string()\n            } else {\n                \"Defined in test-only module\".to_string()\n            }\n        }\n        DeadCodeCategory::Unreachable => \"No callers reachable from entrypoints\".to_string(),\n        DeadCodeCategory::ReachableUnused => \"Reachable but unused in execution\".to_string(),\n    }\n}\n\nfn is_test_path(path: &Path) -> bool {\n    path.components().any(|c| {\n        let name = c.as_os_str().to_str().unwrap_or(\"\");\n        name == \"tests\" || name == \"test\" || name == \"benches\"\n    })\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_layer_utilities.rs",
          "original_content": "//! Layer utility functions for layer-based dependency analysis\n//! This module is at layer 010 to be accessible from all higher layers\n\nuse anyhow::{Context, Result};\nuse clap::Parser;\nuse std::path::{Path, PathBuf};\n\n#[allow(unused_imports)]\npub use crate::cluster_001::{build_file_layers, detect_layer, gather_julia_files, julia_entry_paths};\n#[allow(unused_imports)]\npub use crate::cluster_010::contains_tools;\n\n/// Resolves the source root directory from a given root path\npub fn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\n/// Checks if a directory should be included in analysis\npub fn allow_analysis_dir(root: &Path, dir: &Path) -> bool {\n    let name = dir.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    \n    if name.starts_with('.') || name == \"target\" || name == \"node_modules\" {\n        return false;\n    }\n    \n    if let Ok(rel) = dir.strip_prefix(root) {\n        if rel.components().any(|c| {\n            let s = c.as_os_str().to_str().unwrap_or(\"\");\n            s.starts_with('.') || s == \"target\" || s == \"node_modules\"\n        }) {\n            return false;\n        }\n    }\n    \n    true\n}\n\npub fn gather_rust_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"rs\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n        .collect()\n}\n\n// ============================================================================\n// CLI Entrypoint (from src/000_cluster_011.rs)\n// ============================================================================\n\n#[derive(Parser, Debug)]\n#[command(name = \"mmsb-analyzer\")]\n#[command(about = \"MMSB Intelligence Substrate Analyzer\", long_about = None)]\nstruct Args {\n    /// Root directory to analyze\n    #[arg(short, long, default_value = \"../..\")]\n    root: PathBuf,\n\n    /// Output directory for reports\n    #[arg(short, long, default_value = \"../../docs/analysis\")]\n    output: PathBuf,\n\n    /// Verbose output\n    #[arg(short, long)]\n    verbose: bool,\n\n    /// Skip Julia file analysis\n    #[arg(long)]\n    skip_julia: bool,\n\n    /// Run dead code analysis\n    #[arg(long)]\n    dead_code: bool,\n\n    /// Filter dead code from downstream analysis\n    #[arg(long)]\n    dead_code_filter: bool,\n\n    /// Output JSON dead code report\n    #[arg(long)]\n    dead_code_json: Option<PathBuf>,\n\n    /// Output dead code summary markdown\n    #[arg(long)]\n    dead_code_summary: Option<PathBuf>,\n\n    /// Dead code summary limit\n    #[arg(long, default_value_t = 50)]\n    dead_code_summary_limit: usize,\n\n    /// Dead code policy file\n    #[arg(long)]\n    dead_code_policy: Option<PathBuf>,\n\n    /// Generate correction intelligence JSON\n    #[arg(long)]\n    correction_intelligence: bool,\n\n    /// Override correction intelligence JSON output path\n    #[arg(long)]\n    correction_json: Option<PathBuf>,\n\n    /// Override verification policy JSON output path\n    #[arg(long)]\n    verification_policy_json: Option<PathBuf>,\n}\n\npub fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let root_path = std::env::current_dir()?.join(&args.root).canonicalize()?;\n    let output_path = std::env::current_dir()?\n        .join(&args.output)\n        .canonicalize()\n        .unwrap_or_else(|_| {\n            let p = std::env::current_dir().unwrap().join(&args.output);\n            std::fs::create_dir_all(&p).ok();\n            p.canonicalize().unwrap_or(p)\n        });\n    run_analysis(\n        &root_path,\n        &output_path,\n        args.verbose,\n        args.skip_julia,\n        args.dead_code,\n        args.dead_code_filter,\n        args.dead_code_json,\n        args.dead_code_summary,\n        args.dead_code_summary_limit,\n        args.dead_code_policy,\n        args.correction_intelligence,\n        args.correction_json,\n        args.verification_policy_json,\n    )\n}\n\npub fn run_analysis(\n    root_path: &Path,\n    output_path: &Path,\n    verbose: bool,\n    skip_julia: bool,\n    dead_code: bool,\n    dead_code_filter: bool,\n    dead_code_json: Option<PathBuf>,\n    dead_code_summary: Option<PathBuf>,\n    dead_code_summary_limit: usize,\n    dead_code_policy: Option<PathBuf>,\n    correction_intelligence: bool,\n    correction_json: Option<PathBuf>,\n    verification_policy_json: Option<PathBuf>,\n) -> Result<()> {\n    use crate::control_flow::ControlFlowAnalyzer;\n    use crate::cohesion_analyzer::FunctionCohesionAnalyzer;\n    use crate::dependency::LayerGraph;\n    use crate::directory_analyzer::DirectoryAnalyzer;\n    use crate::dot_exporter::export_program_cfg_to_path;\n    use crate::julia_parser::JuliaAnalyzer;\n    use crate::report::ReportGenerator;\n    use crate::rust_parser::RustAnalyzer;\n    use crate::types::{AnalysisResult, FileOrderingResult};\n\n    let julia_script_path = root_path.join(\"src/000_main.jl\");\n\n    println!(\"MMSB Intelligence Substrate Analyzer\");\n    println!(\"=====================================\\n\");\n    println!(\"Root directory: {:?}\", root_path);\n    println!(\"Output directory: {:?}\", output_path);\n    println!(\"Julia script: {:?}\\n\", julia_script_path);\n\n    let rust_analyzer = RustAnalyzer::new(root_path.to_string_lossy().to_string());\n    let mut combined_result = AnalysisResult::new();\n\n    println!(\"Scanning Rust files (dependency-ordered)...\");\n    let mut rust_count = 0;\n    let rust_files = gather_rust_files(root_path);\n    let (ordered_rust_files, rust_layer_graph) =\n        crate::dependency::order_rust_files_by_dependency(&rust_files, root_path)\n            .context(\"Failed to resolve Rust dependency order\")?;\n    let rust_file_ordering =\n        crate::dependency::analyze_file_ordering(&rust_files, None)\n            .context(\"Failed to analyze Rust file ordering\")?;\n    let julia_file_ordering = FileOrderingResult {\n        ordered_files: Vec::new(),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles: Vec::new(),\n    };\n\n    for path in ordered_rust_files {\n        if verbose {\n            println!(\"  Analyzing: {:?}\", path);\n        }\n\n        match rust_analyzer.analyze_file(&path) {\n            Ok(result) => {\n                rust_count += 1;\n                combined_result.merge(result);\n            }\n            Err(e) => {\n                eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n            }\n        }\n    }\n\n    println!(\"  Analyzed {} Rust files\\n\", rust_count);\n\n    let mut julia_count = 0;\n    let mut julia_layer_graph = LayerGraph {\n        ordered_layers: Vec::new(),\n        edges: Vec::new(),\n        cycles: Vec::new(),\n        unresolved: Vec::new(),\n    };\n    if !skip_julia {\n        println!(\"Scanning Julia files (dependency-ordered)...\");\n        let julia_files = gather_julia_files(root_path);\n        let (ordered_julia_files, jlg) =\n            crate::dependency::order_julia_files_by_dependency(&julia_files, root_path)\n                .context(\"Failed to resolve Julia dependency order\")?;\n        julia_layer_graph = jlg;\n\n        if julia_script_path.exists() {\n            let julia_analyzer = JuliaAnalyzer::new(\n                root_path.to_path_buf(),\n                julia_script_path.clone(),\n                output_path.join(\"30_cfg/dots\"),\n            );\n\n            for path in ordered_julia_files {\n                if verbose {\n                    println!(\"  Analyzing: {:?}\", path);\n                }\n\n                match julia_analyzer.analyze_file(&path) {\n                    Ok(result) => {\n                        julia_count += 1;\n                        combined_result.merge(result);\n                    }\n                    Err(e) => {\n                        eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n                    }\n                }\n            }\n        } else {\n            println!(\"  Skipping Julia analysis (script not found)\");\n        }\n\n        println!(\"  Analyzed {} Julia files\\n\", julia_count);\n    }\n\n    if dead_code || dead_code_filter || dead_code_json.is_some() || dead_code_summary.is_some() {\n        let policy = if let Some(policy_path) = dead_code_policy {\n            Some(\n                crate::dead_code_policy::load_policy(&policy_path)\n                    .context(\"Failed to load dead code policy\")?,\n            )\n        } else {\n            None\n        };\n        let config = crate::dead_code_cli::DeadCodeRunConfig {\n            root: root_path.to_path_buf(),\n            output_dir: output_path.to_path_buf(),\n            policy,\n            write_json: dead_code_json,\n            write_summary: dead_code_summary,\n            summary_limit: dead_code_summary_limit,\n        };\n        let report = crate::dead_code_cli::run_dead_code_pipeline(&combined_result.elements, &config)\n            .context(\"Dead code analysis failed\")?;\n        if dead_code_filter {\n            combined_result.elements =\n                crate::dead_code_filter::filter_dead_code_elements(&combined_result.elements, &report);\n        }\n    }\n\n    println!(\"Building call graph...\");\n    let mut cf_analyzer = ControlFlowAnalyzer::new();\n    cf_analyzer.build_call_graph(&combined_result);\n\n    // NEW: Invariant detection\n    use crate::invariant_integrator::InvariantDetector;\n    println!(\"Detecting invariants...\");\n    let invariants_result = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.detect_all()\n    };\n    let constraints = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.generate_constraints(&invariants_result)\n    };\n    combined_result.invariants = invariants_result;\n    combined_result.constraints = constraints;\n\n    println!(\"Analyzing function cohesion...\");\n    let cohesion_analyzer = FunctionCohesionAnalyzer::new();\n    let placements = cohesion_analyzer.analyze(&combined_result)?;\n    let clusters = cohesion_analyzer.detect_clusters(&combined_result)?;\n\n    println!(\"Analyzing directory structure...\");\n    let dir_analyzer = DirectoryAnalyzer::new(root_path.to_path_buf());\n    let dir_analysis = dir_analyzer.analyze()?;\n\n    println!(\"\\nGenerating reports...\");\n    let report_gen = ReportGenerator::new(output_path.to_string_lossy().to_string());\n    report_gen.generate_all(\n        &combined_result,\n        &cf_analyzer,\n        &rust_layer_graph,\n        &julia_layer_graph,\n        &rust_file_ordering,\n        &julia_file_ordering,\n        &placements,\n        &clusters,\n        &dir_analysis,\n        root_path,\n        correction_intelligence,\n        correction_json,\n        verification_policy_json,\n    )\n    .context(\"Failed to generate reports\")?;\n\n    println!(\"\\nExporting program CFG...\");\n    export_program_cfg_to_path(&combined_result, &cf_analyzer.call_edges(), output_path)?;\n\n    println!(\"\\nGenerating invariant report...\");\n    use crate::invariant_reporter;\n    invariant_reporter::generate_invariant_report(&combined_result.invariants, output_path)\n        .context(\"Failed to generate invariant report\")?;\n    invariant_reporter::export_constraints_json(&combined_result.constraints, output_path)\n        .context(\"Failed to export constraints\")?;\n\n    println!(\"\\n Analysis complete!\");\n    println!(\"  Total elements: {}\", combined_result.elements.len());\n    println!(\"  Rust files: {}\", rust_count);\n    println!(\"  Julia files: {}\", julia_count);\n    println!(\"  Output: {}\\n\", output_path.display());\n\n    Ok(())\n}\n",
          "updated_content": "//! Layer utility functions for layer-based dependency analysis\n//! This module is at layer 010 to be accessible from all higher layers\n\nuse anyhow::{Context, Result};\nuse clap::Parser;\nuse std::path::{Path, PathBuf};\n\n#[allow(unused_imports)]\npub use crate::cluster_001::{build_file_layers, detect_layer, gather_julia_files, julia_entry_paths};\n#[allow(unused_imports)]\npub use crate::cluster_010::contains_tools;\n\n/// Resolves the source root directory from a given root path\npub fn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\n/// Checks if a directory should be included in analysis\npub fn allow_analysis_dir(root: &Path, dir: &Path) -> bool {\n    let name = dir.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    \n    if name.starts_with('.') || name == \"target\" || name == \"node_modules\" {\n        return false;\n    }\n    \n    if let Ok(rel) = dir.strip_prefix(root) {\n        if rel.components().any(|c| {\n            let s = c.as_os_str().to_str().unwrap_or(\"\");\n            s.starts_with('.') || s == \"target\" || s == \"node_modules\"\n        }) {\n            return false;\n        }\n    }\n    \n    true\n}\n\npub fn gather_rust_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"rs\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n        .collect()\n}\n\n// ============================================================================\n// CLI Entrypoint (from src/000_cluster_011.rs)\n// ============================================================================\n\n#[derive(Parser, Debug)]\n#[command(name = \"mmsb-analyzer\")]\n#[command(about = \"MMSB Intelligence Substrate Analyzer\", long_about = None)]\nstruct Args {\n    /// Root directory to analyze\n    #[arg(short, long, default_value = \"../..\")]\n    root: PathBuf,\n\n    /// Output directory for reports\n    #[arg(short, long, default_value = \"../../docs/analysis\")]\n    output: PathBuf,\n\n    /// Verbose output\n    #[arg(short, long)]\n    verbose: bool,\n\n    /// Skip Julia file analysis\n    #[arg(long)]\n    skip_julia: bool,\n\n    /// Run dead code analysis\n    #[arg(long)]\n    dead_code: bool,\n\n    /// Filter dead code from downstream analysis\n    #[arg(long)]\n    dead_code_filter: bool,\n\n    /// Output JSON dead code report\n    #[arg(long)]\n    dead_code_json: Option<PathBuf>,\n\n    /// Output dead code summary markdown\n    #[arg(long)]\n    dead_code_summary: Option<PathBuf>,\n\n    /// Dead code summary limit\n    #[arg(long, default_value_t = 50)]\n    dead_code_summary_limit: usize,\n\n    /// Dead code policy file\n    #[arg(long)]\n    dead_code_policy: Option<PathBuf>,\n\n    /// Generate correction intelligence JSON\n    #[arg(long)]\n    correction_intelligence: bool,\n\n    /// Override correction intelligence JSON output path\n    #[arg(long)]\n    correction_json: Option<PathBuf>,\n\n    /// Override verification policy JSON output path\n    #[arg(long)]\n    verification_policy_json: Option<PathBuf>,\n}\n\npub fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let root_path = std::env::current_dir()?.join(&args.root).canonicalize()?;\n    let output_path = std::env::current_dir()?\n        .join(&args.output)\n        .canonicalize()\n        .unwrap_or_else(|_| {\n            let p = std::env::current_dir().unwrap().join(&args.output);\n            std::fs::create_dir_all(&p).ok();\n            p.canonicalize().unwrap_or(p)\n        });\n    run_analysis(\n        &root_path,\n        &output_path,\n        args.verbose,\n        args.skip_julia,\n        args.dead_code,\n        args.dead_code_filter,\n        args.dead_code_json,\n        args.dead_code_summary,\n        args.dead_code_summary_limit,\n        args.dead_code_policy,\n        args.correction_intelligence,\n        args.correction_json,\n        args.verification_policy_json,\n    )\n}\n\npub fn run_analysis(\n    root_path: &Path,\n    output_path: &Path,\n    verbose: bool,\n    skip_julia: bool,\n    dead_code: bool,\n    dead_code_filter: bool,\n    dead_code_json: Option<PathBuf>,\n    dead_code_summary: Option<PathBuf>,\n    dead_code_summary_limit: usize,\n    dead_code_policy: Option<PathBuf>,\n    correction_intelligence: bool,\n    correction_json: Option<PathBuf>,\n    verification_policy_json: Option<PathBuf>,\n) -> Result<()> {\n    use crate::control_flow::ControlFlowAnalyzer;\n    use crate::cohesion_analyzer::FunctionCohesionAnalyzer;\n    use crate::dependency::LayerGraph;\n    use crate::directory_analyzer::DirectoryAnalyzer;\n    use crate::dot_exporter::export_program_cfg_to_path;\n    use crate::julia_parser::JuliaAnalyzer;\n    use crate::report::ReportGenerator;\n    use crate::rust_parser::RustAnalyzer;\n    use crate::types::{AnalysisResult, FileOrderingResult};\n\n    let julia_script_path = root_path.join(\"src/000_main.jl\");\n\n    println!(\"MMSB Intelligence Substrate Analyzer\");\n    println!(\"=====================================\\n\");\n    println!(\"Root directory: {:?}\", root_path);\n    println!(\"Output directory: {:?}\", output_path);\n    println!(\"Julia script: {:?}\\n\", julia_script_path);\n\n    let rust_analyzer = RustAnalyzer::new(root_path.to_string_lossy().to_string());\n    let mut combined_result = AnalysisResult::new();\n\n    println!(\"Scanning Rust files (dependency-ordered)...\");\n    let mut rust_count = 0;\n    let rust_files = gather_rust_files(root_path);\n    let (ordered_rust_files, rust_layer_graph) =\n        crate::dependency::order_rust_files_by_dependency(&rust_files, root_path)\n            .context(\"Failed to resolve Rust dependency order\")?;\n    let rust_file_ordering =\n        crate::dependency::analyze_file_ordering(&rust_files, None)\n            .context(\"Failed to analyze Rust file ordering\")?;\n    let julia_file_ordering = FileOrderingResult {\n        ordered_files: Vec::new(),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles: Vec::new(),\n    };\n\n    for path in ordered_rust_files {\n        if verbose {\n            println!(\"  Analyzing: {:?}\", path);\n        }\n\n        match rust_analyzer.analyze_file(&path) {\n            Ok(result) => {\n                rust_count += 1;\n                combined_result.merge(result);\n            }\n            Err(e) => {\n                eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n            }\n        }\n    }\n\n    println!(\"  Analyzed {} Rust files\\n\", rust_count);\n\n    let mut julia_count = 0;\n    let mut julia_layer_graph = LayerGraph {\n        ordered_layers: Vec::new(),\n        edges: Vec::new(),\n        cycles: Vec::new(),\n        unresolved: Vec::new(),\n    };\n    if !skip_julia {\n        println!(\"Scanning Julia files (dependency-ordered)...\");\n        let julia_files = gather_julia_files(root_path);\n        let (ordered_julia_files, jlg) =\n            crate::dependency::order_julia_files_by_dependency(&julia_files, root_path)\n                .context(\"Failed to resolve Julia dependency order\")?;\n        julia_layer_graph = jlg;\n\n        if julia_script_path.exists() {\n            let julia_analyzer = JuliaAnalyzer::new(\n                root_path.to_path_buf(),\n                julia_script_path.clone(),\n                output_path.join(\"30_cfg/dots\"),\n            );\n\n            for path in ordered_julia_files {\n                if verbose {\n                    println!(\"  Analyzing: {:?}\", path);\n                }\n\n                match julia_analyzer.analyze_file(&path) {\n                    Ok(result) => {\n                        julia_count += 1;\n                        combined_result.merge(result);\n                    }\n                    Err(e) => {\n                        eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n                    }\n                }\n            }\n        } else {\n            println!(\"  Skipping Julia analysis (script not found)\");\n        }\n\n        println!(\"  Analyzed {} Julia files\\n\", julia_count);\n    }\n\n    if dead_code || dead_code_filter || dead_code_json.is_some() || dead_code_summary.is_some() {\n        let policy = if let Some(policy_path) = dead_code_policy {\n            Some(\n                crate::dead_code_policy::load_policy(&policy_path)\n                    .context(\"Failed to load dead code policy\")?,\n            )\n        } else {\n            None\n        };\n        let config = crate::dead_code_cli::DeadCodeRunConfig {\n            root: root_path.to_path_buf(),\n            output_dir: output_path.to_path_buf(),\n            policy,\n            write_json: dead_code_json,\n            write_summary: dead_code_summary,\n            summary_limit: dead_code_summary_limit,\n        };\n        let report = crate::dead_code_cli::run_dead_code_pipeline(&combined_result.elements, &config)\n            .context(\"Dead code analysis failed\")?;\n        if dead_code_filter {\n            combined_result.elements =\n                crate::dead_code_filter::filter_dead_code_elements(&combined_result.elements, &report);\n        }\n    }\n\n    println!(\"Building call graph...\");\n    let mut cf_analyzer = ControlFlowAnalyzer::new();\n    cf_analyzer.build_call_graph(&combined_result);\n\n    // NEW: Invariant detection\n    use crate::invariant_integrator::InvariantDetector;\n    println!(\"Detecting invariants...\");\n    let invariants_result = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.detect_all()\n    };\n    let constraints = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.generate_constraints(&invariants_result)\n    };\n    combined_result.invariants = invariants_result;\n    combined_result.constraints = constraints;\n\n    println!(\"Analyzing function cohesion...\");\n    let cohesion_analyzer = FunctionCohesionAnalyzer::new();\n    let placements = cohesion_analyzer.analyze(&combined_result)?;\n    let clusters = cohesion_analyzer.detect_clusters(&combined_result)?;\n\n    println!(\"Analyzing directory structure...\");\n    let dir_analyzer = DirectoryAnalyzer::new(root_path.to_path_buf());\n    let dir_analysis = dir_analyzer.analyze()?;\n\n    println!(\"\\nGenerating reports...\");\n    let report_gen = ReportGenerator::new(output_path.to_string_lossy().to_string());\n    report_gen.generate_all(\n        &combined_result,\n        &cf_analyzer,\n        &rust_layer_graph,\n        &julia_layer_graph,\n        &rust_file_ordering,\n        &julia_file_ordering,\n        &placements,\n        &clusters,\n        &dir_analysis,\n        root_path,\n        correction_intelligence,\n        correction_json,\n        verification_policy_json,\n    )\n    .context(\"Failed to generate reports\")?;\n\n    println!(\"\\nExporting program CFG...\");\n    export_program_cfg_to_path(&combined_result, &cf_analyzer.call_edges(), output_path)?;\n\n    println!(\"\\nGenerating invariant report...\");\n    use crate::invariant_reporter;\n    invariant_reporter::generate_invariant_report(&combined_result.invariants, output_path)\n        .context(\"Failed to generate invariant report\")?;\n    invariant_reporter::export_constraints_json(&combined_result.constraints, output_path)\n        .context(\"Failed to export constraints\")?;\n\n    println!(\"\\n Analysis complete!\");\n    println!(\"  Total elements: {}\", combined_result.elements.len());\n    println!(\"  Rust files: {}\", rust_count);\n    println!(\"  Julia files: {}\", julia_count);\n    println!(\"  Output: {}\\n\", output_path.display());\n\n    Ok(())\n}\n\npub fn run_dead_code_pipeline(\n    elements: &[CodeElement],\n    config: &DeadCodeRunConfig,\n) -> Result<DeadCodeReportWithMeta> {\n    let rust_files = gather_rust_files(&config.root);\n    let mut intent_map = HashMap::new();\n    let mut test_boundaries = TestBoundaries::default();\n\n    for file in &rust_files {\n        let intents = detect_intent_signals(file, config.policy.as_ref());\n        merge_intent_map(&mut intent_map, intents);\n        let test_modules = detect_test_modules(file);\n        test_boundaries.test_modules.extend(test_modules);\n        let test_symbols = detect_test_symbols(file);\n        test_boundaries.test_symbols.extend(test_symbols);\n        if is_test_path(file) {\n            test_boundaries.test_files.insert(file.clone());\n        }\n    }\n\n    let call_graph = build_call_graph(elements);\n    let entrypoints = collect_entrypoints(elements, config.policy.as_ref());\n    let exports = collect_exports(&config.root);\n\n    let mut items = Vec::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let category = classify_symbol(\n            &element.name,\n            &call_graph,\n            &intent_map,\n            &test_boundaries,\n            &entrypoints,\n            config.policy.as_ref(),\n        );\n        let intent_tag = intent_map.contains_key(&element.name);\n        let test_reference = test_boundaries.test_symbols.contains(&element.name);\n        let call_graph_proven =\n            category == DeadCodeCategory::Unreachable && is_reachable(&element.name, &call_graph, &entrypoints) == false;\n\n        let mut item = DeadCodeItem {\n            symbol: element.name.clone(),\n            file: PathBuf::from(&element.file_path),\n            line: element.line_number,\n            category,\n            confidence: crate::dead_code_types::ConfidenceLevel::Heuristic,\n            action: crate::dead_code_types::RecommendedAction::ManualReview,\n            reason: reason_for_category(category, intent_tag, test_reference),\n        };\n\n        let confidence = assign_confidence(\n            &item,\n            &Evidence {\n                intent_tag,\n                test_reference,\n                call_graph_proven,\n            },\n        );\n        item.confidence = confidence;\n\n        let public_api = is_public_api(&element.name, &exports)\n            || matches!(element.visibility, Visibility::Public);\n        item.action = recommend_action(category, confidence, public_api);\n\n        items.push(item);\n    }\n\n    let metadata = DeadCodeReportMetadata {\n        analyzer_version: env!(\"CARGO_PKG_VERSION\").to_string(),\n        project_root: config.root.display().to_string(),\n        entrypoints_found: entrypoints.len(),\n    };\n    let report = build_report(\n        chrono::Local::now().to_rfc3339(),\n        items,\n        metadata,\n    );\n\n    write_outputs(&report, config)?;\n    Ok(report)\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_write_outputs_to_src/220_dead_code_report.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/222_dead_code_cli.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code analysis pipeline runner.\n\nuse anyhow::Result;\nuse crate::dead_code_actions::recommend_action;\nuse crate::dead_code_call_graph::build_call_graph;\nuse crate::dead_code_classifier::{classify_symbol, is_reachable};\nuse crate::dead_code_confidence::{assign_confidence, Evidence};\nuse crate::dead_code_entrypoints::{collect_entrypoints, collect_exports, is_public_api};\nuse crate::dead_code_intent::{detect_intent_signals, DeadCodePolicy};\nuse crate::dead_code_report::{build_report, write_report, DeadCodeReportMetadata, DeadCodeReportWithMeta};\nuse crate::dead_code_report_split::write_summary_markdown;\nuse crate::dead_code_test_boundaries::{detect_test_modules, detect_test_symbols, TestBoundaries};\nuse crate::dead_code_types::{DeadCodeCategory, DeadCodeItem};\nuse crate::layer_utilities::gather_rust_files;\nuse crate::types::{CodeElement, ElementType, Language, Visibility};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\n#[derive(Debug, Clone)]\npub struct DeadCodeRunConfig {\n    pub root: PathBuf,\n    pub output_dir: PathBuf,\n    pub policy: Option<DeadCodePolicy>,\n    pub write_json: Option<PathBuf>,\n    pub write_summary: Option<PathBuf>,\n    pub summary_limit: usize,\n}\n\npub fn run_dead_code_pipeline(\n    elements: &[CodeElement],\n    config: &DeadCodeRunConfig,\n) -> Result<DeadCodeReportWithMeta> {\n    let rust_files = gather_rust_files(&config.root);\n    let mut intent_map = HashMap::new();\n    let mut test_boundaries = TestBoundaries::default();\n\n    for file in &rust_files {\n        let intents = detect_intent_signals(file, config.policy.as_ref());\n        merge_intent_map(&mut intent_map, intents);\n        let test_modules = detect_test_modules(file);\n        test_boundaries.test_modules.extend(test_modules);\n        let test_symbols = detect_test_symbols(file);\n        test_boundaries.test_symbols.extend(test_symbols);\n        if is_test_path(file) {\n            test_boundaries.test_files.insert(file.clone());\n        }\n    }\n\n    let call_graph = build_call_graph(elements);\n    let entrypoints = collect_entrypoints(elements, config.policy.as_ref());\n    let exports = collect_exports(&config.root);\n\n    let mut items = Vec::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let category = classify_symbol(\n            &element.name,\n            &call_graph,\n            &intent_map,\n            &test_boundaries,\n            &entrypoints,\n            config.policy.as_ref(),\n        );\n        let intent_tag = intent_map.contains_key(&element.name);\n        let test_reference = test_boundaries.test_symbols.contains(&element.name);\n        let call_graph_proven =\n            category == DeadCodeCategory::Unreachable && is_reachable(&element.name, &call_graph, &entrypoints) == false;\n\n        let mut item = DeadCodeItem {\n            symbol: element.name.clone(),\n            file: PathBuf::from(&element.file_path),\n            line: element.line_number,\n            category,\n            confidence: crate::dead_code_types::ConfidenceLevel::Heuristic,\n            action: crate::dead_code_types::RecommendedAction::ManualReview,\n            reason: reason_for_category(category, intent_tag, test_reference),\n        };\n\n        let confidence = assign_confidence(\n            &item,\n            &Evidence {\n                intent_tag,\n                test_reference,\n                call_graph_proven,\n            },\n        );\n        item.confidence = confidence;\n\n        let public_api = is_public_api(&element.name, &exports)\n            || matches!(element.visibility, Visibility::Public);\n        item.action = recommend_action(category, confidence, public_api);\n\n        items.push(item);\n    }\n\n    let metadata = DeadCodeReportMetadata {\n        analyzer_version: env!(\"CARGO_PKG_VERSION\").to_string(),\n        project_root: config.root.display().to_string(),\n        entrypoints_found: entrypoints.len(),\n    };\n    let report = build_report(\n        chrono::Local::now().to_rfc3339(),\n        items,\n        metadata,\n    );\n\n    write_outputs(&report, config)?;\n    Ok(report)\n}\n\nfn write_outputs(report: &DeadCodeReportWithMeta, config: &DeadCodeRunConfig) -> Result<()> {\n    let json_path = config\n        .write_json\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_full.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_report(&json_path, report)?;\n\n    let summary_path = config\n        .write_summary\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_summary.md\"));\n    if let Some(parent) = summary_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_summary_markdown(&summary_path, report, config.summary_limit)?;\n    Ok(())\n}\n\nfn merge_intent_map(base: &mut HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>, next: HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>) {\n    for (symbol, items) in next {\n        base.entry(symbol).or_default().extend(items);\n    }\n}\n\nfn reason_for_category(category: DeadCodeCategory, intent_tag: bool, test_reference: bool) -> String {\n    match category {\n        DeadCodeCategory::LatentPlanned => {\n            if intent_tag {\n                \"Intent tag present\".to_string()\n            } else {\n                \"Intent directory policy\".to_string()\n            }\n        }\n        DeadCodeCategory::TestOnly => {\n            if test_reference {\n                \"Called only by test symbols\".to_string()\n            } else {\n                \"Defined in test-only module\".to_string()\n            }\n        }\n        DeadCodeCategory::Unreachable => \"No callers reachable from entrypoints\".to_string(),\n        DeadCodeCategory::ReachableUnused => \"Reachable but unused in execution\".to_string(),\n    }\n}\n\nfn is_test_path(path: &Path) -> bool {\n    path.components().any(|c| {\n        let name = c.as_os_str().to_str().unwrap_or(\"\");\n        name == \"tests\" || name == \"test\" || name == \"benches\"\n    })\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Dead code analysis pipeline runner.\n\nuse anyhow::Result;\nuse crate::dead_code_actions::recommend_action;\nuse crate::dead_code_call_graph::build_call_graph;\nuse crate::dead_code_classifier::{classify_symbol, is_reachable};\nuse crate::dead_code_confidence::{assign_confidence, Evidence};\nuse crate::dead_code_entrypoints::{collect_entrypoints, collect_exports, is_public_api};\nuse crate::dead_code_intent::{detect_intent_signals, DeadCodePolicy};\nuse crate::dead_code_report::{build_report, write_report, DeadCodeReportMetadata, DeadCodeReportWithMeta};\nuse crate::dead_code_report_split::write_summary_markdown;\nuse crate::dead_code_test_boundaries::{detect_test_modules, detect_test_symbols, TestBoundaries};\nuse crate::dead_code_types::{DeadCodeCategory, DeadCodeItem};\nuse crate::layer_utilities::gather_rust_files;\nuse crate::types::{CodeElement, ElementType, Language, Visibility};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\n#[derive(Debug, Clone)]\npub struct DeadCodeRunConfig {\n    pub root: PathBuf,\n    pub output_dir: PathBuf,\n    pub policy: Option<DeadCodePolicy>,\n    pub write_json: Option<PathBuf>,\n    pub write_summary: Option<PathBuf>,\n    pub summary_limit: usize,\n}\n\npub fn run_dead_code_pipeline(\n    elements: &[CodeElement],\n    config: &DeadCodeRunConfig,\n) -> Result<DeadCodeReportWithMeta> {\n    let rust_files = gather_rust_files(&config.root);\n    let mut intent_map = HashMap::new();\n    let mut test_boundaries = TestBoundaries::default();\n\n    for file in &rust_files {\n        let intents = detect_intent_signals(file, config.policy.as_ref());\n        merge_intent_map(&mut intent_map, intents);\n        let test_modules = detect_test_modules(file);\n        test_boundaries.test_modules.extend(test_modules);\n        let test_symbols = detect_test_symbols(file);\n        test_boundaries.test_symbols.extend(test_symbols);\n        if is_test_path(file) {\n            test_boundaries.test_files.insert(file.clone());\n        }\n    }\n\n    let call_graph = build_call_graph(elements);\n    let entrypoints = collect_entrypoints(elements, config.policy.as_ref());\n    let exports = collect_exports(&config.root);\n\n    let mut items = Vec::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let category = classify_symbol(\n            &element.name,\n            &call_graph,\n            &intent_map,\n            &test_boundaries,\n            &entrypoints,\n            config.policy.as_ref(),\n        );\n        let intent_tag = intent_map.contains_key(&element.name);\n        let test_reference = test_boundaries.test_symbols.contains(&element.name);\n        let call_graph_proven =\n            category == DeadCodeCategory::Unreachable && is_reachable(&element.name, &call_graph, &entrypoints) == false;\n\n        let mut item = DeadCodeItem {\n            symbol: element.name.clone(),\n            file: PathBuf::from(&element.file_path),\n            line: element.line_number,\n            category,\n            confidence: crate::dead_code_types::ConfidenceLevel::Heuristic,\n            action: crate::dead_code_types::RecommendedAction::ManualReview,\n            reason: reason_for_category(category, intent_tag, test_reference),\n        };\n\n        let confidence = assign_confidence(\n            &item,\n            &Evidence {\n                intent_tag,\n                test_reference,\n                call_graph_proven,\n            },\n        );\n        item.confidence = confidence;\n\n        let public_api = is_public_api(&element.name, &exports)\n            || matches!(element.visibility, Visibility::Public);\n        item.action = recommend_action(category, confidence, public_api);\n\n        items.push(item);\n    }\n\n    let metadata = DeadCodeReportMetadata {\n        analyzer_version: env!(\"CARGO_PKG_VERSION\").to_string(),\n        project_root: config.root.display().to_string(),\n        entrypoints_found: entrypoints.len(),\n    };\n    let report = build_report(\n        chrono::Local::now().to_rfc3339(),\n        items,\n        metadata,\n    );\n\n    write_outputs(&report, config)?;\n    Ok(report)\n}\n\n\n\nfn merge_intent_map(base: &mut HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>, next: HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>) {\n    for (symbol, items) in next {\n        base.entry(symbol).or_default().extend(items);\n    }\n}\n\nfn reason_for_category(category: DeadCodeCategory, intent_tag: bool, test_reference: bool) -> String {\n    match category {\n        DeadCodeCategory::LatentPlanned => {\n            if intent_tag {\n                \"Intent tag present\".to_string()\n            } else {\n                \"Intent directory policy\".to_string()\n            }\n        }\n        DeadCodeCategory::TestOnly => {\n            if test_reference {\n                \"Called only by test symbols\".to_string()\n            } else {\n                \"Defined in test-only module\".to_string()\n            }\n        }\n        DeadCodeCategory::Unreachable => \"No callers reachable from entrypoints\".to_string(),\n        DeadCodeCategory::ReachableUnused => \"Reachable but unused in execution\".to_string(),\n    }\n}\n\nfn is_test_path(path: &Path) -> bool {\n    path.components().any(|c| {\n        let name = c.as_os_str().to_str().unwrap_or(\"\");\n        name == \"tests\" || name == \"test\" || name == \"benches\"\n    })\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/220_dead_code_report.rs",
          "original_content": "#![allow(dead_code)]\n//! JSON report generator for dead code analysis.\n\nuse crate::dead_code_types::{DeadCodeItem, DeadCodeReport, DeadCodeSummary};\nuse serde::{Deserialize, Serialize};\nuse std::path::Path;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadCodeReportMetadata {\n    pub analyzer_version: String,\n    pub project_root: String,\n    pub entrypoints_found: usize,\n}\n\npub fn build_report(\n    timestamp: String,\n    items: Vec<DeadCodeItem>,\n    metadata: DeadCodeReportMetadata,\n) -> DeadCodeReportWithMeta {\n    let summary = DeadCodeSummary::from_items(&items);\n    DeadCodeReportWithMeta {\n        timestamp,\n        summary,\n        items,\n        metadata,\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadCodeReportWithMeta {\n    pub timestamp: String,\n    pub summary: DeadCodeSummary,\n    pub items: Vec<DeadCodeItem>,\n    pub metadata: DeadCodeReportMetadata,\n}\n\npub fn write_report(path: &Path, report: &DeadCodeReportWithMeta) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(report)?;\n    std::fs::write(path, json)\n}\n\npub fn build_basic_report(timestamp: String, items: Vec<DeadCodeItem>) -> DeadCodeReport {\n    let summary = DeadCodeSummary::from_items(&items);\n    DeadCodeReport {\n        timestamp,\n        summary,\n        items,\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! JSON report generator for dead code analysis.\n\nuse crate::dead_code_types::{DeadCodeItem, DeadCodeReport, DeadCodeSummary};\nuse serde::{Deserialize, Serialize};\nuse std::path::Path;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadCodeReportMetadata {\n    pub analyzer_version: String,\n    pub project_root: String,\n    pub entrypoints_found: usize,\n}\n\npub fn build_report(\n    timestamp: String,\n    items: Vec<DeadCodeItem>,\n    metadata: DeadCodeReportMetadata,\n) -> DeadCodeReportWithMeta {\n    let summary = DeadCodeSummary::from_items(&items);\n    DeadCodeReportWithMeta {\n        timestamp,\n        summary,\n        items,\n        metadata,\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadCodeReportWithMeta {\n    pub timestamp: String,\n    pub summary: DeadCodeSummary,\n    pub items: Vec<DeadCodeItem>,\n    pub metadata: DeadCodeReportMetadata,\n}\n\npub fn write_report(path: &Path, report: &DeadCodeReportWithMeta) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(report)?;\n    std::fs::write(path, json)\n}\n\npub fn build_basic_report(timestamp: String, items: Vec<DeadCodeItem>) -> DeadCodeReport {\n    let summary = DeadCodeSummary::from_items(&items);\n    DeadCodeReport {\n        timestamp,\n        summary,\n        items,\n    }\n}\n\nfn write_outputs(report: &DeadCodeReportWithMeta, config: &DeadCodeRunConfig) -> Result<()> {\n    let json_path = config\n        .write_json\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_full.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_report(&json_path, report)?;\n\n    let summary_path = config\n        .write_summary\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_summary.md\"));\n    if let Some(parent) = summary_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_summary_markdown(&summary_path, report, config.summary_limit)?;\n    Ok(())\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_estimate_impact_to_src/229_quality_delta_calculator.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/230_action_impact_estimator.rs",
          "original_content": "#![allow(dead_code)]\n//! Action impact estimator.\n\nuse crate::correction_plan_types::RefactorAction;\nuse crate::quality_delta_calculator::{calculate_quality_delta, Metrics};\nuse crate::quality_delta_types::QualityDelta;\n\n#[derive(Clone, Debug)]\npub struct AnalysisState {\n    pub metrics: Metrics,\n}\n\npub fn estimate_impact(action: &RefactorAction, current_state: &AnalysisState) -> QualityDelta {\n    let simulated = simulate_action(action, current_state);\n    calculate_quality_delta(action, &current_state.metrics, &simulated.metrics)\n}\n\nfn simulate_action(_action: &RefactorAction, state: &AnalysisState) -> AnalysisState {\n    state.clone()\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Action impact estimator.\n\nuse crate::correction_plan_types::RefactorAction;\nuse crate::quality_delta_calculator::{calculate_quality_delta, Metrics};\nuse crate::quality_delta_types::QualityDelta;\n\n#[derive(Clone, Debug)]\npub struct AnalysisState {\n    pub metrics: Metrics,\n}\n\n\n\nfn simulate_action(_action: &RefactorAction, state: &AnalysisState) -> AnalysisState {\n    state.clone()\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/229_quality_delta_calculator.rs",
          "original_content": "#![allow(dead_code)]\n//! Quality delta calculator.\n\nuse crate::correction_plan_types::RefactorAction;\nuse crate::quality_delta_types::QualityDelta;\n\n#[derive(Clone, Debug, Default)]\npub struct Metrics {\n    pub cohesion: f64,\n    pub violations: usize,\n    pub complexity: f64,\n}\n\npub fn calculate_quality_delta(\n    action: &RefactorAction,\n    current: &Metrics,\n    simulated: &Metrics,\n) -> QualityDelta {\n    let cohesion_delta = simulated.cohesion - current.cohesion;\n    let violation_delta = simulated.violations as i32 - current.violations as i32;\n    let complexity_delta = simulated.complexity - current.complexity;\n    let overall = 0.5 * cohesion_delta - 0.3 * violation_delta as f64 - 0.2 * complexity_delta;\n    let acceptable = overall > -0.05 && violation_delta <= 0;\n    let reason = if acceptable {\n        \"Quality improved or maintained\".to_string()\n    } else if overall < -0.1 {\n        \"Quality degradation exceeds threshold\".to_string()\n    } else if violation_delta > 0 {\n        format!(\"Introduced {} new violations\", violation_delta)\n    } else {\n        \"Quality barely acceptable\".to_string()\n    };\n    QualityDelta {\n        action_id: action.action_id(),\n        cohesion_delta,\n        violation_delta,\n        complexity_delta,\n        overall_score_delta: overall,\n        acceptable,\n        reason,\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Quality delta calculator.\n\nuse crate::correction_plan_types::RefactorAction;\nuse crate::quality_delta_types::QualityDelta;\n\n#[derive(Clone, Debug, Default)]\npub struct Metrics {\n    pub cohesion: f64,\n    pub violations: usize,\n    pub complexity: f64,\n}\n\npub fn calculate_quality_delta(\n    action: &RefactorAction,\n    current: &Metrics,\n    simulated: &Metrics,\n) -> QualityDelta {\n    let cohesion_delta = simulated.cohesion - current.cohesion;\n    let violation_delta = simulated.violations as i32 - current.violations as i32;\n    let complexity_delta = simulated.complexity - current.complexity;\n    let overall = 0.5 * cohesion_delta - 0.3 * violation_delta as f64 - 0.2 * complexity_delta;\n    let acceptable = overall > -0.05 && violation_delta <= 0;\n    let reason = if acceptable {\n        \"Quality improved or maintained\".to_string()\n    } else if overall < -0.1 {\n        \"Quality degradation exceeds threshold\".to_string()\n    } else if violation_delta > 0 {\n        format!(\"Introduced {} new violations\", violation_delta)\n    } else {\n        \"Quality barely acceptable\".to_string()\n    };\n    QualityDelta {\n        action_id: action.action_id(),\n        cohesion_delta,\n        violation_delta,\n        complexity_delta,\n        overall_score_delta: overall,\n        acceptable,\n        reason,\n    }\n}\n\npub fn estimate_impact(action: &RefactorAction, current_state: &AnalysisState) -> QualityDelta {\n    let simulated = simulate_action(action, current_state);\n    calculate_quality_delta(action, &current_state.metrics, &simulated.metrics)\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_generate_intelligence_report_to_src/223_violation_predictor.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/233_correction_intelligence_report.rs",
          "original_content": "#![allow(dead_code)]\n//! Correction intelligence report generator.\n\nuse crate::action_impact_estimator::{estimate_impact, AnalysisState as ImpactState};\nuse crate::correction_plan_generator::generate_correction_plan;\nuse crate::correction_plan_serializer::serialize_correction_plan;\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction, ViolationPrediction};\nuse crate::quality_delta_calculator::Metrics;\nuse crate::quality_delta_types::{QualityDelta, RollbackCriteria};\nuse crate::rollback_criteria_builder::build_rollback_criteria;\nuse crate::verification_policy_emitter::emit_verification_policy;\nuse crate::verification_scope_planner::plan_verification_scope;\nuse crate::violation_predictor::predict_violations;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{AnalysisResult, CallGraphNode, CodeElement};\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionIntelligenceReport {\n    pub version: String,\n    pub timestamp: String,\n    pub project_root: PathBuf,\n    pub actions_analyzed: usize,\n    pub correction_plans: Vec<CorrectionPlan>,\n    pub verification_policies: Vec<crate::verification_policy_types::VerificationPolicy>,\n    pub rollback_criteria: Vec<RollbackCriteria>,\n    pub quality_deltas: Vec<QualityDelta>,\n    pub summary: CorrectionSummary,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionSummary {\n    pub trivial_count: usize,\n    pub moderate_count: usize,\n    pub complex_count: usize,\n    pub total_predicted_violations: usize,\n    pub average_confidence: f64,\n    pub estimated_total_fix_time_seconds: u32,\n}\n\n#[derive(Clone, Debug)]\npub struct IntelligenceState<'a> {\n    pub root: PathBuf,\n    pub invariants: &'a InvariantAnalysisResult,\n    pub call_graph: &'a HashMap<String, CallGraphNode>,\n    pub elements: &'a [CodeElement],\n    pub metrics: Metrics,\n}\n\npub fn build_state<'a>(\n    root: &'a Path,\n    analysis: &'a AnalysisResult,\n    metrics: Metrics,\n) -> IntelligenceState<'a> {\n    IntelligenceState {\n        root: root.to_path_buf(),\n        invariants: &analysis.invariants,\n        call_graph: &analysis.call_graph,\n        elements: &analysis.elements,\n        metrics,\n    }\n}\n\npub fn generate_intelligence_report(\n    actions: &[RefactorAction],\n    state: &IntelligenceState<'_>,\n) -> CorrectionIntelligenceReport {\n    let mut plans = Vec::new();\n    let mut policies = Vec::new();\n    let mut criteria = Vec::new();\n    let mut deltas = Vec::new();\n\n    for action in actions {\n        let mut predictions =\n            predict_violations(action, state.invariants, state.call_graph, state.elements);\n        fill_prediction_confidence(&mut predictions);\n        let plan = generate_correction_plan(action, &predictions);\n        let policy = plan_verification_scope(action, &plan);\n        let rollback = build_rollback_criteria(action, &plan);\n        let delta = estimate_impact(action, &ImpactState {\n            metrics: state.metrics.clone(),\n        });\n\n        plans.push(plan);\n        policies.push(policy);\n        criteria.push(rollback);\n        deltas.push(delta);\n    }\n\n    let summary = compute_summary(&plans, &deltas);\n\n    CorrectionIntelligenceReport {\n        version: \"1.0\".to_string(),\n        timestamp: chrono::Utc::now().to_rfc3339(),\n        project_root: state.root.clone(),\n        actions_analyzed: actions.len(),\n        correction_plans: plans,\n        verification_policies: policies,\n        rollback_criteria: criteria,\n        quality_deltas: deltas,\n        summary,\n    }\n}\n\npub fn write_intelligence_outputs(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n) -> std::io::Result<()> {\n    write_intelligence_outputs_at(report, output_dir, None, None)\n}\n\npub fn write_intelligence_outputs_at(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n    correction_json: Option<&Path>,\n    verification_policy_json: Option<&Path>,\n) -> std::io::Result<()> {\n    std::fs::create_dir_all(output_dir)?;\n    let json_path = correction_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"correction_intelligence.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    let contract = serialize_correction_plans(report);\n    std::fs::write(&json_path, serde_json::to_string_pretty(&contract)?)?;\n\n    let policy_path = verification_policy_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"verification_policy.json\"));\n    if let Some(parent) = policy_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    emit_verification_policy(&report.verification_policies, &policy_path)?;\n    Ok(())\n}\n\npub fn serialize_correction_plans(\n    report: &CorrectionIntelligenceReport,\n) -> serde_json::Value {\n    let items = report\n        .correction_plans\n        .iter()\n        .zip(report.verification_policies.iter())\n        .zip(report.rollback_criteria.iter())\n        .map(|((plan, policy), rollback)| serialize_correction_plan(plan, policy, rollback))\n        .collect::<Vec<_>>();\n    json!({\n        \"version\": report.version,\n        \"timestamp\": report.timestamp,\n        \"project_root\": report.project_root,\n        \"actions_analyzed\": report.actions_analyzed,\n        \"correction_plans\": items,\n        \"quality_deltas\": report.quality_deltas,\n        \"summary\": report.summary,\n    })\n}\n\nfn compute_summary(plans: &[CorrectionPlan], deltas: &[QualityDelta]) -> CorrectionSummary {\n    let mut trivial = 0;\n    let mut moderate = 0;\n    let mut complex = 0;\n    let mut total_violations = 0;\n    let mut total_confidence = 0.0;\n    let mut total_time = 0;\n\n    for plan in plans {\n        match plan.tier {\n            ErrorTier::Trivial => trivial += 1,\n            ErrorTier::Moderate => moderate += 1,\n            ErrorTier::Complex => complex += 1,\n        }\n        total_violations += plan.predicted_violations.len();\n        total_confidence += plan.confidence;\n        total_time += plan.estimated_fix_time_seconds;\n    }\n\n    let avg_conf = if plans.is_empty() {\n        0.0\n    } else {\n        total_confidence / plans.len() as f64\n    };\n\n    let _ = deltas;\n\n    CorrectionSummary {\n        trivial_count: trivial,\n        moderate_count: moderate,\n        complex_count: complex,\n        total_predicted_violations: total_violations,\n        average_confidence: avg_conf,\n        estimated_total_fix_time_seconds: total_time,\n    }\n}\n\nfn fill_prediction_confidence(predictions: &mut [ViolationPrediction]) {\n    for prediction in predictions {\n        if prediction.confidence <= 0.0 {\n            prediction.confidence = default_confidence(&prediction.violation_type);\n        }\n    }\n}\n\nfn default_confidence(violation_type: &crate::correction_plan_types::ViolationType) -> f64 {\n    match violation_type {\n        crate::correction_plan_types::ViolationType::UnresolvedImport => 0.95,\n        crate::correction_plan_types::ViolationType::NameCollision => 1.0,\n        crate::correction_plan_types::ViolationType::LayerViolation => 0.9,\n        crate::correction_plan_types::ViolationType::BrokenReference => 0.85,\n        crate::correction_plan_types::ViolationType::TypeMismatch => 0.6,\n        crate::correction_plan_types::ViolationType::OwnershipIssue => 0.5,\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Correction intelligence report generator.\n\nuse crate::action_impact_estimator::{estimate_impact, AnalysisState as ImpactState};\nuse crate::correction_plan_generator::generate_correction_plan;\nuse crate::correction_plan_serializer::serialize_correction_plan;\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction, ViolationPrediction};\nuse crate::quality_delta_calculator::Metrics;\nuse crate::quality_delta_types::{QualityDelta, RollbackCriteria};\nuse crate::rollback_criteria_builder::build_rollback_criteria;\nuse crate::verification_policy_emitter::emit_verification_policy;\nuse crate::verification_scope_planner::plan_verification_scope;\nuse crate::violation_predictor::predict_violations;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{AnalysisResult, CallGraphNode, CodeElement};\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionIntelligenceReport {\n    pub version: String,\n    pub timestamp: String,\n    pub project_root: PathBuf,\n    pub actions_analyzed: usize,\n    pub correction_plans: Vec<CorrectionPlan>,\n    pub verification_policies: Vec<crate::verification_policy_types::VerificationPolicy>,\n    pub rollback_criteria: Vec<RollbackCriteria>,\n    pub quality_deltas: Vec<QualityDelta>,\n    pub summary: CorrectionSummary,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionSummary {\n    pub trivial_count: usize,\n    pub moderate_count: usize,\n    pub complex_count: usize,\n    pub total_predicted_violations: usize,\n    pub average_confidence: f64,\n    pub estimated_total_fix_time_seconds: u32,\n}\n\n#[derive(Clone, Debug)]\npub struct IntelligenceState<'a> {\n    pub root: PathBuf,\n    pub invariants: &'a InvariantAnalysisResult,\n    pub call_graph: &'a HashMap<String, CallGraphNode>,\n    pub elements: &'a [CodeElement],\n    pub metrics: Metrics,\n}\n\npub fn build_state<'a>(\n    root: &'a Path,\n    analysis: &'a AnalysisResult,\n    metrics: Metrics,\n) -> IntelligenceState<'a> {\n    IntelligenceState {\n        root: root.to_path_buf(),\n        invariants: &analysis.invariants,\n        call_graph: &analysis.call_graph,\n        elements: &analysis.elements,\n        metrics,\n    }\n}\n\n\n\npub fn write_intelligence_outputs(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n) -> std::io::Result<()> {\n    write_intelligence_outputs_at(report, output_dir, None, None)\n}\n\npub fn write_intelligence_outputs_at(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n    correction_json: Option<&Path>,\n    verification_policy_json: Option<&Path>,\n) -> std::io::Result<()> {\n    std::fs::create_dir_all(output_dir)?;\n    let json_path = correction_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"correction_intelligence.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    let contract = serialize_correction_plans(report);\n    std::fs::write(&json_path, serde_json::to_string_pretty(&contract)?)?;\n\n    let policy_path = verification_policy_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"verification_policy.json\"));\n    if let Some(parent) = policy_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    emit_verification_policy(&report.verification_policies, &policy_path)?;\n    Ok(())\n}\n\npub fn serialize_correction_plans(\n    report: &CorrectionIntelligenceReport,\n) -> serde_json::Value {\n    let items = report\n        .correction_plans\n        .iter()\n        .zip(report.verification_policies.iter())\n        .zip(report.rollback_criteria.iter())\n        .map(|((plan, policy), rollback)| serialize_correction_plan(plan, policy, rollback))\n        .collect::<Vec<_>>();\n    json!({\n        \"version\": report.version,\n        \"timestamp\": report.timestamp,\n        \"project_root\": report.project_root,\n        \"actions_analyzed\": report.actions_analyzed,\n        \"correction_plans\": items,\n        \"quality_deltas\": report.quality_deltas,\n        \"summary\": report.summary,\n    })\n}\n\nfn compute_summary(plans: &[CorrectionPlan], deltas: &[QualityDelta]) -> CorrectionSummary {\n    let mut trivial = 0;\n    let mut moderate = 0;\n    let mut complex = 0;\n    let mut total_violations = 0;\n    let mut total_confidence = 0.0;\n    let mut total_time = 0;\n\n    for plan in plans {\n        match plan.tier {\n            ErrorTier::Trivial => trivial += 1,\n            ErrorTier::Moderate => moderate += 1,\n            ErrorTier::Complex => complex += 1,\n        }\n        total_violations += plan.predicted_violations.len();\n        total_confidence += plan.confidence;\n        total_time += plan.estimated_fix_time_seconds;\n    }\n\n    let avg_conf = if plans.is_empty() {\n        0.0\n    } else {\n        total_confidence / plans.len() as f64\n    };\n\n    let _ = deltas;\n\n    CorrectionSummary {\n        trivial_count: trivial,\n        moderate_count: moderate,\n        complex_count: complex,\n        total_predicted_violations: total_violations,\n        average_confidence: avg_conf,\n        estimated_total_fix_time_seconds: total_time,\n    }\n}\n\nfn fill_prediction_confidence(predictions: &mut [ViolationPrediction]) {\n    for prediction in predictions {\n        if prediction.confidence <= 0.0 {\n            prediction.confidence = default_confidence(&prediction.violation_type);\n        }\n    }\n}\n\nfn default_confidence(violation_type: &crate::correction_plan_types::ViolationType) -> f64 {\n    match violation_type {\n        crate::correction_plan_types::ViolationType::UnresolvedImport => 0.95,\n        crate::correction_plan_types::ViolationType::NameCollision => 1.0,\n        crate::correction_plan_types::ViolationType::LayerViolation => 0.9,\n        crate::correction_plan_types::ViolationType::BrokenReference => 0.85,\n        crate::correction_plan_types::ViolationType::TypeMismatch => 0.6,\n        crate::correction_plan_types::ViolationType::OwnershipIssue => 0.5,\n    }\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/223_violation_predictor.rs",
          "original_content": "#![allow(dead_code)]\n//! Predict violations for refactor actions.\n\nuse crate::correction_plan_types::{\n    RefactorAction, Severity, ViolationPrediction, ViolationType,\n};\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{CallGraphNode, CodeElement};\nuse std::collections::{HashMap, HashSet};\nuse std::path::PathBuf;\n\npub fn predict_violations(\n    action: &RefactorAction,\n    invariants: &InvariantAnalysisResult,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<ViolationPrediction> {\n    let mut predictions = Vec::new();\n    match action {\n        RefactorAction::MoveFunction { function, from, to, required_layer } => {\n            let callers = find_callers(function, call_graph, elements);\n            if !callers.is_empty() {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::UnresolvedImport,\n                    affected_files: callers,\n                    severity: Severity::Critical,\n                    confidence: 0.95,\n                });\n            }\n            if let Some(layer) = required_layer {\n                if !layer.is_empty() {\n                    predictions.push(ViolationPrediction {\n                        violation_type: ViolationType::LayerViolation,\n                        affected_files: vec![to.clone()],\n                        severity: Severity::High,\n                        confidence: 1.0,\n                    });\n                }\n            } else if move_violates_invariant(function, from, to, invariants) {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::LayerViolation,\n                    affected_files: vec![to.clone()],\n                    severity: Severity::High,\n                    confidence: 0.9,\n                });\n            }\n        }\n        RefactorAction::RenameFunction { old_name, new_name, file } => {\n            if symbol_exists(new_name, elements) {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::NameCollision,\n                    affected_files: vec![file.clone()],\n                    severity: Severity::Critical,\n                    confidence: 1.0,\n                });\n            }\n            let references = find_reference_files(old_name, call_graph, elements);\n            if !references.is_empty() {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::BrokenReference,\n                    affected_files: references,\n                    severity: Severity::Critical,\n                    confidence: 0.85,\n                });\n            }\n        }\n        RefactorAction::RenameFile { from, to } => {\n            predictions.push(ViolationPrediction {\n                violation_type: ViolationType::BrokenReference,\n                affected_files: vec![from.clone(), to.clone()],\n                severity: Severity::High,\n                confidence: 0.7,\n            });\n        }\n        RefactorAction::CreateFile { path } => {\n            predictions.push(ViolationPrediction {\n                violation_type: ViolationType::UnresolvedImport,\n                affected_files: vec![path.clone()],\n                severity: Severity::Low,\n                confidence: 0.5,\n            });\n        }\n    }\n    predictions\n}\n\nfn find_callers(\n    function: &str,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<PathBuf> {\n    let mut files = HashSet::new();\n    if let Some(node) = call_graph.get(function) {\n        for caller in &node.called_by {\n            if let Some(file) = find_element_file(caller, elements) {\n                files.insert(file);\n            }\n        }\n    }\n    files.into_iter().collect()\n}\n\nfn find_reference_files(\n    function: &str,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<PathBuf> {\n    let mut files = HashSet::new();\n    for (caller, node) in call_graph {\n        if node.calls.iter().any(|c| c == function) {\n            if let Some(file) = find_element_file(caller, elements) {\n                files.insert(file);\n            }\n        }\n    }\n    files.into_iter().collect()\n}\n\nfn find_element_file(function: &str, elements: &[CodeElement]) -> Option<PathBuf> {\n    elements\n        .iter()\n        .find(|el| el.name == function)\n        .map(|el| PathBuf::from(&el.file_path))\n}\n\nfn symbol_exists(symbol: &str, elements: &[CodeElement]) -> bool {\n    elements.iter().any(|el| el.name == symbol)\n}\n\nfn move_violates_invariant(\n    _function: &str,\n    _from: &PathBuf,\n    _to: &PathBuf,\n    _invariants: &InvariantAnalysisResult,\n) -> bool {\n    false\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Predict violations for refactor actions.\n\nuse crate::correction_plan_types::{\n    RefactorAction, Severity, ViolationPrediction, ViolationType,\n};\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{CallGraphNode, CodeElement};\nuse std::collections::{HashMap, HashSet};\nuse std::path::PathBuf;\n\npub fn predict_violations(\n    action: &RefactorAction,\n    invariants: &InvariantAnalysisResult,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<ViolationPrediction> {\n    let mut predictions = Vec::new();\n    match action {\n        RefactorAction::MoveFunction { function, from, to, required_layer } => {\n            let callers = find_callers(function, call_graph, elements);\n            if !callers.is_empty() {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::UnresolvedImport,\n                    affected_files: callers,\n                    severity: Severity::Critical,\n                    confidence: 0.95,\n                });\n            }\n            if let Some(layer) = required_layer {\n                if !layer.is_empty() {\n                    predictions.push(ViolationPrediction {\n                        violation_type: ViolationType::LayerViolation,\n                        affected_files: vec![to.clone()],\n                        severity: Severity::High,\n                        confidence: 1.0,\n                    });\n                }\n            } else if move_violates_invariant(function, from, to, invariants) {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::LayerViolation,\n                    affected_files: vec![to.clone()],\n                    severity: Severity::High,\n                    confidence: 0.9,\n                });\n            }\n        }\n        RefactorAction::RenameFunction { old_name, new_name, file } => {\n            if symbol_exists(new_name, elements) {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::NameCollision,\n                    affected_files: vec![file.clone()],\n                    severity: Severity::Critical,\n                    confidence: 1.0,\n                });\n            }\n            let references = find_reference_files(old_name, call_graph, elements);\n            if !references.is_empty() {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::BrokenReference,\n                    affected_files: references,\n                    severity: Severity::Critical,\n                    confidence: 0.85,\n                });\n            }\n        }\n        RefactorAction::RenameFile { from, to } => {\n            predictions.push(ViolationPrediction {\n                violation_type: ViolationType::BrokenReference,\n                affected_files: vec![from.clone(), to.clone()],\n                severity: Severity::High,\n                confidence: 0.7,\n            });\n        }\n        RefactorAction::CreateFile { path } => {\n            predictions.push(ViolationPrediction {\n                violation_type: ViolationType::UnresolvedImport,\n                affected_files: vec![path.clone()],\n                severity: Severity::Low,\n                confidence: 0.5,\n            });\n        }\n    }\n    predictions\n}\n\nfn find_callers(\n    function: &str,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<PathBuf> {\n    let mut files = HashSet::new();\n    if let Some(node) = call_graph.get(function) {\n        for caller in &node.called_by {\n            if let Some(file) = find_element_file(caller, elements) {\n                files.insert(file);\n            }\n        }\n    }\n    files.into_iter().collect()\n}\n\nfn find_reference_files(\n    function: &str,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<PathBuf> {\n    let mut files = HashSet::new();\n    for (caller, node) in call_graph {\n        if node.calls.iter().any(|c| c == function) {\n            if let Some(file) = find_element_file(caller, elements) {\n                files.insert(file);\n            }\n        }\n    }\n    files.into_iter().collect()\n}\n\nfn find_element_file(function: &str, elements: &[CodeElement]) -> Option<PathBuf> {\n    elements\n        .iter()\n        .find(|el| el.name == function)\n        .map(|el| PathBuf::from(&el.file_path))\n}\n\nfn symbol_exists(symbol: &str, elements: &[CodeElement]) -> bool {\n    elements.iter().any(|el| el.name == symbol)\n}\n\nfn move_violates_invariant(\n    _function: &str,\n    _from: &PathBuf,\n    _to: &PathBuf,\n    _invariants: &InvariantAnalysisResult,\n) -> bool {\n    false\n}\n\npub fn generate_intelligence_report(\n    actions: &[RefactorAction],\n    state: &IntelligenceState<'_>,\n) -> CorrectionIntelligenceReport {\n    let mut plans = Vec::new();\n    let mut policies = Vec::new();\n    let mut criteria = Vec::new();\n    let mut deltas = Vec::new();\n\n    for action in actions {\n        let mut predictions =\n            predict_violations(action, state.invariants, state.call_graph, state.elements);\n        fill_prediction_confidence(&mut predictions);\n        let plan = generate_correction_plan(action, &predictions);\n        let policy = plan_verification_scope(action, &plan);\n        let rollback = build_rollback_criteria(action, &plan);\n        let delta = estimate_impact(action, &ImpactState {\n            metrics: state.metrics.clone(),\n        });\n\n        plans.push(plan);\n        policies.push(policy);\n        criteria.push(rollback);\n        deltas.push(delta);\n    }\n\n    let summary = compute_summary(&plans, &deltas);\n\n    CorrectionIntelligenceReport {\n        version: \"1.0\".to_string(),\n        timestamp: chrono::Utc::now().to_rfc3339(),\n        project_root: state.root.clone(),\n        actions_analyzed: actions.len(),\n        correction_plans: plans,\n        verification_policies: policies,\n        rollback_criteria: criteria,\n        quality_deltas: deltas,\n        summary,\n    }\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_write_intelligence_outputs_at_to_src/232_verification_policy_emitter.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/233_correction_intelligence_report.rs",
          "original_content": "#![allow(dead_code)]\n//! Correction intelligence report generator.\n\nuse crate::action_impact_estimator::{estimate_impact, AnalysisState as ImpactState};\nuse crate::correction_plan_generator::generate_correction_plan;\nuse crate::correction_plan_serializer::serialize_correction_plan;\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction, ViolationPrediction};\nuse crate::quality_delta_calculator::Metrics;\nuse crate::quality_delta_types::{QualityDelta, RollbackCriteria};\nuse crate::rollback_criteria_builder::build_rollback_criteria;\nuse crate::verification_policy_emitter::emit_verification_policy;\nuse crate::verification_scope_planner::plan_verification_scope;\nuse crate::violation_predictor::predict_violations;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{AnalysisResult, CallGraphNode, CodeElement};\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionIntelligenceReport {\n    pub version: String,\n    pub timestamp: String,\n    pub project_root: PathBuf,\n    pub actions_analyzed: usize,\n    pub correction_plans: Vec<CorrectionPlan>,\n    pub verification_policies: Vec<crate::verification_policy_types::VerificationPolicy>,\n    pub rollback_criteria: Vec<RollbackCriteria>,\n    pub quality_deltas: Vec<QualityDelta>,\n    pub summary: CorrectionSummary,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionSummary {\n    pub trivial_count: usize,\n    pub moderate_count: usize,\n    pub complex_count: usize,\n    pub total_predicted_violations: usize,\n    pub average_confidence: f64,\n    pub estimated_total_fix_time_seconds: u32,\n}\n\n#[derive(Clone, Debug)]\npub struct IntelligenceState<'a> {\n    pub root: PathBuf,\n    pub invariants: &'a InvariantAnalysisResult,\n    pub call_graph: &'a HashMap<String, CallGraphNode>,\n    pub elements: &'a [CodeElement],\n    pub metrics: Metrics,\n}\n\npub fn build_state<'a>(\n    root: &'a Path,\n    analysis: &'a AnalysisResult,\n    metrics: Metrics,\n) -> IntelligenceState<'a> {\n    IntelligenceState {\n        root: root.to_path_buf(),\n        invariants: &analysis.invariants,\n        call_graph: &analysis.call_graph,\n        elements: &analysis.elements,\n        metrics,\n    }\n}\n\npub fn generate_intelligence_report(\n    actions: &[RefactorAction],\n    state: &IntelligenceState<'_>,\n) -> CorrectionIntelligenceReport {\n    let mut plans = Vec::new();\n    let mut policies = Vec::new();\n    let mut criteria = Vec::new();\n    let mut deltas = Vec::new();\n\n    for action in actions {\n        let mut predictions =\n            predict_violations(action, state.invariants, state.call_graph, state.elements);\n        fill_prediction_confidence(&mut predictions);\n        let plan = generate_correction_plan(action, &predictions);\n        let policy = plan_verification_scope(action, &plan);\n        let rollback = build_rollback_criteria(action, &plan);\n        let delta = estimate_impact(action, &ImpactState {\n            metrics: state.metrics.clone(),\n        });\n\n        plans.push(plan);\n        policies.push(policy);\n        criteria.push(rollback);\n        deltas.push(delta);\n    }\n\n    let summary = compute_summary(&plans, &deltas);\n\n    CorrectionIntelligenceReport {\n        version: \"1.0\".to_string(),\n        timestamp: chrono::Utc::now().to_rfc3339(),\n        project_root: state.root.clone(),\n        actions_analyzed: actions.len(),\n        correction_plans: plans,\n        verification_policies: policies,\n        rollback_criteria: criteria,\n        quality_deltas: deltas,\n        summary,\n    }\n}\n\npub fn write_intelligence_outputs(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n) -> std::io::Result<()> {\n    write_intelligence_outputs_at(report, output_dir, None, None)\n}\n\npub fn write_intelligence_outputs_at(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n    correction_json: Option<&Path>,\n    verification_policy_json: Option<&Path>,\n) -> std::io::Result<()> {\n    std::fs::create_dir_all(output_dir)?;\n    let json_path = correction_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"correction_intelligence.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    let contract = serialize_correction_plans(report);\n    std::fs::write(&json_path, serde_json::to_string_pretty(&contract)?)?;\n\n    let policy_path = verification_policy_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"verification_policy.json\"));\n    if let Some(parent) = policy_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    emit_verification_policy(&report.verification_policies, &policy_path)?;\n    Ok(())\n}\n\npub fn serialize_correction_plans(\n    report: &CorrectionIntelligenceReport,\n) -> serde_json::Value {\n    let items = report\n        .correction_plans\n        .iter()\n        .zip(report.verification_policies.iter())\n        .zip(report.rollback_criteria.iter())\n        .map(|((plan, policy), rollback)| serialize_correction_plan(plan, policy, rollback))\n        .collect::<Vec<_>>();\n    json!({\n        \"version\": report.version,\n        \"timestamp\": report.timestamp,\n        \"project_root\": report.project_root,\n        \"actions_analyzed\": report.actions_analyzed,\n        \"correction_plans\": items,\n        \"quality_deltas\": report.quality_deltas,\n        \"summary\": report.summary,\n    })\n}\n\nfn compute_summary(plans: &[CorrectionPlan], deltas: &[QualityDelta]) -> CorrectionSummary {\n    let mut trivial = 0;\n    let mut moderate = 0;\n    let mut complex = 0;\n    let mut total_violations = 0;\n    let mut total_confidence = 0.0;\n    let mut total_time = 0;\n\n    for plan in plans {\n        match plan.tier {\n            ErrorTier::Trivial => trivial += 1,\n            ErrorTier::Moderate => moderate += 1,\n            ErrorTier::Complex => complex += 1,\n        }\n        total_violations += plan.predicted_violations.len();\n        total_confidence += plan.confidence;\n        total_time += plan.estimated_fix_time_seconds;\n    }\n\n    let avg_conf = if plans.is_empty() {\n        0.0\n    } else {\n        total_confidence / plans.len() as f64\n    };\n\n    let _ = deltas;\n\n    CorrectionSummary {\n        trivial_count: trivial,\n        moderate_count: moderate,\n        complex_count: complex,\n        total_predicted_violations: total_violations,\n        average_confidence: avg_conf,\n        estimated_total_fix_time_seconds: total_time,\n    }\n}\n\nfn fill_prediction_confidence(predictions: &mut [ViolationPrediction]) {\n    for prediction in predictions {\n        if prediction.confidence <= 0.0 {\n            prediction.confidence = default_confidence(&prediction.violation_type);\n        }\n    }\n}\n\nfn default_confidence(violation_type: &crate::correction_plan_types::ViolationType) -> f64 {\n    match violation_type {\n        crate::correction_plan_types::ViolationType::UnresolvedImport => 0.95,\n        crate::correction_plan_types::ViolationType::NameCollision => 1.0,\n        crate::correction_plan_types::ViolationType::LayerViolation => 0.9,\n        crate::correction_plan_types::ViolationType::BrokenReference => 0.85,\n        crate::correction_plan_types::ViolationType::TypeMismatch => 0.6,\n        crate::correction_plan_types::ViolationType::OwnershipIssue => 0.5,\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Correction intelligence report generator.\n\nuse crate::action_impact_estimator::{estimate_impact, AnalysisState as ImpactState};\nuse crate::correction_plan_generator::generate_correction_plan;\nuse crate::correction_plan_serializer::serialize_correction_plan;\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction, ViolationPrediction};\nuse crate::quality_delta_calculator::Metrics;\nuse crate::quality_delta_types::{QualityDelta, RollbackCriteria};\nuse crate::rollback_criteria_builder::build_rollback_criteria;\nuse crate::verification_policy_emitter::emit_verification_policy;\nuse crate::verification_scope_planner::plan_verification_scope;\nuse crate::violation_predictor::predict_violations;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{AnalysisResult, CallGraphNode, CodeElement};\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionIntelligenceReport {\n    pub version: String,\n    pub timestamp: String,\n    pub project_root: PathBuf,\n    pub actions_analyzed: usize,\n    pub correction_plans: Vec<CorrectionPlan>,\n    pub verification_policies: Vec<crate::verification_policy_types::VerificationPolicy>,\n    pub rollback_criteria: Vec<RollbackCriteria>,\n    pub quality_deltas: Vec<QualityDelta>,\n    pub summary: CorrectionSummary,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionSummary {\n    pub trivial_count: usize,\n    pub moderate_count: usize,\n    pub complex_count: usize,\n    pub total_predicted_violations: usize,\n    pub average_confidence: f64,\n    pub estimated_total_fix_time_seconds: u32,\n}\n\n#[derive(Clone, Debug)]\npub struct IntelligenceState<'a> {\n    pub root: PathBuf,\n    pub invariants: &'a InvariantAnalysisResult,\n    pub call_graph: &'a HashMap<String, CallGraphNode>,\n    pub elements: &'a [CodeElement],\n    pub metrics: Metrics,\n}\n\npub fn build_state<'a>(\n    root: &'a Path,\n    analysis: &'a AnalysisResult,\n    metrics: Metrics,\n) -> IntelligenceState<'a> {\n    IntelligenceState {\n        root: root.to_path_buf(),\n        invariants: &analysis.invariants,\n        call_graph: &analysis.call_graph,\n        elements: &analysis.elements,\n        metrics,\n    }\n}\n\npub fn generate_intelligence_report(\n    actions: &[RefactorAction],\n    state: &IntelligenceState<'_>,\n) -> CorrectionIntelligenceReport {\n    let mut plans = Vec::new();\n    let mut policies = Vec::new();\n    let mut criteria = Vec::new();\n    let mut deltas = Vec::new();\n\n    for action in actions {\n        let mut predictions =\n            predict_violations(action, state.invariants, state.call_graph, state.elements);\n        fill_prediction_confidence(&mut predictions);\n        let plan = generate_correction_plan(action, &predictions);\n        let policy = plan_verification_scope(action, &plan);\n        let rollback = build_rollback_criteria(action, &plan);\n        let delta = estimate_impact(action, &ImpactState {\n            metrics: state.metrics.clone(),\n        });\n\n        plans.push(plan);\n        policies.push(policy);\n        criteria.push(rollback);\n        deltas.push(delta);\n    }\n\n    let summary = compute_summary(&plans, &deltas);\n\n    CorrectionIntelligenceReport {\n        version: \"1.0\".to_string(),\n        timestamp: chrono::Utc::now().to_rfc3339(),\n        project_root: state.root.clone(),\n        actions_analyzed: actions.len(),\n        correction_plans: plans,\n        verification_policies: policies,\n        rollback_criteria: criteria,\n        quality_deltas: deltas,\n        summary,\n    }\n}\n\npub fn write_intelligence_outputs(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n) -> std::io::Result<()> {\n    write_intelligence_outputs_at(report, output_dir, None, None)\n}\n\n\n\npub fn serialize_correction_plans(\n    report: &CorrectionIntelligenceReport,\n) -> serde_json::Value {\n    let items = report\n        .correction_plans\n        .iter()\n        .zip(report.verification_policies.iter())\n        .zip(report.rollback_criteria.iter())\n        .map(|((plan, policy), rollback)| serialize_correction_plan(plan, policy, rollback))\n        .collect::<Vec<_>>();\n    json!({\n        \"version\": report.version,\n        \"timestamp\": report.timestamp,\n        \"project_root\": report.project_root,\n        \"actions_analyzed\": report.actions_analyzed,\n        \"correction_plans\": items,\n        \"quality_deltas\": report.quality_deltas,\n        \"summary\": report.summary,\n    })\n}\n\nfn compute_summary(plans: &[CorrectionPlan], deltas: &[QualityDelta]) -> CorrectionSummary {\n    let mut trivial = 0;\n    let mut moderate = 0;\n    let mut complex = 0;\n    let mut total_violations = 0;\n    let mut total_confidence = 0.0;\n    let mut total_time = 0;\n\n    for plan in plans {\n        match plan.tier {\n            ErrorTier::Trivial => trivial += 1,\n            ErrorTier::Moderate => moderate += 1,\n            ErrorTier::Complex => complex += 1,\n        }\n        total_violations += plan.predicted_violations.len();\n        total_confidence += plan.confidence;\n        total_time += plan.estimated_fix_time_seconds;\n    }\n\n    let avg_conf = if plans.is_empty() {\n        0.0\n    } else {\n        total_confidence / plans.len() as f64\n    };\n\n    let _ = deltas;\n\n    CorrectionSummary {\n        trivial_count: trivial,\n        moderate_count: moderate,\n        complex_count: complex,\n        total_predicted_violations: total_violations,\n        average_confidence: avg_conf,\n        estimated_total_fix_time_seconds: total_time,\n    }\n}\n\nfn fill_prediction_confidence(predictions: &mut [ViolationPrediction]) {\n    for prediction in predictions {\n        if prediction.confidence <= 0.0 {\n            prediction.confidence = default_confidence(&prediction.violation_type);\n        }\n    }\n}\n\nfn default_confidence(violation_type: &crate::correction_plan_types::ViolationType) -> f64 {\n    match violation_type {\n        crate::correction_plan_types::ViolationType::UnresolvedImport => 0.95,\n        crate::correction_plan_types::ViolationType::NameCollision => 1.0,\n        crate::correction_plan_types::ViolationType::LayerViolation => 0.9,\n        crate::correction_plan_types::ViolationType::BrokenReference => 0.85,\n        crate::correction_plan_types::ViolationType::TypeMismatch => 0.6,\n        crate::correction_plan_types::ViolationType::OwnershipIssue => 0.5,\n    }\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/232_verification_policy_emitter.rs",
          "original_content": "#![allow(dead_code)]\n//! Emit verification policies to JSON.\n\nuse crate::verification_policy_types::{QualityThresholds, VerificationCheck, VerificationPolicy, VerificationScope};\nuse serde_json::json;\nuse std::path::Path;\n\npub fn emit_verification_policy(\n    policies: &[VerificationPolicy],\n    output_path: &Path,\n) -> std::io::Result<()> {\n    let policy_file = json!({\n        \"version\": \"1.0\",\n        \"policies\": policies.iter().map(|p| json!({\n            \"action_id\": p.action_id,\n            \"scope\": serialize_scope(&p.scope),\n            \"checks\": p.required_checks.iter()\n                .map(serialize_check)\n                .collect::<Vec<_>>(),\n            \"incremental\": p.incremental_eligible,\n            \"estimated_time_seconds\": p.estimated_time_seconds,\n        })).collect::<Vec<_>>()\n    });\n    std::fs::write(output_path, serde_json::to_string_pretty(&policy_file)?)?;\n    Ok(())\n}\n\nfn serialize_scope(scope: &VerificationScope) -> serde_json::Value {\n    match scope {\n        VerificationScope::SyntaxOnly { files } => json!({\n            \"type\": \"SyntaxOnly\",\n            \"files\": files,\n        }),\n        VerificationScope::ModuleLocal { module, transitive_depth } => json!({\n            \"type\": \"ModuleLocal\",\n            \"module\": module,\n            \"transitive_depth\": transitive_depth,\n        }),\n        VerificationScope::CallerChain { root_function } => json!({\n            \"type\": \"CallerChain\",\n            \"root_function\": root_function,\n        }),\n        VerificationScope::FullWorkspace => json!({\n            \"type\": \"FullWorkspace\",\n        }),\n    }\n}\n\nfn serialize_check(check: &VerificationCheck) -> serde_json::Value {\n    match check {\n        VerificationCheck::CargoCheck => json!({ \"type\": \"CargoCheck\" }),\n        VerificationCheck::CargoTest { filter } => json!({\n            \"type\": \"CargoTest\",\n            \"filter\": filter,\n        }),\n        VerificationCheck::InvariantValidation { invariant_ids } => json!({\n            \"type\": \"InvariantValidation\",\n            \"invariant_ids\": invariant_ids,\n        }),\n        VerificationCheck::QualityMetrics { thresholds } => json!({\n            \"type\": \"QualityMetrics\",\n            \"thresholds\": serialize_thresholds(thresholds),\n        }),\n        VerificationCheck::ManualInspection { reason } => json!({\n            \"type\": \"ManualInspection\",\n            \"reason\": reason,\n        }),\n    }\n}\n\nfn serialize_thresholds(thresholds: &QualityThresholds) -> serde_json::Value {\n    json!({\n        \"min_cohesion_delta\": thresholds.min_cohesion_delta,\n        \"max_violation_delta\": thresholds.max_violation_delta,\n        \"max_complexity_delta\": thresholds.max_complexity_delta,\n    })\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Emit verification policies to JSON.\n\nuse crate::verification_policy_types::{QualityThresholds, VerificationCheck, VerificationPolicy, VerificationScope};\nuse serde_json::json;\nuse std::path::Path;\n\npub fn emit_verification_policy(\n    policies: &[VerificationPolicy],\n    output_path: &Path,\n) -> std::io::Result<()> {\n    let policy_file = json!({\n        \"version\": \"1.0\",\n        \"policies\": policies.iter().map(|p| json!({\n            \"action_id\": p.action_id,\n            \"scope\": serialize_scope(&p.scope),\n            \"checks\": p.required_checks.iter()\n                .map(serialize_check)\n                .collect::<Vec<_>>(),\n            \"incremental\": p.incremental_eligible,\n            \"estimated_time_seconds\": p.estimated_time_seconds,\n        })).collect::<Vec<_>>()\n    });\n    std::fs::write(output_path, serde_json::to_string_pretty(&policy_file)?)?;\n    Ok(())\n}\n\nfn serialize_scope(scope: &VerificationScope) -> serde_json::Value {\n    match scope {\n        VerificationScope::SyntaxOnly { files } => json!({\n            \"type\": \"SyntaxOnly\",\n            \"files\": files,\n        }),\n        VerificationScope::ModuleLocal { module, transitive_depth } => json!({\n            \"type\": \"ModuleLocal\",\n            \"module\": module,\n            \"transitive_depth\": transitive_depth,\n        }),\n        VerificationScope::CallerChain { root_function } => json!({\n            \"type\": \"CallerChain\",\n            \"root_function\": root_function,\n        }),\n        VerificationScope::FullWorkspace => json!({\n            \"type\": \"FullWorkspace\",\n        }),\n    }\n}\n\nfn serialize_check(check: &VerificationCheck) -> serde_json::Value {\n    match check {\n        VerificationCheck::CargoCheck => json!({ \"type\": \"CargoCheck\" }),\n        VerificationCheck::CargoTest { filter } => json!({\n            \"type\": \"CargoTest\",\n            \"filter\": filter,\n        }),\n        VerificationCheck::InvariantValidation { invariant_ids } => json!({\n            \"type\": \"InvariantValidation\",\n            \"invariant_ids\": invariant_ids,\n        }),\n        VerificationCheck::QualityMetrics { thresholds } => json!({\n            \"type\": \"QualityMetrics\",\n            \"thresholds\": serialize_thresholds(thresholds),\n        }),\n        VerificationCheck::ManualInspection { reason } => json!({\n            \"type\": \"ManualInspection\",\n            \"reason\": reason,\n        }),\n    }\n}\n\nfn serialize_thresholds(thresholds: &QualityThresholds) -> serde_json::Value {\n    json!({\n        \"min_cohesion_delta\": thresholds.min_cohesion_delta,\n        \"max_violation_delta\": thresholds.max_violation_delta,\n        \"max_complexity_delta\": thresholds.max_complexity_delta,\n    })\n}\n\npub fn write_intelligence_outputs_at(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n    correction_json: Option<&Path>,\n    verification_policy_json: Option<&Path>,\n) -> std::io::Result<()> {\n    std::fs::create_dir_all(output_dir)?;\n    let json_path = correction_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"correction_intelligence.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    let contract = serialize_correction_plans(report);\n    std::fs::write(&json_path, serde_json::to_string_pretty(&contract)?)?;\n\n    let policy_path = verification_policy_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"verification_policy.json\"));\n    if let Some(parent) = policy_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    emit_verification_policy(&report.verification_policies, &policy_path)?;\n    Ok(())\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "move_serialize_correction_plans_to_src/231_correction_plan_serializer.rs",
      "mutations": [
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/233_correction_intelligence_report.rs",
          "original_content": "#![allow(dead_code)]\n//! Correction intelligence report generator.\n\nuse crate::action_impact_estimator::{estimate_impact, AnalysisState as ImpactState};\nuse crate::correction_plan_generator::generate_correction_plan;\nuse crate::correction_plan_serializer::serialize_correction_plan;\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction, ViolationPrediction};\nuse crate::quality_delta_calculator::Metrics;\nuse crate::quality_delta_types::{QualityDelta, RollbackCriteria};\nuse crate::rollback_criteria_builder::build_rollback_criteria;\nuse crate::verification_policy_emitter::emit_verification_policy;\nuse crate::verification_scope_planner::plan_verification_scope;\nuse crate::violation_predictor::predict_violations;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{AnalysisResult, CallGraphNode, CodeElement};\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionIntelligenceReport {\n    pub version: String,\n    pub timestamp: String,\n    pub project_root: PathBuf,\n    pub actions_analyzed: usize,\n    pub correction_plans: Vec<CorrectionPlan>,\n    pub verification_policies: Vec<crate::verification_policy_types::VerificationPolicy>,\n    pub rollback_criteria: Vec<RollbackCriteria>,\n    pub quality_deltas: Vec<QualityDelta>,\n    pub summary: CorrectionSummary,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionSummary {\n    pub trivial_count: usize,\n    pub moderate_count: usize,\n    pub complex_count: usize,\n    pub total_predicted_violations: usize,\n    pub average_confidence: f64,\n    pub estimated_total_fix_time_seconds: u32,\n}\n\n#[derive(Clone, Debug)]\npub struct IntelligenceState<'a> {\n    pub root: PathBuf,\n    pub invariants: &'a InvariantAnalysisResult,\n    pub call_graph: &'a HashMap<String, CallGraphNode>,\n    pub elements: &'a [CodeElement],\n    pub metrics: Metrics,\n}\n\npub fn build_state<'a>(\n    root: &'a Path,\n    analysis: &'a AnalysisResult,\n    metrics: Metrics,\n) -> IntelligenceState<'a> {\n    IntelligenceState {\n        root: root.to_path_buf(),\n        invariants: &analysis.invariants,\n        call_graph: &analysis.call_graph,\n        elements: &analysis.elements,\n        metrics,\n    }\n}\n\npub fn generate_intelligence_report(\n    actions: &[RefactorAction],\n    state: &IntelligenceState<'_>,\n) -> CorrectionIntelligenceReport {\n    let mut plans = Vec::new();\n    let mut policies = Vec::new();\n    let mut criteria = Vec::new();\n    let mut deltas = Vec::new();\n\n    for action in actions {\n        let mut predictions =\n            predict_violations(action, state.invariants, state.call_graph, state.elements);\n        fill_prediction_confidence(&mut predictions);\n        let plan = generate_correction_plan(action, &predictions);\n        let policy = plan_verification_scope(action, &plan);\n        let rollback = build_rollback_criteria(action, &plan);\n        let delta = estimate_impact(action, &ImpactState {\n            metrics: state.metrics.clone(),\n        });\n\n        plans.push(plan);\n        policies.push(policy);\n        criteria.push(rollback);\n        deltas.push(delta);\n    }\n\n    let summary = compute_summary(&plans, &deltas);\n\n    CorrectionIntelligenceReport {\n        version: \"1.0\".to_string(),\n        timestamp: chrono::Utc::now().to_rfc3339(),\n        project_root: state.root.clone(),\n        actions_analyzed: actions.len(),\n        correction_plans: plans,\n        verification_policies: policies,\n        rollback_criteria: criteria,\n        quality_deltas: deltas,\n        summary,\n    }\n}\n\npub fn write_intelligence_outputs(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n) -> std::io::Result<()> {\n    write_intelligence_outputs_at(report, output_dir, None, None)\n}\n\npub fn write_intelligence_outputs_at(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n    correction_json: Option<&Path>,\n    verification_policy_json: Option<&Path>,\n) -> std::io::Result<()> {\n    std::fs::create_dir_all(output_dir)?;\n    let json_path = correction_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"correction_intelligence.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    let contract = serialize_correction_plans(report);\n    std::fs::write(&json_path, serde_json::to_string_pretty(&contract)?)?;\n\n    let policy_path = verification_policy_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"verification_policy.json\"));\n    if let Some(parent) = policy_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    emit_verification_policy(&report.verification_policies, &policy_path)?;\n    Ok(())\n}\n\npub fn serialize_correction_plans(\n    report: &CorrectionIntelligenceReport,\n) -> serde_json::Value {\n    let items = report\n        .correction_plans\n        .iter()\n        .zip(report.verification_policies.iter())\n        .zip(report.rollback_criteria.iter())\n        .map(|((plan, policy), rollback)| serialize_correction_plan(plan, policy, rollback))\n        .collect::<Vec<_>>();\n    json!({\n        \"version\": report.version,\n        \"timestamp\": report.timestamp,\n        \"project_root\": report.project_root,\n        \"actions_analyzed\": report.actions_analyzed,\n        \"correction_plans\": items,\n        \"quality_deltas\": report.quality_deltas,\n        \"summary\": report.summary,\n    })\n}\n\nfn compute_summary(plans: &[CorrectionPlan], deltas: &[QualityDelta]) -> CorrectionSummary {\n    let mut trivial = 0;\n    let mut moderate = 0;\n    let mut complex = 0;\n    let mut total_violations = 0;\n    let mut total_confidence = 0.0;\n    let mut total_time = 0;\n\n    for plan in plans {\n        match plan.tier {\n            ErrorTier::Trivial => trivial += 1,\n            ErrorTier::Moderate => moderate += 1,\n            ErrorTier::Complex => complex += 1,\n        }\n        total_violations += plan.predicted_violations.len();\n        total_confidence += plan.confidence;\n        total_time += plan.estimated_fix_time_seconds;\n    }\n\n    let avg_conf = if plans.is_empty() {\n        0.0\n    } else {\n        total_confidence / plans.len() as f64\n    };\n\n    let _ = deltas;\n\n    CorrectionSummary {\n        trivial_count: trivial,\n        moderate_count: moderate,\n        complex_count: complex,\n        total_predicted_violations: total_violations,\n        average_confidence: avg_conf,\n        estimated_total_fix_time_seconds: total_time,\n    }\n}\n\nfn fill_prediction_confidence(predictions: &mut [ViolationPrediction]) {\n    for prediction in predictions {\n        if prediction.confidence <= 0.0 {\n            prediction.confidence = default_confidence(&prediction.violation_type);\n        }\n    }\n}\n\nfn default_confidence(violation_type: &crate::correction_plan_types::ViolationType) -> f64 {\n    match violation_type {\n        crate::correction_plan_types::ViolationType::UnresolvedImport => 0.95,\n        crate::correction_plan_types::ViolationType::NameCollision => 1.0,\n        crate::correction_plan_types::ViolationType::LayerViolation => 0.9,\n        crate::correction_plan_types::ViolationType::BrokenReference => 0.85,\n        crate::correction_plan_types::ViolationType::TypeMismatch => 0.6,\n        crate::correction_plan_types::ViolationType::OwnershipIssue => 0.5,\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Correction intelligence report generator.\n\nuse crate::action_impact_estimator::{estimate_impact, AnalysisState as ImpactState};\nuse crate::correction_plan_generator::generate_correction_plan;\nuse crate::correction_plan_serializer::serialize_correction_plan;\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction, ViolationPrediction};\nuse crate::quality_delta_calculator::Metrics;\nuse crate::quality_delta_types::{QualityDelta, RollbackCriteria};\nuse crate::rollback_criteria_builder::build_rollback_criteria;\nuse crate::verification_policy_emitter::emit_verification_policy;\nuse crate::verification_scope_planner::plan_verification_scope;\nuse crate::violation_predictor::predict_violations;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{AnalysisResult, CallGraphNode, CodeElement};\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionIntelligenceReport {\n    pub version: String,\n    pub timestamp: String,\n    pub project_root: PathBuf,\n    pub actions_analyzed: usize,\n    pub correction_plans: Vec<CorrectionPlan>,\n    pub verification_policies: Vec<crate::verification_policy_types::VerificationPolicy>,\n    pub rollback_criteria: Vec<RollbackCriteria>,\n    pub quality_deltas: Vec<QualityDelta>,\n    pub summary: CorrectionSummary,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionSummary {\n    pub trivial_count: usize,\n    pub moderate_count: usize,\n    pub complex_count: usize,\n    pub total_predicted_violations: usize,\n    pub average_confidence: f64,\n    pub estimated_total_fix_time_seconds: u32,\n}\n\n#[derive(Clone, Debug)]\npub struct IntelligenceState<'a> {\n    pub root: PathBuf,\n    pub invariants: &'a InvariantAnalysisResult,\n    pub call_graph: &'a HashMap<String, CallGraphNode>,\n    pub elements: &'a [CodeElement],\n    pub metrics: Metrics,\n}\n\npub fn build_state<'a>(\n    root: &'a Path,\n    analysis: &'a AnalysisResult,\n    metrics: Metrics,\n) -> IntelligenceState<'a> {\n    IntelligenceState {\n        root: root.to_path_buf(),\n        invariants: &analysis.invariants,\n        call_graph: &analysis.call_graph,\n        elements: &analysis.elements,\n        metrics,\n    }\n}\n\npub fn generate_intelligence_report(\n    actions: &[RefactorAction],\n    state: &IntelligenceState<'_>,\n) -> CorrectionIntelligenceReport {\n    let mut plans = Vec::new();\n    let mut policies = Vec::new();\n    let mut criteria = Vec::new();\n    let mut deltas = Vec::new();\n\n    for action in actions {\n        let mut predictions =\n            predict_violations(action, state.invariants, state.call_graph, state.elements);\n        fill_prediction_confidence(&mut predictions);\n        let plan = generate_correction_plan(action, &predictions);\n        let policy = plan_verification_scope(action, &plan);\n        let rollback = build_rollback_criteria(action, &plan);\n        let delta = estimate_impact(action, &ImpactState {\n            metrics: state.metrics.clone(),\n        });\n\n        plans.push(plan);\n        policies.push(policy);\n        criteria.push(rollback);\n        deltas.push(delta);\n    }\n\n    let summary = compute_summary(&plans, &deltas);\n\n    CorrectionIntelligenceReport {\n        version: \"1.0\".to_string(),\n        timestamp: chrono::Utc::now().to_rfc3339(),\n        project_root: state.root.clone(),\n        actions_analyzed: actions.len(),\n        correction_plans: plans,\n        verification_policies: policies,\n        rollback_criteria: criteria,\n        quality_deltas: deltas,\n        summary,\n    }\n}\n\npub fn write_intelligence_outputs(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n) -> std::io::Result<()> {\n    write_intelligence_outputs_at(report, output_dir, None, None)\n}\n\npub fn write_intelligence_outputs_at(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n    correction_json: Option<&Path>,\n    verification_policy_json: Option<&Path>,\n) -> std::io::Result<()> {\n    std::fs::create_dir_all(output_dir)?;\n    let json_path = correction_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"correction_intelligence.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    let contract = serialize_correction_plans(report);\n    std::fs::write(&json_path, serde_json::to_string_pretty(&contract)?)?;\n\n    let policy_path = verification_policy_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"verification_policy.json\"));\n    if let Some(parent) = policy_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    emit_verification_policy(&report.verification_policies, &policy_path)?;\n    Ok(())\n}\n\n\n\nfn compute_summary(plans: &[CorrectionPlan], deltas: &[QualityDelta]) -> CorrectionSummary {\n    let mut trivial = 0;\n    let mut moderate = 0;\n    let mut complex = 0;\n    let mut total_violations = 0;\n    let mut total_confidence = 0.0;\n    let mut total_time = 0;\n\n    for plan in plans {\n        match plan.tier {\n            ErrorTier::Trivial => trivial += 1,\n            ErrorTier::Moderate => moderate += 1,\n            ErrorTier::Complex => complex += 1,\n        }\n        total_violations += plan.predicted_violations.len();\n        total_confidence += plan.confidence;\n        total_time += plan.estimated_fix_time_seconds;\n    }\n\n    let avg_conf = if plans.is_empty() {\n        0.0\n    } else {\n        total_confidence / plans.len() as f64\n    };\n\n    let _ = deltas;\n\n    CorrectionSummary {\n        trivial_count: trivial,\n        moderate_count: moderate,\n        complex_count: complex,\n        total_predicted_violations: total_violations,\n        average_confidence: avg_conf,\n        estimated_total_fix_time_seconds: total_time,\n    }\n}\n\nfn fill_prediction_confidence(predictions: &mut [ViolationPrediction]) {\n    for prediction in predictions {\n        if prediction.confidence <= 0.0 {\n            prediction.confidence = default_confidence(&prediction.violation_type);\n        }\n    }\n}\n\nfn default_confidence(violation_type: &crate::correction_plan_types::ViolationType) -> f64 {\n    match violation_type {\n        crate::correction_plan_types::ViolationType::UnresolvedImport => 0.95,\n        crate::correction_plan_types::ViolationType::NameCollision => 1.0,\n        crate::correction_plan_types::ViolationType::LayerViolation => 0.9,\n        crate::correction_plan_types::ViolationType::BrokenReference => 0.85,\n        crate::correction_plan_types::ViolationType::TypeMismatch => 0.6,\n        crate::correction_plan_types::ViolationType::OwnershipIssue => 0.5,\n    }\n}\n"
        },
        {
          "type": "FileEdit",
          "path": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/231_correction_plan_serializer.rs",
          "original_content": "#![allow(dead_code)]\n//! Serialize correction plans to JSON values.\n\nuse crate::correction_plan_types::{CorrectionPlan, CorrectionStrategy};\nuse crate::quality_delta_types::RollbackCriteria;\nuse crate::verification_policy_types::{QualityThresholds, VerificationCheck, VerificationPolicy, VerificationScope};\nuse serde_json::{json, Value};\n\npub fn serialize_correction_plan(\n    plan: &CorrectionPlan,\n    verification: &VerificationPolicy,\n    rollback: &RollbackCriteria,\n) -> Value {\n    json!({\n        \"action_id\": plan.action_id,\n        \"tier\": format!(\"{:?}\", plan.tier),\n        \"confidence\": plan.confidence,\n        \"estimated_fix_time_seconds\": plan.estimated_fix_time_seconds,\n        \"predicted_violations\": plan.predicted_violations.iter().map(|v| json!({\n            \"type\": format!(\"{:?}\", v.violation_type),\n            \"severity\": format!(\"{:?}\", v.severity),\n            \"affected_files\": v.affected_files,\n            \"confidence\": v.confidence,\n        })).collect::<Vec<_>>(),\n        \"correction_strategies\": plan.strategies.iter().map(serialize_strategy).collect::<Vec<_>>(),\n        \"verification_policy\": {\n            \"scope\": serialize_scope(&verification.scope),\n            \"required_checks\": verification.required_checks.iter()\n                .map(serialize_check)\n                .collect::<Vec<_>>(),\n            \"incremental_eligible\": verification.incremental_eligible,\n            \"estimated_time_seconds\": verification.estimated_time_seconds,\n        },\n        \"rollback_criteria\": {\n            \"mandatory\": rollback.mandatory_rollback_if.iter()\n                .map(|c| format!(\"{:?}\", c))\n                .collect::<Vec<_>>(),\n            \"suggested\": rollback.suggested_rollback_if.iter()\n                .map(|c| format!(\"{:?}\", c))\n                .collect::<Vec<_>>(),\n        }\n    })\n}\n\nfn serialize_scope(scope: &VerificationScope) -> Value {\n    match scope {\n        VerificationScope::SyntaxOnly { files } => json!({\n            \"type\": \"SyntaxOnly\",\n            \"files\": files,\n        }),\n        VerificationScope::ModuleLocal { module, transitive_depth } => json!({\n            \"type\": \"ModuleLocal\",\n            \"module\": module,\n            \"transitive_depth\": transitive_depth,\n        }),\n        VerificationScope::CallerChain { root_function } => json!({\n            \"type\": \"CallerChain\",\n            \"root_function\": root_function,\n        }),\n        VerificationScope::FullWorkspace => json!({\n            \"type\": \"FullWorkspace\",\n        }),\n    }\n}\n\nfn serialize_check(check: &VerificationCheck) -> Value {\n    match check {\n        VerificationCheck::CargoCheck => json!({ \"type\": \"CargoCheck\" }),\n        VerificationCheck::CargoTest { filter } => json!({\n            \"type\": \"CargoTest\",\n            \"filter\": filter,\n        }),\n        VerificationCheck::InvariantValidation { invariant_ids } => json!({\n            \"type\": \"InvariantValidation\",\n            \"invariant_ids\": invariant_ids,\n        }),\n        VerificationCheck::QualityMetrics { thresholds } => json!({\n            \"type\": \"QualityMetrics\",\n            \"thresholds\": serialize_thresholds(thresholds),\n        }),\n        VerificationCheck::ManualInspection { reason } => json!({\n            \"type\": \"ManualInspection\",\n            \"reason\": reason,\n        }),\n    }\n}\n\nfn serialize_thresholds(thresholds: &QualityThresholds) -> Value {\n    json!({\n        \"min_cohesion_delta\": thresholds.min_cohesion_delta,\n        \"max_violation_delta\": thresholds.max_violation_delta,\n        \"max_complexity_delta\": thresholds.max_complexity_delta,\n    })\n}\n\nfn serialize_strategy(strategy: &CorrectionStrategy) -> Value {\n    match strategy {\n        CorrectionStrategy::AddImport { module_path, symbol } => json!({\n            \"type\": \"AddImport\",\n            \"module_path\": module_path,\n            \"symbol\": symbol,\n        }),\n        CorrectionStrategy::UpdatePath { old_path, new_path } => json!({\n            \"type\": \"UpdatePath\",\n            \"old_path\": old_path,\n            \"new_path\": new_path,\n        }),\n        CorrectionStrategy::ReExport { from_module, symbol } => json!({\n            \"type\": \"ReExport\",\n            \"from_module\": from_module,\n            \"symbol\": symbol,\n        }),\n        CorrectionStrategy::RenameWithSuffix { original, suffix } => json!({\n            \"type\": \"RenameWithSuffix\",\n            \"original\": original,\n            \"suffix\": suffix,\n        }),\n        CorrectionStrategy::MoveToLayer { function, target_layer } => json!({\n            \"type\": \"MoveToLayer\",\n            \"function\": function,\n            \"target_layer\": target_layer,\n        }),\n        CorrectionStrategy::UpdateCaller { caller_file, old_ref, new_ref } => json!({\n            \"type\": \"UpdateCaller\",\n            \"caller_file\": caller_file,\n            \"old_ref\": old_ref,\n            \"new_ref\": new_ref,\n        }),\n        CorrectionStrategy::ManualReview { reason, context } => json!({\n            \"type\": \"ManualReview\",\n            \"reason\": reason,\n            \"context\": context,\n        }),\n    }\n}\n",
          "updated_content": "#![allow(dead_code)]\n//! Serialize correction plans to JSON values.\n\nuse crate::correction_plan_types::{CorrectionPlan, CorrectionStrategy};\nuse crate::quality_delta_types::RollbackCriteria;\nuse crate::verification_policy_types::{QualityThresholds, VerificationCheck, VerificationPolicy, VerificationScope};\nuse serde_json::{json, Value};\n\npub fn serialize_correction_plan(\n    plan: &CorrectionPlan,\n    verification: &VerificationPolicy,\n    rollback: &RollbackCriteria,\n) -> Value {\n    json!({\n        \"action_id\": plan.action_id,\n        \"tier\": format!(\"{:?}\", plan.tier),\n        \"confidence\": plan.confidence,\n        \"estimated_fix_time_seconds\": plan.estimated_fix_time_seconds,\n        \"predicted_violations\": plan.predicted_violations.iter().map(|v| json!({\n            \"type\": format!(\"{:?}\", v.violation_type),\n            \"severity\": format!(\"{:?}\", v.severity),\n            \"affected_files\": v.affected_files,\n            \"confidence\": v.confidence,\n        })).collect::<Vec<_>>(),\n        \"correction_strategies\": plan.strategies.iter().map(serialize_strategy).collect::<Vec<_>>(),\n        \"verification_policy\": {\n            \"scope\": serialize_scope(&verification.scope),\n            \"required_checks\": verification.required_checks.iter()\n                .map(serialize_check)\n                .collect::<Vec<_>>(),\n            \"incremental_eligible\": verification.incremental_eligible,\n            \"estimated_time_seconds\": verification.estimated_time_seconds,\n        },\n        \"rollback_criteria\": {\n            \"mandatory\": rollback.mandatory_rollback_if.iter()\n                .map(|c| format!(\"{:?}\", c))\n                .collect::<Vec<_>>(),\n            \"suggested\": rollback.suggested_rollback_if.iter()\n                .map(|c| format!(\"{:?}\", c))\n                .collect::<Vec<_>>(),\n        }\n    })\n}\n\nfn serialize_scope(scope: &VerificationScope) -> Value {\n    match scope {\n        VerificationScope::SyntaxOnly { files } => json!({\n            \"type\": \"SyntaxOnly\",\n            \"files\": files,\n        }),\n        VerificationScope::ModuleLocal { module, transitive_depth } => json!({\n            \"type\": \"ModuleLocal\",\n            \"module\": module,\n            \"transitive_depth\": transitive_depth,\n        }),\n        VerificationScope::CallerChain { root_function } => json!({\n            \"type\": \"CallerChain\",\n            \"root_function\": root_function,\n        }),\n        VerificationScope::FullWorkspace => json!({\n            \"type\": \"FullWorkspace\",\n        }),\n    }\n}\n\nfn serialize_check(check: &VerificationCheck) -> Value {\n    match check {\n        VerificationCheck::CargoCheck => json!({ \"type\": \"CargoCheck\" }),\n        VerificationCheck::CargoTest { filter } => json!({\n            \"type\": \"CargoTest\",\n            \"filter\": filter,\n        }),\n        VerificationCheck::InvariantValidation { invariant_ids } => json!({\n            \"type\": \"InvariantValidation\",\n            \"invariant_ids\": invariant_ids,\n        }),\n        VerificationCheck::QualityMetrics { thresholds } => json!({\n            \"type\": \"QualityMetrics\",\n            \"thresholds\": serialize_thresholds(thresholds),\n        }),\n        VerificationCheck::ManualInspection { reason } => json!({\n            \"type\": \"ManualInspection\",\n            \"reason\": reason,\n        }),\n    }\n}\n\nfn serialize_thresholds(thresholds: &QualityThresholds) -> Value {\n    json!({\n        \"min_cohesion_delta\": thresholds.min_cohesion_delta,\n        \"max_violation_delta\": thresholds.max_violation_delta,\n        \"max_complexity_delta\": thresholds.max_complexity_delta,\n    })\n}\n\nfn serialize_strategy(strategy: &CorrectionStrategy) -> Value {\n    match strategy {\n        CorrectionStrategy::AddImport { module_path, symbol } => json!({\n            \"type\": \"AddImport\",\n            \"module_path\": module_path,\n            \"symbol\": symbol,\n        }),\n        CorrectionStrategy::UpdatePath { old_path, new_path } => json!({\n            \"type\": \"UpdatePath\",\n            \"old_path\": old_path,\n            \"new_path\": new_path,\n        }),\n        CorrectionStrategy::ReExport { from_module, symbol } => json!({\n            \"type\": \"ReExport\",\n            \"from_module\": from_module,\n            \"symbol\": symbol,\n        }),\n        CorrectionStrategy::RenameWithSuffix { original, suffix } => json!({\n            \"type\": \"RenameWithSuffix\",\n            \"original\": original,\n            \"suffix\": suffix,\n        }),\n        CorrectionStrategy::MoveToLayer { function, target_layer } => json!({\n            \"type\": \"MoveToLayer\",\n            \"function\": function,\n            \"target_layer\": target_layer,\n        }),\n        CorrectionStrategy::UpdateCaller { caller_file, old_ref, new_ref } => json!({\n            \"type\": \"UpdateCaller\",\n            \"caller_file\": caller_file,\n            \"old_ref\": old_ref,\n            \"new_ref\": new_ref,\n        }),\n        CorrectionStrategy::ManualReview { reason, context } => json!({\n            \"type\": \"ManualReview\",\n            \"reason\": reason,\n            \"context\": context,\n        }),\n    }\n}\n\npub fn serialize_correction_plans(\n    report: &CorrectionIntelligenceReport,\n) -> serde_json::Value {\n    let items = report\n        .correction_plans\n        .iter()\n        .zip(report.verification_policies.iter())\n        .zip(report.rollback_criteria.iter())\n        .map(|((plan, policy), rollback)| serialize_correction_plan(plan, policy, rollback))\n        .collect::<Vec<_>>();\n    json!({\n        \"version\": report.version,\n        \"timestamp\": report.timestamp,\n        \"project_root\": report.project_root,\n        \"actions_analyzed\": report.actions_analyzed,\n        \"correction_plans\": items,\n        \"quality_deltas\": report.quality_deltas,\n        \"summary\": report.summary,\n    })\n}\n"
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/000_invariant_types.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/010_invariant_types.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/000_invariant_types.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/010_invariant_types.rs",
          "original_content": "//! Core types for invariant detection and analysis\n//!\n//! This module defines the fundamental data structures for detecting and representing\n//! invariants in code. Invariants are properties that remain stable across refactorings\n//! and serve as constraints for automated code transformations.\n\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\nuse std::fmt;\n\n/// Strength classification for invariants\n///\n/// CRITICAL: Heuristics are NOT invariants - they are signals that need verification\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum InvariantStrength {\n    /// Mathematically proven from graph structure (e.g., SCC membership, degree)\n    Proven,\n\n    /// Observed empirically across many paths/samples with high confidence\n    Empirical { paths_checked: usize },\n\n    /// Name-based or keyword detection - LOW confidence, needs review\n    Heuristic,\n}\n\nimpl fmt::Display for InvariantStrength {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            InvariantStrength::Proven => write!(f, \"PROVEN\"),\n            InvariantStrength::Empirical { paths_checked } => {\n                write!(f, \"EMPIRICAL (paths: {})\", paths_checked)\n            }\n            InvariantStrength::Heuristic => write!(f, \"HEURISTIC (low confidence)\"),\n        }\n    }\n}\n\n/// Confidence score for an invariant [0.0, 1.0]\n#[derive(Debug, Clone, Copy, PartialEq, PartialOrd, Serialize, Deserialize)]\npub struct Confidence(pub f64);\n\nimpl Confidence {\n    pub fn from_strength(strength: &InvariantStrength) -> Self {\n        match strength {\n            InvariantStrength::Proven => Confidence(1.0),\n            InvariantStrength::Empirical { paths_checked } => {\n                if *paths_checked > 100 {\n                    Confidence(0.9)\n                } else if *paths_checked > 10 {\n                    Confidence(0.7)\n                } else {\n                    Confidence(0.5)\n                }\n            }\n            InvariantStrength::Heuristic => Confidence(0.3), // Always low!\n        }\n    }\n\n    pub fn value(&self) -> f64 {\n        self.0\n    }\n}\n\n/// Structural invariants - properties derived from graph topology\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub enum StructuralInvariant {\n    /// Layer assignment is fixed and cannot change\n    LayerFixed { layer: usize },\n\n    /// In-degree and out-degree are stable\n    DegreeStable { in_degree: usize, out_degree: usize },\n\n    /// Node is a leaf (out-degree = 0)\n    Leaf,\n\n    /// Node is a root (in-degree = 0)\n    Root,\n\n    /// Node is a bridge (removal disconnects graph)\n    Bridge,\n\n    /// Node belongs to a specific SCC\n    SccMembership { scc_id: usize, scc_size: usize },\n}\n\n/// Semantic invariants - properties of function behavior\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub enum SemanticInvariant {\n    /// Function signature is stable (type-based)\n    TypeStable { signature: String },\n\n    /// Function is pure (no side effects, deterministic)\n    PureFunction,\n\n    /// Function is idempotent: f(f(x)) = f(x)\n    Idempotent,\n\n    /// Function has specific effect signature\n    EffectStable { effects: Vec<String> },\n}\n\n/// Delta invariants - properties about rate of change\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub enum DeltaInvariant {\n    /// Value is monotonically increasing/decreasing\n    Monotonic { direction: MonotonicDirection },\n\n    /// Change rate is bounded\n    Stable { epsilon: f64 },\n\n    /// Only appends, never deletes\n    AppendOnly,\n\n    /// Sum of inputs equals sum of outputs\n    Conservative,\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum MonotonicDirection {\n    Increasing,\n    Decreasing,\n}\n\n/// Path-intersection invariant - facts that hold on ALL execution paths\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct PathIntersectionInvariant {\n    /// Symbolic facts that hold on all paths\n    pub facts: HashSet<String>,\n\n    /// Number of paths analyzed\n    pub paths_analyzed: usize,\n\n    /// Maximum path depth explored\n    pub max_depth: usize,\n}\n\n/// Complete invariant kind taxonomy\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub enum InvariantKind {\n    Structural(StructuralInvariant),\n    Semantic(SemanticInvariant),\n    Delta(DeltaInvariant),\n    PathIntersection(PathIntersectionInvariant),\n}\n\nimpl fmt::Display for InvariantKind {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            InvariantKind::Structural(s) => write!(f, \"Structural: {:?}\", s),\n            InvariantKind::Semantic(s) => write!(f, \"Semantic: {:?}\", s),\n            InvariantKind::Delta(d) => write!(f, \"Delta: {:?}\", d),\n            InvariantKind::PathIntersection(p) => {\n                write!(f, \"PathIntersection: {} facts across {} paths\",\n                       p.facts.len(), p.paths_analyzed)\n            }\n        }\n    }\n}\n\n/// A detected invariant with metadata\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Invariant {\n    /// Target of the invariant (function name, module name, etc.)\n    pub target: String,\n\n    /// File where the target is located\n    pub file_path: String,\n\n    /// Kind of invariant\n    pub kind: InvariantKind,\n\n    /// Strength classification\n    pub strength: InvariantStrength,\n\n    /// Confidence score\n    pub confidence: Confidence,\n\n    /// Human-readable description\n    pub description: String,\n\n    /// Evidence supporting this invariant\n    pub evidence: Vec<String>,\n}\n\nimpl Invariant {\n    /// Create a new invariant\n    pub fn new(\n        target: String,\n        file_path: String,\n        kind: InvariantKind,\n        strength: InvariantStrength,\n        description: String,\n    ) -> Self {\n        let confidence = Confidence::from_strength(&strength);\n        Self {\n            target,\n            file_path,\n            kind,\n            strength,\n            confidence,\n            description,\n            evidence: Vec::new(),\n        }\n    }\n\n    /// Add evidence supporting this invariant\n    pub fn add_evidence(&mut self, evidence: String) {\n        self.evidence.push(evidence);\n    }\n\n    /// Check if this invariant should block refactorings\n    pub fn is_blocking(&self) -> bool {\n        match self.strength {\n            InvariantStrength::Proven => true,\n            InvariantStrength::Empirical { paths_checked } => paths_checked >= 10,\n            InvariantStrength::Heuristic => false, // Heuristics never block\n        }\n    }\n}\n\n/// A violation of a detected invariant\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct InvariantViolation {\n    /// The invariant that was violated\n    pub invariant: Invariant,\n\n    /// Description of the violation\n    pub violation_description: String,\n\n    /// Severity level\n    pub severity: ViolationSeverity,\n\n    /// Suggested fix\n    pub suggested_fix: Option<String>,\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]\npub enum ViolationSeverity {\n    Critical, // Breaks proven invariant\n    High,     // Breaks empirical invariant with high confidence\n    Medium,   // Breaks empirical invariant with medium confidence\n    Low,      // Violates heuristic\n}\n\nimpl fmt::Display for ViolationSeverity {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            ViolationSeverity::Critical => write!(f, \"CRITICAL\"),\n            ViolationSeverity::High => write!(f, \"HIGH\"),\n            ViolationSeverity::Medium => write!(f, \"MEDIUM\"),\n            ViolationSeverity::Low => write!(f, \"LOW\"),\n        }\n    }\n}\n\n/// Layer information inferred from call graph\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct LayerInfo {\n    /// Function or module name\n    pub name: String,\n\n    /// Inferred layer number (0 = leaf, higher = more dependencies)\n    pub layer: usize,\n\n    /// Names of direct dependencies (callees)\n    pub dependencies: Vec<String>,\n\n    /// Maximum layer of all dependencies\n    pub max_dependency_layer: Option<usize>,\n}\n\n/// Complete invariant analysis result\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct InvariantAnalysisResult {\n    /// All detected invariants\n    pub invariants: Vec<Invariant>,\n\n    /// Detected violations\n    pub violations: Vec<InvariantViolation>,\n\n    /// Layer assignments for all functions/modules\n    pub layer_assignments: HashMap<String, LayerInfo>,\n\n    /// Statistics\n    pub stats: InvariantStats,\n}\n\nimpl InvariantAnalysisResult {\n    pub fn new() -> Self {\n        Self {\n            invariants: Vec::new(),\n            violations: Vec::new(),\n            layer_assignments: HashMap::new(),\n            stats: InvariantStats::default(),\n        }\n    }\n\n    /// Add an invariant to the result\n    pub fn add_invariant(&mut self, invariant: Invariant) {\n        // Update statistics\n        match invariant.strength {\n            InvariantStrength::Proven => self.stats.proven_count += 1,\n            InvariantStrength::Empirical { .. } => self.stats.empirical_count += 1,\n            InvariantStrength::Heuristic => self.stats.heuristic_count += 1,\n        }\n\n        match &invariant.kind {\n            InvariantKind::Structural(_) => self.stats.structural_count += 1,\n            InvariantKind::Semantic(_) => self.stats.semantic_count += 1,\n            InvariantKind::Delta(_) => self.stats.delta_count += 1,\n            InvariantKind::PathIntersection(_) => self.stats.path_intersection_count += 1,\n        }\n\n        self.invariants.push(invariant);\n    }\n\n    /// Add a violation to the result\n    pub fn add_violation(&mut self, violation: InvariantViolation) {\n        self.stats.violation_count += 1;\n        self.violations.push(violation);\n    }\n\n    /// Get all blocking invariants for a specific target\n    #[allow(dead_code)]\n    pub fn get_blocking_invariants(&self, target: &str) -> Vec<&Invariant> {\n        self.invariants\n            .iter()\n            .filter(|inv| inv.target == target && inv.is_blocking())\n            .collect()\n    }\n}\n\nimpl Default for InvariantAnalysisResult {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n/// Statistics about detected invariants\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct InvariantStats {\n    /// Total number of invariants\n    pub total_count: usize,\n\n    /// Count by strength\n    pub proven_count: usize,\n    pub empirical_count: usize,\n    pub heuristic_count: usize,\n\n    /// Count by kind\n    pub structural_count: usize,\n    pub semantic_count: usize,\n    pub delta_count: usize,\n    pub path_intersection_count: usize,\n\n    /// Number of violations\n    pub violation_count: usize,\n}\n\nimpl InvariantStats {\n    pub fn update_totals(&mut self) {\n        self.total_count = self.proven_count + self.empirical_count + self.heuristic_count;\n    }\n\n    /// Get percentage of proven invariants\n    pub fn proven_percentage(&self) -> f64 {\n        if self.total_count == 0 {\n            0.0\n        } else {\n            (self.proven_count as f64 / self.total_count as f64) * 100.0\n        }\n    }\n\n    /// Get percentage of heuristics\n    pub fn heuristic_percentage(&self) -> f64 {\n        if self.total_count == 0 {\n            0.0\n        } else {\n            (self.heuristic_count as f64 / self.total_count as f64) * 100.0\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_confidence_from_strength() {\n        assert_eq!(Confidence::from_strength(&InvariantStrength::Proven).value(), 1.0);\n        assert_eq!(\n            Confidence::from_strength(&InvariantStrength::Empirical { paths_checked: 150 }).value(),\n            0.9\n        );\n        assert_eq!(Confidence::from_strength(&InvariantStrength::Heuristic).value(), 0.3);\n    }\n\n    #[test]\n    fn test_is_blocking() {\n        let proven_inv = Invariant::new(\n            \"test_fn\".to_string(),\n            \"test.rs\".to_string(),\n            InvariantKind::Structural(StructuralInvariant::LayerFixed { layer: 0 }),\n            InvariantStrength::Proven,\n            \"Test invariant\".to_string(),\n        );\n        assert!(proven_inv.is_blocking());\n\n        let heuristic_inv = Invariant::new(\n            \"test_fn\".to_string(),\n            \"test.rs\".to_string(),\n            InvariantKind::Semantic(SemanticInvariant::PureFunction),\n            InvariantStrength::Heuristic,\n            \"Test heuristic\".to_string(),\n        );\n        assert!(!heuristic_inv.is_blocking());\n    }\n\n    #[test]\n    fn test_stats_calculation() {\n        let mut result = InvariantAnalysisResult::new();\n\n        result.add_invariant(Invariant::new(\n            \"fn1\".to_string(),\n            \"test.rs\".to_string(),\n            InvariantKind::Structural(StructuralInvariant::Leaf),\n            InvariantStrength::Proven,\n            \"Leaf node\".to_string(),\n        ));\n\n        result.add_invariant(Invariant::new(\n            \"fn2\".to_string(),\n            \"test.rs\".to_string(),\n            InvariantKind::Semantic(SemanticInvariant::PureFunction),\n            InvariantStrength::Heuristic,\n            \"Pure function\".to_string(),\n        ));\n\n        result.stats.update_totals();\n\n        assert_eq!(result.stats.total_count, 2);\n        assert_eq!(result.stats.proven_count, 1);\n        assert_eq!(result.stats.heuristic_count, 1);\n        assert_eq!(result.stats.proven_percentage(), 50.0);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/005_refactor_constraints.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/020_refactor_constraints.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/005_refactor_constraints.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/020_refactor_constraints.rs",
          "original_content": "//! Refactoring constraints derived from invariants\n//!\n//! This module converts detected invariants into mechanical constraints that\n//! enforce safe refactorings. Constraints are machine-readable rules that\n//! determine what refactorings are allowed.\n\nuse crate::invariant_types::*;\nuse serde::{Deserialize, Serialize};\nuse std::fmt;\n\n/// A constraint that restricts refactoring operations\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub enum RefactorConstraint {\n    /// Cannot move this item to a different file\n    NoMove {\n        target: String,\n        reason: String,\n        strength: InvariantStrength,\n    },\n\n    /// Cannot change the signature of this item\n    PreserveSignature {\n        target: String,\n        signature: String,\n        strength: InvariantStrength,\n    },\n\n    /// Cannot change the layer assignment\n    FixedLayer {\n        target: String,\n        layer: usize,\n        strength: InvariantStrength,\n    },\n\n    /// Must preserve ordering relative to other items\n    PreserveOrdering {\n        target: String,\n        must_come_before: Vec<String>,\n        strength: InvariantStrength,\n    },\n\n    /// Must preserve specific facts\n    MustPreserve {\n        target: String,\n        facts: Vec<String>,\n        strength: InvariantStrength,\n    },\n\n    /// Cannot delete this item (has critical dependencies)\n    NoDelete {\n        target: String,\n        dependents: Vec<String>,\n        strength: InvariantStrength,\n    },\n\n    /// Must maintain specific degree (in/out edges)\n    PreserveDegree {\n        target: String,\n        in_degree: usize,\n        out_degree: usize,\n        strength: InvariantStrength,\n    },\n}\n\nimpl fmt::Display for RefactorConstraint {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            RefactorConstraint::NoMove { target, reason, strength } => {\n                write!(f, \"[{}] NoMove: {} (reason: {})\", strength, target, reason)\n            }\n            RefactorConstraint::PreserveSignature { target, signature, strength } => {\n                write!(f, \"[{}] PreserveSignature: {} -> {}\", strength, target, signature)\n            }\n            RefactorConstraint::FixedLayer { target, layer, strength } => {\n                write!(f, \"[{}] FixedLayer: {} @ layer {}\", strength, target, layer)\n            }\n            RefactorConstraint::PreserveOrdering { target, must_come_before, strength } => {\n                write!(f, \"[{}] PreserveOrdering: {} before {:?}\", strength, target, must_come_before)\n            }\n            RefactorConstraint::MustPreserve { target, facts, strength } => {\n                write!(f, \"[{}] MustPreserve: {} -> {} facts\", strength, target, facts.len())\n            }\n            RefactorConstraint::NoDelete { target, dependents, strength } => {\n                write!(f, \"[{}] NoDelete: {} (dependents: {})\", strength, target, dependents.len())\n            }\n            RefactorConstraint::PreserveDegree { target, in_degree, out_degree, strength } => {\n                write!(f, \"[{}] PreserveDegree: {} (in={}, out={})\", strength, target, in_degree, out_degree)\n            }\n        }\n    }\n}\n\nimpl RefactorConstraint {\n    /// Get the target of this constraint\n    #[allow(dead_code)]\n    pub fn target(&self) -> &str {\n        match self {\n            RefactorConstraint::NoMove { target, .. } => target,\n            RefactorConstraint::PreserveSignature { target, .. } => target,\n            RefactorConstraint::FixedLayer { target, .. } => target,\n            RefactorConstraint::PreserveOrdering { target, .. } => target,\n            RefactorConstraint::MustPreserve { target, .. } => target,\n            RefactorConstraint::NoDelete { target, .. } => target,\n            RefactorConstraint::PreserveDegree { target, .. } => target,\n        }\n    }\n\n    /// Get the strength of the invariant backing this constraint\n    #[allow(dead_code)]\n    pub fn strength(&self) -> &InvariantStrength {\n        match self {\n            RefactorConstraint::NoMove { strength, .. } => strength,\n            RefactorConstraint::PreserveSignature { strength, .. } => strength,\n            RefactorConstraint::FixedLayer { strength, .. } => strength,\n            RefactorConstraint::PreserveOrdering { strength, .. } => strength,\n            RefactorConstraint::MustPreserve { strength, .. } => strength,\n            RefactorConstraint::NoDelete { strength, .. } => strength,\n            RefactorConstraint::PreserveDegree { strength, .. } => strength,\n        }\n    }\n\n    /// Check if this constraint should block a refactoring\n    #[allow(dead_code)]\n    pub fn is_blocking(&self) -> bool {\n        match self.strength() {\n            InvariantStrength::Proven => true,\n            InvariantStrength::Empirical { paths_checked } => *paths_checked >= 10,\n            InvariantStrength::Heuristic => false,\n        }\n    }\n}\n\n/// Convert an invariant to a refactoring constraint\npub fn from_invariant(invariant: &Invariant) -> Option<RefactorConstraint> {\n    match &invariant.kind {\n        InvariantKind::Structural(s) => match s {\n            StructuralInvariant::LayerFixed { layer } => Some(RefactorConstraint::FixedLayer {\n                target: invariant.target.clone(),\n                layer: *layer,\n                strength: invariant.strength,\n            }),\n            StructuralInvariant::DegreeStable { in_degree, out_degree } => {\n                Some(RefactorConstraint::PreserveDegree {\n                    target: invariant.target.clone(),\n                    in_degree: *in_degree,\n                    out_degree: *out_degree,\n                    strength: invariant.strength,\n                })\n            }\n            StructuralInvariant::Leaf | StructuralInvariant::Root => {\n                Some(RefactorConstraint::NoMove {\n                    target: invariant.target.clone(),\n                    reason: \"graph topology fixed\".to_string(),\n                    strength: invariant.strength,\n                })\n            }\n            StructuralInvariant::Bridge => Some(RefactorConstraint::NoDelete {\n                target: invariant.target.clone(),\n                dependents: vec![\"graph connectivity\".to_string()],\n                strength: invariant.strength,\n            }),\n            StructuralInvariant::SccMembership { .. } => Some(RefactorConstraint::NoMove {\n                target: invariant.target.clone(),\n                reason: \"SCC membership fixed\".to_string(),\n                strength: invariant.strength,\n            }),\n        },\n        InvariantKind::Semantic(s) => match s {\n            SemanticInvariant::TypeStable { signature } => {\n                Some(RefactorConstraint::PreserveSignature {\n                    target: invariant.target.clone(),\n                    signature: signature.clone(),\n                    strength: invariant.strength,\n                })\n            }\n            SemanticInvariant::PureFunction | SemanticInvariant::Idempotent => {\n                Some(RefactorConstraint::PreserveSignature {\n                    target: invariant.target.clone(),\n                    signature: \"effects must remain pure\".to_string(),\n                    strength: invariant.strength,\n                })\n            }\n            SemanticInvariant::EffectStable { .. } => Some(RefactorConstraint::PreserveSignature {\n                target: invariant.target.clone(),\n                signature: \"effect signature fixed\".to_string(),\n                strength: invariant.strength,\n            }),\n        },\n        InvariantKind::Delta(d) => match d {\n            DeltaInvariant::Monotonic { .. } => Some(RefactorConstraint::PreserveOrdering {\n                target: invariant.target.clone(),\n                must_come_before: Vec::new(),\n                strength: invariant.strength,\n            }),\n            _ => None, // Other delta invariants don't directly map to refactor constraints\n        },\n        InvariantKind::PathIntersection(p) => Some(RefactorConstraint::MustPreserve {\n            target: invariant.target.clone(),\n            facts: p.facts.iter().cloned().collect(),\n            strength: invariant.strength,\n        }),\n    }\n}\n\n/// Check if a move operation is allowed by constraints\n#[allow(dead_code)]\npub fn check_move_allowed(\n    target: &str,\n    current_file: &str,\n    suggested_file: &str,\n    constraints: &[RefactorConstraint],\n) -> Result<(), String> {\n    for constraint in constraints {\n        if constraint.target() == target && constraint.is_blocking() {\n            match constraint {\n                RefactorConstraint::NoMove { reason, .. } => {\n                    return Err(format!(\n                        \"Cannot move {} from {} to {}: {}\",\n                        target, current_file, suggested_file, reason\n                    ));\n                }\n                RefactorConstraint::FixedLayer { layer, .. } => {\n                    return Err(format!(\n                        \"Cannot move {} from {} to {}: layer {} is fixed\",\n                        target, current_file, suggested_file, layer\n                    ));\n                }\n                _ => {} // Other constraints don't block moves\n            }\n        }\n    }\n    Ok(())\n}\n\n/// Generate all constraints from an invariant analysis result\npub fn generate_constraints(analysis: &InvariantAnalysisResult) -> Vec<RefactorConstraint> {\n    analysis\n        .invariants\n        .iter()\n        .filter_map(from_invariant)\n        .collect()\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::collections::HashSet;\n\n    #[test]\n    fn test_from_invariant_layer_fixed() {\n        let inv = Invariant::new(\n            \"test_fn\".to_string(),\n            \"test.rs\".to_string(),\n            InvariantKind::Structural(StructuralInvariant::LayerFixed { layer: 5 }),\n            InvariantStrength::Proven,\n            \"Layer is fixed\".to_string(),\n        );\n\n        let constraint = from_invariant(&inv).unwrap();\n        match constraint {\n            RefactorConstraint::FixedLayer { target, layer, .. } => {\n                assert_eq!(target, \"test_fn\");\n                assert_eq!(layer, 5);\n            }\n            _ => panic!(\"Wrong constraint type\"),\n        }\n    }\n\n    #[test]\n    fn test_check_move_allowed_blocking() {\n        let constraints = vec![RefactorConstraint::NoMove {\n            target: \"test_fn\".to_string(),\n            reason: \"layer fixed\".to_string(),\n            strength: InvariantStrength::Proven,\n        }];\n\n        let result = check_move_allowed(\"test_fn\", \"old.rs\", \"new.rs\", &constraints);\n        assert!(result.is_err());\n        assert!(result.unwrap_err().contains(\"layer fixed\"));\n    }\n\n    #[test]\n    fn test_check_move_allowed_non_blocking() {\n        let constraints = vec![RefactorConstraint::NoMove {\n            target: \"test_fn\".to_string(),\n            reason: \"heuristic\".to_string(),\n            strength: InvariantStrength::Heuristic,\n        }];\n\n        let result = check_move_allowed(\"test_fn\", \"old.rs\", \"new.rs\", &constraints);\n        assert!(result.is_ok()); // Heuristics don't block\n    }\n\n    #[test]\n    fn test_constraint_is_blocking() {\n        let proven = RefactorConstraint::NoMove {\n            target: \"fn1\".to_string(),\n            reason: \"test\".to_string(),\n            strength: InvariantStrength::Proven,\n        };\n        assert!(proven.is_blocking());\n\n        let heuristic = RefactorConstraint::NoMove {\n            target: \"fn2\".to_string(),\n            reason: \"test\".to_string(),\n            strength: InvariantStrength::Heuristic,\n        };\n        assert!(!heuristic.is_blocking());\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/010_cluster_008.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/030_cluster_008.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/010_cluster_008.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/030_cluster_008.rs",
          "original_content": "//! Cluster 008: Core dependency analysis and layer utilities\n//!\n//! This module contains foundational functions for:\n//! - Rust file dependency ordering and resolution\n//! - Layer violation detection\n//! - Layer name extraction\n//! - Graph visualization node styling\n//! - Cyclomatic complexity calculation\n//!\n//! Functions moved from multiple modules as part of Phase 2 refactoring.\n\nuse anyhow::Result;\nuse std::cmp::Ordering;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, VecDeque};\nuse std::path::{Path, PathBuf};\nuse petgraph::graph::DiGraph;\nuse petgraph::visit::EdgeRef;\n\nuse crate::dependency::{\n    LayerEdge, LayerGraph, ReferenceDetail, UnresolvedDependency,\n};\nuse crate::types::{FileLayerViolation, NodeType};\n\n// ============================================================================\n// From src/000_dependency.rs\n// ============================================================================\n\npub fn build_result(\n    files: &[PathBuf],\n    file_layers: HashMap<PathBuf, String>,\n    nodes: BTreeSet<String>,\n    edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n    unresolved: Vec<UnresolvedDependency>,\n    entry_files: &BTreeSet<PathBuf>,\n) -> Result<(Vec<PathBuf>, LayerGraph)> {\n    let adjacency = adjacency_from_edges(&edges_map);\n    let (mut ordered_layers, cycles) = topo_sort(&nodes, &adjacency);\n    if let Some(pos) = ordered_layers.iter().position(|layer| layer == \"root\") {\n        let root_layer = ordered_layers.remove(pos);\n        ordered_layers.insert(0, root_layer);\n    }\n    let rank = layer_rank_map(&ordered_layers);\n\n    let mut ordered_files = files.to_vec();\n    ordered_files.sort_by(|a, b| {\n        let mmsb_a = is_mmsb_main(a);\n        let mmsb_b = is_mmsb_main(b);\n        if mmsb_a && !mmsb_b {\n            return Ordering::Less;\n        } else if mmsb_b && !mmsb_a {\n            return Ordering::Greater;\n        }\n        let entry_a = entry_files.contains(a);\n        let entry_b = entry_files.contains(b);\n        if entry_a && !entry_b {\n            return Ordering::Less;\n        } else if entry_b && !entry_a {\n            return Ordering::Greater;\n        }\n        let layer_a = file_layers\n            .get(a)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let layer_b = file_layers\n            .get(b)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let rank_a = rank.get(&layer_a).cloned().unwrap_or(ordered_layers.len());\n        let rank_b = rank.get(&layer_b).cloned().unwrap_or(ordered_layers.len());\n        rank_a\n            .cmp(&rank_b)\n            .then_with(|| layer_a.cmp(&layer_b))\n            .then_with(|| a.cmp(b))\n    });\n\n    let edges = edges_map\n        .into_iter()\n        .map(|((from, to), references)| LayerEdge {\n            violation: is_layer_violation(&from, &to),\n            from,\n            to,\n            references: references.into_iter().collect(),\n        })\n        .collect();\n\n    let graph = LayerGraph {\n        ordered_layers,\n        edges,\n        cycles,\n        unresolved,\n    };\n\n    Ok((ordered_files, graph))\n}\n\nfn adjacency_from_edges(\n    edges_map: &BTreeMap<(String, String), BTreeSet<ReferenceDetail>>,\n) -> HashMap<String, BTreeSet<String>> {\n    let mut adjacency: HashMap<String, BTreeSet<String>> = HashMap::new();\n    for ((from, to), _) in edges_map {\n        adjacency\n            .entry(from.clone())\n            .or_default()\n            .insert(to.clone());\n    }\n    adjacency\n}\n\nfn topo_sort(\n    nodes: &BTreeSet<String>,\n    adjacency: &HashMap<String, BTreeSet<String>>,\n) -> (Vec<String>, Vec<String>) {\n    let mut indegree: HashMap<String, usize> = HashMap::new();\n    for node in nodes {\n        indegree.entry(node.clone()).or_insert(0);\n    }\n    for targets in adjacency.values() {\n        for target in targets {\n            *indegree.entry(target.clone()).or_insert(0) += 1;\n        }\n    }\n\n    let mut queue: VecDeque<String> = indegree\n        .iter()\n        .filter_map(|(node, &deg)| if deg == 0 { Some(node.clone()) } else { None })\n        .collect();\n    queue.make_contiguous().sort();\n\n    let mut order = Vec::new();\n    while let Some(node) = queue.pop_front() {\n        order.push(node.clone());\n        if let Some(targets) = adjacency.get(&node) {\n            for target in targets {\n                if let Some(entry) = indegree.get_mut(target) {\n                    *entry -= 1;\n                    if *entry == 0 {\n                        insert_sorted(&mut queue, target.clone());\n                    }\n                }\n            }\n        }\n    }\n\n    if order.len() != nodes.len() {\n        let mut remaining: Vec<_> = nodes\n            .iter()\n            .filter(|layer| !order.contains(layer))\n            .cloned()\n            .collect();\n        remaining.sort();\n        let cycles = remaining.clone();\n        order.extend(remaining);\n        return (order, cycles);\n    }\n\n    (order, Vec::new())\n}\n\nfn layer_rank_map(order: &[String]) -> HashMap<String, usize> {\n    let mut rank = HashMap::new();\n    for (idx, layer) in order.iter().enumerate() {\n        rank.insert(layer.clone(), idx);\n    }\n    rank\n}\n\nfn insert_sorted(queue: &mut VecDeque<String>, value: String) {\n    let mut inserted = false;\n    for idx in 0..queue.len() {\n        if value < queue[idx] {\n            queue.insert(idx, value.clone());\n            inserted = true;\n            break;\n        }\n    }\n    if !inserted {\n        queue.push_back(value);\n    }\n}\n\nfn is_mmsb_main(path: &Path) -> bool {\n    path.file_name()\n        .and_then(|n| n.to_str())\n        .map(|n| n == \"MMSB.jl\")\n        .unwrap_or(false)\n}\n\n// ============================================================================\n// From src/010_layer_core.rs\n// ============================================================================\n\n/// Checks if a dependency from one layer to another violates layer ordering\n/// Returns true if from_layer > to_layer (violation: higher depends on lower)\npub fn is_layer_violation(from: &str, to: &str) -> bool {\n    match (layer_prefix_value(from), layer_prefix_value(to)) {\n        (Some(a), Some(b)) => a > b,\n        _ => false,\n    }\n}\n\n/// Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)\nfn layer_prefix_value(layer: &str) -> Option<i32> {\n    let mut chars = layer.chars();\n    let mut digits = String::new();\n    while let Some(ch) = chars.next() {\n        if ch.is_ascii_digit() {\n            digits.push(ch);\n        } else {\n            break;\n        }\n    }\n    if digits.is_empty() {\n        None\n    } else {\n        digits.parse::<i32>().ok()\n    }\n}\n\n// ============================================================================\n// From src/010_layer_core.rs (continued)\n// ============================================================================\n\npub fn compare_dir_layers(a: &Path, b: &Path) -> Ordering {\n    let a_name = a.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let b_name = b.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    let a_layer = layer_prefix_value(a_name).unwrap_or(i32::MAX);\n    let b_layer = layer_prefix_value(b_name).unwrap_or(i32::MAX);\n    a_layer.cmp(&b_layer).then_with(|| a_name.cmp(b_name))\n}\n\npub fn compare_path_components(a: &Path, b: &Path) -> Ordering {\n    let a_components: Vec<_> = a.components().collect();\n    let b_components: Vec<_> = b.components().collect();\n    let min_len = a_components.len().min(b_components.len());\n\n    for idx in 0..min_len {\n        let a_name = a_components[idx].as_os_str().to_string_lossy();\n        let b_name = b_components[idx].as_os_str().to_string_lossy();\n        let a_prefix = layer_prefix_value(&a_name);\n        let b_prefix = layer_prefix_value(&b_name);\n        let cmp = match (a_prefix, b_prefix) {\n            (Some(a_val), Some(b_val)) => a_val.cmp(&b_val),\n            _ => a_name.cmp(&b_name),\n        };\n        if cmp != Ordering::Equal {\n            return cmp;\n        }\n    }\n\n    a_components.len().cmp(&b_components.len())\n}\n\npub fn layer_adheres(current_layer: &str, target_layer: &str) -> bool {\n    match (layer_prefix_value(current_layer), layer_prefix_value(target_layer)) {\n        (Some(curr), Some(target)) => curr <= target,\n        _ => true,\n    }\n}\n\npub(crate) fn structural_layer_value(layer: &Option<String>, default: i32) -> i32 {\n    layer\n        .as_ref()\n        .and_then(|value| layer_prefix_value(value))\n        .unwrap_or(default)\n}\n\npub fn detect_layer_violations(\n    graph: &DiGraph<PathBuf, ()>,\n    file_layers: &HashMap<PathBuf, String>,\n) -> Vec<FileLayerViolation> {\n    let mut violations = Vec::new();\n    for edge in graph.edge_references() {\n        let from = &graph[edge.source()];\n        let to = &graph[edge.target()];\n        let from_layer = file_layers\n            .get(from)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        let to_layer = file_layers\n            .get(to)\n            .cloned()\n            .unwrap_or_else(|| \"root\".to_string());\n        if let (Some(from_val), Some(to_val)) =\n            (layer_prefix_value(&from_layer), layer_prefix_value(&to_layer))\n        {\n            if from_val > to_val {\n                violations.push(FileLayerViolation {\n                    from: from.clone(),\n                    to: to.clone(),\n                    from_layer,\n                    to_layer,\n                });\n            }\n        }\n    }\n    violations\n}\n\n#[derive(Clone)]\npub struct FunctionInfo {\n    pub name: String,\n    pub signature: String,\n    pub file_path: String,\n    pub layer: String,\n    pub calls: Vec<String>,\n}\n\npub fn detect_layer_violation(\n    func: &FunctionInfo,\n    functions: &[FunctionInfo],\n    outgoing: &HashMap<usize, usize>,\n    file_layers: &HashMap<String, String>,\n) -> Option<(String, String)> {\n    let current_layer = file_layers\n        .get(&func.file_path)\n        .cloned()\n        .unwrap_or_else(|| func.layer.clone());\n    let current_value = layer_prefix_value(&current_layer)?;\n\n    let mut violation: Option<(i32, String)> = None;\n    for (callee_idx, _) in outgoing {\n        let callee = &functions[*callee_idx];\n        let target_layer = file_layers\n            .get(&callee.file_path)\n            .cloned()\n            .unwrap_or_else(|| callee.layer.clone());\n        if let Some(target_value) = layer_prefix_value(&target_layer) {\n            if target_value < current_value {\n                match violation {\n                    Some((best_value, _)) if target_value >= best_value => {}\n                    _ => {\n                        violation = Some((target_value, target_layer));\n                    }\n                }\n            }\n        }\n    }\n\n    violation.map(|(_, target_layer)| (current_layer, target_layer))\n}\n\n// ============================================================================\n// From src/020_layer_utilities.rs (report planning helpers)\n// ============================================================================\n\npub fn parse_cluster_members(\n    cluster: &crate::types::FunctionCluster,\n) -> Vec<crate::report::ClusterMember> {\n    cluster\n        .members\n        .iter()\n        .filter_map(|member| {\n            let (file, name) = member.rsplit_once(\"::\")?;\n            Some(crate::report::ClusterMember {\n                file: PathBuf::from(file),\n                name: name.to_string(),\n            })\n        })\n        .collect()\n}\n\npub fn is_core_module_path(path: &Path) -> bool {\n    let Some(stem) = path.file_stem().and_then(|name| name.to_str()) else {\n        return false;\n    };\n    stem.starts_with(\"040_dependency\") || stem.starts_with(\"060_layer_core\")\n}\n\npub fn cluster_target_path(\n    target: PathBuf,\n    members: &[crate::report::ClusterMember],\n    root_path: &Path,\n    idx: usize,\n) -> PathBuf {\n    if !is_core_module_path(&target) {\n        return target;\n    }\n    let prefix = target\n        .file_stem()\n        .and_then(|name| name.to_str())\n        .and_then(|stem| layer_prefix_value(stem))\n        .unwrap_or(900);\n    let file_name = format!(\"{:03}_cluster_{:03}.rs\", prefix, idx + 1);\n    let dir = members\n        .first()\n        .and_then(|member| member.file.parent())\n        .unwrap_or(root_path);\n    dir.join(file_name)\n}\n\npub fn collect_cluster_plans(\n    clusters: &[crate::types::FunctionCluster],\n    root_path: &Path,\n) -> Vec<crate::report::ClusterPlan> {\n    let mut plans = Vec::new();\n    for (idx, cluster) in clusters.iter().enumerate() {\n        let all_members = parse_cluster_members(cluster);\n        let target = if let Some(suggested) = &cluster.suggested_file {\n            suggested.clone()\n        } else if let Some(first) = all_members.first() {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            first\n                .file\n                .parent()\n                .unwrap_or(root_path)\n                .join(file_name)\n        } else {\n            let file_name = format!(\"900_cluster_{:03}.rs\", idx + 1);\n            root_path.join(file_name)\n        };\n        let target = cluster_target_path(target, &all_members, root_path, idx);\n        let members = all_members\n            .into_iter()\n            .filter(|member| member.file != target)\n            .collect::<Vec<_>>();\n        if members.len() < 2 {\n            continue;\n        }\n        plans.push(crate::report::ClusterPlan {\n            target,\n            cohesion: cluster.cohesion,\n            members,\n        });\n    }\n    plans.sort_by(|a, b| {\n        use std::cmp::Ordering;\n        b.cohesion\n            .partial_cmp(&a.cohesion)\n            .unwrap_or(Ordering::Equal)\n            .then_with(|| b.members.len().cmp(&a.members.len()))\n            .then_with(|| a.target.cmp(&b.target))\n    });\n    plans\n}\n\n// ============================================================================\n// From src/120_dot_exporter.rs\n// ============================================================================\n\npub fn node_style(node_type: &NodeType) -> (&str, &str, &str) {\n    match node_type {\n        NodeType::Entry => (\"ellipse\", \"lightgreen\", \"\\\"filled,bold\\\"\"),\n        NodeType::Exit => (\"doubleoctagon\", \"lightcoral\", \"\\\"filled,bold\\\"\"),\n        NodeType::BasicBlock => (\"box\", \"lightblue\", \"filled\"),\n        NodeType::Branch => (\"diamond\", \"yellow\", \"filled\"),\n        NodeType::LoopHeader => (\"box\", \"orange\", \"\\\"filled,rounded\\\"\"),\n    }\n}\n\npub fn cyclomatic_complexity(cfg: &crate::types::FunctionCfg) -> usize {\n    let edges = cfg.edges.len() as isize;\n    let nodes = cfg.nodes.len() as isize;\n    let exits = 1isize; // assume one exit\n    let cc = edges - nodes + 2 * exits;\n    if cc <= 0 {\n        1\n    } else {\n        cc as usize\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/010_scc_compressor.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/040_scc_compressor.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/010_scc_compressor.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/040_scc_compressor.rs",
          "original_content": "//! SCC (Strongly Connected Components) Compression\n//!\n//! This module compresses strongly connected components in a directed graph\n//! to prevent path explosion during invariant detection. Each SCC is compressed\n//! into a single node, guaranteeing the resulting graph is a DAG.\n\nuse petgraph::algo::tarjan_scc;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse petgraph::visit::EdgeRef;\nuse std::collections::HashMap;\n\n/// A compressed graph where each SCC is represented as a single node\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct SccCompression {\n    /// Original graph\n    pub original_graph: DiGraph<String, ()>,\n\n    /// Compressed graph (guaranteed to be a DAG)\n    pub compressed_graph: DiGraph<Vec<String>, ()>,\n\n    /// Mapping from original node index to SCC ID\n    pub node_to_scc: HashMap<NodeIndex, usize>,\n\n    /// Mapping from SCC ID to compressed node index\n    pub scc_to_node: HashMap<usize, NodeIndex>,\n\n    /// All SCCs detected\n    pub sccs: Vec<Vec<NodeIndex>>,\n}\n\n#[allow(dead_code)]\nimpl SccCompression {\n    /// Create a new SCC compression from a call graph\n    ///\n    /// # Arguments\n    /// * `graph` - The directed graph to compress\n    ///\n    /// # Returns\n    /// A compressed representation where each SCC is a single node\n    pub fn new(graph: DiGraph<String, ()>) -> Self {\n        // Step 1: Detect SCCs using Tarjan's algorithm\n        let sccs = tarjan_scc(&graph);\n\n        // Step 2: Build mapping from node to SCC ID\n        let mut node_to_scc = HashMap::new();\n        for (scc_id, scc) in sccs.iter().enumerate() {\n            for &node in scc {\n                node_to_scc.insert(node, scc_id);\n            }\n        }\n\n        // Step 3: Build compressed graph\n        let mut compressed_graph = DiGraph::new();\n        let mut scc_to_node = HashMap::new();\n\n        // Add one node per SCC\n        for (scc_id, scc) in sccs.iter().enumerate() {\n            let members: Vec<String> = scc\n                .iter()\n                .map(|&node_idx| graph[node_idx].clone())\n                .collect();\n            let compressed_node = compressed_graph.add_node(members);\n            scc_to_node.insert(scc_id, compressed_node);\n        }\n\n        // Add edges between SCCs (but not within SCCs)\n        for edge in graph.edge_references() {\n            let source_scc = node_to_scc[&edge.source()];\n            let target_scc = node_to_scc[&edge.target()];\n\n            // Only add edge if it goes between different SCCs\n            if source_scc != target_scc {\n                let source_node = scc_to_node[&source_scc];\n                let target_node = scc_to_node[&target_scc];\n\n                // Check if edge already exists to avoid duplicates\n                if !compressed_graph.contains_edge(source_node, target_node) {\n                    compressed_graph.add_edge(source_node, target_node, ());\n                }\n            }\n        }\n\n        Self {\n            original_graph: graph,\n            compressed_graph,\n            node_to_scc,\n            scc_to_node,\n            sccs,\n        }\n    }\n\n    /// Check if the compressed graph is a DAG\n    ///\n    /// This should always return true after SCC compression\n    pub fn is_dag(&self) -> bool {\n        petgraph::algo::is_cyclic_directed(&self.compressed_graph) == false\n    }\n\n    /// Get the SCC containing a node name\n    pub fn get_scc_for_node(&self, node_name: &str) -> Option<usize> {\n        for (node_idx, name) in self.original_graph.node_weights().enumerate() {\n            if name == node_name {\n                let node_index = NodeIndex::new(node_idx);\n                return self.node_to_scc.get(&node_index).copied();\n            }\n        }\n        None\n    }\n\n    /// Get all members of an SCC\n    pub fn get_scc_members(&self, scc_id: usize) -> Vec<String> {\n        if let Some(scc) = self.sccs.get(scc_id) {\n            scc.iter()\n                .map(|&node_idx| self.original_graph[node_idx].clone())\n                .collect()\n        } else {\n            Vec::new()\n        }\n    }\n\n    /// Count trivial SCCs (size 1)\n    pub fn count_trivial_sccs(&self) -> usize {\n        self.sccs.iter().filter(|scc| scc.len() == 1).count()\n    }\n\n    /// Count non-trivial SCCs (size > 1, i.e., cycles)\n    pub fn count_nontrivial_sccs(&self) -> usize {\n        self.sccs.iter().filter(|scc| scc.len() > 1).count()\n    }\n\n    /// Get statistics about the compression\n    pub fn stats(&self) -> SccStats {\n        SccStats {\n            original_node_count: self.original_graph.node_count(),\n            compressed_node_count: self.compressed_graph.node_count(),\n            trivial_sccs: self.count_trivial_sccs(),\n            nontrivial_sccs: self.count_nontrivial_sccs(),\n            is_dag: self.is_dag(),\n        }\n    }\n}\n\n/// Statistics about SCC compression\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct SccStats {\n    pub original_node_count: usize,\n    pub compressed_node_count: usize,\n    pub trivial_sccs: usize,\n    pub nontrivial_sccs: usize,\n    pub is_dag: bool,\n}\n\nimpl std::fmt::Display for SccStats {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"SCC Compression: {} nodes  {} SCCs ({} trivial, {} cycles) - DAG: {}\",\n            self.original_node_count,\n            self.compressed_node_count,\n            self.trivial_sccs,\n            self.nontrivial_sccs,\n            self.is_dag\n        )\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_scc_compression_dag() {\n        // Create a simple DAG: A -> B -> C\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n\n        let compression = SccCompression::new(graph);\n\n        assert!(compression.is_dag());\n        assert_eq!(compression.count_trivial_sccs(), 3);\n        assert_eq!(compression.count_nontrivial_sccs(), 0);\n    }\n\n    #[test]\n    fn test_scc_compression_cycle() {\n        // Create a graph with a cycle: A -> B -> C -> A\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n        graph.add_edge(c, a, ());\n\n        let compression = SccCompression::new(graph);\n\n        assert!(compression.is_dag());\n        assert_eq!(compression.count_nontrivial_sccs(), 1);\n        assert_eq!(compression.compressed_graph.node_count(), 1);\n    }\n\n    #[test]\n    fn test_scc_compression_mixed() {\n        // Create a graph with a cycle and a leaf: A -> B -> C -> B, A -> D\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        let d = graph.add_node(\"D\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n        graph.add_edge(c, b, ()); // Cycle: B <-> C\n        graph.add_edge(a, d, ());\n\n        let compression = SccCompression::new(graph);\n\n        assert!(compression.is_dag());\n        assert_eq!(compression.count_nontrivial_sccs(), 1); // B-C cycle\n        assert_eq!(compression.count_trivial_sccs(), 2); // A and D\n        assert_eq!(compression.compressed_graph.node_count(), 3);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/020_cluster_010.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/050_cluster_010.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/020_cluster_010.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/050_cluster_010.rs",
          "original_content": "//! Cluster 010: Module resolution and dependency extraction utilities\n//!\n//! This module contains foundational functions for:\n//! - Module name resolution and path mapping\n//! - Rust dependency extraction from source files\n//! - Julia dependency extraction from source files\n//!\n//! Functions moved from src/000_dependency.rs as part of Phase 2, Batch 6 refactoring.\n\nuse anyhow::{Context, Result};\nuse once_cell::sync::Lazy;\nuse regex::Regex;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::ItemUse;\nuse walkdir::WalkDir;\n\nuse crate::dependency::RootState;\n\n// ============================================================================\n// Module Resolution (from src/000_dependency.rs)\n// ============================================================================\n\npub fn normalize_module_name(name: &str) -> String {\n    if let Some(pos) = name.find('_') {\n        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n            return name[pos + 1..].to_string();\n        }\n    }\n    name.to_string()\n}\n\npub fn resolve_module(\n    root: &str,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Option<PathBuf> {\n    let key = normalize_module_name(root);\n    if let Some(path) = module_map.get(&key) {\n        return Some(path.clone());\n    }\n    module_map\n        .iter()\n        .find(|(name, _)| name == &&key)\n        .map(|(_, path)| path.clone())\n        .or_else(|| {\n            module_map\n                .iter()\n                .find(|(name, _)| key.starts_with(name.as_str()))\n                .map(|(_, path)| path.clone())\n        })\n        .or_else(|| crate::cluster_011::resolve_path(&PathBuf::from(root), file_set, module_map))\n}\n\npub fn contains_tools(path: &Path) -> bool {\n    path.components().any(|c| c.as_os_str() == \"tools\")\n}\n\n#[derive(Clone)]\npub struct ModuleRoot {\n    pub layer: String,\n}\n\npub fn build_module_root_map(root: &Path) -> Result<HashMap<String, ModuleRoot>, std::io::Error> {\n    let src_dir = root.join(\"src\");\n    let mut map = HashMap::new();\n    if src_dir.is_dir() {\n        for entry in fs::read_dir(&src_dir)? {\n            let entry = entry?;\n            let path = entry.path();\n            if contains_tools(&path) {\n                continue;\n            }\n            let name = entry\n                .file_name()\n                .to_string_lossy()\n                .to_string()\n                .trim_end_matches(\".rs\")\n                .to_string();\n            if path.is_dir() {\n                let normalized = normalize_module_name(&name);\n                map.insert(\n                    normalized,\n                    ModuleRoot {\n                        layer: name.clone(),\n                    },\n                );\n            } else if path.extension().map(|ext| ext == \"rs\").unwrap_or(false) {\n                map.insert(\n                    name.clone(),\n                    ModuleRoot {\n                        layer: crate::cluster_001::detect_layer(&path),\n                    },\n                );\n            }\n        }\n    }\n    Ok(map)\n}\n\n// ============================================================================\n// Julia Dependency Ordering (from src/020_layer_utilities.rs)\n// ============================================================================\n\nstruct LayerResolver {\n    aliases: HashMap<String, String>,\n}\n\nfn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\nimpl LayerResolver {\n    fn build(root: &Path) -> Result<Self> {\n        let mut resolver = LayerResolver {\n            aliases: HashMap::new(),\n        };\n        let src_dir = resolve_source_root(root);\n        if src_dir.is_dir() {\n            for entry in WalkDir::new(&src_dir).into_iter().filter_map(|e| e.ok()) {\n                let path = entry.path();\n                if contains_tools(path) {\n                    continue;\n                }\n                let layer = crate::cluster_001::detect_layer(path);\n                if layer == \"root\" {\n                    continue;\n                }\n                if path.is_dir() {\n                    if let Some(name) = path.file_name().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(name, &layer);\n                    }\n                } else if path.extension().map_or(false, |ext| ext == \"jl\") {\n                    if let Some(stem) = path.file_stem().and_then(|n| n.to_str()) {\n                        resolver.add_aliases(stem, &layer);\n                    }\n                }\n            }\n        }\n        Ok(resolver)\n    }\n\n    fn add_aliases(&mut self, name: &str, layer: &str) {\n        let lower = name.to_lowercase();\n        self.aliases\n            .entry(lower.clone())\n            .or_insert_with(|| layer.to_string());\n        let condensed = lower.replace('_', \"\");\n        self.aliases\n            .entry(condensed)\n            .or_insert_with(|| layer.to_string());\n    }\n\n    fn resolve_module(&self, module: &str) -> Option<String> {\n        let key = module.to_lowercase();\n        if let Some(layer) = self.aliases.get(&key) {\n            return Some(layer.clone());\n        }\n        let condensed = key.replace('_', \"\");\n        if let Some(layer) = self.aliases.get(&condensed) {\n            return Some(layer.clone());\n        }\n        self.aliases\n            .iter()\n            .filter(|(alias, _)| !alias.is_empty())\n            .find(|(alias, _)| key.starts_with(alias.as_str()))\n            .map(|(_, layer)| layer.clone())\n    }\n}\n\npub fn order_julia_files_by_dependency(\n    files: &[PathBuf],\n    root: &Path,\n) -> Result<(Vec<PathBuf>, crate::dependency::LayerGraph)> {\n    use crate::cluster_001::{collect_julia_dependencies, JuliaTarget};\n    use crate::dependency::ReferenceDetail;\n\n    let mut file_layers: HashMap<PathBuf, String> = HashMap::new();\n    let mut nodes: BTreeSet<String> = BTreeSet::new();\n    let mut edges_map: BTreeMap<(String, String), BTreeSet<ReferenceDetail>> = BTreeMap::new();\n    let mut unresolved = Vec::new();\n    let resolver = LayerResolver::build(root)?;\n    let entry_files = crate::cluster_001::julia_entry_paths(root);\n\n    for file in files {\n        let layer = crate::cluster_001::detect_layer(file);\n        nodes.insert(layer.clone());\n        file_layers.insert(file.clone(), layer.clone());\n\n        let references = collect_julia_dependencies(file)\n            .with_context(|| format!(\"Failed to analyze Julia dependencies for {:?}\", file))?;\n        for dep in references {\n            match dep.target {\n                JuliaTarget::Include(include_path) => {\n                    let resolved = if include_path.is_absolute() {\n                        include_path.clone()\n                    } else {\n                        file.parent()\n                            .map(|p| p.join(&include_path))\n                            .unwrap_or(include_path.clone())\n                    };\n\n                    if resolved.exists() {\n                        let target_layer = crate::cluster_001::detect_layer(&resolved);\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n                JuliaTarget::Module(module) => {\n                    if let Some(target_layer) = resolver.resolve_module(&module) {\n                        nodes.insert(target_layer.clone());\n                        if target_layer != layer {\n                            edges_map\n                                .entry((target_layer.clone(), layer.clone()))\n                                .or_default()\n                                .insert(ReferenceDetail {\n                                    file: file.clone(),\n                                    reference: dep.detail.clone(),\n                                });\n                        }\n                    } else {\n                        unresolved.push(crate::dependency::UnresolvedDependency {\n                            file: file.clone(),\n                            reference: dep.detail.clone(),\n                        });\n                    }\n                }\n            }\n        }\n    }\n\n    crate::cluster_008::build_result(\n        files,\n        file_layers,\n        nodes,\n        edges_map,\n        unresolved,\n        &entry_files,\n    )\n}\n\n// ============================================================================\n// Rust Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_rust_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    #[derive(Default)]\n    struct UseCollector {\n        roots: BTreeSet<String>,\n        mods: BTreeSet<String>,\n    }\n\n    impl<'ast> Visit<'ast> for UseCollector {\n        fn visit_item_use(&mut self, node: &'ast ItemUse) {\n            crate::dependency::collect_roots(&node.tree, RootState::Start, &mut self.roots);\n        }\n\n        fn visit_item_mod(&mut self, node: &'ast syn::ItemMod) {\n            if node.content.is_none() {\n                self.mods.insert(node.ident.to_string());\n            }\n        }\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let syntax = syn::parse_file(&content)\n        .with_context(|| format!(\"Unable to parse Rust file {:?}\", file))?;\n    let mut collector = UseCollector::default();\n    collector.visit_file(&syntax);\n    let mut deps = Vec::new();\n    for root in collector.roots {\n        if let Some(path) = resolve_module(&root, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    for module in collector.mods {\n        if let Some(path) = resolve_module(&module, file_set, module_map) {\n            deps.push(path);\n        }\n    }\n    Ok(deps)\n}\n\n// ============================================================================\n// Julia Dependency Extraction (from src/000_dependency.rs)\n// ============================================================================\n\npub fn extract_julia_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    static INCLUDE_RE: Lazy<Regex> =\n        Lazy::new(|| Regex::new(r#\"include\\s*\\(\\s*[\"']([^\"']+)[\"']\"#).unwrap());\n    static MMSB_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\.([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static MMSB_SYMBOL_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+MMSB\\s*:\\s*([A-Za-z0-9_,\\s]+)\"#).unwrap()\n    });\n    static LOCAL_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+\\.\\s*([A-Za-z0-9_\\.]+)\"#).unwrap()\n    });\n    static PLAIN_USING_RE: Lazy<Regex> = Lazy::new(|| {\n        Regex::new(r#\"(?m)^\\s*(?:using|import)\\s+([A-Za-z_][A-Za-z0-9_\\.]*)\"#).unwrap()\n    });\n\n    fn resolve_module_name(\n        module: &str,\n        file_set: &HashSet<PathBuf>,\n        module_map: &HashMap<String, PathBuf>,\n    ) -> Option<PathBuf> {\n        let primary = module.split('.').next().unwrap_or(module);\n        resolve_module(primary, file_set, module_map)\n    }\n\n    let content =\n        fs::read_to_string(file).with_context(|| format!(\"Unable to read {:?}\", file))?;\n    let mut deps = Vec::new();\n\n    for cap in INCLUDE_RE.captures_iter(&content) {\n        if let Some(path_match) = cap.get(1) {\n            let raw = path_match.as_str();\n            let mut candidate = PathBuf::from(raw);\n            if candidate.extension().is_none() {\n                candidate.set_extension(\"jl\");\n            }\n            let resolved = if candidate.is_absolute() {\n                candidate\n            } else {\n                file.parent()\n                    .map(|p| p.join(&candidate))\n                    .unwrap_or(candidate)\n            };\n            if let Some(path) = crate::cluster_011::resolve_path(&resolved, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in MMSB_SYMBOL_RE.captures_iter(&content) {\n        if let Some(symbols) = cap.get(1) {\n            for symbol in symbols\n                .as_str()\n                .split(',')\n                .map(|s| s.trim())\n                .filter(|s| !s.is_empty())\n            {\n                if let Some(path) = resolve_module_name(symbol, file_set, module_map) {\n                    deps.push(path);\n                }\n            }\n        }\n    }\n\n    for cap in LOCAL_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            if let Some(path) = resolve_module_name(module_match.as_str(), file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    for cap in PLAIN_USING_RE.captures_iter(&content) {\n        if let Some(module_match) = cap.get(1) {\n            let module = module_match.as_str();\n            if module.starts_with(\"MMSB\") {\n                continue;\n            }\n            if let Some(path) = resolve_module_name(module, file_set, module_map) {\n                deps.push(path);\n            }\n        }\n    }\n\n    Ok(deps)\n}\n\n// ============================================================================\n// File Dependency Mapping (from src/090_file_ordering.rs)\n// ============================================================================\n\npub fn build_dependency_map(\n    files: &[PathBuf],\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<HashMap<PathBuf, Vec<PathBuf>>> {\n    let mut dep_map: HashMap<PathBuf, Vec<PathBuf>> = HashMap::new();\n    for file in files {\n        let deps = extract_dependencies(file, file_set, module_map)\n            .with_context(|| format!(\"Failed to extract dependencies for {:?}\", file))?;\n        dep_map.insert(file.clone(), deps);\n    }\n    Ok(dep_map)\n}\n\npub(crate) fn extract_dependencies(\n    file: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Result<Vec<PathBuf>> {\n    let ext = file.extension().and_then(|s| s.to_str()).unwrap_or(\"\");\n    match ext {\n        \"rs\" => extract_rust_dependencies(file, file_set, module_map),\n        \"jl\" => extract_julia_dependencies(file, file_set, module_map),\n        _ => Ok(Vec::new()),\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/020_layer_inference.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/060_layer_inference.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/020_layer_inference.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/060_layer_inference.rs",
          "original_content": "//! Layer Inference from Call Graph\n//!\n//! This module infers layer assignments from the call graph structure,\n//! NOT from filename prefixes. This makes the system self-correcting\n//! and prevents silent drift.\n//!\n//! Algorithm:\n//! 1. Assign layer 0 to all leaf nodes (out-degree = 0)\n//! 2. Repeat until fixpoint:\n//!    - For each unassigned node:\n//!      - If all callees have layers assigned:\n//!        - layer(node) = max(layer(callee)) + 1\n//! 3. Remaining nodes get max + 1\n\nuse crate::invariant_types::LayerInfo;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse petgraph::visit::EdgeRef;\nuse petgraph::Direction;\nuse std::collections::HashMap;\n\n/// Infer layers from call graph structure\n///\n/// # Arguments\n/// * `graph` - Call graph (edge from caller to callee)\n/// * `max_iterations` - Maximum fixpoint iterations (default: 100)\n///\n/// # Returns\n/// HashMap mapping node name to layer information\npub fn infer_layers(graph: &DiGraph<String, ()>, max_iterations: usize) -> HashMap<String, LayerInfo> {\n    let mut layers: HashMap<NodeIndex, usize> = HashMap::new();\n    let mut result: HashMap<String, LayerInfo> = HashMap::new();\n\n    // Step 1: Assign layer 0 to all leaf nodes (out-degree = 0)\n    for node_idx in graph.node_indices() {\n        let out_degree = graph.neighbors_directed(node_idx, Direction::Outgoing).count();\n        if out_degree == 0 {\n            layers.insert(node_idx, 0);\n        }\n    }\n\n    // Step 2: Fixpoint iteration\n    let mut changed = true;\n    let mut iteration = 0;\n\n    while changed && iteration < max_iterations {\n        changed = false;\n        iteration += 1;\n\n        for node_idx in graph.node_indices() {\n            // Skip if already assigned\n            if layers.contains_key(&node_idx) {\n                continue;\n            }\n\n            // Get all callees (outgoing edges)\n            let callees: Vec<NodeIndex> = graph\n                .neighbors_directed(node_idx, Direction::Outgoing)\n                .collect();\n\n            // Check if all callees have layer assignments\n            let all_assigned = callees.iter().all(|callee| layers.contains_key(callee));\n\n            if all_assigned && !callees.is_empty() {\n                // Compute layer = max(callee layers) + 1\n                let max_callee_layer = callees\n                    .iter()\n                    .filter_map(|callee| layers.get(callee))\n                    .max()\n                    .copied()\n                    .unwrap_or(0);\n\n                layers.insert(node_idx, max_callee_layer + 1);\n                changed = true;\n            }\n        }\n    }\n\n    // Step 3: Assign remaining nodes (those in cycles or unreachable)\n    let max_layer = layers.values().max().copied().unwrap_or(0);\n    for node_idx in graph.node_indices() {\n        if !layers.contains_key(&node_idx) {\n            layers.insert(node_idx, max_layer + 1);\n        }\n    }\n\n    // Step 4: Build LayerInfo for each node\n    for (node_idx, &layer) in &layers {\n        let name = graph[*node_idx].clone();\n\n        // Get dependencies (callees)\n        let dependencies: Vec<String> = graph\n            .neighbors_directed(*node_idx, Direction::Outgoing)\n            .map(|callee_idx| graph[callee_idx].clone())\n            .collect();\n\n        // Get max dependency layer\n        let max_dependency_layer = if dependencies.is_empty() {\n            None\n        } else {\n            dependencies\n                .iter()\n                .filter_map(|dep_name| {\n                    graph\n                        .node_indices()\n                        .find(|idx| &graph[*idx] == dep_name)\n                        .and_then(|idx| layers.get(&idx))\n                        .copied()\n                })\n                .max()\n        };\n\n        result.insert(\n            name.clone(),\n            LayerInfo {\n                name,\n                layer,\n                dependencies,\n                max_dependency_layer,\n            },\n        );\n    }\n\n    result\n}\n\n/// Detect layer violations in the call graph\n///\n/// A violation occurs when a lower-layer function calls a higher-layer function\n///\n/// # Arguments\n/// * `layer_assignments` - Layer information for each node\n/// * `graph` - The call graph\n///\n/// # Returns\n/// Vec of (caller, callee, caller_layer, callee_layer) tuples\npub fn detect_layer_violations(\n    layer_assignments: &HashMap<String, LayerInfo>,\n    graph: &DiGraph<String, ()>,\n) -> Vec<(String, String, usize, usize)> {\n    let mut violations = Vec::new();\n\n    for edge in graph.edge_references() {\n        let caller = &graph[edge.source()];\n        let callee = &graph[edge.target()];\n\n        if let (Some(caller_info), Some(callee_info)) =\n            (layer_assignments.get(caller), layer_assignments.get(callee))\n        {\n            // Violation: caller layer <= callee layer (should be strictly greater)\n            // Actually, caller should be > callee (higher layers call lower layers)\n            if caller_info.layer <= callee_info.layer {\n                violations.push((\n                    caller.clone(),\n                    callee.clone(),\n                    caller_info.layer,\n                    callee_info.layer,\n                ));\n            }\n        }\n    }\n\n    violations\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_layer_inference_simple_dag() {\n        // Create a simple DAG: A -> B -> C\n        // Expected layers: C=0, B=1, A=2\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n\n        let layers = infer_layers(&graph, 100);\n\n        assert_eq!(layers.get(\"C\").unwrap().layer, 0); // Leaf\n        assert_eq!(layers.get(\"B\").unwrap().layer, 1);\n        assert_eq!(layers.get(\"A\").unwrap().layer, 2);\n    }\n\n    #[test]\n    fn test_layer_inference_diamond() {\n        // Diamond: A -> B -> D, A -> C -> D\n        // Expected layers: D=0, B=1, C=1, A=2\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        let d = graph.add_node(\"D\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(a, c, ());\n        graph.add_edge(b, d, ());\n        graph.add_edge(c, d, ());\n\n        let layers = infer_layers(&graph, 100);\n\n        assert_eq!(layers.get(\"D\").unwrap().layer, 0);\n        assert_eq!(layers.get(\"B\").unwrap().layer, 1);\n        assert_eq!(layers.get(\"C\").unwrap().layer, 1);\n        assert_eq!(layers.get(\"A\").unwrap().layer, 2);\n    }\n\n    #[test]\n    fn test_detect_layer_violations_none() {\n        // Proper layering: A(2) -> B(1) -> C(0)\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n\n        let layers = infer_layers(&graph, 100);\n        let violations = detect_layer_violations(&layers, &graph);\n\n        assert_eq!(violations.len(), 0);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/030_cluster_011.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_cluster_011.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/030_cluster_011.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_cluster_011.rs",
          "original_content": "//! Cluster 011: File ordering and analysis utilities extracted from core modules.\n//!\n//! This module consolidates:\n//! - Module name normalization\n//! - Module path resolution\n//! - Directory/file graph utilities\n\nuse anyhow::Result;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse std::collections::{HashMap, HashSet};\nuse std::path::{Path, PathBuf};\n\npub fn build_module_map(files: &[PathBuf]) -> HashMap<String, PathBuf> {\n    let mut map = HashMap::new();\n    for file in files {\n        if let Some(stem) = file.file_stem().and_then(|s| s.to_str()) {\n            let normalized = crate::cluster_010::normalize_module_name(stem);\n            map.insert(normalized.clone(), file.clone());\n            if stem == \"mod\" {\n                if let Some(parent) = file.parent().and_then(|p| p.file_name()) {\n                    if let Some(name) = parent.to_str() {\n                        map.insert(crate::cluster_010::normalize_module_name(name), file.clone());\n                    }\n                }\n            }\n        }\n    }\n    map\n}\n\npub fn resolve_path(\n    candidate: &Path,\n    file_set: &HashSet<PathBuf>,\n    module_map: &HashMap<String, PathBuf>,\n) -> Option<PathBuf> {\n    if file_set.contains(candidate) {\n        return Some(candidate.to_path_buf());\n    }\n    if let Some(file_name) = candidate.file_stem().and_then(|s| s.to_str()) {\n        let key = crate::cluster_010::normalize_module_name(file_name);\n        if let Some(path) = module_map.get(&key) {\n            return Some(path.clone());\n        }\n    }\n    None\n}\n\npub fn build_directory_dag(dir: &PathBuf) -> Result<DiGraph<PathBuf, ()>> {\n    let files: Vec<PathBuf> = walkdir::WalkDir::new(dir)\n        .into_iter()\n        .filter_map(|e| e.ok())\n        .filter(|e| {\n            e.path()\n                .extension()\n                .and_then(|ext| ext.to_str())\n                .map(|ext| ext == \"rs\" || ext == \"jl\")\n                .unwrap_or(false)\n        })\n        .map(|entry| entry.into_path())\n        .collect();\n\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = build_module_map(&files);\n    let dep_map = crate::cluster_010::build_dependency_map(&files, &file_set, &module_map)?;\n    let (graph, _) = build_file_dag(&files, &dep_map);\n    Ok(graph)\n}\n\npub fn build_file_dependency_graph(files: &[PathBuf]) -> Result<DiGraph<PathBuf, ()>> {\n    let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n    let module_map = build_module_map(files);\n    let dep_map = crate::cluster_010::build_dependency_map(files, &file_set, &module_map)?;\n    let (graph, _) = build_file_dag(files, &dep_map);\n    Ok(graph)\n}\n\npub fn export_program_cfg_to_path(\n    result: &crate::types::AnalysisResult,\n    call_edges: &[(String, String)],\n    output_path: &Path,\n) -> std::io::Result<()> {\n    use crate::types::ProgramCFG;\n\n    let mut program_cfg = ProgramCFG {\n        functions: HashMap::new(),\n        call_edges: Vec::new(),\n    };\n\n    for cfg in &result.cfgs {\n        program_cfg\n            .functions\n            .insert(cfg.function.clone(), cfg.clone());\n    }\n\n    for (caller, callee) in call_edges {\n        let caller_name = caller.split(\"::\").last().unwrap_or(caller).to_string();\n        let callee_name = callee.split(\"::\").last().unwrap_or(callee).to_string();\n        if program_cfg.functions.contains_key(&caller_name)\n            && program_cfg.functions.contains_key(&callee_name)\n        {\n            program_cfg.call_edges.push((caller_name, callee_name));\n        }\n    }\n\n    let cfg_dir = output_path.join(\"30_cfg\");\n    std::fs::create_dir_all(&cfg_dir)?;\n    let dot_path = cfg_dir.join(\"complete_program.dot\");\n    crate::cluster_001::export_complete_program_dot(\n        &program_cfg,\n        dot_path.to_string_lossy().as_ref(),\n    )?;\n\n    #[cfg(feature = \"png\")]\n    {\n        let png_path = cfg_dir.join(\"complete_program.png\");\n        if let (Some(dot_path_str), Some(png_path_str)) =\n            (dot_path.to_str(), png_path.to_str())\n        {\n            let _ = std::process::Command::new(\"dot\")\n                .args([\"-Tpng\", dot_path_str, \"-o\", png_path_str])\n                .status();\n        }\n    }\n\n    Ok(())\n}\n\npub(crate) fn build_file_dag(\n    files: &[PathBuf],\n    dep_map: &HashMap<PathBuf, Vec<PathBuf>>,\n) -> (DiGraph<PathBuf, ()>, HashMap<PathBuf, NodeIndex>) {\n    let mut graph = DiGraph::new();\n    let mut node_map = HashMap::new();\n\n    for file in files {\n        let node = graph.add_node(file.clone());\n        node_map.insert(file.clone(), node);\n    }\n\n    for (file, deps) in dep_map {\n        if let Some(&file_node) = node_map.get(file) {\n            for dep in deps {\n                if let Some(&dep_node) = node_map.get(dep) {\n                    graph.add_edge(dep_node, file_node, ());\n                }\n            }\n        }\n    }\n\n    (graph, node_map)\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/030_fixpoint_solver.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/080_fixpoint_solver.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/030_fixpoint_solver.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/080_fixpoint_solver.rs",
          "original_content": "//! Fixpoint Solver for Symbolic Abstraction Propagation\n//!\n//! This module propagates symbolic abstractions through the call graph\n//! until a fixpoint is reached. Abstractions that remain stable at the\n//! fixpoint are candidate invariants.\n\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse petgraph::Direction;\nuse std::collections::{HashMap, HashSet};\n\n/// Symbolic abstraction of program state\n#[derive(Debug, Clone, PartialEq, Eq)]\n#[allow(dead_code)]\npub struct SymbolicAbstraction {\n    /// Type signature\n    pub type_sig: Option<String>,\n\n    /// Effect signature (I/O, mutation, etc.)\n    pub effects: HashSet<String>,\n\n    /// Layer assignment\n    pub layer: Option<usize>,\n\n    /// Ownership/visibility\n    pub visibility: Option<String>,\n\n    /// Additional properties\n    pub properties: HashSet<String>,\n}\n\nimpl SymbolicAbstraction {\n    pub fn new() -> Self {\n        Self {\n            type_sig: None,\n            effects: HashSet::new(),\n            layer: None,\n            visibility: None,\n            properties: HashSet::new(),\n        }\n    }\n\n    /// Check if two abstractions are approximately equal\n    pub fn approx_eq(&self, other: &Self) -> bool {\n        self.type_sig == other.type_sig\n            && self.effects == other.effects\n            && self.layer == other.layer\n            && self.visibility == other.visibility\n            && self.properties == other.properties\n    }\n\n    /// Merge two abstractions (used during propagation)\n    pub fn merge(&mut self, other: &Self) {\n        // Type signature: take most specific (if both exist, keep current)\n        if self.type_sig.is_none() && other.type_sig.is_some() {\n            self.type_sig = other.type_sig.clone();\n        }\n\n        // Effects: union\n        self.effects.extend(other.effects.clone());\n\n        // Layer: take maximum\n        match (self.layer, other.layer) {\n            (Some(l1), Some(l2)) => self.layer = Some(l1.max(l2)),\n            (None, Some(l)) => self.layer = Some(l),\n            _ => {}\n        }\n\n        // Visibility: take most restrictive\n        if self.visibility.is_none() && other.visibility.is_some() {\n            self.visibility = other.visibility.clone();\n        }\n\n        // Properties: union\n        self.properties.extend(other.properties.clone());\n    }\n}\n\nimpl Default for SymbolicAbstraction {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n/// Result of fixpoint computation\n#[derive(Debug)]\n#[allow(dead_code)]\npub struct FixpointResult {\n    /// Final abstraction for each node\n    pub abstractions: HashMap<String, SymbolicAbstraction>,\n\n    /// Number of iterations to convergence\n    pub iterations: usize,\n\n    /// Whether fixpoint was reached\n    pub converged: bool,\n\n    /// Nodes whose abstractions were stable\n    pub stable_nodes: Vec<String>,\n}\n\n/// Propagate symbolic abstractions until fixpoint\n///\n/// # Arguments\n/// * `graph` - Call graph\n/// * `initial` - Initial abstractions for each node\n/// * `max_iterations` - Maximum iterations before giving up\n///\n/// # Returns\n/// FixpointResult containing final abstractions and metadata\n#[allow(dead_code)]\npub fn propagate_to_fixpoint(\n    graph: &DiGraph<String, ()>,\n    initial: HashMap<String, SymbolicAbstraction>,\n    max_iterations: usize,\n) -> FixpointResult {\n    // Build mapping from node name to index\n    let mut name_to_idx: HashMap<String, NodeIndex> = HashMap::new();\n    for idx in graph.node_indices() {\n        name_to_idx.insert(graph[idx].clone(), idx);\n    }\n\n    // Initialize abstractions\n    let mut current: HashMap<NodeIndex, SymbolicAbstraction> = HashMap::new();\n    for (name, abstraction) in &initial {\n        if let Some(&idx) = name_to_idx.get(name) {\n            current.insert(idx, abstraction.clone());\n        }\n    }\n\n    // Fixpoint iteration\n    let mut iteration = 0;\n    let mut converged = false;\n\n    while iteration < max_iterations {\n        iteration += 1;\n        let mut next = current.clone();\n        let mut changed = false;\n\n        // Propagate from callees to callers\n        for node_idx in graph.node_indices() {\n            let mut new_abstraction = current\n                .get(&node_idx)\n                .cloned()\n                .unwrap_or_else(SymbolicAbstraction::new);\n\n            // Get all callees (outgoing edges)\n            for callee_idx in graph.neighbors_directed(node_idx, Direction::Outgoing) {\n                if let Some(callee_abs) = current.get(&callee_idx) {\n                    new_abstraction.merge(callee_abs);\n                }\n            }\n\n            // Check if changed\n            if let Some(old) = current.get(&node_idx) {\n                if !old.approx_eq(&new_abstraction) {\n                    changed = true;\n                }\n            } else {\n                changed = true;\n            }\n\n            next.insert(node_idx, new_abstraction);\n        }\n\n        current = next;\n\n        if !changed {\n            converged = true;\n            break;\n        }\n    }\n\n    // Convert back to name-based mapping\n    let mut abstractions = HashMap::new();\n    let mut stable_nodes = Vec::new();\n\n    for (idx, abstraction) in &current {\n        let name = graph[*idx].clone();\n\n        // Check if this abstraction matches initial (stable)\n        if let Some(initial_abs) = initial.get(&name) {\n            if initial_abs.approx_eq(abstraction) {\n                stable_nodes.push(name.clone());\n            }\n        }\n\n        abstractions.insert(name, abstraction.clone());\n    }\n\n    FixpointResult {\n        abstractions,\n        iterations: iteration,\n        converged,\n        stable_nodes,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_symbolic_abstraction_merge() {\n        let mut abs1 = SymbolicAbstraction::new();\n        abs1.type_sig = Some(\"String\".to_string());\n        abs1.effects.insert(\"IO\".to_string());\n        abs1.layer = Some(1);\n\n        let mut abs2 = SymbolicAbstraction::new();\n        abs2.effects.insert(\"Mutation\".to_string());\n        abs2.layer = Some(2);\n\n        abs1.merge(&abs2);\n\n        assert_eq!(abs1.type_sig, Some(\"String\".to_string()));\n        assert!(abs1.effects.contains(\"IO\"));\n        assert!(abs1.effects.contains(\"Mutation\"));\n        assert_eq!(abs1.layer, Some(2)); // max\n    }\n\n    #[test]\n    fn test_fixpoint_simple() {\n        // Graph: A -> B -> C\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n\n        // Initial: C has effect \"Pure\"\n        let mut initial = HashMap::new();\n        let mut c_abs = SymbolicAbstraction::new();\n        c_abs.effects.insert(\"Pure\".to_string());\n        initial.insert(\"C\".to_string(), c_abs);\n\n        initial.insert(\"A\".to_string(), SymbolicAbstraction::new());\n        initial.insert(\"B\".to_string(), SymbolicAbstraction::new());\n\n        let result = propagate_to_fixpoint(&graph, initial, 100);\n\n        assert!(result.converged);\n        // A and B should propagate \"Pure\" from C\n        assert!(result.abstractions[\"A\"].effects.contains(\"Pure\"));\n        assert!(result.abstractions[\"B\"].effects.contains(\"Pure\"));\n    }\n\n    #[test]\n    fn test_fixpoint_convergence() {\n        // Simple chain should converge quickly\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        graph.add_edge(a, b, ());\n\n        let mut initial = HashMap::new();\n        initial.insert(\"A\".to_string(), SymbolicAbstraction::new());\n        initial.insert(\"B\".to_string(), SymbolicAbstraction::new());\n\n        let result = propagate_to_fixpoint(&graph, initial, 100);\n\n        assert!(result.converged);\n        assert!(result.iterations < 10);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/040_dependency.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/090_dependency.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/040_dependency.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/090_dependency.rs",
          "original_content": "//! Dependency-aware ordering and layer graph extraction.\n\npub use crate::cluster_010::order_julia_files_by_dependency;\n#[allow(unused_imports)]\npub use crate::cluster_001::{detect_layer, julia_entry_paths, order_rust_files_by_dependency};\npub use crate::cluster_011::build_module_map;\npub use crate::cluster_001::{build_directory_entry_map, collect_naming_warnings};\n#[allow(unused_imports)]\npub use crate::cluster_010::{extract_julia_dependencies, extract_rust_dependencies};\npub use crate::cluster_001::{analyze_file_ordering, naming_score_for_file};\npub use crate::cluster_011::{build_directory_dag, build_file_dependency_graph};\nuse std::collections::BTreeSet;\nuse std::path::PathBuf;\nuse syn::UseTree;\n\n#[allow(dead_code)]\n\n#[derive(Clone, Copy, Debug, PartialEq, Eq)]\npub enum RootState {\n    Start,\n    AfterRoot,\n}\n\n#[derive(Debug, Clone)]\npub struct LayerGraph {\n    pub ordered_layers: Vec<String>,\n    pub edges: Vec<LayerEdge>,\n    pub cycles: Vec<String>,\n    pub unresolved: Vec<UnresolvedDependency>,\n}\n\nimpl Default for LayerGraph {\n    fn default() -> Self {\n        LayerGraph {\n            ordered_layers: Vec::new(),\n            edges: Vec::new(),\n            cycles: Vec::new(),\n            unresolved: Vec::new(),\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct LayerEdge {\n    pub from: String,\n    pub to: String,\n    pub references: Vec<ReferenceDetail>,\n    pub violation: bool,\n}\n\n#[derive(Debug, Clone)]\npub struct UnresolvedDependency {\n    pub file: PathBuf,\n    pub reference: String,\n}\n\n#[derive(Debug, Clone, Eq, PartialEq, Ord, PartialOrd)]\npub struct ReferenceDetail {\n    pub file: PathBuf,\n    pub reference: String,\n}\n\npub fn collect_roots(tree: &UseTree, state: RootState, acc: &mut BTreeSet<String>) {\n    match tree {\n        UseTree::Path(path) => {\n            let ident = path.ident.to_string();\n            if state == RootState::Start && matches!(ident.as_str(), \"crate\" | \"self\" | \"super\") {\n                collect_roots(&path.tree, RootState::AfterRoot, acc);\n            } else if state == RootState::AfterRoot {\n                acc.insert(ident);\n            } else {\n                acc.insert(ident);\n            }\n        }\n        UseTree::Group(group) => {\n            for tree in &group.items {\n                collect_roots(tree, state, acc);\n            }\n        }\n        UseTree::Name(name) => {\n            acc.insert(name.ident.to_string());\n        }\n        UseTree::Rename(rename) => {\n            acc.insert(rename.ident.to_string());\n        }\n        UseTree::Glob(_) => {}\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/040_structural_detector.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/100_structural_detector.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/040_structural_detector.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/100_structural_detector.rs",
          "original_content": "//! Structural Invariant Detection\n//!\n//! This module detects invariants based on graph topology and structure.\n//! All structural invariants are PROVEN (not heuristic) because they are\n//! derived from mathematical properties of the graph.\n\nuse crate::invariant_types::*;\nuse crate::layer_inference::infer_layers;\nuse crate::scc_compressor::SccCompression;\nuse crate::types::{AnalysisResult, ElementType};\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse petgraph::Direction;\nuse std::collections::HashMap;\n\n/// Structural invariant detector\npub struct StructuralDetector {\n    graph: DiGraph<String, ()>,\n}\n\nimpl StructuralDetector {\n    pub fn new(graph: DiGraph<String, ()>) -> Self {\n        Self { graph }\n    }\n\n    /// Build call graph from analysis result\n    pub fn build_call_graph(analysis: &AnalysisResult) -> DiGraph<String, ()> {\n        let mut graph = DiGraph::new();\n        let mut node_map: HashMap<String, NodeIndex> = HashMap::new();\n\n        // Add nodes for all functions\n        for element in &analysis.elements {\n            if element.element_type == ElementType::Function {\n                let node = graph.add_node(element.name.clone());\n                node_map.insert(element.name.clone(), node);\n            }\n        }\n\n        // Add edges from calls\n        for element in &analysis.elements {\n            if element.element_type == ElementType::Function {\n                if let Some(&source) = node_map.get(&element.name) {\n                    for callee in &element.calls {\n                        if let Some(&target) = node_map.get(callee) {\n                            if source != target {\n                                // No self-loops\n                                graph.add_edge(source, target, ());\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        graph\n    }\n\n    /// Detect PROVEN structural invariants from analysis result's call graph\n    pub fn detect_from_analysis(analysis: &AnalysisResult) -> Vec<Invariant> {\n        let graph = Self::build_call_graph(analysis);\n        let detector = Self::new(graph);\n\n        // Create a map from function name to file path\n        let mut name_to_file: HashMap<String, String> = HashMap::new();\n        for element in &analysis.elements {\n            if element.element_type == ElementType::Function {\n                name_to_file.insert(element.name.clone(), element.file_path.clone());\n            }\n        }\n\n        let mut invariants = detector.detect_all();\n\n        // Update file paths in invariants\n        for inv in &mut invariants {\n            if let Some(file_path) = name_to_file.get(&inv.target) {\n                inv.file_path = file_path.clone();\n            }\n        }\n\n        invariants\n    }\n\n    /// Detect all structural invariants\n    pub fn detect_all(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        // Detect layer-fixed invariants\n        invariants.extend(self.detect_layer_fixed());\n\n        // Detect degree-stable invariants\n        invariants.extend(self.detect_degree_stable());\n\n        // Detect leaf and root nodes\n        invariants.extend(self.detect_leaf_root());\n\n        // Detect bridges\n        invariants.extend(self.detect_bridges());\n\n        // Detect SCC membership\n        invariants.extend(self.detect_scc_membership());\n\n        invariants\n    }\n\n    /// Detect layer-fixed invariants (PROVEN)\n    ///\n    /// Once a layer is assigned via the call graph, it's mathematically fixed\n    fn detect_layer_fixed(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n        let layers = infer_layers(&self.graph, 100);\n\n        for (name, layer_info) in layers {\n            let inv = Invariant::new(\n                name.clone(),\n                \"\".to_string(), // File path unknown at this level\n                InvariantKind::Structural(StructuralInvariant::LayerFixed {\n                    layer: layer_info.layer,\n                }),\n                InvariantStrength::Proven,\n                format!(\"Layer {} assignment is proven from call graph\", layer_info.layer),\n            );\n\n            invariants.push(inv);\n        }\n\n        invariants\n    }\n\n    /// Detect degree-stable invariants (PROVEN)\n    ///\n    /// In-degree and out-degree are structural properties\n    fn detect_degree_stable(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        for node_idx in self.graph.node_indices() {\n            let name = self.graph[node_idx].clone();\n            let in_degree = self\n                .graph\n                .neighbors_directed(node_idx, Direction::Incoming)\n                .count();\n            let out_degree = self\n                .graph\n                .neighbors_directed(node_idx, Direction::Outgoing)\n                .count();\n\n            let mut inv = Invariant::new(\n                name.clone(),\n                \"\".to_string(),\n                InvariantKind::Structural(StructuralInvariant::DegreeStable {\n                    in_degree,\n                    out_degree,\n                }),\n                InvariantStrength::Proven,\n                format!(\n                    \"Degree: in={}, out={} (proven from graph)\",\n                    in_degree, out_degree\n                ),\n            );\n\n            inv.add_evidence(format!(\"In-degree: {}\", in_degree));\n            inv.add_evidence(format!(\"Out-degree: {}\", out_degree));\n\n            invariants.push(inv);\n        }\n\n        invariants\n    }\n\n    /// Detect leaf and root nodes (PROVEN)\n    fn detect_leaf_root(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        for node_idx in self.graph.node_indices() {\n            let name = self.graph[node_idx].clone();\n            let in_degree = self\n                .graph\n                .neighbors_directed(node_idx, Direction::Incoming)\n                .count();\n            let out_degree = self\n                .graph\n                .neighbors_directed(node_idx, Direction::Outgoing)\n                .count();\n\n            // Leaf node (out-degree = 0)\n            if out_degree == 0 {\n                let inv = Invariant::new(\n                    name.clone(),\n                    \"\".to_string(),\n                    InvariantKind::Structural(StructuralInvariant::Leaf),\n                    InvariantStrength::Proven,\n                    \"Leaf node (calls no other functions)\".to_string(),\n                );\n                invariants.push(inv);\n            }\n\n            // Root node (in-degree = 0)\n            if in_degree == 0 {\n                let inv = Invariant::new(\n                    name.clone(),\n                    \"\".to_string(),\n                    InvariantKind::Structural(StructuralInvariant::Root),\n                    InvariantStrength::Proven,\n                    \"Root node (called by no other functions)\".to_string(),\n                );\n                invariants.push(inv);\n            }\n        }\n\n        invariants\n    }\n\n    /// Detect bridges (PROVEN)\n    ///\n    /// A bridge is an edge whose removal disconnects the graph\n    fn detect_bridges(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        // Simple approach: for each node, check if removing it increases connected components\n        // This is a simplified version - a full implementation would use Tarjan's bridge-finding algorithm\n\n        for node_idx in self.graph.node_indices() {\n            let name = self.graph[node_idx].clone();\n\n            // Check if this node is critical for connectivity\n            // (This is a heuristic approximation - true bridge detection is more complex)\n            let in_degree = self\n                .graph\n                .neighbors_directed(node_idx, Direction::Incoming)\n                .count();\n            let out_degree = self\n                .graph\n                .neighbors_directed(node_idx, Direction::Outgoing)\n                .count();\n\n            // A node with in_degree=1 and out_degree=1 might be a bridge\n            if in_degree == 1 && out_degree == 1 {\n                let inv = Invariant::new(\n                    name.clone(),\n                    \"\".to_string(),\n                    InvariantKind::Structural(StructuralInvariant::Bridge),\n                    InvariantStrength::Empirical { paths_checked: 1 }, // Conservative - needs proper algorithm\n                    \"Potential bridge node in call graph\".to_string(),\n                );\n                invariants.push(inv);\n            }\n        }\n\n        invariants\n    }\n\n    /// Detect SCC membership (PROVEN)\n    ///\n    /// Membership in a strongly connected component is a proven structural property\n    fn detect_scc_membership(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n        let compression = SccCompression::new(self.graph.clone());\n\n        for (scc_id, scc) in compression.sccs.iter().enumerate() {\n            for &node_idx in scc {\n                let name = self.graph[node_idx].clone();\n\n                let inv = Invariant::new(\n                    name.clone(),\n                    \"\".to_string(),\n                    InvariantKind::Structural(StructuralInvariant::SccMembership {\n                        scc_id,\n                        scc_size: scc.len(),\n                    }),\n                    InvariantStrength::Proven,\n                    format!(\"Member of SCC {} (size: {})\", scc_id, scc.len()),\n                );\n\n                invariants.push(inv);\n            }\n        }\n\n        invariants\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_detect_leaf_root() {\n        // Create graph: A -> B -> C\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n\n        let detector = StructuralDetector::new(graph);\n        let invariants = detector.detect_leaf_root();\n\n        // A should be root, C should be leaf\n        let roots: Vec<_> = invariants\n            .iter()\n            .filter(|inv| matches!(inv.kind, InvariantKind::Structural(StructuralInvariant::Root)))\n            .collect();\n        let leaves: Vec<_> = invariants\n            .iter()\n            .filter(|inv| matches!(inv.kind, InvariantKind::Structural(StructuralInvariant::Leaf)))\n            .collect();\n\n        assert_eq!(roots.len(), 1);\n        assert_eq!(leaves.len(), 1);\n        assert_eq!(roots[0].target, \"A\");\n        assert_eq!(leaves[0].target, \"C\");\n    }\n\n    #[test]\n    fn test_detect_degree_stable() {\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        graph.add_edge(a, b, ());\n\n        let detector = StructuralDetector::new(graph);\n        let invariants = detector.detect_degree_stable();\n\n        assert_eq!(invariants.len(), 2); // One for A, one for B\n\n        // Find A's invariant\n        let a_inv = invariants.iter().find(|inv| inv.target == \"A\").unwrap();\n        if let InvariantKind::Structural(StructuralInvariant::DegreeStable {\n            in_degree,\n            out_degree,\n        }) = &a_inv.kind\n        {\n            assert_eq!(*in_degree, 0);\n            assert_eq!(*out_degree, 1);\n        } else {\n            panic!(\"Wrong invariant kind\");\n        }\n    }\n\n    #[test]\n    fn test_all_structural_invariants_proven() {\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        graph.add_edge(a, b, ());\n\n        let detector = StructuralDetector::new(graph);\n        let invariants = detector.detect_all();\n\n        // Most should be proven (except bridges which are conservative)\n        let proven_count = invariants\n            .iter()\n            .filter(|inv| matches!(inv.strength, InvariantStrength::Proven))\n            .count();\n\n        assert!(proven_count > 0);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/050_cluster_006.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/110_cluster_006.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/050_cluster_006.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/110_cluster_006.rs",
          "original_content": "//! Cluster 006: Dependency ordering and layer analysis utilities\n//!\n//! This module contains functions for:\n//! - Julia dependency collection and file ordering\n//! - Layer prefix extraction and comparison utilities\n//! - Layer violation detection\n//! - Directory ordering based on dependencies\n//! - Cluster planning and path resolution\n\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::path::{Path, PathBuf};\n\nuse crate::cluster_008::FunctionInfo;\n// ============================================================================\n// Layer Utilities (from src/010_layer_core.rs and src/020_layer_utilities.rs)\n// ============================================================================\n\n/// Extracts numeric layer prefix from a layer string (e.g., \"060_file_ordering\" -> 60)\npub fn layer_prefix_value(layer: &str) -> Option<i32> {\n    let mut chars = layer.chars();\n    let mut digits = String::new();\n    while let Some(ch) = chars.next() {\n        if ch.is_ascii_digit() {\n            digits.push(ch);\n        } else {\n            break;\n        }\n    }\n    if digits.is_empty() {\n        None\n    } else {\n        digits.parse::<i32>().ok()\n    }\n}\n\npub fn order_directories(\n    files: &[PathBuf],\n    dep_map: &HashMap<PathBuf, Vec<PathBuf>>,\n) -> Vec<PathBuf> {\n    let root = common_root(files);\n    let mut dirs: HashSet<PathBuf> = HashSet::new();\n    for file in files {\n        let mut current = file.parent().map(Path::to_path_buf);\n        while let Some(dir) = current {\n            if let Some(ref root_path) = root {\n                if !dir.starts_with(root_path) {\n                    break;\n                }\n            }\n            dirs.insert(dir.clone());\n            current = dir.parent().map(Path::to_path_buf);\n        }\n    }\n\n    let mut ordered: Vec<PathBuf> = dirs.into_iter().collect();\n    ordered.sort_by(|a, b| crate::cluster_008::compare_path_components(a, b));\n\n    let mut node_map = HashMap::new();\n    for (idx, dir) in ordered.iter().enumerate() {\n        node_map.insert(dir.clone(), idx);\n    }\n\n    let mut adjacency: Vec<BTreeSet<usize>> = vec![BTreeSet::new(); ordered.len()];\n    let mut indegree = vec![0usize; ordered.len()];\n\n    for (file, deps) in dep_map {\n        let Some(from_dir) = file.parent().map(Path::to_path_buf) else {\n            continue;\n        };\n        let Some(&from_idx) = node_map.get(&from_dir) else {\n            continue;\n        };\n        for dep in deps {\n            let Some(to_dir) = dep.parent().map(Path::to_path_buf) else {\n                continue;\n            };\n            if to_dir == from_dir {\n                continue;\n            }\n            let Some(&to_idx) = node_map.get(&to_dir) else {\n                continue;\n            };\n            if adjacency[to_idx].insert(from_idx) {\n                indegree[from_idx] += 1;\n            }\n        }\n    }\n\n    let mut queue: BTreeSet<usize> = indegree\n        .iter()\n        .enumerate()\n        .filter_map(|(idx, &deg)| if deg == 0 { Some(idx) } else { None })\n        .collect();\n\n    let mut result = Vec::with_capacity(ordered.len());\n    while let Some(&idx) = queue.iter().next() {\n        queue.remove(&idx);\n        result.push(ordered[idx].clone());\n        let neighbors = adjacency[idx].clone();\n        for neighbor in neighbors {\n            let entry = &mut indegree[neighbor];\n            if *entry > 0 {\n                *entry -= 1;\n                if *entry == 0 {\n                    queue.insert(neighbor);\n                }\n            }\n        }\n    }\n\n    if result.len() < ordered.len() {\n        for (idx, dir) in ordered.iter().enumerate() {\n            if indegree[idx] > 0 {\n                result.push(dir.clone());\n            }\n        }\n    }\n\n    result\n}\n\npub fn common_root(files: &[PathBuf]) -> Option<PathBuf> {\n    let mut iter = files.iter();\n    let first = iter.next()?.components().collect::<Vec<_>>();\n    let mut prefix_len = first.len();\n\n    for path in iter {\n        let comps = path.components().collect::<Vec<_>>();\n        let mut idx = 0;\n        while idx < prefix_len && idx < comps.len() && comps[idx] == first[idx] {\n            idx += 1;\n        }\n        prefix_len = idx;\n    }\n\n    if prefix_len == 0 {\n        None\n    } else {\n        let mut root = PathBuf::new();\n        for comp in first.into_iter().take(prefix_len) {\n            root.push(comp.as_os_str());\n        }\n        Some(root)\n    }\n}\n\npub(crate) fn strip_numeric_prefix(name: &str) -> &str {\n    use once_cell::sync::Lazy;\n    use regex::Regex;\n\n    static PREFIX_RE: Lazy<Regex> =\n        Lazy::new(|| Regex::new(r\"^\\d+_(.*)$\").unwrap());\n    PREFIX_RE\n        .captures(name)\n        .and_then(|cap| cap.get(1))\n        .map(|m| m.as_str())\n        .unwrap_or(name)\n}\n\npub fn generate_canonical_name(path: &Path, number: usize) -> String {\n    let stem = path\n        .file_stem()\n        .and_then(|s| s.to_str())\n        .unwrap_or(\"unknown\");\n    let ext = path\n        .extension()\n        .and_then(|s| s.to_str())\n        .unwrap_or(\"\");\n    let clean_stem = strip_numeric_prefix(stem);\n    if ext.is_empty() {\n        format!(\"{:03}_{}\", number, clean_stem)\n    } else {\n        format!(\"{:03}_{}.{}\", number, clean_stem, ext)\n    }\n}\n\npub fn collect_directory_moves(\n    ordering: &crate::types::FileOrderingResult,\n    root_path: &Path,\n) -> Vec<crate::file_ordering::DirectoryMove> {\n    let mut moves = Vec::new();\n    let mut by_parent: BTreeMap<PathBuf, Vec<PathBuf>> = BTreeMap::new();\n    let src_dir = root_path.join(\"src\");\n\n    for dir in &ordering.ordered_directories {\n        if dir == root_path {\n            continue;\n        }\n        if dir == &src_dir {\n            continue;\n        }\n        if let Some(parent) = dir.parent() {\n            by_parent\n                .entry(parent.to_path_buf())\n                .or_default()\n                .push(dir.clone());\n        }\n    }\n\n    for (parent, mut dirs) in by_parent {\n        dirs.sort_by(|a, b| crate::cluster_008::compare_dir_layers(a, b));\n        for (idx, dir) in dirs.iter().enumerate() {\n            let Some(name) = dir.file_name().and_then(|n| n.to_str()) else {\n                continue;\n            };\n            let clean = strip_numeric_prefix(name);\n            let suggested = format!(\"{:03}_{}\", idx * 10, clean);\n            if name == suggested {\n                continue;\n            }\n            let to = parent.join(&suggested);\n            moves.push(crate::file_ordering::DirectoryMove {\n                from: dir.clone(),\n                to,\n            });\n        }\n    }\n\n    moves\n}\n\n// ============================================================================\n// Cohesion Analysis (from src/060_cohesion_analyzer.rs)\n// ============================================================================\n\npub fn compute_cohesion_score(\n    func: &FunctionInfo,\n    functions: &[FunctionInfo],\n    outgoing: &HashMap<usize, usize>,\n    file_layers: &HashMap<String, String>,\n    call_analysis: &crate::types::CallAnalysis,\n) -> f64 {\n    let mut total_calls = 0usize;\n    let mut intra_calls = 0usize;\n    let mut external_calls = 0usize;\n    let mut layer_ok = 0usize;\n\n    for (callee_idx, count) in outgoing {\n        total_calls += count;\n        let callee = &functions[*callee_idx];\n        if callee.file_path == func.file_path {\n            intra_calls += count;\n        } else {\n            external_calls += count;\n        }\n        let current_layer = file_layers\n            .get(&func.file_path)\n            .cloned()\n            .unwrap_or_else(|| func.layer.clone());\n        let target_layer = file_layers\n            .get(&callee.file_path)\n            .cloned()\n            .unwrap_or_else(|| callee.layer.clone());\n        if crate::cluster_008::layer_adheres(&current_layer, &target_layer) {\n            layer_ok += count;\n        }\n    }\n\n    let total_calls_f = total_calls as f64;\n    let call_locality = if total_calls == 0 {\n        1.0\n    } else {\n        intra_calls as f64 / total_calls_f\n    };\n    let layer_adherence = if total_calls == 0 {\n        1.0\n    } else {\n        layer_ok as f64 / total_calls_f\n    };\n    let cross_file_calls = if total_calls == 0 {\n        0.0\n    } else {\n        external_calls as f64 / total_calls_f\n    };\n\n    let total_type_refs = call_analysis.same_file_type_refs + call_analysis.other_file_type_refs;\n    let type_coupling = if total_type_refs == 0 {\n        1.0\n    } else {\n        call_analysis.same_file_type_refs as f64 / total_type_refs as f64\n    };\n\n    let score = 0.4 * call_locality\n        + 0.3 * type_coupling\n        + 0.2 * layer_adherence\n        - 0.1 * cross_file_calls;\n    score.clamp(0.0, 1.0)\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/050_semantic_detector.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/120_semantic_detector.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/050_semantic_detector.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/120_semantic_detector.rs",
          "original_content": "//! Semantic Invariant Detection\n//!\n//! This module detects invariants based on function semantics, types, and effects.\n//! Some are EMPIRICAL (derived from signatures), others are HEURISTIC (name-based).\n//! All heuristics are clearly marked as low confidence.\n\nuse crate::invariant_types::*;\nuse crate::types::{CodeElement, ElementType};\nuse regex::Regex;\n\n/// Semantic invariant detector\npub struct SemanticDetector<'a> {\n    elements: &'a [CodeElement],\n}\n\nimpl<'a> SemanticDetector<'a> {\n    pub fn new(elements: &'a [CodeElement]) -> Self {\n        Self { elements }\n    }\n\n    /// Detect all semantic invariants\n    pub fn detect_all(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        // Detect type-stable functions\n        invariants.extend(self.detect_type_stable());\n\n        // Detect pure functions (heuristic - based on name/signature patterns)\n        invariants.extend(self.detect_pure_function());\n\n        // Detect idempotent functions (heuristic)\n        invariants.extend(self.detect_idempotent());\n\n        invariants\n    }\n\n    /// Detect type-stable functions (EMPIRICAL)\n    ///\n    /// Functions with explicit type signatures have stable types\n    fn detect_type_stable(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        for element in self.elements {\n            if !matches!(element.element_type, ElementType::Function) {\n                continue;\n            }\n\n            // If signature is non-empty and looks like it has type info\n            if !element.signature.is_empty() && element.signature.contains(\"->\") {\n                let inv = Invariant::new(\n                    element.name.clone(),\n                    element.file_path.clone(),\n                    InvariantKind::Semantic(SemanticInvariant::TypeStable {\n                        signature: element.signature.clone(),\n                    }),\n                    InvariantStrength::Empirical { paths_checked: 1 },\n                    format!(\"Type signature: {}\", element.signature),\n                );\n\n                invariants.push(inv);\n            }\n        }\n\n        invariants\n    }\n\n    /// Detect pure functions (HEURISTIC)\n    ///\n    /// IMPORTANT: This is a heuristic based on naming patterns and signature analysis.\n    /// It has LOW confidence and should not block refactorings.\n    fn detect_pure_function(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        // Heuristic patterns for pure functions\n        let pure_patterns = [\n            r\"^is_\",        // is_valid, is_empty\n            r\"^has_\",       // has_property\n            r\"^get_\",       // get_value (might be pure)\n            r\"^calculate_\", // calculate_sum\n            r\"^compute_\",   // compute_hash\n            r\"^parse_\",     // parse_input\n            r\"^format_\",    // format_output\n            r\"^convert_\",   // convert_type\n        ];\n\n        let mut pure_regex_set = Vec::new();\n        for pattern in &pure_patterns {\n            if let Ok(re) = Regex::new(pattern) {\n                pure_regex_set.push(re);\n            }\n        }\n\n        for element in self.elements {\n            if !matches!(element.element_type, ElementType::Function) {\n                continue;\n            }\n\n            let mut is_likely_pure = false;\n            let mut evidence = Vec::new();\n\n            // Check naming patterns\n            for re in &pure_regex_set {\n                if re.is_match(&element.name) {\n                    is_likely_pure = true;\n                    evidence.push(format!(\"Name matches pure pattern: {}\", re.as_str()));\n                }\n            }\n\n            // Check signature for no &mut parameters (simple heuristic)\n            if !element.signature.contains(\"&mut\") {\n                evidence.push(\"No mutable references in signature\".to_string());\n            } else {\n                is_likely_pure = false; // &mut suggests mutation\n                evidence.clear(); // Clear evidence since this is not pure\n            }\n\n            // Check if function doesn't call many other functions (might be pure)\n            // Only consider this if we haven't already ruled it out\n            if element.calls.is_empty() && is_likely_pure {\n                evidence.push(\"Calls no other functions\".to_string());\n            }\n\n            if is_likely_pure {\n                let mut inv = Invariant::new(\n                    element.name.clone(),\n                    element.file_path.clone(),\n                    InvariantKind::Semantic(SemanticInvariant::PureFunction),\n                    InvariantStrength::Heuristic, // Always heuristic - needs verification!\n                    \"Likely pure function (HEURISTIC - verify manually)\".to_string(),\n                );\n\n                for e in evidence {\n                    inv.add_evidence(e);\n                }\n\n                invariants.push(inv);\n            }\n        }\n\n        invariants\n    }\n\n    /// Detect idempotent functions (HEURISTIC)\n    ///\n    /// Functions that are likely idempotent based on naming patterns\n    fn detect_idempotent(&self) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        let idempotent_patterns = [\n            r\"^set_\",       // set_value (idempotent if pure set)\n            r\"^clear_\",     // clear_buffer\n            r\"^reset_\",     // reset_state\n            r\"^initialize_\", // initialize_config\n            r\"^normalize_\", // normalize_value\n        ];\n\n        let mut idempotent_regex_set = Vec::new();\n        for pattern in &idempotent_patterns {\n            if let Ok(re) = Regex::new(pattern) {\n                idempotent_regex_set.push(re);\n            }\n        }\n\n        for element in self.elements {\n            if !matches!(element.element_type, ElementType::Function) {\n                continue;\n            }\n\n            for re in &idempotent_regex_set {\n                if re.is_match(&element.name) {\n                    let mut inv = Invariant::new(\n                        element.name.clone(),\n                        element.file_path.clone(),\n                        InvariantKind::Semantic(SemanticInvariant::Idempotent),\n                        InvariantStrength::Heuristic, // Always heuristic!\n                        \"Likely idempotent function (HEURISTIC - verify manually)\".to_string(),\n                    );\n\n                    inv.add_evidence(format!(\"Name matches idempotent pattern: {}\", re.as_str()));\n\n                    invariants.push(inv);\n                    break; // Only add once per function\n                }\n            }\n        }\n\n        invariants\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::types::{Language, Visibility};\n\n    fn make_function(name: &str, signature: &str, calls: Vec<String>) -> CodeElement {\n        CodeElement {\n            name: name.to_string(),\n            file_path: \"test.rs\".to_string(),\n            line_number: 1,\n            element_type: ElementType::Function,\n            signature: signature.to_string(),\n            visibility: Visibility::Public,\n            generic_params: Vec::new(),\n            language: Language::Rust,\n            layer: \"0\".to_string(),\n            calls,\n        }\n    }\n\n    #[test]\n    fn test_detect_type_stable() {\n        let elements = vec![make_function(\n            \"test_fn\",\n            \"fn test_fn(x: i32) -> i32\",\n            Vec::new(),\n        )];\n\n        let detector = SemanticDetector::new(&elements);\n        let invariants = detector.detect_type_stable();\n\n        assert_eq!(invariants.len(), 1);\n        assert_eq!(invariants[0].target, \"test_fn\");\n        assert!(matches!(\n            invariants[0].strength,\n            InvariantStrength::Empirical { .. }\n        ));\n    }\n\n    #[test]\n    fn test_detect_pure_function_heuristic() {\n        let elements = vec![\n            make_function(\"is_valid\", \"fn is_valid(x: i32) -> bool\", Vec::new()),\n            make_function(\"get_value\", \"fn get_value() -> i32\", Vec::new()),\n        ];\n\n        let detector = SemanticDetector::new(&elements);\n        let invariants = detector.detect_pure_function();\n\n        // Should detect both as potentially pure\n        assert!(invariants.len() >= 2);\n\n        // All should be heuristic\n        for inv in &invariants {\n            assert!(matches!(inv.strength, InvariantStrength::Heuristic));\n        }\n    }\n\n    #[test]\n    fn test_detect_idempotent_heuristic() {\n        let elements = vec![\n            make_function(\"set_value\", \"fn set_value(x: i32)\", Vec::new()),\n            make_function(\"reset_state\", \"fn reset_state()\", Vec::new()),\n        ];\n\n        let detector = SemanticDetector::new(&elements);\n        let invariants = detector.detect_idempotent();\n\n        assert_eq!(invariants.len(), 2);\n\n        // All should be heuristic\n        for inv in &invariants {\n            assert!(matches!(inv.strength, InvariantStrength::Heuristic));\n            assert!(matches!(\n                inv.kind,\n                InvariantKind::Semantic(SemanticInvariant::Idempotent)\n            ));\n        }\n    }\n\n    #[test]\n    fn test_no_pure_for_mutable() {\n        let elements = vec![make_function(\n            \"is_valid\",\n            \"fn is_valid(x: &mut i32) -> bool\",\n            Vec::new(),\n        )];\n\n        let detector = SemanticDetector::new(&elements);\n        let invariants = detector.detect_pure_function();\n\n        // Should not detect as pure due to &mut\n        let pure_count = invariants\n            .iter()\n            .filter(|inv| inv.target == \"is_valid\")\n            .count();\n        assert_eq!(pure_count, 0);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/060_layer_core.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/130_layer_core.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/060_layer_core.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/130_layer_core.rs",
          "original_content": "//! Core layer computation functions - lowest level utilities\n//! This module must have no dependencies on other local modules to avoid circular dependencies.\n//! It provides fundamental layer prefix extraction and violation detection.\n\n#[allow(unused_imports)]\npub use crate::cluster_006::{\n    layer_prefix_value, order_directories, collect_directory_moves,\n};\n#[allow(unused_imports)]\npub use crate::cluster_008::detect_layer_violations;\n#[allow(unused_imports)]\npub use crate::cluster_001::{layer_constrained_sort, topo_sort_within};\nuse crate::cluster_008::structural_layer_value;\n\npub fn structural_cmp(a: &crate::report::PlanItem, b: &crate::report::PlanItem) -> std::cmp::Ordering {\n    let a_required = structural_layer_value(&a.required_layer, i32::MAX);\n    let b_required = structural_layer_value(&b.required_layer, i32::MAX);\n    let a_current = structural_layer_value(&a.current_layer, i32::MIN);\n    let b_current = structural_layer_value(&b.current_layer, i32::MIN);\n    let a_benefit = if a.cost == 0 {\n        0\n    } else {\n        (a.benefit.saturating_mul(1000)) / a.cost\n    };\n    let b_benefit = if b.cost == 0 {\n        0\n    } else {\n        (b.benefit.saturating_mul(1000)) / b.cost\n    };\n    a_required\n        .cmp(&b_required)\n        .then_with(|| b.is_utility.cmp(&a.is_utility))\n        .then_with(|| b_benefit.cmp(&a_benefit))\n        .then_with(|| b.impact_weight.cmp(&a.impact_weight))\n        .then_with(|| b_current.cmp(&a_current))\n        .then_with(|| a.description.cmp(&b.description))\n}\n\npub fn sort_structural_items(items: &mut Vec<crate::report::PlanItem>) {\n    use std::collections::HashMap;\n    use std::path::PathBuf;\n\n    if items.len() <= 1 {\n        return;\n    }\n\n    let count = items.len();\n    let mut edges: Vec<Vec<usize>> = vec![Vec::new(); count];\n    let mut indegree = vec![0usize; count];\n\n    let mut file_to_items: HashMap<PathBuf, Vec<usize>> = HashMap::new();\n    for (idx, item) in items.iter().enumerate() {\n        if let Some(path) = &item.current_file {\n            file_to_items.entry(path.clone()).or_default().push(idx);\n        }\n    }\n\n    for i in 0..count {\n        for j in (i + 1)..count {\n            let req_i = structural_layer_value(&items[i].required_layer, i32::MAX);\n            let req_j = structural_layer_value(&items[j].required_layer, i32::MAX);\n            let mut edge = None;\n            if req_i != req_j {\n                edge = if req_i < req_j { Some((i, j)) } else { Some((j, i)) };\n            } else if items[i].is_utility != items[j].is_utility {\n                edge = if items[i].is_utility {\n                    Some((i, j))\n                } else {\n                    Some((j, i))\n                };\n            }\n            if let Some((from, to)) = edge {\n                edges[from].push(to);\n                indegree[to] += 1;\n            }\n        }\n    }\n\n    for (idx, item) in items.iter().enumerate() {\n        for file in &item.outgoing_files {\n            if let Some(dependents) = file_to_items.get(file) {\n                for &dependent_idx in dependents {\n                    if dependent_idx == idx {\n                        continue;\n                    }\n                    edges[dependent_idx].push(idx);\n                    indegree[idx] += 1;\n                }\n            }\n        }\n    }\n\n    let mut ordered_indices = Vec::with_capacity(count);\n    let mut available: Vec<usize> = (0..count).filter(|&i| indegree[i] == 0).collect();\n    while !available.is_empty() {\n        available.sort_by(|&a, &b| structural_cmp(&items[a], &items[b]));\n        let next = available.remove(0);\n        ordered_indices.push(next);\n        for &neighbor in &edges[next] {\n            indegree[neighbor] = indegree[neighbor].saturating_sub(1);\n            if indegree[neighbor] == 0 {\n                available.push(neighbor);\n            }\n        }\n    }\n\n    if ordered_indices.len() != count {\n        items.sort_by(structural_cmp);\n        return;\n    }\n\n    let mut reordered = Vec::with_capacity(count);\n    for idx in ordered_indices {\n        reordered.push(items[idx].clone());\n    }\n    *items = reordered;\n}\n\n// Moved to layer_utilities in Batch 4 - no re-export needed\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/060_path_detector.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/140_path_detector.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/060_path_detector.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/140_path_detector.rs",
          "original_content": "//! Path-Intersection Invariant Detection\n//!\n//! This is the MOST POWERFUL detector - it finds facts that hold on ALL execution paths.\n//! Uses SCC compression to prevent path explosion.\n\nuse crate::invariant_types::*;\nuse crate::scc_compressor::SccCompression;\nuse petgraph::algo::all_simple_paths;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse petgraph::Direction;\nuse std::collections::HashSet;\n\n/// Path invariant detector\npub struct PathDetector {\n    graph: DiGraph<String, ()>,\n    compression: SccCompression,\n}\n\nimpl PathDetector {\n    pub fn new(graph: DiGraph<String, ()>) -> Self {\n        let compression = SccCompression::new(graph.clone());\n        Self { graph, compression }\n    }\n\n    /// Detect all path-intersection invariants\n    ///\n    /// # Arguments\n    /// * `max_depth` - Maximum path depth to explore\n    /// * `max_paths_per_node` - Maximum paths to check per node\n    pub fn detect_all(&self, max_depth: usize, max_paths_per_node: usize) -> Vec<Invariant> {\n        let mut invariants = Vec::new();\n\n        // Enumerate paths on the SCC-compressed graph (guaranteed DAG)\n        for node_idx in self.compression.compressed_graph.node_indices() {\n            let members = &self.compression.compressed_graph[node_idx];\n\n            // For each member in this SCC\n            for member in members {\n                if let Some(inv) =\n                    self.compute_path_intersection(member, max_depth, max_paths_per_node)\n                {\n                    invariants.push(inv);\n                }\n            }\n        }\n\n        invariants\n    }\n\n    /// Compute path-intersection invariant for a specific node\n    fn compute_path_intersection(\n        &self,\n        target: &str,\n        max_depth: usize,\n        max_paths: usize,\n    ) -> Option<Invariant> {\n        // Find the node index in the original graph\n        let target_idx = self\n            .graph\n            .node_indices()\n            .find(|&idx| self.graph[idx] == target)?;\n\n        // Get all leaf nodes (out-degree = 0) reachable from this node\n        let leaf_nodes: Vec<NodeIndex> = self\n            .graph\n            .node_indices()\n            .filter(|&idx| {\n                self.graph\n                    .neighbors_directed(idx, Direction::Outgoing)\n                    .count()\n                    == 0\n            })\n            .collect();\n\n        let mut all_paths = Vec::new();\n        let mut paths_found = 0;\n\n        // Enumerate paths to all reachable leaves\n        for leaf_idx in leaf_nodes {\n            // Use petgraph's all_simple_paths (bounded by max_depth)\n            let paths: Vec<Vec<NodeIndex>> = all_simple_paths(\n                &self.graph,\n                target_idx,\n                leaf_idx,\n                0,\n                Some(max_depth),\n            )\n            .take(max_paths - paths_found)\n            .collect();\n\n            paths_found += paths.len();\n            all_paths.extend(paths);\n\n            if paths_found >= max_paths {\n                break;\n            }\n        }\n\n        // If we found no paths or only one path, no useful invariant\n        if all_paths.len() < 2 {\n            return None;\n        }\n\n        // Extract facts from each path\n        let path_facts: Vec<HashSet<String>> = all_paths\n            .iter()\n            .map(|path| self.extract_facts_from_path(path))\n            .collect();\n\n        // Compute intersection of all path facts\n        let mut intersection = path_facts[0].clone();\n        for facts in &path_facts[1..] {\n            intersection = intersection.intersection(facts).cloned().collect();\n        }\n\n        // If intersection is empty, no common facts\n        if intersection.is_empty() {\n            return None;\n        }\n\n        // Create invariant\n        Some(Invariant::new(\n            target.to_string(),\n            \"\".to_string(),\n            InvariantKind::PathIntersection(PathIntersectionInvariant {\n                facts: intersection,\n                paths_analyzed: all_paths.len(),\n                max_depth,\n            }),\n            InvariantStrength::Empirical {\n                paths_checked: all_paths.len(),\n            },\n            format!(\n                \"Facts hold on all {} paths (max depth: {})\",\n                all_paths.len(),\n                max_depth\n            ),\n        ))\n    }\n\n    /// Extract symbolic facts from a path\n    ///\n    /// This is a simplified version - a full implementation would use\n    /// abstract interpretation or symbolic execution\n    fn extract_facts_from_path(&self, path: &[NodeIndex]) -> HashSet<String> {\n        let mut facts = HashSet::new();\n\n        // Fact: path length\n        facts.insert(format!(\"path_length_{}\", path.len()));\n\n        // Fact: nodes visited\n        for &node_idx in path {\n            let name = &self.graph[node_idx];\n            facts.insert(format!(\"visits_{}\", name));\n        }\n\n        // Fact: edges traversed\n        for window in path.windows(2) {\n            let from = &self.graph[window[0]];\n            let to = &self.graph[window[1]];\n            facts.insert(format!(\"edge_{}_to_{}\", from, to));\n        }\n\n        // Fact: always reaches leaf (if path ends at a leaf)\n        if let Some(&last) = path.last() {\n            let out_degree = self\n                .graph\n                .neighbors_directed(last, Direction::Outgoing)\n                .count();\n            if out_degree == 0 {\n                facts.insert(\"reaches_leaf\".to_string());\n            }\n        }\n\n        facts\n    }\n\n    /// Get statistics about path enumeration\n    #[allow(dead_code)]\n    pub fn get_stats(&self) -> PathStats {\n        let total_nodes = self.graph.node_count();\n        let compressed_nodes = self.compression.compressed_graph.node_count();\n\n        PathStats {\n            original_nodes: total_nodes,\n            compressed_nodes,\n            is_dag: self.compression.is_dag(),\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct PathStats {\n    pub original_nodes: usize,\n    pub compressed_nodes: usize,\n    pub is_dag: bool,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_path_detector_simple() {\n        // Create graph: A -> B -> C\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n\n        let detector = PathDetector::new(graph);\n        let invariants = detector.detect_all(10, 100);\n\n        // Should find some invariants (at least for A which has a path to C)\n        // Note: might be 0 if only one path exists (no intersection needed)\n        assert!(invariants.len() >= 0);\n    }\n\n    #[test]\n    fn test_path_detector_diamond() {\n        // Diamond: A -> B -> D, A -> C -> D\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        let d = graph.add_node(\"D\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(a, c, ());\n        graph.add_edge(b, d, ());\n        graph.add_edge(c, d, ());\n\n        let detector = PathDetector::new(graph);\n        let invariants = detector.detect_all(10, 100);\n\n        // A has two paths to D, should find intersection invariant\n        let a_invs: Vec<_> = invariants.iter().filter(|inv| inv.target == \"A\").collect();\n\n        if !a_invs.is_empty() {\n            let inv = a_invs[0];\n            if let InvariantKind::PathIntersection(ref pi) = inv.kind {\n                assert_eq!(pi.paths_analyzed, 2);\n                // Both paths should reach D\n                assert!(pi.facts.contains(\"visits_D\"));\n            }\n        }\n    }\n\n    #[test]\n    fn test_extract_facts_from_path() {\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        let c = graph.add_node(\"C\".to_string());\n        graph.add_edge(a, b, ());\n        graph.add_edge(b, c, ());\n\n        let detector = PathDetector::new(graph);\n        let path = vec![a, b, c];\n        let facts = detector.extract_facts_from_path(&path);\n\n        assert!(facts.contains(\"visits_A\"));\n        assert!(facts.contains(\"visits_B\"));\n        assert!(facts.contains(\"visits_C\"));\n        assert!(facts.contains(\"reaches_leaf\"));\n        assert!(facts.contains(\"path_length_3\"));\n    }\n\n    #[test]\n    fn test_path_stats() {\n        let mut graph = DiGraph::new();\n        let a = graph.add_node(\"A\".to_string());\n        let b = graph.add_node(\"B\".to_string());\n        graph.add_edge(a, b, ());\n\n        let detector = PathDetector::new(graph);\n        let stats = detector.get_stats();\n\n        assert_eq!(stats.original_nodes, 2);\n        assert!(stats.is_dag);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_invariant_integrator.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/150_invariant_integrator.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_invariant_integrator.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/150_invariant_integrator.rs",
          "original_content": "//! Invariant Detection Integration\n//!\n//! This module orchestrates all invariant detectors and produces a unified result.\n\nuse crate::invariant_types::*;\nuse crate::layer_inference::{detect_layer_violations, infer_layers};\nuse crate::path_detector::PathDetector;\nuse crate::refactor_constraints::generate_constraints;\nuse crate::semantic_detector::SemanticDetector;\nuse crate::structural_detector::StructuralDetector;\nuse crate::types::{AnalysisResult, CallGraphNode};\nuse petgraph::graph::DiGraph;\nuse std::collections::HashMap;\n\n/// Main invariant detector that orchestrates all detection strategies\npub struct InvariantDetector<'a> {\n    analysis_result: &'a AnalysisResult,\n    call_graph: DiGraph<String, ()>,\n}\n\nimpl<'a> InvariantDetector<'a> {\n    /// Create a new invariant detector\n    pub fn new(\n        analysis_result: &'a AnalysisResult,\n        call_graph_nodes: &HashMap<String, CallGraphNode>,\n    ) -> Self {\n        // Build petgraph from call graph nodes\n        let mut graph = DiGraph::new();\n        let mut node_indices = HashMap::new();\n\n        // Add all nodes\n        for (name, _) in call_graph_nodes {\n            let idx = graph.add_node(name.clone());\n            node_indices.insert(name.clone(), idx);\n        }\n\n        // Add all edges\n        for (caller, node) in call_graph_nodes {\n            if let Some(&caller_idx) = node_indices.get(caller) {\n                for callee in &node.calls {\n                    if let Some(&callee_idx) = node_indices.get(callee) {\n                        graph.add_edge(caller_idx, callee_idx, ());\n                    }\n                }\n            }\n        }\n\n        Self {\n            analysis_result,\n            call_graph: graph,\n        }\n    }\n\n    /// Detect all invariants across all detection strategies\n    pub fn detect_all(&self) -> InvariantAnalysisResult {\n        let mut result = InvariantAnalysisResult::new();\n\n        println!(\" Detecting invariants...\");\n\n        // 1. Structural invariants (PROVEN)\n        println!(\"   Detecting structural invariants (graph-based)...\");\n        let structural_detector = StructuralDetector::new(self.call_graph.clone());\n        let structural_invs = structural_detector.detect_all();\n        println!(\"     Found {} structural invariants (from call graph)\", structural_invs.len());\n        for inv in structural_invs {\n            result.add_invariant(inv);\n        }\n\n        // 1b. Enhanced structural detection from AnalysisResult\n        println!(\"   Enhanced structural detection (call graph from analysis)...\");\n        let enhanced_structural_invs = StructuralDetector::detect_from_analysis(self.analysis_result);\n        println!(\"     Found {} enhanced structural invariants\", enhanced_structural_invs.len());\n        for inv in enhanced_structural_invs {\n            result.add_invariant(inv);\n        }\n\n        // 2. Semantic invariants (EMPIRICAL/HEURISTIC)\n        println!(\"   Detecting semantic invariants...\");\n        let semantic_detector = SemanticDetector::new(&self.analysis_result.elements);\n        let semantic_invs = semantic_detector.detect_all();\n        println!(\"     Found {} semantic invariants\", semantic_invs.len());\n        for inv in semantic_invs {\n            result.add_invariant(inv);\n        }\n\n        // 3. Path-intersection invariants (EMPIRICAL)\n        println!(\"   Detecting path-intersection invariants...\");\n        let path_detector = PathDetector::new(self.call_graph.clone());\n        let path_invs = path_detector.detect_all(10, 100); // max_depth=10, max_paths=100\n        println!(\"     Found {} path-intersection invariants\", path_invs.len());\n        for inv in path_invs {\n            result.add_invariant(inv);\n        }\n\n        // 4. Layer assignments\n        println!(\"   Inferring layers from call graph...\");\n        let layers = infer_layers(&self.call_graph, 100);\n        result.layer_assignments = layers.clone();\n        println!(\"     Inferred {} layer assignments\", layers.len());\n\n        // 5. Check for violations\n        println!(\"   Checking for violations...\");\n        let violations = detect_layer_violations(&layers, &self.call_graph);\n        for (caller, callee, caller_layer, callee_layer) in violations {\n            let violation = InvariantViolation {\n                invariant: Invariant::new(\n                    caller.clone(),\n                    \"\".to_string(),\n                    InvariantKind::Structural(StructuralInvariant::LayerFixed {\n                        layer: caller_layer,\n                    }),\n                    InvariantStrength::Proven,\n                    \"Layer violation detected\".to_string(),\n                ),\n                violation_description: format!(\n                    \"{} (layer {}) calls {} (layer {}) - violation!\",\n                    caller, caller_layer, callee, callee_layer\n                ),\n                severity: ViolationSeverity::Critical,\n                suggested_fix: Some(format!(\n                    \"Move {} to layer > {} or refactor call\",\n                    caller, callee_layer\n                )),\n            };\n            result.add_violation(violation);\n        }\n\n        result.stats.update_totals();\n\n        println!();\n        println!(\n            \" Invariants: {} (proven: {}, empirical: {}, heuristic: {})\",\n            result.stats.total_count,\n            result.stats.proven_count,\n            result.stats.empirical_count,\n            result.stats.heuristic_count\n        );\n        println!(\" Violations: {}\", result.stats.violation_count);\n\n        result\n    }\n\n    /// Check for violations of existing invariants\n    #[allow(dead_code)]\n    pub fn check_violations(&self, invariants: &[Invariant]) -> Vec<InvariantViolation> {\n        let mut violations = Vec::new();\n\n        // Check layer violations\n        let layers = infer_layers(&self.call_graph, 100);\n        let layer_violations = detect_layer_violations(&layers, &self.call_graph);\n\n        for (caller, callee, caller_layer, callee_layer) in layer_violations {\n            // Find the relevant invariant\n            if let Some(inv) = invariants.iter().find(|i| i.target == caller) {\n                let violation = InvariantViolation {\n                    invariant: inv.clone(),\n                    violation_description: format!(\n                        \"{} (layer {}) calls {} (layer {})\",\n                        caller, caller_layer, callee, callee_layer\n                    ),\n                    severity: ViolationSeverity::Critical,\n                    suggested_fix: Some(format!(\"Refactor {} to respect layer constraints\", caller)),\n                };\n                violations.push(violation);\n            }\n        }\n\n        violations\n    }\n\n    /// Generate refactoring constraints from detected invariants\n    pub fn generate_constraints(&self, result: &InvariantAnalysisResult) -> Vec<crate::refactor_constraints::RefactorConstraint> {\n        generate_constraints(result)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::types::{CodeElement, ElementType, Language, Visibility};\n\n    fn make_simple_analysis() -> AnalysisResult {\n        let mut result = AnalysisResult::new();\n\n        result.add_element(CodeElement {\n            name: \"test_fn\".to_string(),\n            file_path: \"test.rs\".to_string(),\n            line_number: 1,\n            element_type: ElementType::Function,\n            signature: \"fn test_fn() -> i32\".to_string(),\n            visibility: Visibility::Public,\n            generic_params: Vec::new(),\n            language: Language::Rust,\n            layer: \"0\".to_string(),\n            calls: Vec::new(),\n        });\n\n        let mut call_graph = HashMap::new();\n        call_graph.insert(\n            \"test_fn\".to_string(),\n            CallGraphNode {\n                function_name: \"test_fn\".to_string(),\n                file_path: \"test.rs\".to_string(),\n                calls: Vec::new(),\n                called_by: Vec::new(),\n            },\n        );\n        result.call_graph = call_graph;\n\n        result\n    }\n\n    #[test]\n    fn test_invariant_detector_creation() {\n        let analysis = make_simple_analysis();\n        let detector = InvariantDetector::new(&analysis, &analysis.call_graph);\n\n        assert_eq!(detector.call_graph.node_count(), 1);\n    }\n\n    #[test]\n    fn test_detect_all() {\n        let analysis = make_simple_analysis();\n        let detector = InvariantDetector::new(&analysis, &analysis.call_graph);\n        let result = detector.detect_all();\n\n        // Should have at least some structural invariants\n        assert!(result.stats.structural_count > 0);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_layer_utilities.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/160_layer_utilities.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/070_layer_utilities.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/160_layer_utilities.rs",
          "original_content": "//! Layer utility functions for layer-based dependency analysis\n//! This module is at layer 010 to be accessible from all higher layers\n\nuse anyhow::{Context, Result};\nuse clap::Parser;\nuse std::path::{Path, PathBuf};\n\n#[allow(unused_imports)]\npub use crate::cluster_001::{build_file_layers, detect_layer, gather_julia_files, julia_entry_paths};\n#[allow(unused_imports)]\npub use crate::cluster_010::contains_tools;\n\n/// Resolves the source root directory from a given root path\npub fn resolve_source_root(root: &Path) -> PathBuf {\n    let src_candidate = root.join(\"src\");\n    if src_candidate.exists() && src_candidate.is_dir() {\n        src_candidate\n    } else {\n        root.to_path_buf()\n    }\n}\n\n/// Checks if a directory should be included in analysis\npub fn allow_analysis_dir(root: &Path, dir: &Path) -> bool {\n    let name = dir.file_name().and_then(|n| n.to_str()).unwrap_or(\"\");\n    \n    if name.starts_with('.') || name == \"target\" || name == \"node_modules\" {\n        return false;\n    }\n    \n    if let Ok(rel) = dir.strip_prefix(root) {\n        if rel.components().any(|c| {\n            let s = c.as_os_str().to_str().unwrap_or(\"\");\n            s.starts_with('.') || s == \"target\" || s == \"node_modules\"\n        }) {\n            return false;\n        }\n    }\n    \n    true\n}\n\npub fn gather_rust_files(root: &Path) -> Vec<PathBuf> {\n    use walkdir::WalkDir;\n\n    let src_root = resolve_source_root(root);\n    WalkDir::new(&src_root)\n        .into_iter()\n        .filter_entry(|entry| {\n            if entry.depth() == 0 {\n                return true;\n            }\n            if !entry.file_type().is_dir() {\n                return true;\n            }\n            allow_analysis_dir(&src_root, entry.path())\n        })\n        .filter_map(|e| e.ok())\n        .filter(|e| e.path().extension().map_or(false, |ext| ext == \"rs\"))\n        .filter(|e| {\n            let rel = e.path().strip_prefix(&src_root).unwrap_or(e.path());\n            rel.components().count() == 1 || e.path().starts_with(src_root.join(\"src\"))\n        })\n        .map(|entry| entry.into_path())\n        .collect()\n}\n\n// ============================================================================\n// CLI Entrypoint (from src/000_cluster_011.rs)\n// ============================================================================\n\n#[derive(Parser, Debug)]\n#[command(name = \"mmsb-analyzer\")]\n#[command(about = \"MMSB Intelligence Substrate Analyzer\", long_about = None)]\nstruct Args {\n    /// Root directory to analyze\n    #[arg(short, long, default_value = \"../..\")]\n    root: PathBuf,\n\n    /// Output directory for reports\n    #[arg(short, long, default_value = \"../../docs/analysis\")]\n    output: PathBuf,\n\n    /// Verbose output\n    #[arg(short, long)]\n    verbose: bool,\n\n    /// Skip Julia file analysis\n    #[arg(long)]\n    skip_julia: bool,\n\n    /// Run dead code analysis\n    #[arg(long)]\n    dead_code: bool,\n\n    /// Filter dead code from downstream analysis\n    #[arg(long)]\n    dead_code_filter: bool,\n\n    /// Output JSON dead code report\n    #[arg(long)]\n    dead_code_json: Option<PathBuf>,\n\n    /// Output dead code summary markdown\n    #[arg(long)]\n    dead_code_summary: Option<PathBuf>,\n\n    /// Dead code summary limit\n    #[arg(long, default_value_t = 50)]\n    dead_code_summary_limit: usize,\n\n    /// Dead code policy file\n    #[arg(long)]\n    dead_code_policy: Option<PathBuf>,\n\n    /// Generate correction intelligence JSON\n    #[arg(long)]\n    correction_intelligence: bool,\n\n    /// Override correction intelligence JSON output path\n    #[arg(long)]\n    correction_json: Option<PathBuf>,\n\n    /// Override verification policy JSON output path\n    #[arg(long)]\n    verification_policy_json: Option<PathBuf>,\n}\n\npub fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let root_path = std::env::current_dir()?.join(&args.root).canonicalize()?;\n    let output_path = std::env::current_dir()?\n        .join(&args.output)\n        .canonicalize()\n        .unwrap_or_else(|_| {\n            let p = std::env::current_dir().unwrap().join(&args.output);\n            std::fs::create_dir_all(&p).ok();\n            p.canonicalize().unwrap_or(p)\n        });\n    run_analysis(\n        &root_path,\n        &output_path,\n        args.verbose,\n        args.skip_julia,\n        args.dead_code,\n        args.dead_code_filter,\n        args.dead_code_json,\n        args.dead_code_summary,\n        args.dead_code_summary_limit,\n        args.dead_code_policy,\n        args.correction_intelligence,\n        args.correction_json,\n        args.verification_policy_json,\n    )\n}\n\npub fn run_analysis(\n    root_path: &Path,\n    output_path: &Path,\n    verbose: bool,\n    skip_julia: bool,\n    dead_code: bool,\n    dead_code_filter: bool,\n    dead_code_json: Option<PathBuf>,\n    dead_code_summary: Option<PathBuf>,\n    dead_code_summary_limit: usize,\n    dead_code_policy: Option<PathBuf>,\n    correction_intelligence: bool,\n    correction_json: Option<PathBuf>,\n    verification_policy_json: Option<PathBuf>,\n) -> Result<()> {\n    use crate::control_flow::ControlFlowAnalyzer;\n    use crate::cohesion_analyzer::FunctionCohesionAnalyzer;\n    use crate::dependency::LayerGraph;\n    use crate::directory_analyzer::DirectoryAnalyzer;\n    use crate::dot_exporter::export_program_cfg_to_path;\n    use crate::julia_parser::JuliaAnalyzer;\n    use crate::report::ReportGenerator;\n    use crate::rust_parser::RustAnalyzer;\n    use crate::types::{AnalysisResult, FileOrderingResult};\n\n    let julia_script_path = root_path.join(\"src/000_main.jl\");\n\n    println!(\"MMSB Intelligence Substrate Analyzer\");\n    println!(\"=====================================\\n\");\n    println!(\"Root directory: {:?}\", root_path);\n    println!(\"Output directory: {:?}\", output_path);\n    println!(\"Julia script: {:?}\\n\", julia_script_path);\n\n    let rust_analyzer = RustAnalyzer::new(root_path.to_string_lossy().to_string());\n    let mut combined_result = AnalysisResult::new();\n\n    println!(\"Scanning Rust files (dependency-ordered)...\");\n    let mut rust_count = 0;\n    let rust_files = gather_rust_files(root_path);\n    let (ordered_rust_files, rust_layer_graph) =\n        crate::dependency::order_rust_files_by_dependency(&rust_files, root_path)\n            .context(\"Failed to resolve Rust dependency order\")?;\n    let rust_file_ordering =\n        crate::dependency::analyze_file_ordering(&rust_files, None)\n            .context(\"Failed to analyze Rust file ordering\")?;\n    let julia_file_ordering = FileOrderingResult {\n        ordered_files: Vec::new(),\n        violations: Vec::new(),\n        layer_violations: Vec::new(),\n        ordered_directories: Vec::new(),\n        cycles: Vec::new(),\n    };\n\n    for path in ordered_rust_files {\n        if verbose {\n            println!(\"  Analyzing: {:?}\", path);\n        }\n\n        match rust_analyzer.analyze_file(&path) {\n            Ok(result) => {\n                rust_count += 1;\n                combined_result.merge(result);\n            }\n            Err(e) => {\n                eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n            }\n        }\n    }\n\n    println!(\"  Analyzed {} Rust files\\n\", rust_count);\n\n    let mut julia_count = 0;\n    let mut julia_layer_graph = LayerGraph {\n        ordered_layers: Vec::new(),\n        edges: Vec::new(),\n        cycles: Vec::new(),\n        unresolved: Vec::new(),\n    };\n    if !skip_julia {\n        println!(\"Scanning Julia files (dependency-ordered)...\");\n        let julia_files = gather_julia_files(root_path);\n        let (ordered_julia_files, jlg) =\n            crate::dependency::order_julia_files_by_dependency(&julia_files, root_path)\n                .context(\"Failed to resolve Julia dependency order\")?;\n        julia_layer_graph = jlg;\n\n        if julia_script_path.exists() {\n            let julia_analyzer = JuliaAnalyzer::new(\n                root_path.to_path_buf(),\n                julia_script_path.clone(),\n                output_path.join(\"30_cfg/dots\"),\n            );\n\n            for path in ordered_julia_files {\n                if verbose {\n                    println!(\"  Analyzing: {:?}\", path);\n                }\n\n                match julia_analyzer.analyze_file(&path) {\n                    Ok(result) => {\n                        julia_count += 1;\n                        combined_result.merge(result);\n                    }\n                    Err(e) => {\n                        eprintln!(\"Warning: Failed to analyze {:?}: {}\", path, e);\n                    }\n                }\n            }\n        } else {\n            println!(\"  Skipping Julia analysis (script not found)\");\n        }\n\n        println!(\"  Analyzed {} Julia files\\n\", julia_count);\n    }\n\n    if dead_code || dead_code_filter || dead_code_json.is_some() || dead_code_summary.is_some() {\n        let policy = if let Some(policy_path) = dead_code_policy {\n            Some(\n                crate::dead_code_policy::load_policy(&policy_path)\n                    .context(\"Failed to load dead code policy\")?,\n            )\n        } else {\n            None\n        };\n        let config = crate::dead_code_cli::DeadCodeRunConfig {\n            root: root_path.to_path_buf(),\n            output_dir: output_path.to_path_buf(),\n            policy,\n            write_json: dead_code_json,\n            write_summary: dead_code_summary,\n            summary_limit: dead_code_summary_limit,\n        };\n        let report = crate::dead_code_cli::run_dead_code_pipeline(&combined_result.elements, &config)\n            .context(\"Dead code analysis failed\")?;\n        if dead_code_filter {\n            combined_result.elements =\n                crate::dead_code_filter::filter_dead_code_elements(&combined_result.elements, &report);\n        }\n    }\n\n    println!(\"Building call graph...\");\n    let mut cf_analyzer = ControlFlowAnalyzer::new();\n    cf_analyzer.build_call_graph(&combined_result);\n\n    // NEW: Invariant detection\n    use crate::invariant_integrator::InvariantDetector;\n    println!(\"Detecting invariants...\");\n    let invariants_result = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.detect_all()\n    };\n    let constraints = {\n        let invariant_detector = InvariantDetector::new(\n            &combined_result,\n            &combined_result.call_graph,\n        );\n        invariant_detector.generate_constraints(&invariants_result)\n    };\n    combined_result.invariants = invariants_result;\n    combined_result.constraints = constraints;\n\n    println!(\"Analyzing function cohesion...\");\n    let cohesion_analyzer = FunctionCohesionAnalyzer::new();\n    let placements = cohesion_analyzer.analyze(&combined_result)?;\n    let clusters = cohesion_analyzer.detect_clusters(&combined_result)?;\n\n    println!(\"Analyzing directory structure...\");\n    let dir_analyzer = DirectoryAnalyzer::new(root_path.to_path_buf());\n    let dir_analysis = dir_analyzer.analyze()?;\n\n    println!(\"\\nGenerating reports...\");\n    let report_gen = ReportGenerator::new(output_path.to_string_lossy().to_string());\n    report_gen.generate_all(\n        &combined_result,\n        &cf_analyzer,\n        &rust_layer_graph,\n        &julia_layer_graph,\n        &rust_file_ordering,\n        &julia_file_ordering,\n        &placements,\n        &clusters,\n        &dir_analysis,\n        root_path,\n        correction_intelligence,\n        correction_json,\n        verification_policy_json,\n    )\n    .context(\"Failed to generate reports\")?;\n\n    println!(\"\\nExporting program CFG...\");\n    export_program_cfg_to_path(&combined_result, &cf_analyzer.call_edges(), output_path)?;\n\n    println!(\"\\nGenerating invariant report...\");\n    use crate::invariant_reporter;\n    invariant_reporter::generate_invariant_report(&combined_result.invariants, output_path)\n        .context(\"Failed to generate invariant report\")?;\n    invariant_reporter::export_constraints_json(&combined_result.constraints, output_path)\n        .context(\"Failed to export constraints\")?;\n\n    println!(\"\\n Analysis complete!\");\n    println!(\"  Total elements: {}\", combined_result.elements.len());\n    println!(\"  Rust files: {}\", rust_count);\n    println!(\"  Julia files: {}\", julia_count);\n    println!(\"  Output: {}\\n\", output_path.display());\n\n    Ok(())\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/080_invariant_reporter.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/170_invariant_reporter.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/080_invariant_reporter.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/170_invariant_reporter.rs",
          "original_content": "//! Invariant Reporting\n//!\n//! This module generates reports for detected invariants, exports them to JSON,\n//! and integrates with the existing reporting system.\n\nuse crate::invariant_types::*;\nuse crate::refactor_constraints::RefactorConstraint;\nuse serde_json;\nuse std::fs;\nuse std::path::Path;\n\n/// Generate invariant report in markdown format\npub fn generate_invariant_report(\n    result: &InvariantAnalysisResult,\n    output_dir: &Path,\n) -> Result<(), std::io::Error> {\n    let report_dir = output_dir.join(\"95_invariants\");\n    fs::create_dir_all(&report_dir)?;\n\n    let report_path = report_dir.join(\"index.md\");\n    let mut report = String::new();\n\n    // Header\n    report.push_str(\"# Invariant Analysis Report\\n\\n\");\n    report.push_str(&format!(\n        \"Generated: {}\\n\\n\",\n        chrono::Local::now().format(\"%Y-%m-%d %H:%M:%S\")\n    ));\n\n    // Summary\n    report.push_str(\"## Summary\\n\\n\");\n    report.push_str(&format!(\"- **Total Invariants**: {}\\n\", result.stats.total_count));\n    report.push_str(&format!(\"- **Proven**: {} ({:.1}%)\\n\",\n        result.stats.proven_count,\n        result.stats.proven_percentage()\n    ));\n    report.push_str(&format!(\"- **Empirical**: {}\\n\", result.stats.empirical_count));\n    report.push_str(&format!(\"- **Heuristic**: {} ({:.1}%)  LOW CONFIDENCE\\n\",\n        result.stats.heuristic_count,\n        result.stats.heuristic_percentage()\n    ));\n    report.push_str(&format!(\"- **Violations**: {}\\n\\n\", result.stats.violation_count));\n\n    report.push_str(\"### By Kind\\n\\n\");\n    report.push_str(&format!(\"- **Structural**: {}\\n\", result.stats.structural_count));\n    report.push_str(&format!(\"- **Semantic**: {}\\n\", result.stats.semantic_count));\n    report.push_str(&format!(\"- **Delta**: {}\\n\", result.stats.delta_count));\n    report.push_str(&format!(\"- **Path-Intersection**: {}\\n\\n\", result.stats.path_intersection_count));\n\n    // Proven Invariants\n    report.push_str(\"## Proven Invariants (Mechanical Truth)\\n\\n\");\n    report.push_str(\"These invariants are mathematically proven from graph structure and should **always block refactorings**.\\n\\n\");\n\n    let proven: Vec<_> = result\n        .invariants\n        .iter()\n        .filter(|inv| matches!(inv.strength, InvariantStrength::Proven))\n        .collect();\n\n    if proven.is_empty() {\n        report.push_str(\"*None detected*\\n\\n\");\n    } else {\n        for inv in &proven {\n            report.push_str(&format!(\"### {}\\n\\n\", inv.target));\n            report.push_str(&format!(\"- **Type**: {}\\n\", inv.kind));\n            report.push_str(&format!(\"- **File**: {}\\n\", inv.file_path));\n            report.push_str(&format!(\"- **Description**: {}\\n\", inv.description));\n            if !inv.evidence.is_empty() {\n                report.push_str(\"- **Evidence**:\\n\");\n                for e in &inv.evidence {\n                    report.push_str(&format!(\"  - {}\\n\", e));\n                }\n            }\n            report.push_str(\"\\n\");\n        }\n    }\n\n    // Empirical Invariants\n    report.push_str(\"## Empirical Invariants (High Confidence)\\n\\n\");\n    report.push_str(\"These invariants were observed across multiple paths/samples and have high confidence.\\n\\n\");\n\n    let empirical: Vec<_> = result\n        .invariants\n        .iter()\n        .filter(|inv| matches!(inv.strength, InvariantStrength::Empirical { .. }))\n        .collect();\n\n    if empirical.is_empty() {\n        report.push_str(\"*None detected*\\n\\n\");\n    } else {\n        for inv in empirical.iter().take(20) {\n            // Limit to first 20 for readability\n            report.push_str(&format!(\"### {}\\n\\n\", inv.target));\n            report.push_str(&format!(\"- **Type**: {}\\n\", inv.kind));\n            report.push_str(&format!(\"- **Strength**: {}\\n\", inv.strength));\n            report.push_str(&format!(\"- **Confidence**: {:.2}\\n\", inv.confidence.value()));\n            report.push_str(&format!(\"- **Description**: {}\\n\\n\", inv.description));\n        }\n        if empirical.len() > 20 {\n            report.push_str(&format!(\"*... and {} more*\\n\\n\", empirical.len() - 20));\n        }\n    }\n\n    // Heuristic Signals\n    report.push_str(\"## Heuristic Signals (Low Confidence - Review Required)\\n\\n\");\n    report.push_str(\" **WARNING**: These are based on naming patterns and heuristics. They require manual verification and should **NOT block refactorings**.\\n\\n\");\n\n    let heuristic: Vec<_> = result\n        .invariants\n        .iter()\n        .filter(|inv| matches!(inv.strength, InvariantStrength::Heuristic))\n        .collect();\n\n    if heuristic.is_empty() {\n        report.push_str(\"*None detected*\\n\\n\");\n    } else {\n        for inv in heuristic.iter().take(10) {\n            // Limit to first 10\n            report.push_str(&format!(\"- **{}**: {} ({})\\n\", inv.target, inv.description, inv.file_path));\n        }\n        if heuristic.len() > 10 {\n            report.push_str(&format!(\"\\n*... and {} more (see JSON export)*\\n\\n\", heuristic.len() - 10));\n        } else {\n            report.push_str(\"\\n\");\n        }\n    }\n\n    // Violations\n    if !result.violations.is_empty() {\n        report.push_str(\"## Violations\\n\\n\");\n        report.push_str(\"Detected violations of invariants, grouped by severity.\\n\\n\");\n\n        let mut critical: Vec<_> = result.violations.iter()\n            .filter(|v| matches!(v.severity, ViolationSeverity::Critical))\n            .collect();\n        critical.sort_by_key(|v| &v.invariant.target);\n\n        if !critical.is_empty() {\n            report.push_str(\"### Critical\\n\\n\");\n            for violation in critical {\n                report.push_str(&format!(\"- **{}**: {}\\n\",\n                    violation.invariant.target,\n                    violation.violation_description\n                ));\n                if let Some(ref fix) = violation.suggested_fix {\n                    report.push_str(&format!(\"  - *Suggested fix*: {}\\n\", fix));\n                }\n            }\n            report.push_str(\"\\n\");\n        }\n    }\n\n    // Layer Assignments\n    if !result.layer_assignments.is_empty() {\n        report.push_str(\"## Layer Assignments (Inferred from Call Graph)\\n\\n\");\n        report.push_str(\"Layers are **NOT** based on filename prefixes. They are computed from the call graph structure.\\n\\n\");\n\n        let mut layers: Vec<_> = result.layer_assignments.iter().collect();\n        layers.sort_by_key(|(_, info)| info.layer);\n\n        for (name, info) in layers.iter().take(20) {\n            report.push_str(&format!(\"- **{}**: Layer {} (dependencies: {})\\n\",\n                name,\n                info.layer,\n                info.dependencies.len()\n            ));\n        }\n        if layers.len() > 20 {\n            report.push_str(&format!(\"\\n*... and {} more (see JSON export)*\\n\\n\", layers.len() - 20));\n        } else {\n            report.push_str(\"\\n\");\n        }\n    }\n\n    fs::write(&report_path, report)?;\n    println!(\" Invariant report written to: {}\", report_path.display());\n\n    // Export JSON\n    export_json(result, &report_dir)?;\n\n    // Generate conscience map\n    let conscience_map_path = report_dir.join(\"conscience_map.md\");\n    crate::conscience_graph::generate_conscience_map(&result.invariants, &conscience_map_path)?;\n    println!(\" Conscience map written to: {}\", conscience_map_path.display());\n\n    Ok(())\n}\n\n/// Export invariants to JSON for agent consumption\npub fn export_json(\n    result: &InvariantAnalysisResult,\n    output_dir: &Path,\n) -> Result<(), std::io::Error> {\n    let json_path = output_dir.join(\"invariants.json\");\n    let json = serde_json::to_string_pretty(result)\n        .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))?;\n    fs::write(&json_path, json)?;\n    println!(\" JSON export written to: {}\", json_path.display());\n    Ok(())\n}\n\n/// Export refactoring constraints to JSON\npub fn export_constraints_json(\n    constraints: &[RefactorConstraint],\n    output_dir: &Path,\n) -> Result<(), std::io::Error> {\n    let constraints_dir = output_dir.join(\"96_constraints\");\n    fs::create_dir_all(&constraints_dir)?;\n\n    let json_path = constraints_dir.join(\"refactor_constraints.json\");\n    let json = serde_json::to_string_pretty(constraints)\n        .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))?;\n    fs::write(&json_path, json)?;\n    println!(\" Constraints written to: {}\", json_path.display());\n    Ok(())\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::collections::HashMap;\n\n    #[test]\n    fn test_generate_report() {\n        let result = InvariantAnalysisResult {\n            invariants: vec![\n                Invariant::new(\n                    \"test_fn\".to_string(),\n                    \"test.rs\".to_string(),\n                    InvariantKind::Structural(StructuralInvariant::Leaf),\n                    InvariantStrength::Proven,\n                    \"Test invariant\".to_string(),\n                ),\n            ],\n            violations: Vec::new(),\n            layer_assignments: HashMap::new(),\n            stats: InvariantStats {\n                total_count: 1,\n                proven_count: 1,\n                empirical_count: 0,\n                heuristic_count: 0,\n                structural_count: 1,\n                semantic_count: 0,\n                delta_count: 0,\n                path_intersection_count: 0,\n                violation_count: 0,\n            },\n        };\n\n        let temp_dir = std::env::temp_dir().join(\"mmsb_test_invariants\");\n        fs::create_dir_all(&temp_dir).unwrap();\n\n        let res = generate_invariant_report(&result, &temp_dir);\n        assert!(res.is_ok());\n\n        // Cleanup\n        let _ = fs::remove_dir_all(&temp_dir);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/082_conscience_graph.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/180_conscience_graph.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/082_conscience_graph.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/180_conscience_graph.rs",
          "original_content": "//! Conscience Visualization\n//!\n//! Generates visual map of conscience: which functions are protected by invariants.\n\nuse crate::invariant_types::*;\nuse std::collections::HashMap;\nuse std::path::Path;\n\n/// Generate conscience map showing protection levels per function\npub fn generate_conscience_map(\n    invariants: &[Invariant],\n    output_path: &Path,\n) -> std::io::Result<()> {\n    let mut content = String::new();\n\n    content.push_str(\"# Conscience Map\\n\\n\");\n    content.push_str(\"## Overview\\n\\n\");\n    content.push_str(\"This map shows which functions are protected by mechanical constraints.\\n\");\n    content.push_str(\"Functions with blocking invariants cannot be refactored without violating proven properties.\\n\\n\");\n\n    // Group invariants by function\n    let mut by_function: HashMap<String, Vec<&Invariant>> = HashMap::new();\n\n    for inv in invariants {\n        by_function\n            .entry(inv.target.clone())\n            .or_default()\n            .push(inv);\n    }\n\n    // Calculate statistics\n    let total_functions = by_function.len();\n    let protected_functions = by_function\n        .values()\n        .filter(|invs| invs.iter().any(|i| i.is_blocking()))\n        .count();\n\n    content.push_str(&format!(\n        \"**Total Functions**: {}\\n\\n\",\n        total_functions\n    ));\n    content.push_str(&format!(\n        \"**Protected Functions**: {} ({:.1}%)\\n\\n\",\n        protected_functions,\n        (protected_functions as f64 / total_functions as f64) * 100.0\n    ));\n\n    // Sort by protection level (most protected first)\n    let mut funcs: Vec<_> = by_function.into_iter().collect();\n    funcs.sort_by_key(|(_, invs)| {\n        -(invs.iter().filter(|i| i.is_blocking()).count() as i32)\n    });\n\n    content.push_str(\"---\\n\\n\");\n    content.push_str(\"## Functions by Protection Level\\n\\n\");\n\n    for (func, invs) in funcs {\n        let blocking_count = invs.iter().filter(|i| i.is_blocking()).count();\n        let total_count = invs.len();\n\n        if blocking_count == 0 {\n            continue; // Skip unprotected functions\n        }\n\n        let protection_percent = (blocking_count * 100) / total_count;\n\n        content.push_str(&format!(\n            \"### `{}` ({}% protected)\\n\\n\",\n            func, protection_percent\n        ));\n\n        // Show file path if available\n        if let Some(inv) = invs.first() {\n            if !inv.file_path.is_empty() {\n                content.push_str(&format!(\"**File**: `{}`\\n\\n\", inv.file_path));\n            }\n        }\n\n        // List blocking invariants\n        for inv in invs.iter().filter(|i| i.is_blocking()) {\n            content.push_str(&format!(\n                \"-  {} **{}**: {}\\n\",\n                strength_emoji(inv),\n                kind_name(inv),\n                inv.description\n            ));\n        }\n\n        content.push_str(\"\\n\");\n    }\n\n    // Legend\n    content.push_str(\"---\\n\\n\");\n    content.push_str(\"## Legend\\n\\n\");\n    content.push_str(\"-  **PROVEN**: Mathematical certainty from graph topology\\n\");\n    content.push_str(\"-  **EMPIRICAL**: Observed across multiple paths (high confidence)\\n\");\n    content.push_str(\"- ? **HEURISTIC**: Name-based guess (LOW CONFIDENCE - verify manually)\\n\\n\");\n    content.push_str(\"-  **Blocking**: Constraint mechanically enforced\\n\\n\");\n\n    std::fs::write(output_path, content)?;\n    Ok(())\n}\n\n/// Get emoji for invariant strength\nfn strength_emoji(inv: &Invariant) -> &'static str {\n    match inv.strength {\n        InvariantStrength::Proven => \"\",\n        InvariantStrength::Empirical { .. } => \"\",\n        InvariantStrength::Heuristic => \"?\",\n    }\n}\n\n/// Get short name for invariant kind\nfn kind_name(inv: &Invariant) -> String {\n    match &inv.kind {\n        InvariantKind::Structural(s) => match s {\n            StructuralInvariant::LayerFixed { layer } => format!(\"LayerFixed({})\", layer),\n            StructuralInvariant::DegreeStable { in_degree, out_degree } => {\n                format!(\"DegreeStable(in={}, out={})\", in_degree, out_degree)\n            }\n            StructuralInvariant::Leaf => \"Leaf\".to_string(),\n            StructuralInvariant::Root => \"Root\".to_string(),\n            StructuralInvariant::Bridge => \"Bridge\".to_string(),\n            StructuralInvariant::SccMembership { scc_id, scc_size } => {\n                format!(\"SCC({}, size={})\", scc_id, scc_size)\n            }\n        },\n        InvariantKind::Semantic(s) => match s {\n            SemanticInvariant::TypeStable { .. } => \"TypeStable\".to_string(),\n            SemanticInvariant::PureFunction => \"PureFunction\".to_string(),\n            SemanticInvariant::Idempotent => \"Idempotent\".to_string(),\n            SemanticInvariant::EffectStable { .. } => \"EffectStable\".to_string(),\n        },\n        InvariantKind::Delta(_) => \"Delta\".to_string(),\n        InvariantKind::PathIntersection { .. } => \"PathIntersection\".to_string(),\n    }\n}\n\n/// Generate summary statistics\n#[allow(dead_code)]\npub fn generate_conscience_stats(invariants: &[Invariant]) -> ConscienceStats {\n    let mut by_function: HashMap<String, Vec<&Invariant>> = HashMap::new();\n\n    for inv in invariants {\n        by_function\n            .entry(inv.target.clone())\n            .or_default()\n            .push(inv);\n    }\n\n    let total_functions = by_function.len();\n    let protected_functions = by_function\n        .values()\n        .filter(|invs| invs.iter().any(|i| i.is_blocking()))\n        .count();\n\n    let proven_count = invariants\n        .iter()\n        .filter(|i| matches!(i.strength, InvariantStrength::Proven))\n        .count();\n\n    let empirical_count = invariants\n        .iter()\n        .filter(|i| matches!(i.strength, InvariantStrength::Empirical { .. }))\n        .count();\n\n    let heuristic_count = invariants\n        .iter()\n        .filter(|i| matches!(i.strength, InvariantStrength::Heuristic))\n        .count();\n\n    ConscienceStats {\n        total_functions,\n        protected_functions,\n        total_invariants: invariants.len(),\n        blocking_invariants: invariants.iter().filter(|i| i.is_blocking()).count(),\n        proven_count,\n        empirical_count,\n        heuristic_count,\n    }\n}\n\n#[allow(dead_code)]\npub struct ConscienceStats {\n    pub total_functions: usize,\n    pub protected_functions: usize,\n    pub total_invariants: usize,\n    pub blocking_invariants: usize,\n    pub proven_count: usize,\n    pub empirical_count: usize,\n    pub heuristic_count: usize,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_test_invariant(\n        name: &str,\n        kind: InvariantKind,\n        strength: InvariantStrength,\n    ) -> Invariant {\n        Invariant::new(\n            name.to_string(),\n            \"test.rs\".to_string(),\n            kind,\n            strength,\n            \"test\".to_string(),\n        )\n    }\n\n    #[test]\n    fn test_generate_stats() {\n        let invariants = vec![\n            make_test_invariant(\n                \"fn1\",\n                InvariantKind::Structural(StructuralInvariant::LayerFixed { layer: 0 }),\n                InvariantStrength::Proven,\n            ),\n            make_test_invariant(\n                \"fn1\",\n                InvariantKind::Semantic(SemanticInvariant::PureFunction),\n                InvariantStrength::Heuristic,\n            ),\n            make_test_invariant(\n                \"fn2\",\n                InvariantKind::Structural(StructuralInvariant::Leaf),\n                InvariantStrength::Proven,\n            ),\n        ];\n\n        let stats = generate_conscience_stats(&invariants);\n\n        assert_eq!(stats.total_functions, 2); // fn1 and fn2\n        assert_eq!(stats.total_invariants, 3);\n        assert_eq!(stats.proven_count, 2);\n        assert_eq!(stats.heuristic_count, 1);\n    }\n\n    #[test]\n    fn test_strength_emoji() {\n        let proven = make_test_invariant(\n            \"test\",\n            InvariantKind::Structural(StructuralInvariant::Leaf),\n            InvariantStrength::Proven,\n        );\n        let empirical = make_test_invariant(\n            \"test\",\n            InvariantKind::Semantic(SemanticInvariant::TypeStable {\n                signature: \"sig\".to_string(),\n            }),\n            InvariantStrength::Empirical { paths_checked: 5 },\n        );\n        let heuristic = make_test_invariant(\n            \"test\",\n            InvariantKind::Semantic(SemanticInvariant::PureFunction),\n            InvariantStrength::Heuristic,\n        );\n\n        assert_eq!(strength_emoji(&proven), \"\");\n        assert_eq!(strength_emoji(&empirical), \"\");\n        assert_eq!(strength_emoji(&heuristic), \"?\");\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/083_action_validator.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/190_action_validator.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/083_action_validator.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/190_action_validator.rs",
          "original_content": "//! Action Validation\n//!\n//! Core validation logic for agent actions against refactoring constraints.\n//! Matches proposed actions against mechanical constraints and generates violation reports.\n\nuse crate::refactor_constraints::RefactorConstraint;\nuse serde::{Deserialize, Serialize};\nuse std::path::PathBuf;\n\n/// Agent action proposal\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum AgentAction {\n    /// Move function to different file\n    MoveFunction {\n        name: String,\n        from: PathBuf,\n        to: PathBuf,\n    },\n\n    /// Rename function\n    RenameFunction {\n        old_name: String,\n        new_name: String,\n        file: PathBuf,\n    },\n\n    /// Change function signature\n    ChangeSignature {\n        name: String,\n        old_sig: String,\n        new_sig: String,\n        file: PathBuf,\n    },\n\n    /// Delete function\n    DeleteFunction { name: String, file: PathBuf },\n\n    /// Create new function\n    CreateFunction {\n        name: String,\n        file: PathBuf,\n        signature: String,\n    },\n}\n\n/// Violation severity levels\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord)]\npub enum ViolationSeverity {\n    Info,\n    Warning,\n    High,\n    Critical,\n}\n\n/// Constraint violation with explanation\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ConstraintViolation {\n    pub constraint_id: usize,\n    pub invariant_id: usize,\n    pub reason: String,\n    pub severity: ViolationSeverity,\n    pub blocking: bool,\n}\n\n/// Extract layer number from file path (e.g., \"src/040_test.rs\" -> Some(40))\nfn extract_layer(path: &PathBuf) -> Option<usize> {\n    path.file_name()\n        .and_then(|n| n.to_str())\n        .and_then(|s| {\n            let parts: Vec<&str> = s.split('_').collect();\n            if !parts.is_empty() {\n                parts[0].parse::<usize>().ok()\n            } else {\n                None\n            }\n        })\n}\n\n/// Validate action against all constraints\npub fn validate_action(\n    action: &AgentAction,\n    constraints: &[RefactorConstraint],\n) -> Result<(), Vec<ConstraintViolation>> {\n    let mut violations = Vec::new();\n\n    for (idx, constraint) in constraints.iter().enumerate() {\n        match (action, constraint) {\n            // MoveFunction violations\n            (\n                AgentAction::MoveFunction { name, from, to },\n                RefactorConstraint::NoMove {\n                    target,\n                    reason,\n                    strength,\n                },\n            ) if target == name => {\n                violations.push(ConstraintViolation {\n                    constraint_id: idx,\n                    invariant_id: 0, // No direct link to invariant\n                    reason: format!(\"Cannot move {}: {} (strength: {:?})\", name, reason, strength),\n                    severity: ViolationSeverity::Critical,\n                    blocking: true,\n                });\n            }\n\n            (\n                AgentAction::MoveFunction { name, from, to },\n                RefactorConstraint::FixedLayer {\n                    target,\n                    layer,\n                    strength,\n                },\n            ) if target == name => {\n                let from_layer = extract_layer(from);\n                let to_layer = extract_layer(to);\n\n                if from_layer != to_layer {\n                    violations.push(ConstraintViolation {\n                        constraint_id: idx,\n                        invariant_id: 0,\n                        reason: format!(\n                            \"Cannot move {} across layers: layer {} fixed (strength: {:?})\",\n                            name, layer, strength\n                        ),\n                        severity: ViolationSeverity::Critical,\n                        blocking: true,\n                    });\n                }\n            }\n\n            // ChangeSignature violations\n            (\n                AgentAction::ChangeSignature { name, old_sig, .. },\n                RefactorConstraint::PreserveSignature {\n                    target,\n                    signature,\n                    strength,\n                },\n            ) if target == name => {\n                violations.push(ConstraintViolation {\n                    constraint_id: idx,\n                    invariant_id: 0,\n                    reason: format!(\n                        \"Cannot change signature of {}: type-stable (strength: {:?})\",\n                        name, strength\n                    ),\n                    severity: ViolationSeverity::High,\n                    blocking: true,\n                });\n            }\n\n            // DeleteFunction violations\n            (\n                AgentAction::DeleteFunction { name, .. },\n                RefactorConstraint::NoMove {\n                    target,\n                    reason,\n                    strength,\n                },\n            ) if target == name && reason.contains(\"utility\") => {\n                // High in-degree functions shouldn't be deleted\n                violations.push(ConstraintViolation {\n                    constraint_id: idx,\n                    invariant_id: 0,\n                    reason: format!(\n                        \"Cannot delete {}: widely used utility function (strength: {:?})\",\n                        name, strength\n                    ),\n                    severity: ViolationSeverity::Critical,\n                    blocking: true,\n                });\n            }\n\n            _ => {}\n        }\n    }\n\n    if violations.is_empty() {\n        Ok(())\n    } else {\n        Err(violations)\n    }\n}\n\n/// Check if a specific move is allowed\npub fn check_move_allowed(\n    name: &str,\n    from: &PathBuf,\n    to: &PathBuf,\n    constraints: &[RefactorConstraint],\n) -> Result<(), String> {\n    let action = AgentAction::MoveFunction {\n        name: name.to_string(),\n        from: from.clone(),\n        to: to.clone(),\n    };\n\n    match validate_action(&action, constraints) {\n        Ok(_) => Ok(()),\n        Err(violations) => {\n            let reasons: Vec<String> = violations.iter().map(|v| v.reason.clone()).collect();\n            Err(reasons.join(\"; \"))\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::invariant_types::InvariantStrength;\n\n    #[test]\n    fn test_extract_layer() {\n        assert_eq!(extract_layer(&PathBuf::from(\"src/040_test.rs\")), Some(40));\n        assert_eq!(extract_layer(&PathBuf::from(\"src/000_utils.rs\")), Some(0));\n        assert_eq!(extract_layer(&PathBuf::from(\"src/test.rs\")), None);\n    }\n\n    #[test]\n    fn test_validate_no_move_constraint() {\n        let constraints = vec![RefactorConstraint::NoMove {\n            target: \"test_fn\".to_string(),\n            reason: \"layer 0 is fixed\".to_string(),\n            strength: InvariantStrength::Proven,\n        }];\n\n        let action = AgentAction::MoveFunction {\n            name: \"test_fn\".to_string(),\n            from: PathBuf::from(\"src/000_test.rs\"),\n            to: PathBuf::from(\"src/010_test.rs\"),\n        };\n\n        let result = validate_action(&action, &constraints);\n        assert!(result.is_err());\n\n        let violations = result.unwrap_err();\n        assert_eq!(violations.len(), 1);\n        assert_eq!(violations[0].severity, ViolationSeverity::Critical);\n        assert!(violations[0].blocking);\n    }\n\n    #[test]\n    fn test_validate_layer_fixed_constraint() {\n        let constraints = vec![RefactorConstraint::FixedLayer {\n            target: \"test_fn\".to_string(),\n            layer: 0,\n            strength: InvariantStrength::Proven,\n        }];\n\n        let action = AgentAction::MoveFunction {\n            name: \"test_fn\".to_string(),\n            from: PathBuf::from(\"src/000_test.rs\"),\n            to: PathBuf::from(\"src/010_test.rs\"),\n        };\n\n        let result = validate_action(&action, &constraints);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_validate_preserve_signature() {\n        let constraints = vec![RefactorConstraint::PreserveSignature {\n            target: \"test_fn\".to_string(),\n            signature: \"fn test_fn() -> i32\".to_string(),\n            strength: InvariantStrength::Empirical { paths_checked: 5 },\n        }];\n\n        let action = AgentAction::ChangeSignature {\n            name: \"test_fn\".to_string(),\n            old_sig: \"fn test_fn() -> i32\".to_string(),\n            new_sig: \"fn test_fn() -> String\".to_string(),\n            file: PathBuf::from(\"src/test.rs\"),\n        };\n\n        let result = validate_action(&action, &constraints);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_validate_allowed_action() {\n        let constraints = vec![RefactorConstraint::NoMove {\n            target: \"other_fn\".to_string(),\n            reason: \"layer fixed\".to_string(),\n            strength: InvariantStrength::Proven,\n        }];\n\n        let action = AgentAction::MoveFunction {\n            name: \"test_fn\".to_string(), // Different function\n            from: PathBuf::from(\"src/000_test.rs\"),\n            to: PathBuf::from(\"src/010_test.rs\"),\n        };\n\n        let result = validate_action(&action, &constraints);\n        assert!(result.is_ok());\n    }\n\n    #[test]\n    fn test_check_move_allowed() {\n        let constraints = vec![RefactorConstraint::NoMove {\n            target: \"test_fn\".to_string(),\n            reason: \"critical function\".to_string(),\n            strength: InvariantStrength::Proven,\n        }];\n\n        let result = check_move_allowed(\n            \"test_fn\",\n            &PathBuf::from(\"a.rs\"),\n            &PathBuf::from(\"b.rs\"),\n            &constraints,\n        );\n\n        assert!(result.is_err());\n        assert!(result.unwrap_err().contains(\"critical function\"));\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/085_agent_conscience.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/200_agent_conscience.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/085_agent_conscience.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/200_agent_conscience.rs",
          "original_content": "//! Agent Conscience\n//!\n//! Conscience-driven agent architecture: Validates actions against invariants.\n//! Philosophy: An agent with \"conscience\" mechanically enforces invariants.\n//! No interpretation, no values - just constraint checking.\n\nuse crate::action_validator::{validate_action, AgentAction, ConstraintViolation};\nuse crate::invariant_types::{Invariant, InvariantStrength};\nuse crate::refactor_constraints::{from_invariant, RefactorConstraint};\nuse serde::{Deserialize, Serialize};\nuse std::path::{Path, PathBuf};\n\n/// Action permission result\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ActionPermission {\n    pub allowed: bool,\n    pub action: AgentAction,\n    pub violations: Vec<ConstraintViolation>,\n    pub warnings: Vec<String>,\n}\n\n/// Agent conscience: Validates actions against invariants\npub struct AgentConscience {\n    invariants: Vec<Invariant>,\n    constraints: Vec<RefactorConstraint>,\n}\n\nimpl AgentConscience {\n    /// Create conscience from detected invariants\n    pub fn new(invariants: Vec<Invariant>) -> Self {\n        let constraints = invariants\n            .iter()\n            .filter_map(from_invariant)\n            .collect();\n\n        Self {\n            invariants,\n            constraints,\n        }\n    }\n\n    /// Load conscience from JSON file\n    #[allow(dead_code)]\n    pub fn load(path: &Path) -> Result<Self, Box<dyn std::error::Error>> {\n        let json = std::fs::read_to_string(path)?;\n\n        #[derive(Deserialize)]\n        struct ConscienceExport {\n            invariants: Vec<Invariant>,\n        }\n\n        let export: ConscienceExport = serde_json::from_str(&json)?;\n        Ok(Self::new(export.invariants))\n    }\n\n    /// Check if action is morally permissible (preserves invariants)\n    ///\n    /// This is the humility gate: H(A, I) =  iff I_k  I : violates(A, I_k)\n    pub fn check_action(&self, action: &AgentAction) -> ActionPermission {\n        match validate_action(action, &self.constraints) {\n            Ok(_) => ActionPermission {\n                allowed: true,\n                action: action.clone(),\n                violations: Vec::new(),\n                warnings: self.generate_warnings(action),\n            },\n            Err(violations) => ActionPermission {\n                allowed: false,\n                action: action.clone(),\n                violations,\n                warnings: Vec::new(),\n            },\n        }\n    }\n\n    /// Generate warnings for non-blocking concerns\n    fn generate_warnings(&self, action: &AgentAction) -> Vec<String> {\n        let mut warnings = Vec::new();\n\n        // Warn about heuristic invariants\n        for inv in &self.invariants {\n            if matches!(inv.strength, InvariantStrength::Heuristic) {\n                if let Some(warning) = self.check_heuristic_warning(action, inv) {\n                    warnings.push(warning);\n                }\n            }\n        }\n\n        warnings\n    }\n\n    /// Check if heuristic invariant suggests caution\n    fn check_heuristic_warning(&self, action: &AgentAction, inv: &Invariant) -> Option<String> {\n        use crate::invariant_types::{InvariantKind, SemanticInvariant};\n\n        match (&action, &inv.kind) {\n            (\n                AgentAction::MoveFunction { name, .. },\n                InvariantKind::Semantic(SemanticInvariant::PureFunction),\n            ) if inv.target == *name => {\n                Some(format!(\n                    \"Warning: {} may be pure function (HEURISTIC - verify manually)\",\n                    name\n                ))\n            }\n            (\n                AgentAction::ChangeSignature { name, .. },\n                InvariantKind::Semantic(SemanticInvariant::Idempotent),\n            ) if inv.target == *name => {\n                Some(format!(\n                    \"Warning: {} may be idempotent (HEURISTIC - verify manually)\",\n                    name\n                ))\n            }\n            _ => None,\n        }\n    }\n\n    /// Query: \"What can I safely do to this function?\"\n    pub fn query_allowed_actions(&self, function: &str) -> Vec<AgentAction> {\n        let mut allowed = Vec::new();\n\n        // Test rename (usually safe if no signature change)\n        let rename_action = AgentAction::RenameFunction {\n            old_name: function.to_string(),\n            new_name: format!(\"{}_renamed\", function),\n            file: PathBuf::from(\"test.rs\"),\n        };\n\n        if self.check_action(&rename_action).allowed {\n            allowed.push(rename_action);\n        }\n\n        // Test creation (always safe)\n        allowed.push(AgentAction::CreateFunction {\n            name: format!(\"new_{}\", function),\n            file: PathBuf::from(\"test.rs\"),\n            signature: \"fn new() -> ()\".to_string(),\n        });\n\n        allowed\n    }\n\n    /// Export conscience as JSON (for agents to load)\n    #[allow(dead_code)]\n    pub fn export_json(&self, path: &Path) -> std::io::Result<()> {\n        #[derive(Serialize)]\n        struct ConscienceExport {\n            invariants: Vec<Invariant>,\n            constraints: Vec<RefactorConstraint>,\n            meta: ConscMeta,\n        }\n\n        #[derive(Serialize)]\n        struct ConscMeta {\n            total_invariants: usize,\n            blocking_invariants: usize,\n            total_constraints: usize,\n        }\n\n        let export = ConscienceExport {\n            invariants: self.invariants.clone(),\n            constraints: self.constraints.clone(),\n            meta: ConscMeta {\n                total_invariants: self.invariants.len(),\n                blocking_invariants: self.invariants.iter().filter(|i| i.is_blocking()).count(),\n                total_constraints: self.constraints.len(),\n            },\n        };\n\n        let json = serde_json::to_string_pretty(&export)?;\n        std::fs::write(path, json)?;\n        Ok(())\n    }\n\n    /// Get statistics about conscience state\n    #[allow(dead_code)]\n    pub fn stats(&self) -> ConscienceStats {\n        let blocking = self.invariants.iter().filter(|i| i.is_blocking()).count();\n        let proven = self\n            .invariants\n            .iter()\n            .filter(|i| matches!(i.strength, InvariantStrength::Proven))\n            .count();\n        let empirical = self\n            .invariants\n            .iter()\n            .filter(|i| matches!(i.strength, InvariantStrength::Empirical { .. }))\n            .count();\n        let heuristic = self\n            .invariants\n            .iter()\n            .filter(|i| matches!(i.strength, InvariantStrength::Heuristic))\n            .count();\n\n        ConscienceStats {\n            total_invariants: self.invariants.len(),\n            blocking_invariants: blocking,\n            total_constraints: self.constraints.len(),\n            proven_count: proven,\n            empirical_count: empirical,\n            heuristic_count: heuristic,\n        }\n    }\n}\n\n#[derive(Debug, Serialize, Deserialize)]\n#[allow(dead_code)]\npub struct ConscienceStats {\n    pub total_invariants: usize,\n    pub blocking_invariants: usize,\n    pub total_constraints: usize,\n    pub proven_count: usize,\n    pub empirical_count: usize,\n    pub heuristic_count: usize,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::invariant_types::{InvariantKind, StructuralInvariant};\n\n    fn make_test_invariant(name: &str, layer: usize, strength: InvariantStrength) -> Invariant {\n        Invariant::new(\n            name.to_string(),\n            format!(\"src/{:03}_test.rs\", layer * 10),\n            InvariantKind::Structural(StructuralInvariant::LayerFixed { layer }),\n            strength,\n            format!(\"Layer {} fixed\", layer),\n        )\n    }\n\n    #[test]\n    fn test_conscience_blocks_invalid_move() {\n        let inv = make_test_invariant(\"test_fn\", 0, InvariantStrength::Proven);\n        let conscience = AgentConscience::new(vec![inv]);\n\n        let action = AgentAction::MoveFunction {\n            name: \"test_fn\".to_string(),\n            from: PathBuf::from(\"src/000_test.rs\"),\n            to: PathBuf::from(\"src/010_test.rs\"),\n        };\n\n        let result = conscience.check_action(&action);\n        assert!(!result.allowed);\n        assert!(!result.violations.is_empty());\n    }\n\n    #[test]\n    fn test_conscience_allows_valid_action() {\n        let inv = make_test_invariant(\"other_fn\", 0, InvariantStrength::Proven);\n        let conscience = AgentConscience::new(vec![inv]);\n\n        let action = AgentAction::MoveFunction {\n            name: \"test_fn\".to_string(), // Different function\n            from: PathBuf::from(\"src/000_test.rs\"),\n            to: PathBuf::from(\"src/010_test.rs\"),\n        };\n\n        let result = conscience.check_action(&action);\n        assert!(result.allowed);\n        assert!(result.violations.is_empty());\n    }\n\n    #[test]\n    fn test_conscience_stats() {\n        let invariants = vec![\n            make_test_invariant(\"fn1\", 0, InvariantStrength::Proven),\n            make_test_invariant(\"fn2\", 1, InvariantStrength::Empirical { paths_checked: 3 }),\n            make_test_invariant(\"fn3\", 2, InvariantStrength::Heuristic),\n        ];\n\n        let conscience = AgentConscience::new(invariants);\n        let stats = conscience.stats();\n\n        assert_eq!(stats.total_invariants, 3);\n        assert_eq!(stats.proven_count, 1);\n        assert_eq!(stats.empirical_count, 1);\n        assert_eq!(stats.heuristic_count, 1);\n    }\n\n    #[test]\n    fn test_query_allowed_actions() {\n        let inv = make_test_invariant(\"test_fn\", 0, InvariantStrength::Proven);\n        let conscience = AgentConscience::new(vec![inv]);\n\n        let allowed = conscience.query_allowed_actions(\"test_fn\");\n        assert!(!allowed.is_empty()); // Should return at least some safe actions\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/090_utilities.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/210_utilities.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/090_utilities.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/210_utilities.rs",
          "original_content": "//! Utility functions shared across modules\n\nuse std::collections::{BTreeSet, HashMap};\nuse std::path::{Path, PathBuf};\nuse crate::types::{DirectoryAnalysis, FunctionPlacement, PlacementStatus};\nuse crate::report::{PlanItem, Priority, ActionKind, ClusterPlan};\n\n/// Compress absolute paths to MMSB-relative format\npub fn compress_path(path: &str) -> String {\n    // Find MMSB in the path and return everything from there\n    if let Some(idx) = path.find(\"/MMSB/\") {\n        return format!(\"MMSB{}\", &path[idx + 5..]);\n    }\n    // If already starts with MMSB/, return as-is\n    if path.starts_with(\"MMSB/\") {\n        return path.to_string();\n    }\n    // Fallback: try to find src/ or other common markers\n    if let Some(idx) = path.rfind(\"/src/\") {\n        return format!(\"MMSB/src{}\", &path[idx + 4..]);\n    }\n    // Last resort: return original\n    path.to_string()\n}\n\n// Layer helpers live in 070_layer_utilities.rs.\n\npub fn collect_directory_files(directory: &DirectoryAnalysis, out: &mut Vec<PathBuf>) {\n    out.extend(directory.files.iter().cloned());\n    for sub in &directory.subdirectories {\n        collect_directory_files(sub, out);\n    }\n}\n\npub fn path_common_prefix_len(a: &Path, b: &Path) -> isize {\n    let mut count = 0isize;\n    for (a_comp, b_comp) in a.components().zip(b.components()) {\n        if a_comp == b_comp {\n            count += 1;\n        } else {\n            break;\n        }\n    }\n    count\n}\n\npub fn resolve_required_layer_path(\n    required_layer: &str,\n    current_file: &Path,\n    directory: &DirectoryAnalysis,\n    root_path: &Path,\n) -> PathBuf {\n    let mut files = Vec::new();\n    collect_directory_files(directory, &mut files);\n    let candidates = files\n        .into_iter()\n        .filter(|path| {\n            path.file_name()\n                .and_then(|name| name.to_str())\n                .map(|name| name == required_layer)\n                .unwrap_or(false)\n        })\n        .collect::<Vec<_>>();\n    if candidates.is_empty() {\n        return current_file\n            .parent()\n            .unwrap_or(root_path)\n            .join(required_layer);\n    }\n\n    let current_dir = current_file.parent().unwrap_or(root_path);\n    let mut best = None;\n    let mut best_score = -1isize;\n    for candidate in candidates {\n        let candidate_dir = candidate.parent().unwrap_or(root_path);\n        let score = path_common_prefix_len(current_dir, candidate_dir);\n        let length = candidate.components().count() as isize;\n        let combined = score * 1000 - length;\n        if combined > best_score {\n            best_score = combined;\n            best = Some(candidate);\n        }\n    }\n    best.unwrap_or_else(|| {\n        current_file\n            .parent()\n            .unwrap_or(root_path)\n            .join(required_layer)\n    })\n}\n\npub fn compute_move_metrics(\n    placement: &FunctionPlacement,\n) -> (usize, usize, usize, usize, Vec<PathBuf>, Vec<PathBuf>) {\n    let incoming_calls = placement\n        .call_analysis\n        .calls_from_other_files\n        .iter()\n        .map(|(_, count)| *count)\n        .sum::<usize>();\n    let callers = placement.call_analysis.calls_from_other_files.len();\n    let mut touched = BTreeSet::new();\n    touched.insert(placement.current_file.clone());\n    let mut outgoing_files = Vec::new();\n    for (path, _) in &placement.call_analysis.inter_file_calls {\n        touched.insert(path.clone());\n        outgoing_files.push(path.clone());\n    }\n    let mut caller_files = Vec::new();\n    for (path, _) in &placement.call_analysis.calls_from_other_files {\n        touched.insert(path.clone());\n        caller_files.push(path.clone());\n    }\n    let cost = touched.len().max(1);\n    let benefit = 1 + callers;\n    (incoming_calls, benefit, cost, callers, caller_files, outgoing_files)\n}\n\npub fn collect_move_items(\n    placements: &[FunctionPlacement],\n    utility_names: &BTreeSet<String>,\n    directory: &DirectoryAnalysis,\n    root_path: &Path,\n) -> Vec<PlanItem> {\n    let mut items = Vec::new();\n    for placement in placements {\n        match &placement.placement_status {\n            PlacementStatus::ShouldMove { reason, impact } => {\n                let priority = if *impact >= 0.5 {\n                    Priority::Critical\n                } else if *impact >= 0.2 {\n                    Priority::High\n                } else if *impact >= 0.1 {\n                    Priority::Medium\n                } else {\n                    Priority::Low\n                };\n                let (impact_weight, benefit, cost, callers, caller_files, outgoing_files) =\n                    compute_move_metrics(placement);\n                let to = placement\n                    .suggested_file\n                    .as_ref()\n                    .map(|p| compress_path(p.to_string_lossy().as_ref()))\n                    .unwrap_or_else(|| \"-\".to_string());\n                items.push(PlanItem {\n                    kind: ActionKind::Cohesion,\n                    priority,\n                    description: format!(\n                        \"`{}` from `{}` to `{}`: {} (impact {:.2})\",\n                        placement.name,\n                        compress_path(placement.current_file.to_string_lossy().as_ref()),\n                        to,\n                        reason,\n                        impact\n                    ),\n                    command: String::new(),\n                    current_layer: None,\n                    required_layer: None,\n                    is_utility: utility_names.contains(&placement.name),\n                    impact_weight,\n                    benefit,\n                    cost,\n                    callers,\n                    caller_files,\n                    current_file: Some(placement.current_file.clone()),\n                    target_file: placement.suggested_file.clone(),\n                    outgoing_files,\n                    name: Some(placement.name.clone()),\n                    cluster_cohesion: 0.0,\n                    member_count: 0,\n                });\n            }\n            PlacementStatus::LayerViolation {\n                current_layer,\n                required_layer,\n            } => {\n                let target_path = resolve_required_layer_path(\n                    required_layer,\n                    &placement.current_file,\n                    directory,\n                    root_path,\n                );\n                let to = compress_path(target_path.to_string_lossy().as_ref());\n                let (impact_weight, benefit, cost, callers, caller_files, outgoing_files) =\n                    compute_move_metrics(placement);\n                items.push(PlanItem {\n                    kind: ActionKind::Structural,\n                    priority: Priority::Critical,\n                    description: format!(\n                        \"`{}` from `{}` to `{}`: layer violation {} -> {}\",\n                        placement.name,\n                        compress_path(placement.current_file.to_string_lossy().as_ref()),\n                        to,\n                        current_layer,\n                        required_layer\n                    ),\n                    command: String::new(),\n                    current_layer: Some(current_layer.clone()),\n                    required_layer: Some(required_layer.clone()),\n                    is_utility: utility_names.contains(&placement.name),\n                    impact_weight,\n                    benefit,\n                    cost,\n                    callers,\n                    caller_files,\n                    current_file: Some(placement.current_file.clone()),\n                    target_file: Some(target_path),\n                    outgoing_files,\n                    name: Some(placement.name.clone()),\n                    cluster_cohesion: 0.0,\n                    member_count: 0,\n                });\n            }\n            _ => {}\n        }\n    }\n    items\n}\n\npub fn write_structural_batches(content: &mut String, items: &[PlanItem]) {\n    if items.is_empty() {\n        return;\n    }\n\n    let mut ordered_targets = Vec::new();\n    let mut batches: HashMap<PathBuf, Vec<&PlanItem>> = HashMap::new();\n    for item in items {\n        let Some(target) = &item.target_file else {\n            continue;\n        };\n        let entry = batches.entry(target.clone()).or_default();\n        if entry.is_empty() {\n            ordered_targets.push(target.clone());\n        }\n        entry.push(item);\n    }\n\n    content.push_str(\"### Phase 3 Batches\\n\\n\");\n    content.push_str(\"Action: execute batches in order and verify after each batch.\\n\");\n    content.push_str(\"Note: each batch targets one destination module.\\n\\n\");\n    for (idx, target) in ordered_targets.iter().enumerate() {\n        let empty: Vec<&PlanItem> = Vec::new();\n        let items = batches.get(target).unwrap_or(&empty);\n        content.push_str(&format!(\n            \"#### Batch {}: target `{}`\\n\\n\",\n            idx + 1,\n            compress_path(target.to_string_lossy().as_ref())\n        ));\n        content.push_str(\"Action: move the listed functions into the target module.\\n\");\n        content.push_str(\"Note: use the rg commands to locate definitions and callers.\\n\\n\");\n        let mut commands: Vec<String> = Vec::new();\n        if !target.exists() {\n            let target_label = compress_path(target.to_string_lossy().as_ref());\n            content.push_str(&format!(\n                \"- Create target file: `{}`\\n\",\n                target_label\n            ));\n            commands.push(format!(\"touch \\\"{}\\\"\", target.to_string_lossy()));\n        }\n        for item in items {\n            let name = item.name.as_deref().unwrap_or(\"function\");\n            let current = item\n                .current_file\n                .as_ref()\n                .map(|p| compress_path(p.to_string_lossy().as_ref()))\n                .unwrap_or_else(|| \"-\".to_string());\n            let ratio = if item.cost == 0 {\n                0.0\n            } else {\n                item.benefit as f64 / item.cost as f64\n            };\n            let caller_hint = if item.callers == 0 {\n                \"no external callers\".to_string()\n            } else {\n                format!(\"update {} caller files\", item.callers)\n            };\n            content.push_str(&format!(\n                \"- Move `{}` from `{}` (impact {}, benefit/cost {:.2}, touches {} files; {})\\n\",\n                name,\n                current,\n                item.impact_weight,\n                ratio,\n                item.cost,\n                caller_hint\n            ));\n            if let Some(current_file) = &item.current_file {\n                commands.push(format!(\n                    \"rg -n \\\"{}\\\" \\\"{}\\\"\",\n                    name,\n                    current_file.to_string_lossy()\n                ));\n            }\n            let mut callers = item.caller_files.clone();\n            callers.sort();\n            callers.dedup();\n            if !callers.is_empty() {\n                content.push_str(\"- Update imports in:\\n\");\n                for caller in callers {\n                    content.push_str(&format!(\n                        \"  - `{}`\\n\",\n                        compress_path(caller.to_string_lossy().as_ref())\n                    ));\n                    commands.push(format!(\n                        \"rg -n \\\"{}\\\" \\\"{}\\\"\",\n                        name,\n                        caller.to_string_lossy()\n                    ));\n                }\n            }\n        }\n        content.push_str(\"- Verification gate: `cargo test`\\n\");\n        if !commands.is_empty() {\n            content.push_str(\"\\n```bash\\n\");\n            for command in commands {\n                content.push_str(&format!(\"{}\\n\", command));\n            }\n            content.push_str(\"```\\n\");\n        }\n        content.push('\\n');\n    }\n}\n\npub fn write_cluster_batches(content: &mut String, plans: &[ClusterPlan], root_path: &Path) {\n    if plans.is_empty() {\n        return;\n    }\n    content.push_str(\"### Phase 2 Batches\\n\\n\");\n    content.push_str(\"Action: execute batches in order and verify after each batch.\\n\");\n    content.push_str(\"Note: each batch creates or fills a cluster file.\\n\\n\");\n    for (idx, plan) in plans.iter().enumerate() {\n        content.push_str(&format!(\n            \"#### Batch {}: target `{}`\\n\\n\",\n            idx + 1,\n            compress_path(plan.target.to_string_lossy().as_ref())\n        ));\n        content.push_str(\"Action: move the listed functions into the target module.\\n\");\n        content.push_str(\"Note: use the rg commands to locate definitions and callers.\\n\\n\");\n        let mut commands = Vec::new();\n        if !plan.target.exists() {\n            content.push_str(&format!(\n                \"- Create target file: `{}`\\n\",\n                compress_path(plan.target.to_string_lossy().as_ref())\n            ));\n            commands.push(format!(\"touch \\\"{}\\\"\", plan.target.to_string_lossy()));\n        }\n        content.push_str(&format!(\n            \"- Cluster cohesion {:.2}, {} functions\\n\",\n            plan.cohesion,\n            plan.members.len()\n        ));\n        for member in &plan.members {\n            let file = compress_path(member.file.to_string_lossy().as_ref());\n            content.push_str(&format!(\"- Move `{}` from `{}`\\n\", member.name, file));\n            commands.push(format!(\n                \"rg -n \\\"{}\\\" \\\"{}\\\"\",\n                member.name,\n                member.file.to_string_lossy()\n            ));\n            commands.push(format!(\n                \"rg -n \\\"{}\\\" \\\"{}\\\"\",\n                member.name,\n                root_path.to_string_lossy()\n            ));\n        }\n        content.push_str(\"- Verification gate: `cargo test`\\n\");\n        if !commands.is_empty() {\n            content.push_str(\"\\n```bash\\n\");\n            for command in commands {\n                content.push_str(&format!(\"{}\\n\", command));\n            }\n            content.push_str(\"```\\n\");\n        }\n        content.push('\\n');\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/100_types.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/220_types.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/100_types.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/220_types.rs",
          "original_content": "//! Type definitions for code analysis\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::path::PathBuf;\n\n// Import from sibling modules\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::refactor_constraints::RefactorConstraint;\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum ElementType {\n    Function,\n    Struct,\n    Enum,\n    Trait,\n    Impl,\n    Module,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum Language {\n    Rust,\n    Julia,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CodeElement {\n    pub name: String,\n    pub file_path: String,\n    pub line_number: usize,\n    pub element_type: ElementType,\n    pub signature: String,\n    pub visibility: Visibility,\n    pub generic_params: Vec<String>,\n    pub language: Language,\n    pub layer: String,\n    pub calls: Vec<String>,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum Visibility {\n    Public,\n    Crate,\n    Private,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ModuleInfo {\n    pub name: String,\n    pub file_path: String,\n    pub imports: Vec<String>,\n    pub exports: Vec<String>,\n    pub submodules: Vec<String>,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CallGraphNode {\n    pub function_name: String,\n    pub file_path: String,\n    pub calls: Vec<String>,\n    pub called_by: Vec<String>,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum NodeType {\n    Entry,\n    Exit,\n    BasicBlock,\n    Branch,\n    LoopHeader,\n}\n\n#[derive(Debug, Clone)]\npub struct CfgNode {\n    pub id: usize,\n    pub node_type: NodeType,\n    pub label: String,\n    pub lines: Vec<u32>,  // Source line numbers (empty for Rust currently)\n}\n\n#[derive(Debug, Clone)]\npub struct CfgEdge {\n    pub from: usize,\n    pub to: usize,\n    pub condition: Option<bool>,  // Some(true)=taken/true branch, Some(false)=false/else, None=unconditional\n}\n\n#[derive(Debug, Clone)]\npub struct FunctionCfg {\n    pub function: String,\n    pub file_path: String,\n    pub entry_id: usize,\n    pub exit_id: usize,\n    pub nodes: Vec<CfgNode>,\n    pub edges: Vec<CfgEdge>,\n    pub branch_count: usize,\n    pub loop_count: usize,\n}\n\n#[derive(Debug)]\npub struct ProgramCFG {\n    pub functions: HashMap<String, FunctionCfg>,  // Key: function name (assume unique)\n    pub call_edges: Vec<(String, String)>,  // (caller, callee)\n}\n\n#[derive(Debug)]\npub struct AnalysisResult {\n    pub elements: Vec<CodeElement>,\n    pub modules: Vec<ModuleInfo>,\n    pub call_graph: HashMap<String, CallGraphNode>,\n    pub type_hierarchy: HashMap<String, Vec<String>>,\n    pub cfgs: Vec<FunctionCfg>,\n    pub invariants: InvariantAnalysisResult,\n    pub constraints: Vec<RefactorConstraint>,\n}\n\n#[allow(dead_code)]\n#[derive(Debug, Clone)]\npub struct DirectoryAnalysis {\n    pub path: PathBuf,\n    pub layer: String,\n    pub parent: Option<PathBuf>,\n    pub files: Vec<PathBuf>,\n    pub subdirectories: Vec<DirectoryAnalysis>,\n    pub has_sources: bool,\n}\n\n#[derive(Debug, Clone)]\npub struct FileOrderEntry {\n    pub current_path: PathBuf,\n    pub canonical_order: usize,\n    pub suggested_name: String,\n    pub needs_rename: bool,\n}\n\n#[derive(Debug, Clone)]\npub struct OrderViolation {\n    pub file: PathBuf,\n    pub current_position: usize,\n    pub required_position: usize,\n    pub blocking_dependencies: Vec<PathBuf>,\n}\n\n#[derive(Debug, Clone)]\npub struct FileLayerViolation {\n    pub from: PathBuf,\n    pub to: PathBuf,\n    pub from_layer: String,\n    pub to_layer: String,\n}\n\n#[derive(Debug, Clone)]\npub struct FileOrderingResult {\n    pub ordered_files: Vec<FileOrderEntry>,\n    pub violations: Vec<OrderViolation>,\n    pub layer_violations: Vec<FileLayerViolation>,\n    pub ordered_directories: Vec<PathBuf>,\n    pub cycles: Vec<Vec<PathBuf>>,\n}\n\n#[derive(Debug, Clone)]\npub struct CallAnalysis {\n    pub intra_file_calls: usize,\n    pub inter_file_calls: Vec<(PathBuf, usize)>,\n    pub calls_from_same_file: usize,\n    pub calls_from_other_files: Vec<(PathBuf, usize)>,\n    pub same_file_type_refs: usize,\n    pub other_file_type_refs: usize,\n}\n\n#[derive(Debug, Clone)]\npub enum PlacementStatus {\n    Correct,\n    ShouldMove { reason: String, impact: f64 },\n    Orphaned { suggested_module: String },\n    LayerViolation { current_layer: String, required_layer: String },\n}\n\n#[derive(Debug, Clone)]\npub struct FunctionPlacement {\n    pub name: String,\n    pub signature: String,\n    pub current_file: PathBuf,\n    pub suggested_file: Option<PathBuf>,\n    pub placement_status: PlacementStatus,\n    pub cohesion_score: f64,\n    pub call_analysis: CallAnalysis,\n}\n\n#[derive(Debug, Clone)]\npub struct FunctionCluster {\n    pub members: Vec<String>,\n    pub cohesion: f64,\n    pub suggested_file: Option<PathBuf>,\n}\n\nimpl AnalysisResult {\n    pub fn new() -> Self {\n        Self {\n            elements: Vec::new(),\n            modules: Vec::new(),\n            call_graph: HashMap::new(),\n            type_hierarchy: HashMap::new(),\n            cfgs: Vec::new(),\n            invariants: InvariantAnalysisResult::new(),\n            constraints: Vec::new(),\n        }\n    }\n\n    pub fn add_element(&mut self, element: CodeElement) {\n        self.elements.push(element);\n    }\n\n    pub fn add_cfg(&mut self, cfg: FunctionCfg) {\n        self.cfgs.push(cfg);\n    }\n\n    pub fn merge(&mut self, other: AnalysisResult) {\n        self.elements.extend(other.elements);\n        self.modules.extend(other.modules);\n        self.call_graph.extend(other.call_graph);\n        for (key, mut values) in other.type_hierarchy {\n            self.type_hierarchy\n                .entry(key)\n                .or_insert_with(Vec::new)\n                .append(&mut values);\n        }\n        self.cfgs.extend(other.cfgs);\n\n        // Merge invariants\n        self.invariants.invariants.extend(other.invariants.invariants);\n        self.invariants.violations.extend(other.invariants.violations);\n        self.invariants.layer_assignments.extend(other.invariants.layer_assignments);\n\n        // Merge constraints\n        self.constraints.extend(other.constraints);\n    }\n}\n\n#[derive(Debug, Deserialize)]\npub struct JuliaElement {\n    pub element_type: String,\n    pub name: String,\n    pub file_path: String,\n    pub line_number: usize,\n    pub signature: String,\n    pub calls: Vec<String>,\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/110_cohesion_analyzer.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/230_cohesion_analyzer.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/110_cohesion_analyzer.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/230_cohesion_analyzer.rs",
          "original_content": "//! Function cohesion analysis and placement suggestions.\n\nuse crate::cluster_006::compute_cohesion_score;\nuse crate::cluster_008::{detect_layer_violation, FunctionInfo};\nuse crate::types::{CallAnalysis, FunctionCluster, FunctionPlacement, PlacementStatus};\nuse crate::types::{AnalysisResult, ElementType};\nuse anyhow::Result;\nuse std::collections::{BTreeMap, HashMap, HashSet};\nuse std::path::PathBuf;\n\nconst DEFAULT_THRESHOLD: f64 = 0.6;\n\n#[derive(Default)]\npub struct FunctionCohesionAnalyzer {\n    threshold: f64,\n}\n\nimpl FunctionCohesionAnalyzer {\n    pub fn new() -> Self {\n        Self {\n            threshold: DEFAULT_THRESHOLD,\n        }\n    }\n\n    #[allow(dead_code)]\n    pub fn with_threshold(threshold: f64) -> Self {\n        Self { threshold }\n    }\n\n    pub fn analyze(&self, result: &AnalysisResult) -> Result<Vec<FunctionPlacement>> {\n        let (functions, outgoing_counts, incoming_counts) = build_call_edges(result);\n        let file_layers = build_function_layers(&functions);\n        let (file_types, all_types) = build_type_maps(result);\n\n        let mut placements = Vec::new();\n        for (idx, func) in functions.iter().enumerate() {\n            let call_analysis = build_call_analysis(\n                func,\n                &functions,\n                &outgoing_counts[idx],\n                &incoming_counts[idx],\n                &file_types,\n                &all_types,\n            );\n            let cohesion_score = compute_cohesion_score(\n                func,\n                &functions,\n                &outgoing_counts[idx],\n                &file_layers,\n                &call_analysis,\n            );\n            let layer_violation =\n                detect_layer_violation(func, &functions, &outgoing_counts[idx], &file_layers);\n            let placement_status =\n                determine_status(&call_analysis, cohesion_score, self.threshold, layer_violation);\n            let suggested_file = match &placement_status {\n                PlacementStatus::ShouldMove { .. } => suggest_file(&call_analysis),\n                PlacementStatus::Orphaned { .. } => None,\n                PlacementStatus::LayerViolation { .. } => None,\n                PlacementStatus::Correct => None,\n            };\n\n            placements.push(FunctionPlacement {\n                name: func.name.clone(),\n                signature: func.signature.clone(),\n                current_file: PathBuf::from(&func.file_path),\n                suggested_file,\n                placement_status,\n                cohesion_score,\n                call_analysis,\n            });\n        }\n\n        Ok(placements)\n    }\n\n    pub fn detect_clusters(&self, result: &AnalysisResult) -> Result<Vec<FunctionCluster>> {\n        let (functions, outgoing_counts, _) = build_call_edges(result);\n        let assignments = louvain_communities(&outgoing_counts);\n        let mut clusters_map: HashMap<usize, Vec<usize>> = HashMap::new();\n        for (idx, comm) in assignments.into_iter().enumerate() {\n            clusters_map.entry(comm).or_default().push(idx);\n        }\n\n        let mut clusters = Vec::new();\n        for component in clusters_map.values() {\n            if component.len() < 3 {\n                continue;\n            }\n            let cohesion = compute_cluster_cohesion(component, &outgoing_counts);\n            let suggested_file = suggest_cluster_file(component, &functions);\n            let members = component\n                .iter()\n                .map(|idx| {\n                    format!(\"{}::{}\", functions[*idx].file_path, functions[*idx].name)\n                })\n                .collect();\n            clusters.push(FunctionCluster {\n                members,\n                cohesion,\n                suggested_file,\n            });\n        }\n\n        Ok(clusters)\n    }\n}\n\nfn collect_functions(result: &AnalysisResult) -> Vec<FunctionInfo> {\n    result\n        .elements\n        .iter()\n        .filter(|elem| matches!(elem.element_type, ElementType::Function))\n        .map(|elem| FunctionInfo {\n            name: elem.name.clone(),\n            signature: elem.signature.clone(),\n            file_path: elem.file_path.clone(),\n            layer: elem.layer.clone(),\n            calls: elem.calls.clone(),\n        })\n        .collect()\n}\n\nfn build_call_edges(\n    result: &AnalysisResult,\n) -> (\n    Vec<FunctionInfo>,\n    Vec<HashMap<usize, usize>>,\n    Vec<HashMap<usize, usize>>,\n) {\n    let functions = collect_functions(result);\n    let name_map = build_name_map(&functions);\n\n    let mut outgoing_counts: Vec<HashMap<usize, usize>> = vec![HashMap::new(); functions.len()];\n    let mut incoming_counts: Vec<HashMap<usize, usize>> = vec![HashMap::new(); functions.len()];\n\n    for (idx, func) in functions.iter().enumerate() {\n        for call in &func.calls {\n            if let Some(callee_idxs) = name_map.get(call) {\n                for &callee_idx in callee_idxs {\n                    *outgoing_counts[idx].entry(callee_idx).or_insert(0) += 1;\n                    *incoming_counts[callee_idx].entry(idx).or_insert(0) += 1;\n                }\n            }\n        }\n    }\n\n    (functions, outgoing_counts, incoming_counts)\n}\n\nfn build_function_layers(functions: &[FunctionInfo]) -> HashMap<String, String> {\n    let mut map = HashMap::new();\n    for func in functions {\n        map.entry(func.file_path.clone())\n            .or_insert_with(|| func.layer.clone());\n    }\n    map\n}\n\nfn build_type_maps(\n    result: &AnalysisResult,\n) -> (HashMap<String, HashSet<String>>, HashSet<String>) {\n    let mut file_types: HashMap<String, HashSet<String>> = HashMap::new();\n    let mut all_types: HashSet<String> = HashSet::new();\n\n    for elem in &result.elements {\n        if matches!(elem.element_type, ElementType::Struct | ElementType::Enum | ElementType::Trait)\n        {\n            let name = elem.name.clone();\n            all_types.insert(name.clone());\n            file_types\n                .entry(elem.file_path.clone())\n                .or_default()\n                .insert(name);\n        }\n    }\n\n    (file_types, all_types)\n}\n\nfn build_name_map(functions: &[FunctionInfo]) -> HashMap<String, Vec<usize>> {\n    let mut map: HashMap<String, Vec<usize>> = HashMap::new();\n    for (idx, func) in functions.iter().enumerate() {\n        map.entry(func.name.clone()).or_default().push(idx);\n    }\n    map\n}\n\nfn build_call_analysis(\n    func: &FunctionInfo,\n    functions: &[FunctionInfo],\n    outgoing: &HashMap<usize, usize>,\n    incoming: &HashMap<usize, usize>,\n    file_types: &HashMap<String, HashSet<String>>,\n    all_types: &HashSet<String>,\n) -> CallAnalysis {\n    let mut intra_file_calls = 0usize;\n    let mut inter_file_calls: BTreeMap<PathBuf, usize> = BTreeMap::new();\n\n    for (callee_idx, count) in outgoing {\n        let callee = &functions[*callee_idx];\n        if callee.file_path == func.file_path {\n            intra_file_calls += count;\n        } else {\n            *inter_file_calls\n                .entry(PathBuf::from(&callee.file_path))\n                .or_insert(0) += count;\n        }\n    }\n\n    let mut calls_from_same_file = 0usize;\n    let mut calls_from_other_files: BTreeMap<PathBuf, usize> = BTreeMap::new();\n    for (caller_idx, count) in incoming {\n        let caller = &functions[*caller_idx];\n        if caller.file_path == func.file_path {\n            calls_from_same_file += count;\n        } else {\n            *calls_from_other_files\n                .entry(PathBuf::from(&caller.file_path))\n                .or_insert(0) += count;\n        }\n    }\n\n    let (same_file_type_refs, other_file_type_refs) =\n        compute_type_coupling(func, file_types, all_types);\n\n    CallAnalysis {\n        intra_file_calls,\n        inter_file_calls: inter_file_calls.into_iter().collect(),\n        calls_from_same_file,\n        calls_from_other_files: calls_from_other_files.into_iter().collect(),\n        same_file_type_refs,\n        other_file_type_refs,\n    }\n}\n\nfn determine_status(\n    call_analysis: &CallAnalysis,\n    cohesion_score: f64,\n    threshold: f64,\n    layer_violation: Option<(String, String)>,\n) -> PlacementStatus {\n    let total_activity =\n        call_analysis.intra_file_calls + call_analysis.calls_from_same_file;\n    let external_activity: usize = call_analysis\n        .calls_from_other_files\n        .iter()\n        .map(|(_, count)| *count)\n        .sum();\n\n    if total_activity == 0 && external_activity == 0 {\n        return PlacementStatus::Orphaned {\n            suggested_module: \"utilities\".to_string(),\n        };\n    }\n\n    if let Some((current_layer, required_layer)) = layer_violation {\n        return PlacementStatus::LayerViolation {\n            current_layer,\n            required_layer,\n        };\n    }\n\n    if cohesion_score >= threshold {\n        return PlacementStatus::Correct;\n    }\n\n    let mut impact = threshold - cohesion_score;\n    if impact < 0.0 {\n        impact = 0.0;\n    }\n\n    PlacementStatus::ShouldMove {\n        reason: format!(\n            \"cohesion {:.2} below threshold {:.2}\",\n            cohesion_score, threshold\n        ),\n        impact,\n    }\n}\n\nfn suggest_file(call_analysis: &CallAnalysis) -> Option<PathBuf> {\n    let mut best_file: Option<PathBuf> = None;\n    let mut best_score = 0usize;\n\n    for (file, count) in &call_analysis.calls_from_other_files {\n        if *count > best_score {\n            best_score = *count;\n            best_file = Some(file.clone());\n        }\n    }\n\n    if best_score == 0 {\n        return None;\n    }\n\n    if let Some(candidate) = &best_file {\n        let outgoing_to_candidate = call_analysis\n            .inter_file_calls\n            .iter()\n            .find(|(path, _)| path == candidate)\n            .map(|(_, count)| *count)\n            .unwrap_or(0);\n        if outgoing_to_candidate == 0 && call_analysis.intra_file_calls >= best_score {\n            return None;\n        }\n    }\n\n    best_file\n}\n\nfn compute_cluster_cohesion(\n    members: &[usize],\n    outgoing_counts: &[HashMap<usize, usize>],\n) -> f64 {\n    let member_set: HashSet<usize> = members.iter().copied().collect();\n    let mut internal = 0usize;\n    let mut total = 0usize;\n\n    for idx in members {\n        if let Some(outgoing) = outgoing_counts.get(*idx) {\n            for (target, count) in outgoing {\n                total += count;\n                if member_set.contains(target) {\n                    internal += count;\n                }\n            }\n        }\n    }\n\n    if total == 0 {\n        0.0\n    } else {\n        internal as f64 / total as f64\n    }\n}\n\nfn suggest_cluster_file(\n    members: &[usize],\n    functions: &[FunctionInfo],\n) -> Option<PathBuf> {\n    let mut counts: HashMap<&str, usize> = HashMap::new();\n    for idx in members {\n        let path = functions[*idx].file_path.as_str();\n        *counts.entry(path).or_insert(0) += 1;\n    }\n    counts\n        .into_iter()\n        .max_by_key(|(_, count)| *count)\n        .map(|(path, _)| PathBuf::from(path))\n}\n\nfn compute_type_coupling(\n    func: &FunctionInfo,\n    file_types: &HashMap<String, HashSet<String>>,\n    all_types: &HashSet<String>,\n) -> (usize, usize) {\n    let mut same_file = 0usize;\n    let mut other_file = 0usize;\n    let tokens = extract_identifiers(&func.signature);\n    let same_set = file_types.get(&func.file_path);\n\n    for token in tokens {\n        if let Some(types) = same_set {\n            if types.contains(&token) {\n                same_file += 1;\n                continue;\n            }\n        }\n        if all_types.contains(&token) {\n            other_file += 1;\n        }\n    }\n\n    (same_file, other_file)\n}\n\nfn extract_identifiers(text: &str) -> Vec<String> {\n    let mut tokens = Vec::new();\n    let mut current = String::new();\n    for ch in text.chars() {\n        if ch.is_ascii_alphanumeric() || ch == '_' {\n            current.push(ch);\n        } else if !current.is_empty() {\n            tokens.push(current.clone());\n            current.clear();\n        }\n    }\n    if !current.is_empty() {\n        tokens.push(current);\n    }\n    tokens\n}\n\nfn louvain_communities(outgoing_counts: &[HashMap<usize, usize>]) -> Vec<usize> {\n    let n = outgoing_counts.len();\n    if n == 0 {\n        return Vec::new();\n    }\n\n    let (neighbors, degrees, total_weight) = build_undirected_graph(outgoing_counts);\n    if total_weight == 0 {\n        return (0..n).collect();\n    }\n\n    let two_m = (2 * total_weight) as f64;\n    let mut community: Vec<usize> = (0..n).collect();\n    let mut sum_tot = degrees.clone();\n\n    let max_iters = 25;\n    for iter in 0..max_iters {\n        let mut moved = false;\n        for node in 0..n {\n            let current = community[node];\n            let mut neighbor_comms: HashMap<usize, usize> = HashMap::new();\n            for (neighbor, weight) in &neighbors[node] {\n                let comm = community[*neighbor];\n                *neighbor_comms.entry(comm).or_insert(0) += *weight;\n            }\n\n            let mut best_comm = current;\n            let mut best_gain = 0.0f64;\n            for (comm, k_i_in) in neighbor_comms {\n                if comm == current {\n                    continue;\n                }\n                let gain = (k_i_in as f64\n                    - (degrees[node] as f64 * sum_tot[comm] as f64) / two_m)\n                    / two_m;\n                if gain > best_gain {\n                    best_gain = gain;\n                    best_comm = comm;\n                }\n            }\n\n            if best_comm != current && best_gain > 0.0 {\n                community[node] = best_comm;\n                sum_tot[current] = sum_tot[current].saturating_sub(degrees[node]);\n                sum_tot[best_comm] += degrees[node];\n                moved = true;\n            }\n        }\n        if !moved {\n            break;\n        }\n        if iter % 5 == 0 {\n            println!(\"  Louvain pass {}...\", iter + 1);\n        }\n    }\n\n    community\n}\n\nfn build_undirected_graph(\n    outgoing_counts: &[HashMap<usize, usize>],\n) -> (Vec<Vec<(usize, usize)>>, Vec<usize>, usize) {\n    let n = outgoing_counts.len();\n    let mut edge_weights: HashMap<(usize, usize), usize> = HashMap::new();\n\n    for (src, outgoing) in outgoing_counts.iter().enumerate() {\n        for (dst, weight) in outgoing {\n            let (a, b) = if src <= *dst { (src, *dst) } else { (*dst, src) };\n            *edge_weights.entry((a, b)).or_insert(0) += *weight;\n        }\n    }\n\n    let mut neighbors = vec![Vec::new(); n];\n    let mut degrees = vec![0usize; n];\n    let mut total_weight = 0usize;\n\n    for ((a, b), weight) in edge_weights {\n        if a == b {\n            continue;\n        }\n        neighbors[a].push((b, weight));\n        neighbors[b].push((a, weight));\n        degrees[a] += weight;\n        degrees[b] += weight;\n        total_weight += weight;\n    }\n\n    (neighbors, degrees, total_weight)\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/120_directory_analyzer.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/240_directory_analyzer.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/120_directory_analyzer.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/240_directory_analyzer.rs",
          "original_content": "//! Directory structure analysis for source-aware organization.\n\nuse crate::dependency::detect_layer;\nuse crate::layer_utilities::allow_analysis_dir;\nuse crate::types::DirectoryAnalysis;\nuse anyhow::Result;\nuse std::fs;\nuse std::path::{Path, PathBuf};\n\npub struct DirectoryAnalyzer {\n    root: PathBuf,\n}\n\nimpl DirectoryAnalyzer {\n    pub fn new(root: PathBuf) -> Self {\n        Self { root }\n    }\n\n    pub fn analyze(&self) -> Result<DirectoryAnalysis> {\n        self.analyze_directory(&self.root, None)\n    }\n\n    fn analyze_directory(&self, path: &Path, parent: Option<PathBuf>) -> Result<DirectoryAnalysis> {\n        let mut files = Vec::new();\n        let mut subdirectories = Vec::new();\n\n        let mut entries: Vec<PathBuf> = fs::read_dir(path)?\n            .filter_map(|entry| entry.ok().map(|e| e.path()))\n            .collect();\n        entries.sort();\n\n        for entry_path in entries {\n            if entry_path.is_dir() && !allow_analysis_dir(&self.root, &entry_path) {\n                continue;\n            }\n            if should_skip_path(&entry_path) {\n                continue;\n            }\n            if entry_path.is_dir() {\n                let child = self.analyze_directory(&entry_path, Some(path.to_path_buf()))?;\n                if child.has_sources || !child.subdirectories.is_empty() {\n                    subdirectories.push(child);\n                }\n            } else if is_source_file(&entry_path) {\n                files.push(entry_path.clone());\n            }\n        }\n\n        let has_sources = !files.is_empty();\n        Ok(DirectoryAnalysis {\n            path: path.to_path_buf(),\n            layer: detect_layer(path),\n            parent,\n            files,\n            subdirectories,\n            has_sources,\n        })\n    }\n}\n\nfn is_source_file(path: &Path) -> bool {\n    matches!(path.extension().and_then(|e| e.to_str()), Some(\"rs\") | Some(\"jl\"))\n}\n\nfn should_skip_path(path: &Path) -> bool {\n    let Some(name) = path.file_name() else {\n        return false;\n    };\n    name == \"target\"\n        || name == \".git\"\n        || name == \"tools\"\n        || name == \"examples\"\n        || name == \"docs\"\n        || name == \"xtask\"\n        || name == \".julia\"\n        || name == \"test\"\n        || name == \"tests\"\n        || name == \"benches\"\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/130_control_flow.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/250_control_flow.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/130_control_flow.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/250_control_flow.rs",
          "original_content": "//! Control flow and call graph analysis\n\nuse crate::types::*;\nuse crate::utilities::compress_path;\nuse petgraph::dot::Dot;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse std::collections::HashMap;\n\npub struct ControlFlowAnalyzer {\n    graph: DiGraph<String, String>,\n    node_map: HashMap<String, NodeIndex>,\n    cfgs: Vec<FunctionCfg>,\n}\n\nimpl ControlFlowAnalyzer {\n    pub fn new() -> Self {\n        Self {\n            graph: DiGraph::new(),\n            node_map: HashMap::new(),\n            cfgs: Vec::new(),\n        }\n    }\n\n    pub fn build_call_graph(&mut self, result: &AnalysisResult) {\n        self.graph = DiGraph::new();\n        self.node_map.clear();\n        self.cfgs = result.cfgs.clone();\n\n        // First pass: create nodes for all functions\n        for elem in &result.elements {\n            if matches!(elem.element_type, ElementType::Function) {\n                let node_name = format!(\"{}::{}\", compress_path(&elem.file_path), elem.name);\n                let node_idx = self.graph.add_node(node_name.clone());\n                self.node_map.insert(node_name, node_idx);\n            }\n        }\n\n        let total_funcs = result\n            .elements\n            .iter()\n            .filter(|elem| matches!(elem.element_type, ElementType::Function))\n            .count();\n        let mut processed = 0usize;\n        let mut last_report = 0usize;\n\n        // Second pass: create edges for function calls\n        for elem in &result.elements {\n            if matches!(elem.element_type, ElementType::Function) {\n                processed += 1;\n                if processed % 50 == 0 || processed == total_funcs {\n                    if processed != last_report {\n                        println!(\n                            \"  Call graph progress: {}/{} functions\",\n                            processed, total_funcs\n                        );\n                        last_report = processed;\n                    }\n                }\n                let caller_name = format!(\"{}::{}\", compress_path(&elem.file_path), elem.name);\n\n                if let Some(&caller_idx) = self.node_map.get(&caller_name) {\n                    for called in &elem.calls {\n                        // Try to find the called function\n                        for target_elem in &result.elements {\n                            if matches!(target_elem.element_type, ElementType::Function)\n                                && (target_elem.name == *called\n                                    || called.ends_with(&target_elem.name))\n                            {\n                                let callee_name = format!(\n                                    \"{}::{}\",\n                                    compress_path(&target_elem.file_path),\n                                    target_elem.name\n                                );\n                                if let Some(&callee_idx) = self.node_map.get(&callee_name) {\n                                    self.graph.add_edge(caller_idx, callee_idx, called.clone());\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    #[allow(dead_code)]\n    pub fn generate_dot(&self) -> String {\n        format!(\"{:?}\", Dot::new(&self.graph))\n    }\n\n    pub fn generate_mermaid(&self) -> String {\n        let mut output = String::from(\"```mermaid\\ngraph TD\\n\");\n\n        for node_idx in self.graph.node_indices() {\n            let node_name = &self.graph[node_idx];\n            let safe_name = sanitize_identifier(node_name);\n            output.push_str(&format!(\"    {}[\\\"{}\\\"]\\n\", safe_name, node_name));\n        }\n\n        for edge in self.graph.edge_indices() {\n            if let Some((source, target)) = self.graph.edge_endpoints(edge) {\n                let source_name = &self.graph[source];\n                let target_name = &self.graph[target];\n                let safe_source = sanitize_identifier(source_name);\n                let safe_target = sanitize_identifier(target_name);\n                output.push_str(&format!(\"    {} --> {}\\n\", safe_source, safe_target));\n            }\n        }\n\n        output.push_str(\"```\\n\");\n        output\n    }\n\n    pub fn call_edges(&self) -> Vec<(String, String)> {\n        let mut edges = Vec::new();\n        for edge_idx in self.graph.edge_indices() {\n            if let Some((source, target)) = self.graph.edge_endpoints(edge_idx) {\n                edges.push((self.graph[source].clone(), self.graph[target].clone()));\n            }\n        }\n        edges\n    }\n\n    pub fn cfgs(&self) -> &[FunctionCfg] {\n        &self.cfgs\n    }\n\n    pub fn get_statistics(&self) -> CallGraphStats {\n        let node_count = self.graph.node_count();\n        let edge_count = self.graph.edge_count();\n\n        let mut max_depth = 0;\n        let mut leaf_functions = 0;\n\n        for node_idx in self.graph.node_indices() {\n            let outgoing = self.graph.edges(node_idx).count();\n            if outgoing == 0 {\n                leaf_functions += 1;\n            }\n\n            // Simple depth calculation (could be improved with proper traversal)\n            let depth = self.calculate_depth(node_idx);\n            if depth > max_depth {\n                max_depth = depth;\n            }\n        }\n\n        CallGraphStats {\n            total_functions: node_count,\n            total_calls: edge_count,\n            max_depth,\n            leaf_functions,\n        }\n    }\n\n    fn calculate_depth(&self, start: NodeIndex) -> usize {\n        let mut visited = std::collections::HashSet::new();\n        self.dfs_depth(start, &mut visited)\n    }\n\n    fn dfs_depth(\n        &self,\n        node: NodeIndex,\n        visited: &mut std::collections::HashSet<NodeIndex>,\n    ) -> usize {\n        if visited.contains(&node) {\n            return 0;\n        }\n        visited.insert(node);\n\n        let mut max = 0;\n        for neighbor in self.graph.neighbors(node) {\n            let depth = self.dfs_depth(neighbor, visited);\n            if depth > max {\n                max = depth;\n            }\n        }\n\n        visited.remove(&node);\n        max + 1\n    }\n}\n\npub struct CallGraphStats {\n    pub total_functions: usize,\n    pub total_calls: usize,\n    pub max_depth: usize,\n    pub leaf_functions: usize,\n}\n\nfn sanitize_identifier(name: &str) -> String {\n    name.chars()\n        .map(|c| if c.is_ascii_alphanumeric() { c } else { '_' })\n        .collect()\n}\n\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/140_file_ordering.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/260_file_ordering.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/140_file_ordering.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/260_file_ordering.rs",
          "original_content": "//! File-level dependency ordering using a DAG.\n\nuse anyhow::Result;\nuse petgraph::graph::{DiGraph, NodeIndex};\nuse petgraph::visit::EdgeRef;\nuse rayon::prelude::*;\nuse std::collections::{HashMap, HashSet};\nuse std::fs;\nuse std::path::PathBuf;\nuse std::time::SystemTime;\n\nuse crate::dependency::build_module_map;\nuse crate::cluster_010::extract_dependencies;\n\npub use crate::cluster_001::{ordered_by_name, topological_sort};\npub use crate::cluster_010::build_dependency_map;\n\n#[derive(Clone, Debug)]\npub struct DirectoryMove {\n    pub from: PathBuf,\n    pub to: PathBuf,\n}\n\n#[allow(dead_code)]\npub struct DagCache {\n    graph: DiGraph<PathBuf, ()>,\n    node_map: HashMap<PathBuf, NodeIndex>,\n    topo_order: Vec<NodeIndex>,\n    last_modified: HashMap<PathBuf, SystemTime>,\n    file_set: HashSet<PathBuf>,\n    module_map: HashMap<String, PathBuf>,\n}\n\n#[allow(dead_code)]\nimpl DagCache {\n    pub fn new(files: &[PathBuf]) -> Result<Self> {\n        let file_set: HashSet<PathBuf> = files.iter().cloned().collect();\n        let module_map = build_module_map(files);\n        let dep_map = build_dependency_map(files, &file_set, &module_map)?;\n        let (graph, node_map) = build_file_dag(files, &dep_map);\n        let topo_order = topological_sort(&graph).unwrap_or_else(|_| {\n            graph\n                .node_indices()\n                .collect::<Vec<_>>()\n        });\n\n        let mut last_modified = HashMap::new();\n        for file in files {\n            if let Ok(meta) = fs::metadata(file) {\n                if let Ok(modified) = meta.modified() {\n                    last_modified.insert(file.clone(), modified);\n                }\n            }\n        }\n\n        Ok(Self {\n            graph,\n            node_map,\n            topo_order,\n            last_modified,\n            file_set,\n            module_map,\n        })\n    }\n\n    pub fn incremental_update(&mut self, changed_files: &[PathBuf]) -> Result<()> {\n        for file in changed_files {\n            if !self.file_set.contains(file) {\n                continue;\n            }\n            let Some(&node) = self.node_map.get(file) else {\n                continue;\n            };\n            let old_edges: Vec<_> = self.graph.edges(node).map(|e| e.id()).collect();\n            for edge in old_edges {\n                self.graph.remove_edge(edge);\n            }\n\n            let deps = extract_dependencies(file, &self.file_set, &self.module_map)?;\n            for dep in deps {\n                if let Some(&dep_node) = self.node_map.get(&dep) {\n                    self.graph.add_edge(dep_node, node, ());\n                }\n            }\n\n            if let Ok(meta) = fs::metadata(file) {\n                if let Ok(modified) = meta.modified() {\n                    self.last_modified.insert(file.clone(), modified);\n                }\n            }\n        }\n\n        if let Ok(order) = topological_sort(&self.graph) {\n            self.topo_order = order;\n        }\n        Ok(())\n    }\n\n    pub fn topo_files(&self) -> Vec<PathBuf> {\n        self.topo_order\n            .iter()\n            .map(|idx| self.graph[*idx].clone())\n            .collect()\n    }\n}\n\n#[allow(dead_code)]\npub fn parallel_build_file_dag(directories: &[PathBuf]) -> Result<DiGraph<PathBuf, ()>> {\n    let subgraphs: Vec<DiGraph<PathBuf, ()>> = directories\n        .par_iter()\n        .map(|dir| crate::dependency::build_directory_dag(dir))\n        .collect::<Result<_>>()?;\n\n    let mut merged = DiGraph::new();\n    let mut node_map: HashMap<PathBuf, NodeIndex> = HashMap::new();\n\n    for subgraph in subgraphs {\n        for node in subgraph.node_indices() {\n            let file = subgraph[node].clone();\n            node_map.entry(file.clone()).or_insert_with(|| merged.add_node(file));\n        }\n        for edge in subgraph.edge_indices() {\n            if let Some((src, dst)) = subgraph.edge_endpoints(edge) {\n                let src_file = subgraph[src].clone();\n                let dst_file = subgraph[dst].clone();\n                let src_node = *node_map.get(&src_file).expect(\"missing source node\");\n                let dst_node = *node_map.get(&dst_file).expect(\"missing target node\");\n                merged.add_edge(src_node, dst_node, ());\n            }\n        }\n    }\n\n    Ok(merged)\n}\n\n// Re-exported from other modules for file ordering utilities\npub(crate) use crate::cluster_001::build_entries;\npub(crate) use crate::cluster_011::build_file_dag;\npub(crate) use crate::cluster_001::detect_cycles;\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/150_julia_parser.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/270_julia_parser.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/150_julia_parser.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/270_julia_parser.rs",
          "original_content": "//! Julia file analyzer via Julia script when available, falling back to a built-in parser.\n\nuse crate::types::*;\nuse anyhow::{anyhow, Context, Result};\nuse once_cell::sync::Lazy;\nuse regex::Regex;\nuse std::env;\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse std::process::{Command, Stdio};\nuse std::sync::atomic::{AtomicBool, Ordering};\n\nstatic INLINE_FN_RE: Lazy<Regex> =\n    Lazy::new(|| Regex::new(r\"^\\s*([A-Za-z_][A-Za-z0-9_!.]*)\\s*(\\([^=]*\\))\\s*=\\s*(.+)$\").unwrap());\nstatic CALL_RE: Lazy<Regex> = Lazy::new(|| Regex::new(r\"([A-Za-z_][A-Za-z0-9_!.]*)\\s*\\(\").unwrap());\n\npub struct JuliaAnalyzer {\n    script_path: PathBuf,\n    root_path: PathBuf,\n    project_dir: PathBuf,\n    dot_root: PathBuf,\n    julia_bin: PathBuf,\n    script_disabled: AtomicBool,\n    global_cfg_generated: AtomicBool,\n}\n\nimpl JuliaAnalyzer {\n    pub fn new(root_path: PathBuf, script_path: PathBuf, dot_root: PathBuf) -> Self {\n        let _base_dir = script_path\n            .parent()\n            .map(Path::to_path_buf)\n            .unwrap_or_else(|| PathBuf::from(\".\"));\n\n        let julia_bin = resolve_julia_binary();\n        let project_dir = find_julia_project_dir(&script_path);\n        Self {\n            root_path,\n            script_path,\n            project_dir,\n            dot_root,\n            julia_bin,\n            script_disabled: AtomicBool::new(false),\n            global_cfg_generated: AtomicBool::new(false),\n        }\n    }\n\n    #[allow(dead_code)]\n    pub fn generate_global_cfgs(&self) -> Result<()> {\n        self.ensure_global_cfgs()\n    }\n\n    pub fn analyze_file(&self, file_path: &Path) -> Result<AnalysisResult> {\n        println!(\"    [Julia] Analyzing {:?}\", file_path);\n\n        if !self.script_disabled.load(Ordering::Relaxed) {\n            self.ensure_global_cfgs()?;\n\n            match self.run_script(file_path) {\n                Ok(result) => return Ok(result),\n                Err(err) => {\n                    eprintln!(\n                        \"Warning: Julia script execution failed ({err}). Falling back to internal parser for the remaining files.\"\n                    );\n                    self.script_disabled.store(true, Ordering::Relaxed);\n                }\n            }\n        }\n\n        self.fallback_parse(file_path)\n    }\n\n    fn ensure_global_cfgs(&self) -> Result<()> {\n        if self.global_cfg_generated.load(Ordering::Relaxed) {\n            return Ok(());\n        }\n\n        let status = Command::new(&self.julia_bin)\n            .arg(\"--startup-file=no\")\n            .arg(&self.script_path)\n            .arg(\"--global-cfgs\")\n            .arg(&self.root_path)\n            .arg(&self.dot_root)\n            .env(\"JULIA_PROJECT\", &self.project_dir)\n            .stderr(Stdio::inherit())\n            .stdout(Stdio::inherit())\n            .status()\n            .context(\"Failed to execute Julia analyzer for global CFG generation\")?;\n\n        if !status.success() {\n            return Err(anyhow!(\n                \"Julia analyzer exited with {} while generating global CFGs\",\n                status\n            ));\n        }\n\n        self.global_cfg_generated.store(true, Ordering::Relaxed);\n        Ok(())\n    }\n\n    fn run_script(&self, file_path: &Path) -> Result<AnalysisResult> {\n        let dot_dir = self.compute_dot_dir(file_path);\n        fs::create_dir_all(&dot_dir)?;\n        let output = Command::new(&self.julia_bin)\n            .arg(\"--startup-file=no\")\n            .arg(&self.script_path)\n            .arg(file_path)\n            .arg(&dot_dir)\n            .arg(&self.root_path)\n            .env(\"JULIA_PROJECT\", &self.project_dir)\n            .stderr(Stdio::inherit())\n            .output()\n            .with_context(|| format!(\"Failed to execute Julia analyzer on {:?}\", file_path))?;\n\n        if !output.status.success() {\n            return Err(anyhow!(\n                \"Julia analyzer exited with {}: {}\",\n                output.status,\n                \"check stderr output above\"\n            ));\n        }\n\n        let stdout = String::from_utf8_lossy(&output.stdout);\n        let json_line = stdout\n            .lines()\n            .find(|line| line.trim_start().starts_with('[') || line.trim_start().starts_with('{'))\n            .unwrap_or(\"\");\n\n        let julia_elements: Vec<JuliaElement> = serde_json::from_str(&stdout)\n            .or_else(|_| serde_json::from_str(json_line))\n            .with_context(|| {\n                format!(\n                    \"Failed to parse Julia analyzer output. Raw output: {}\",\n                    stdout\n                )\n            })?;\n\n        let mut result = AnalysisResult::new();\n        let layer = self.extract_layer(file_path);\n\n        for elem in julia_elements {\n            let element_type = match elem.element_type.as_str() {\n                \"struct\" => ElementType::Struct,\n                \"function\" => ElementType::Function,\n                \"module\" => ElementType::Module,\n                _ => continue,\n            };\n\n            let file_path = relativize_path(Path::new(&elem.file_path), &self.root_path);\n\n            result.add_element(CodeElement {\n                element_type,\n                name: elem.name,\n                file_path,\n                line_number: elem.line_number,\n                language: Language::Julia,\n                layer: layer.clone(),\n                signature: elem.signature,\n                calls: elem.calls,\n                visibility: Visibility::Public,\n                generic_params: Vec::new(),\n            });\n        }\n\n        Ok(result)\n    }\n\n    fn fallback_parse(&self, file_path: &Path) -> Result<AnalysisResult> {\n        let content = fs::read_to_string(file_path)\n            .with_context(|| format!(\"Failed to read Julia file {:?}\", file_path))?;\n        let mut result = AnalysisResult::new();\n        let layer = self.extract_layer(file_path);\n        let file_path_str = relativize_path(file_path, &self.root_path);\n        let lines: Vec<&str> = content.lines().collect();\n        let mut idx = 0;\n\n        while idx < lines.len() {\n            let line = lines[idx];\n            let trimmed = line.trim();\n            if trimmed.is_empty() || trimmed.starts_with('#') {\n                idx += 1;\n                continue;\n            }\n\n            if let Some(module_name) = parse_module_name(trimmed) {\n                result.add_element(CodeElement {\n                    element_type: ElementType::Module,\n                    name: module_name,\n                    file_path: file_path_str.clone(),\n                    line_number: idx + 1,\n                    language: Language::Julia,\n                    layer: layer.clone(),\n                    signature: trimmed.to_string(),\n                    calls: Vec::new(),\n                    visibility: Visibility::Public,\n                    generic_params: Vec::new(),\n                });\n                idx += 1;\n                continue;\n            }\n\n            if let Some(struct_name) = parse_struct_name(trimmed) {\n                result.add_element(CodeElement {\n                    element_type: ElementType::Struct,\n                    name: struct_name,\n                    file_path: file_path_str.clone(),\n                    line_number: idx + 1,\n                    language: Language::Julia,\n                    layer: layer.clone(),\n                    signature: trimmed.to_string(),\n                    calls: Vec::new(),\n                    visibility: Visibility::Public,\n                    generic_params: Vec::new(),\n                });\n                idx += 1;\n                continue;\n            }\n\n            if trimmed.starts_with(\"function \") {\n                let (consumed, maybe_element) =\n                    self.parse_function_block(&file_path_str, &layer, &lines, idx);\n                if let Some(element) = maybe_element {\n                    result.add_element(element);\n                }\n                idx += consumed.max(1);\n                continue;\n            }\n\n            if let Some(element) =\n                self.parse_inline_function(&file_path_str, &layer, lines[idx], idx + 1)\n            {\n                result.add_element(element);\n            }\n\n            idx += 1;\n        }\n\n        Ok(result)\n    }\n\n    fn parse_function_block(\n        &self,\n        file_path: &str,\n        layer: &str,\n        lines: &[&str],\n        start_idx: usize,\n    ) -> (usize, Option<CodeElement>) {\n        let mut header = lines[start_idx].trim().to_string();\n        let mut consumed = 1usize;\n        while paren_balance(&header) > 0 && start_idx + consumed < lines.len() {\n            header.push(' ');\n            header.push_str(lines[start_idx + consumed].trim());\n            consumed += 1;\n        }\n\n        let mut inline_body = None;\n        if let Some(eq_idx) = header.find('=') {\n            inline_body = Some(header[eq_idx + 1..].trim().to_string());\n            header = header[..eq_idx].trim().to_string();\n        }\n\n        let mut body_lines = Vec::new();\n        let mut total_consumed = consumed;\n        if inline_body.is_none() {\n            let mut depth = 1i32;\n            let mut idx = start_idx + consumed;\n            while idx < lines.len() {\n                let current = lines[idx];\n                let trimmed = current.trim();\n                if trimmed.starts_with(\"function \")\n                    || trimmed.starts_with(\"let\")\n                    || trimmed.starts_with(\"if \")\n                    || trimmed == \"if\"\n                    || trimmed.starts_with(\"for \")\n                    || trimmed.starts_with(\"while \")\n                    || trimmed.starts_with(\"begin\")\n                    || trimmed.starts_with(\"try\")\n                    || trimmed.starts_with(\"module \")\n                    || trimmed.starts_with(\"baremodule \")\n                    || trimmed.starts_with(\"struct \")\n                    || trimmed.starts_with(\"mutable struct \")\n                    || trimmed.starts_with(\"quote\")\n                {\n                    depth += 1;\n                }\n\n                if trimmed == \"end\" || trimmed.starts_with(\"end \") {\n                    depth -= 1;\n                    idx += 1;\n                    if depth == 0 {\n                        break;\n                    } else {\n                        continue;\n                    }\n                }\n\n                body_lines.push(current.to_string());\n                idx += 1;\n            }\n            total_consumed = idx - start_idx;\n        }\n\n        let signature = header\n            .trim()\n            .trim_start_matches(\"function\")\n            .trim()\n            .to_string();\n        if signature.is_empty() {\n            return (total_consumed, None);\n        }\n\n        let func_name = signature\n            .split(|c: char| c == '(' || c.is_whitespace())\n            .next()\n            .unwrap_or(\"anonymous\")\n            .to_string();\n\n        let calls = if let Some(inline) = inline_body {\n            extract_calls_from_text(&inline)\n        } else {\n            extract_calls_from_lines(&body_lines)\n        };\n\n        let element = CodeElement {\n            element_type: ElementType::Function,\n            name: func_name,\n            file_path: file_path.to_string(),\n            line_number: start_idx + 1,\n            language: Language::Julia,\n            layer: layer.to_string(),\n            signature,\n            calls,\n            visibility: Visibility::Public,\n            generic_params: Vec::new(),\n        };\n\n        (total_consumed, Some(element))\n    }\n\n    fn parse_inline_function(\n        &self,\n        file_path: &str,\n        layer: &str,\n        line: &str,\n        line_number: usize,\n    ) -> Option<CodeElement> {\n        let trimmed = line.trim();\n        if trimmed.starts_with(\"function \") || trimmed.starts_with('#') {\n            return None;\n        }\n\n        let captures = INLINE_FN_RE.captures(trimmed)?;\n        let func_name = captures.get(1)?.as_str().to_string();\n        let args = captures.get(2).map(|m| m.as_str()).unwrap_or(\"()\");\n        let body = captures\n            .get(3)\n            .map(|m| m.as_str())\n            .unwrap_or(\"\")\n            .to_string();\n        let signature = format!(\"{}{}\", func_name, args);\n        let calls = extract_calls_from_text(&body);\n\n        Some(CodeElement {\n            element_type: ElementType::Function,\n            name: func_name,\n            file_path: file_path.to_string(),\n            line_number,\n            language: Language::Julia,\n            layer: layer.to_string(),\n            signature,\n            calls,\n            visibility: Visibility::Public,\n            generic_params: Vec::new(),\n        })\n    }\n\n    fn extract_layer(&self, path: &Path) -> String {\n        for component in path.components() {\n            if let Some(name) = component.as_os_str().to_str() {\n                if name.chars().next().map_or(false, |c| c.is_ascii_digit()) {\n                    if let Some(pos) = name.find('_') {\n                        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n                            return name.to_string();\n                        }\n                    }\n                }\n            }\n        }\n        \"root\".to_string()\n    }\n\n    fn compute_dot_dir(&self, path: &Path) -> PathBuf {\n        if let Ok(relative) = path.strip_prefix(&self.root_path) {\n            if let Some(parent) = relative.parent() {\n                return self.dot_root.join(parent);\n            }\n            return self.dot_root.clone();\n        }\n\n        let slug = slugify_relative(&self.root_path, path);\n        self.dot_root.join(slug)\n    }\n}\n\nfn slugify_relative(root: &Path, path: &Path) -> String {\n    let relative = path.strip_prefix(root).unwrap_or(path);\n    relative\n        .components()\n        .map(|c| c.as_os_str().to_string_lossy().replace('.', \"_\"))\n        .collect::<Vec<_>>()\n        .join(\"-\")\n}\n\nfn resolve_julia_binary() -> PathBuf {\n    if let Ok(custom) = env::var(\"JULIA_BINARY\") {\n        let candidate = PathBuf::from(custom);\n        if candidate.exists() {\n            return candidate;\n        }\n    }\n    if let Ok(home) = env::var(\"HOME\") {\n        let juliaup_root = Path::new(&home).join(\".julia/juliaup\");\n        if let Ok(entries) = fs::read_dir(&juliaup_root) {\n            for entry in entries.flatten() {\n                let path = entry.path();\n                if !path.is_dir() {\n                    continue;\n                }\n                if let Some(name) = path.file_name().and_then(|n| n.to_str()) {\n                    if name.starts_with(\"julia-\") {\n                        let candidate = path.join(\"bin/julia\");\n                        if candidate.exists() {\n                            return candidate;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    let alt = PathBuf::from(\"/home/cicero-arch-omen/git/julia/usr/bin/julia\");\n    if alt.exists() {\n        return alt;\n    }\n    PathBuf::from(\"julia\")\n}\n\nfn find_julia_project_dir(script_path: &Path) -> PathBuf {\n    let mut current = script_path.parent();\n    while let Some(dir) = current {\n        if dir.join(\"Project.toml\").exists() {\n            return dir.to_path_buf();\n        }\n        current = dir.parent();\n    }\n    script_path\n        .parent()\n        .unwrap_or_else(|| Path::new(\".\"))\n        .to_path_buf()\n}\n\nfn parse_module_name(line: &str) -> Option<String> {\n    if line.starts_with(\"module \") {\n        return line\n            .split_whitespace()\n            .nth(1)\n            .map(|name| name.trim_end_matches(';').to_string());\n    }\n    if line.starts_with(\"baremodule \") {\n        return line\n            .split_whitespace()\n            .nth(1)\n            .map(|name| name.trim_end_matches(';').to_string());\n    }\n    None\n}\n\nfn parse_struct_name(line: &str) -> Option<String> {\n    if line.starts_with(\"mutable struct \") || line.starts_with(\"struct \") {\n        let offset = if line.starts_with(\"mutable struct \") {\n            2\n        } else {\n            1\n        };\n        let tokens: Vec<&str> = line.split_whitespace().collect();\n        return tokens.get(offset).map(|name| {\n            name.split(\"<:\")\n                .next()\n                .unwrap_or(name)\n                .trim_end_matches('{')\n                .to_string()\n        });\n    }\n    None\n}\n\nfn relativize_path(path: &Path, root: &Path) -> String {\n    if let Ok(stripped) = path.strip_prefix(root) {\n        stripped.to_string_lossy().to_string()\n    } else {\n        path.to_string_lossy().to_string()\n    }\n}\n\nfn extract_calls_from_lines(lines: &[String]) -> Vec<String> {\n    let joined = lines.join(\"\\n\");\n    extract_calls_from_text(&joined)\n}\n\nfn extract_calls_from_text(text: &str) -> Vec<String> {\n    let mut calls = Vec::new();\n    for capture in CALL_RE.captures_iter(text) {\n        if let Some(name) = capture.get(1) {\n            let identifier = name.as_str();\n            if is_reserved(identifier) {\n                continue;\n            }\n            calls.push(identifier.to_string());\n        }\n    }\n    calls.sort();\n    calls.dedup();\n    calls\n}\n\nfn is_reserved(name: &str) -> bool {\n    matches!(\n        name,\n        \"if\" | \"for\"\n            | \"while\"\n            | \"begin\"\n            | \"let\"\n            | \"struct\"\n            | \"mutable\"\n            | \"quote\"\n            | \"macro\"\n            | \"module\"\n            | \"end\"\n            | \"baremodule\"\n            | \"function\"\n    )\n}\n\nfn paren_balance(input: &str) -> i32 {\n    let mut balance = 0i32;\n    for ch in input.chars() {\n        if ch == '(' {\n            balance += 1;\n        } else if ch == ')' {\n            balance -= 1;\n        }\n    }\n    balance\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/160_rust_parser.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/280_rust_parser.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/160_rust_parser.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/280_rust_parser.rs",
          "original_content": "use anyhow::{Context, Result};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse syn::visit::Visit;\nuse syn::{ItemEnum, ItemFn, ItemImpl, ItemMod, ItemStruct, ItemTrait, ItemUse};\n\nuse crate::types::{\n    AnalysisResult, CfgEdge, CfgNode, CodeElement, ElementType, FunctionCfg, Language, ModuleInfo,\n    NodeType, Visibility,\n};\n\npub struct RustAnalyzer {\n    root_path: PathBuf,\n}\n\nimpl RustAnalyzer {\n    pub fn new(root_path: String) -> Self {\n        Self {\n            root_path: PathBuf::from(root_path),\n        }\n    }\n\n    pub fn analyze_file(&self, file_path: &Path) -> Result<AnalysisResult> {\n        let content = fs::read_to_string(file_path)\n            .with_context(|| format!(\"Failed to read file: {:?}\", file_path))?;\n\n        let syntax_tree = syn::parse_file(&content)\n            .with_context(|| format!(\"Failed to parse Rust file: {:?}\", file_path))?;\n\n        let mut result = AnalysisResult::new();\n        let layer = self.extract_layer(file_path);\n        let file_path_str = relativize_path(file_path, &self.root_path);\n\n        let mut visitor = RustVisitor {\n            file_path: file_path_str.clone(),\n            layer: layer.clone(),\n            result: &mut result,\n        };\n\n        visitor.visit_file(&syntax_tree);\n\n        Ok(result)\n    }\n\n    fn extract_layer(&self, path: &Path) -> String {\n        for component in path.components() {\n            if let Some(name) = component.as_os_str().to_str() {\n                if name.chars().next().map_or(false, |c| c.is_ascii_digit()) {\n                    if let Some(pos) = name.find('_') {\n                        if name[..pos].chars().all(|c| c.is_ascii_digit()) {\n                            return name.to_string();\n                        }\n                    }\n                }\n            }\n        }\n        \"root\".to_string()\n    }\n}\n\nfn relativize_path(path: &Path, root: &Path) -> String {\n    if let Ok(stripped) = path.strip_prefix(root) {\n        stripped.to_string_lossy().to_string()\n    } else {\n        path.to_string_lossy().to_string()\n    }\n}\n\nstruct RustVisitor<'a> {\n    file_path: String,\n    layer: String,\n    result: &'a mut AnalysisResult,\n}\n\nimpl<'a> RustVisitor<'a> {\n    fn get_visibility(&self, vis: &syn::Visibility) -> Visibility {\n        match vis {\n            syn::Visibility::Public(_) => Visibility::Public,\n            syn::Visibility::Restricted(r) => {\n                if let Some(first) = r.path.segments.first() {\n                    if first.ident == \"crate\" {\n                        return Visibility::Crate;\n                    }\n                }\n                Visibility::Private\n            }\n            syn::Visibility::Inherited => Visibility::Private,\n        }\n    }\n\n    fn extract_function_calls(&self, block: &syn::Block) -> Vec<String> {\n        let mut calls = Vec::new();\n\n        struct CallVisitor<'b> {\n            calls: &'b mut Vec<String>,\n        }\n\n        impl<'b> Visit<'_> for CallVisitor<'b> {\n            fn visit_expr_call(&mut self, node: &syn::ExprCall) {\n                if let syn::Expr::Path(path_expr) = &*node.func {\n                    let name = path_expr\n                        .path\n                        .segments\n                        .iter()\n                        .map(|s| s.ident.to_string())\n                        .collect::<Vec<_>>()\n                        .join(\"::\");\n                    self.calls.push(name);\n                }\n                syn::visit::visit_expr_call(self, node);\n            }\n\n            fn visit_expr_method_call(&mut self, node: &syn::ExprMethodCall) {\n                self.calls.push(node.method.to_string());\n                syn::visit::visit_expr_method_call(self, node);\n            }\n        }\n\n        let mut call_visitor = CallVisitor { calls: &mut calls };\n        call_visitor.visit_block(block);\n\n        calls\n    }\n\n    fn ensure_module_entry(&mut self) -> usize {\n        if let Some(idx) = self\n            .result\n            .modules\n            .iter()\n            .position(|m| m.file_path == self.file_path)\n        {\n            idx\n        } else {\n            let module = ModuleInfo {\n                name: self\n                    .file_path\n                    .split('/')\n                    .last()\n                    .unwrap_or(&self.file_path)\n                    .trim_end_matches(\".rs\")\n                    .to_string(),\n                file_path: self.file_path.clone(),\n                imports: Vec::new(),\n                exports: Vec::new(),\n                submodules: Vec::new(),\n            };\n            self.result.modules.push(module);\n            self.result.modules.len() - 1\n        }\n    }\n}\n\nimpl<'a> Visit<'_> for RustVisitor<'a> {\n    fn visit_item_struct(&mut self, node: &ItemStruct) {\n        let generic_params = node\n            .generics\n            .params\n            .iter()\n            .filter_map(|p| match p {\n                syn::GenericParam::Type(t) => Some(t.ident.to_string()),\n                syn::GenericParam::Lifetime(l) => Some(l.lifetime.to_string()),\n                _ => None,\n            })\n            .collect();\n\n        self.result.add_element(CodeElement {\n            element_type: ElementType::Struct,\n            name: node.ident.to_string(),\n            file_path: self.file_path.clone(),\n            line_number: 0, // syn doesn't provide line numbers easily\n            language: Language::Rust,\n            layer: self.layer.clone(),\n            signature: quote::quote!(#node).to_string(),\n            calls: Vec::new(),\n            visibility: self.get_visibility(&node.vis),\n            generic_params,\n        });\n\n        syn::visit::visit_item_struct(self, node);\n    }\n\n    fn visit_item_enum(&mut self, node: &ItemEnum) {\n        let generic_params = node\n            .generics\n            .params\n            .iter()\n            .filter_map(|p| match p {\n                syn::GenericParam::Type(t) => Some(t.ident.to_string()),\n                _ => None,\n            })\n            .collect();\n\n        self.result.add_element(CodeElement {\n            element_type: ElementType::Enum,\n            name: node.ident.to_string(),\n            file_path: self.file_path.clone(),\n            line_number: 0,\n            language: Language::Rust,\n            layer: self.layer.clone(),\n            signature: format!(\"enum {}\", node.ident),\n            calls: Vec::new(),\n            visibility: self.get_visibility(&node.vis),\n            generic_params,\n        });\n\n        syn::visit::visit_item_enum(self, node);\n    }\n\n    fn visit_item_trait(&mut self, node: &ItemTrait) {\n        self.result.add_element(CodeElement {\n            element_type: ElementType::Trait,\n            name: node.ident.to_string(),\n            file_path: self.file_path.clone(),\n            line_number: 0,\n            language: Language::Rust,\n            layer: self.layer.clone(),\n            signature: format!(\"trait {}\", node.ident),\n            calls: Vec::new(),\n            visibility: self.get_visibility(&node.vis),\n            generic_params: Vec::new(),\n        });\n\n        syn::visit::visit_item_trait(self, node);\n    }\n\n    fn visit_item_impl(&mut self, node: &ItemImpl) {\n        let impl_name = if let Some((_, path, _)) = &node.trait_ {\n            format!(\n                \"{} for {}\",\n                path.segments.last().unwrap().ident,\n                quote::quote!(#node.self_ty)\n            )\n        } else {\n            quote::quote!(#node.self_ty).to_string()\n        };\n\n        self.result.add_element(CodeElement {\n            element_type: ElementType::Impl,\n            name: impl_name,\n            file_path: self.file_path.clone(),\n            line_number: 0,\n            language: Language::Rust,\n            layer: self.layer.clone(),\n            signature: quote::quote!(#node).to_string(),\n            calls: Vec::new(),\n            visibility: Visibility::Private,\n            generic_params: Vec::new(),\n        });\n\n        syn::visit::visit_item_impl(self, node);\n    }\n\n    fn visit_item_fn(&mut self, node: &ItemFn) {\n        let calls = self.extract_function_calls(&node.block);\n\n        let generic_params = node\n            .sig\n            .generics\n            .params\n            .iter()\n            .filter_map(|p| match p {\n                syn::GenericParam::Type(t) => Some(t.ident.to_string()),\n                syn::GenericParam::Lifetime(l) => Some(l.lifetime.to_string()),\n                _ => None,\n            })\n            .collect();\n\n        self.result.add_element(CodeElement {\n            element_type: ElementType::Function,\n            name: node.sig.ident.to_string(),\n            file_path: self.file_path.clone(),\n            line_number: 0,\n            language: Language::Rust,\n            layer: self.layer.clone(),\n            signature: quote::quote!(#node.sig).to_string(),\n            calls,\n            visibility: self.get_visibility(&node.vis),\n            generic_params,\n        });\n\n        let cfg = CfgExtractor::from_function(\n            node.sig.ident.to_string(),\n            self.file_path.clone(),\n            &node.block,\n        );\n        self.result.add_cfg(cfg);\n\n        syn::visit::visit_item_fn(self, node);\n    }\n\n    fn visit_item_mod(&mut self, node: &ItemMod) {\n        self.result.add_element(CodeElement {\n            element_type: ElementType::Module,\n            name: node.ident.to_string(),\n            file_path: self.file_path.clone(),\n            line_number: 0,\n            language: Language::Rust,\n            layer: self.layer.clone(),\n            signature: format!(\"mod {}\", node.ident),\n            calls: Vec::new(),\n            visibility: self.get_visibility(&node.vis),\n            generic_params: Vec::new(),\n        });\n\n        let submodule_name = node.ident.to_string();\n        let idx = self.ensure_module_entry();\n        if !self.result.modules[idx]\n            .submodules\n            .contains(&submodule_name)\n        {\n            self.result.modules[idx].submodules.push(submodule_name);\n        }\n\n        syn::visit::visit_item_mod(self, node);\n    }\n\n    fn visit_item_use(&mut self, node: &ItemUse) {\n        let stmt = quote::quote!(#node).to_string();\n        let idx = self.ensure_module_entry();\n\n        match node.vis {\n            syn::Visibility::Public(_) => self.result.modules[idx].exports.push(stmt),\n            _ => self.result.modules[idx].imports.push(stmt),\n        }\n\n        syn::visit::visit_item_use(self, node);\n    }\n}\n\nstruct CfgExtractor {\n    nodes: Vec<CfgNode>,\n    edges: Vec<CfgEdge>,\n    next_id: usize,\n    branch_count: usize,\n    loop_count: usize,\n}\n\nimpl CfgExtractor {\n    fn from_function(function: String, file_path: String, block: &syn::Block) -> FunctionCfg {\n        let mut extractor = Self {\n            nodes: Vec::new(),\n            edges: Vec::new(),\n            next_id: 0,\n            branch_count: 0,\n            loop_count: 0,\n        };\n\n        let entry_id = extractor.add_node(\"ENTRY\".to_string(), NodeType::Entry, vec![]);\n        let body_exit = extractor.build_block(&block.stmts, entry_id);\n        let exit_id = extractor.add_node(\"EXIT\".to_string(), NodeType::Exit, vec![]);\n        extractor.add_edge(body_exit, exit_id, None);\n\n        FunctionCfg {\n            function,\n            file_path,\n            entry_id,\n            exit_id,\n            nodes: extractor.nodes,\n            edges: extractor.edges,\n            branch_count: extractor.branch_count,\n            loop_count: extractor.loop_count,\n        }\n    }\n\n    fn add_node(&mut self, label: String, node_type: NodeType, lines: Vec<u32>) -> usize {\n        let id = self.next_id;\n        self.next_id += 1;\n        self.nodes.push(CfgNode { id, node_type, label, lines });\n        id\n    }\n\n    fn add_edge(&mut self, from: usize, to: usize, condition: Option<bool>) {\n        self.edges.push(CfgEdge { from, to, condition });\n    }\n\n    fn build_block(&mut self, stmts: &[syn::Stmt], from: usize) -> usize {\n        let mut cursor = from;\n        for stmt in stmts {\n            cursor = self.process_stmt(stmt, cursor);\n        }\n        cursor\n    }\n\n    fn process_stmt(&mut self, stmt: &syn::Stmt, from: usize) -> usize {\n        match stmt {\n            syn::Stmt::Local(local) => {\n                let mut label = format!(\"let {}\", pat_snippet(&local.pat));\n                if let Some(init) = &local.init {\n                    label.push_str(\" = \");\n                    label.push_str(&expr_snippet(&init.expr));\n                    if let Some((_else_token, diverge)) = &init.diverge {\n                        label.push_str(\" else \");\n                        label.push_str(&expr_snippet(diverge));\n                    }\n                }\n                let node_id = self.add_node(label, NodeType::BasicBlock, vec![]);\n                self.add_edge(from, node_id, None);\n                node_id\n            }\n            syn::Stmt::Item(item) => {\n                let label = match item {\n                    syn::Item::Struct(i) => format!(\"struct {}\", i.ident),\n                    syn::Item::Enum(i) => format!(\"enum {}\", i.ident),\n                    syn::Item::Trait(i) => format!(\"trait {}\", i.ident),\n                    syn::Item::Impl(_) => \"impl block\".to_string(),\n                    syn::Item::Use(_) => \"use\".to_string(),\n                    syn::Item::Mod(i) => format!(\"mod {}\", i.ident),\n                    _ => \"item\".to_string(),\n                };\n                let node_id = self.add_node(label, NodeType::BasicBlock, vec![]);\n                self.add_edge(from, node_id, None);\n                node_id\n            }\n            syn::Stmt::Expr(expr, _) => self.process_expr(expr, from),\n            syn::Stmt::Macro(mac) => {\n                let label = format!(\n                    \"macro {}\",\n                    mac.mac\n                        .path\n                        .segments\n                        .last()\n                        .map(|s| s.ident.to_string())\n                        .unwrap_or_else(|| \"?\".into())\n                );\n                let node_id = self.add_node(label, NodeType::BasicBlock, vec![]);\n                self.add_edge(from, node_id, None);\n                node_id\n            }\n        }\n    }\n\n    fn process_expr(&mut self, expr: &syn::Expr, from: usize) -> usize {\n        match expr {\n            syn::Expr::If(expr_if) => {\n                self.branch_count += 1;\n                let cond_label = truncate_label(format!(\"if {}\", expr_snippet(&expr_if.cond)));\n                let branch_id = self.add_node(cond_label, NodeType::Branch, vec![]);\n                self.add_edge(from, branch_id, None);\n\n                // Then branch\n                let then_start = self.add_node(\"THEN BB\".to_string(), NodeType::BasicBlock, vec![]);\n                self.add_edge(branch_id, then_start, Some(true));\n                let then_exit = self.build_block(&expr_if.then_branch.stmts, then_start);\n\n                // Else branch (or direct from branch if no else)\n                let else_exit = if let Some((_, else_branch)) = &expr_if.else_branch {\n                    let else_start = self.add_node(\"ELSE BB\".to_string(), NodeType::BasicBlock, vec![]);\n                    self.add_edge(branch_id, else_start, Some(false));\n                    self.process_expr(else_branch, else_start)\n                } else {\n                    let empty_else = self.add_node(\"EMPTY ELSE\".to_string(), NodeType::BasicBlock, vec![]);\n                    self.add_edge(branch_id, empty_else, Some(false));\n                    empty_else\n                };\n\n                // Join\n                let join_id = self.add_node(\"IF JOIN\".to_string(), NodeType::BasicBlock, vec![]);\n                self.add_edge(then_exit, join_id, None);\n                self.add_edge(else_exit, join_id, None);\n                join_id\n            }\n            syn::Expr::Loop(expr_loop) => {\n                self.loop_count += 1;\n                let loop_label = expr_loop.label.as_ref().map_or(\"LOOP\".to_string(), |l| format!(\"LOOP {}\", l.name.ident));\n                let header_id = self.add_node(loop_label, NodeType::LoopHeader, vec![]);\n                self.add_edge(from, header_id, None);\n\n                let body_start = self.add_node(\"LOOP BB\".to_string(), NodeType::BasicBlock, vec![]);\n                self.add_edge(header_id, body_start, None);\n                let body_exit = self.build_block(&expr_loop.body.stmts, body_start);\n\n                // Back edge\n                self.add_edge(body_exit, header_id, None);\n\n                // Exit after loop\n                let after_id = self.add_node(\"AFTER LOOP\".to_string(), NodeType::BasicBlock, vec![]);\n                self.add_edge(body_exit, after_id, None);  // Break would go here, but simplified\n                after_id\n            }\n            // Add more for while, for, match, etc., as needed\n            _ => {\n                let label = truncate_label(expr_snippet(expr));\n                let node_id = self.add_node(label, NodeType::BasicBlock, vec![]);\n                self.add_edge(from, node_id, None);\n                node_id\n            }\n        }\n    }\n}\n\nfn expr_snippet(expr: &syn::Expr) -> String {\n    truncate_label(quote::quote!(#expr).to_string())\n}\n\nfn pat_snippet(pat: &syn::Pat) -> String {\n    truncate_label(quote::quote!(#pat).to_string())\n}\n\nfn truncate_label(text: String) -> String {\n    let collapsed = text.split_whitespace().collect::<Vec<_>>().join(\" \");\n    let mut label = collapsed;\n    if label.len() > 80 {\n        label.truncate(77);\n        label.push_str(\"...\");\n    }\n    label\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/170_dot_exporter.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/290_dot_exporter.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/170_dot_exporter.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/290_dot_exporter.rs",
          "original_content": "// Re-exported from cluster_001 (moved in Batch 3)\n#[allow(unused_imports)]\npub use crate::cluster_001::export_complete_program_dot;\npub use crate::cluster_011::export_program_cfg_to_path;\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/170_file_gathering.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/300_file_gathering.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/170_file_gathering.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/300_file_gathering.rs",
          "original_content": "//! File gathering module retained for legacy references.\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/180_report.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/310_report.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/180_report.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/310_report.rs",
          "original_content": "//! Markdown report generation\n\nuse crate::cluster_008::collect_cluster_plans;\nuse crate::layer_core::{collect_directory_moves, sort_structural_items};\nuse crate::utilities::{compress_path, collect_move_items, write_structural_batches, write_cluster_batches};\nuse crate::control_flow::ControlFlowAnalyzer;\nuse crate::dependency::{LayerGraph, build_directory_entry_map, build_file_dependency_graph, collect_naming_warnings};\nuse crate::file_ordering::DirectoryMove;\nuse crate::types::*;\nuse anyhow::Result;\nuse std::cmp::Ordering;\nuse std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse walkdir::WalkDir;\nfn display_path(path: &Path, root_path: &Path) -> String {\n    let relative = path.strip_prefix(root_path).unwrap_or(path);\n    relative.to_string_lossy().to_string()\n}\n\nfn placement_status_label(status: &PlacementStatus) -> String {\n    match status {\n        PlacementStatus::Correct => \"ok\".to_string(),\n        PlacementStatus::ShouldMove { .. } => \"move\".to_string(),\n        PlacementStatus::Orphaned { .. } => \"orphaned\".to_string(),\n        PlacementStatus::LayerViolation { .. } => \"layer violation\".to_string(),\n    }\n}\n\nfn placement_status_notes(status: &PlacementStatus) -> String {\n    match status {\n        PlacementStatus::Correct => String::new(),\n        PlacementStatus::ShouldMove { reason, impact } => {\n            format!(\"{} (impact {:.2})\", reason, impact)\n        }\n        PlacementStatus::Orphaned { suggested_module } => {\n            format!(\"suggest module {}\", suggested_module)\n        }\n        PlacementStatus::LayerViolation {\n            current_layer,\n            required_layer,\n        } => format!(\"{} -> {}\", current_layer, required_layer),\n    }\n}\n\n#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd)]\npub enum Priority {\n    Critical,\n    High,\n    Medium,\n    Low,\n}\n\n#[derive(Clone, Copy, Debug, Eq, PartialEq)]\npub enum ActionKind {\n    Cluster,\n    Structural,\n    Cohesion,\n    Ordering,\n}\n\n#[derive(Clone)]\npub struct PlanItem {\n    pub kind: ActionKind,\n    pub priority: Priority,\n    pub description: String,\n    pub command: String,\n    pub current_layer: Option<String>,\n    pub required_layer: Option<String>,\n    pub is_utility: bool,\n    pub impact_weight: usize,\n    pub benefit: usize,\n    pub cost: usize,\n    pub callers: usize,\n    pub caller_files: Vec<PathBuf>,\n    pub current_file: Option<PathBuf>,\n    pub target_file: Option<PathBuf>,\n    pub outgoing_files: Vec<PathBuf>,\n    pub name: Option<String>,\n    pub cluster_cohesion: f64,\n    pub member_count: usize,\n}\n\n#[derive(Clone)]\npub struct ClusterMember {\n    pub file: PathBuf,\n    pub name: String,\n}\n\n#[derive(Clone)]\npub struct ClusterPlan {\n    pub target: PathBuf,\n    pub cohesion: f64,\n    pub members: Vec<ClusterMember>,\n}\n\nfn collect_rename_items(ordering: &FileOrderingResult, label: &str) -> Vec<PlanItem> {\n    let mut layer_violation_files = BTreeSet::new();\n    for violation in &ordering.layer_violations {\n        layer_violation_files.insert(violation.to.clone());\n    }\n\n    ordering\n        .ordered_files\n        .iter()\n        .filter(|entry| entry.needs_rename)\n        .map(|entry| {\n            let from = entry.current_path.clone();\n            let to = entry\n                .current_path\n                .parent()\n                .map(|p| p.join(&entry.suggested_name))\n                .unwrap_or_else(|| PathBuf::from(&entry.suggested_name));\n            let priority = if layer_violation_files.contains(&entry.current_path) {\n                Priority::Critical\n            } else {\n                Priority::Medium\n            };\n            PlanItem {\n                kind: ActionKind::Ordering,\n                priority,\n                description: format!(\n                    \"[{}] `{}` -> `{}`\",\n                    label,\n                    compress_path(from.to_string_lossy().as_ref()),\n                    compress_path(to.to_string_lossy().as_ref())\n                ),\n                command: format!(\n                    \"git mv \\\"{}\\\" \\\"{}\\\"\",\n                    from.to_string_lossy(),\n                    to.to_string_lossy()\n                ),\n                current_layer: None,\n                required_layer: None,\n                is_utility: false,\n                impact_weight: 0,\n                benefit: 0,\n                cost: 1,\n                callers: 0,\n                caller_files: Vec::new(),\n                current_file: Some(from.clone()),\n                target_file: Some(to.clone()),\n                outgoing_files: Vec::new(),\n                name: None,\n                cluster_cohesion: 0.0,\n                member_count: 0,\n            }\n        })\n        .collect()\n}\n\nfn collect_utility_candidates(placements: &[FunctionPlacement]) -> Vec<String> {\n    let mut candidates = BTreeSet::new();\n    for placement in placements {\n        let external_files = placement.call_analysis.calls_from_other_files.len();\n        if external_files >= 3 {\n            candidates.insert(format!(\n                \"`{}` called by {} files (suggest `utilities`)\",\n                placement.name, external_files\n            ));\n        }\n    }\n    candidates.into_iter().collect()\n}\n\nfn directory_moves_to_plan(label: &str, moves: Vec<DirectoryMove>) -> Vec<PlanItem> {\n    moves\n        .into_iter()\n        .map(|item| PlanItem {\n            kind: ActionKind::Ordering,\n            priority: Priority::Medium,\n            description: format!(\n                \"[{}] dir `{}` -> `{}`\",\n                label,\n                compress_path(item.from.to_string_lossy().as_ref()),\n                compress_path(item.to.to_string_lossy().as_ref())\n            ),\n            command: format!(\n                \"git mv \\\"{}\\\" \\\"{}\\\"\",\n                item.from.to_string_lossy(),\n                item.to.to_string_lossy()\n            ),\n            current_layer: None,\n            required_layer: None,\n            is_utility: false,\n            impact_weight: 0,\n            benefit: 0,\n            cost: 1,\n            callers: 0,\n            caller_files: Vec::new(),\n            current_file: Some(item.from.clone()),\n            target_file: Some(item.to.clone()),\n            outgoing_files: Vec::new(),\n            name: None,\n            cluster_cohesion: 0.0,\n            member_count: 0,\n        })\n        .collect()\n}\n\nfn write_priority_section(content: &mut String, title: &str, items: &[PlanItem]) {\n    content.push_str(&format!(\"## {}\\n\\n\", title));\n    let (action, note) = match title {\n        \"Phase 1: Correctness Blockers\" => (\n            \"fix these first; they block correctness or builds.\",\n            \"empty means no critical blockers detected.\",\n        ),\n        \"Phase 2: Cluster Extraction\" => (\n            \"create the listed cluster files and move the grouped functions.\",\n            \"use the batches below to keep changes small.\",\n        ),\n        \"Phase 3: Structural Constraints\" => (\n            \"resolve the layer violations by moving functions to target modules.\",\n            \"follow batch order to avoid cascading dependency churn.\",\n        ),\n        \"Phase 4: Cohesion Improvements\" => (\n            \"optional: improve cohesion by moving functions to better-fit modules.\",\n            \"safe to defer unless you are actively refactoring.\",\n        ),\n        \"Phase 5: Ordering & Renames\" => (\n            \"optional: rename files to match ordering conventions.\",\n            \"update module paths and imports after renames.\",\n        ),\n        _ => (\"review items\", \"no additional guidance available.\"),\n    };\n    content.push_str(&format!(\"Action: {}\\n\", action));\n    content.push_str(&format!(\"Note: {}\\n\\n\", note));\n    if items.is_empty() {\n        content.push_str(\"- None.\\n\\n\");\n        return;\n    }\n\n    let mut commands = Vec::new();\n    for item in items {\n        content.push_str(&format!(\"- {}\\n\", item.description));\n        if !item.command.is_empty() {\n            commands.push(item.command.clone());\n        }\n    }\n    content.push('\\n');\n\n    if !commands.is_empty() {\n        content.push_str(\"```bash\\n\");\n        for cmd in commands {\n            content.push_str(&format!(\"{}\\n\", cmd));\n        }\n        content.push_str(\"```\\n\\n\");\n    }\n}\n\nfn write_structural_tips(content: &mut String, items: &[PlanItem]) {\n    if items.is_empty() {\n        return;\n    }\n    content.push_str(\"### Phase 3 Tips\\n\\n\");\n    content.push_str(\"Action: apply these guidelines while executing Phase 3 batches.\\n\");\n    content.push_str(\"Note: these are advisory, not checklist items.\\n\\n\");\n    content.push_str(\"- Move lowest-layer helpers first; higher layers should depend on stable primitives.\\n\");\n    content.push_str(\"- Keep moves small: move one function + update imports + rerun tests.\\n\");\n    content.push_str(\"- If a target module is missing, create it before moving functions.\\n\");\n    content.push_str(\"- Prefer consolidating shared utilities into their destination layer once.\\n\");\n    content.push_str(\"- Avoid touching `_old/` unless explicitly refactoring archives.\\n\\n\");\n}\n\nfn write_cluster_tips(content: &mut String, plans: &[ClusterPlan]) {\n    if plans.is_empty() {\n        return;\n    }\n    content.push_str(\"### Phase 2 Tips\\n\\n\");\n    content.push_str(\"Action: apply these guidelines while executing Phase 2 batches.\\n\");\n    content.push_str(\"Note: these are advisory, not checklist items.\\n\\n\");\n    content.push_str(\"- Extract clusters as a unit; avoid splitting a cluster across files.\\n\");\n    content.push_str(\"- Prefer creating new files before moving functions to keep diffs small.\\n\");\n    content.push_str(\"- After each batch, update imports and run tests to lock in behavior.\\n\\n\");\n}\n\nfn sort_plan_items(items: &mut Vec<PlanItem>) {\n    items.sort_by(|a, b| {\n        a.priority\n            .cmp(&b.priority)\n            .then_with(|| a.description.cmp(&b.description))\n    });\n}\n\nfn sort_cluster_items(items: &mut Vec<PlanItem>) {\n    items.sort_by(|a, b| {\n        b.cluster_cohesion\n            .partial_cmp(&a.cluster_cohesion)\n            .unwrap_or(Ordering::Equal)\n            .then_with(|| b.member_count.cmp(&a.member_count))\n            .then_with(|| a.description.cmp(&b.description))\n    });\n}\n\nfn cluster_priority(cohesion: f64) -> Priority {\n    if cohesion >= 0.8 {\n        Priority::Critical\n    } else if cohesion >= 0.6 {\n        Priority::High\n    } else if cohesion >= 0.4 {\n        Priority::Medium\n    } else {\n        Priority::Low\n    }\n}\n\nfn collect_cluster_items(plans: &[ClusterPlan]) -> Vec<PlanItem> {\n    plans\n        .iter()\n        .map(|plan| PlanItem {\n            kind: ActionKind::Cluster,\n            priority: cluster_priority(plan.cohesion),\n            description: format!(\n                \"Create cluster file `{}` with {} functions (cohesion {:.2})\",\n                compress_path(plan.target.to_string_lossy().as_ref()),\n                plan.members.len(),\n                plan.cohesion\n            ),\n            command: format!(\"touch \\\"{}\\\"\", plan.target.to_string_lossy()),\n            current_layer: None,\n            required_layer: None,\n            is_utility: false,\n            impact_weight: 0,\n            benefit: 0,\n            cost: 1,\n            callers: 0,\n            caller_files: Vec::new(),\n            current_file: None,\n            target_file: Some(plan.target.clone()),\n            outgoing_files: Vec::new(),\n            name: None,\n            cluster_cohesion: plan.cohesion,\n            member_count: plan.members.len(),\n        })\n        .collect()\n}\n\nfn collect_refactor_actions(\n    rust_ordering: &FileOrderingResult,\n    julia_ordering: &FileOrderingResult,\n    placements: &[FunctionPlacement],\n    clusters: &[FunctionCluster],\n    directory: &DirectoryAnalysis,\n    root_path: &Path,\n) -> Vec<crate::correction_plan_types::RefactorAction> {\n    let mut actions = Vec::new();\n\n    let mut renames = collect_rename_items(rust_ordering, \"Rust\")\n        .into_iter()\n        .chain(collect_rename_items(julia_ordering, \"Julia\"))\n        .collect::<Vec<_>>();\n    renames.extend(directory_moves_to_plan(\n        \"Rust\",\n        collect_directory_moves(rust_ordering, root_path),\n    ));\n    renames.extend(directory_moves_to_plan(\n        \"Julia\",\n        collect_directory_moves(julia_ordering, root_path),\n    ));\n\n    let cluster_plans = collect_cluster_plans(clusters, root_path);\n    let cluster_items = collect_cluster_items(&cluster_plans);\n\n    let mut utility_names = BTreeSet::new();\n    for placement in placements {\n        if placement.call_analysis.calls_from_other_files.len() >= 3 {\n            utility_names.insert(placement.name.clone());\n        }\n    }\n    let moves = collect_move_items(placements, &utility_names, directory, root_path);\n\n    for item in renames {\n        if let (Some(from), Some(to)) = (item.current_file, item.target_file) {\n            actions.push(crate::correction_plan_types::RefactorAction::RenameFile { from, to });\n        }\n    }\n    for item in cluster_items {\n        if let Some(path) = item.target_file {\n            actions.push(crate::correction_plan_types::RefactorAction::CreateFile { path });\n        }\n    }\n    for item in moves {\n        if let (Some(name), Some(from), Some(to)) =\n            (item.name.clone(), item.current_file, item.target_file)\n        {\n            actions.push(crate::correction_plan_types::RefactorAction::MoveFunction {\n                function: name,\n                from,\n                to,\n                required_layer: item.required_layer.clone(),\n            });\n        }\n    }\n\n    actions\n}\n\nfn build_correction_metrics(\n    placements: &[FunctionPlacement],\n    _directory: &DirectoryAnalysis,\n) -> crate::quality_delta_calculator::Metrics {\n    let cohesion = compute_directory_cohesion(placements);\n    let violations = placements\n        .iter()\n        .filter(|p| matches!(p.placement_status, PlacementStatus::LayerViolation { .. }))\n        .count();\n    crate::quality_delta_calculator::Metrics {\n        cohesion,\n        violations,\n        complexity: 0.0,\n    }\n}\n\nfn load_cargo_warnings(output_dir: &str) -> Option<String> {\n    let path = Path::new(output_dir).join(\"cargo_warnings.txt\");\n    if !path.exists() {\n        return None;\n    }\n    fs::read_to_string(path).ok()\n}\n\nfn parse_dead_code_warnings(warnings: &str) -> HashMap<String, HashSet<PathBuf>> {\n    let mut dead_code = HashMap::new();\n    let mut lines = warnings.lines().peekable();\n    while let Some(line) = lines.next() {\n        let trimmed = line.trim();\n        if !trimmed.starts_with(\"warning:\") {\n            continue;\n        }\n        let Some(name_start) = trimmed.find(\"function `\") else {\n            continue;\n        };\n        let rest = &trimmed[name_start + \"function `\".len()..];\n        let Some(name_end) = rest.find('`') else {\n            continue;\n        };\n        let name = &rest[..name_end];\n        if !trimmed.contains(\"is never used\") {\n            continue;\n        }\n\n        let mut warn_path: Option<PathBuf> = None;\n        if let Some(next) = lines.peek() {\n            let next_trimmed = next.trim();\n            if let Some(path_start) = next_trimmed.find(\"--> \") {\n                let path_part = &next_trimmed[path_start + 4..];\n                if let Some(path_end) = path_part.find(':') {\n                    warn_path = Some(PathBuf::from(&path_part[..path_end]));\n                }\n            }\n        }\n\n        dead_code\n            .entry(name.to_string())\n            .or_insert_with(HashSet::new)\n            .extend(warn_path);\n    }\n    dead_code\n}\n\nfn parse_use_symbols(line: &str) -> Vec<String> {\n    let mut symbols = Vec::new();\n    let Some(use_idx) = line.find(\"use \") else {\n        return symbols;\n    };\n    let mut clause = line[use_idx + 4..].trim();\n    if let Some(end_idx) = clause.find(';') {\n        clause = clause[..end_idx].trim();\n    }\n    clause = clause.strip_prefix(\"crate::\").unwrap_or(clause);\n    clause = clause.strip_prefix(\"self::\").unwrap_or(clause);\n\n    if let Some(brace_start) = clause.find('{') {\n        let brace_end = clause.rfind('}').unwrap_or(clause.len());\n        let inner = &clause[brace_start + 1..brace_end];\n        for item in inner.split(',') {\n            let item = item.trim();\n            if item.is_empty() || item == \"*\" || item == \"self\" || item == \"super\" {\n                continue;\n            }\n            let item = item.split(\" as \").next().unwrap_or(item).trim();\n            let last = item.rsplit(\"::\").next().unwrap_or(item);\n            if !last.is_empty() {\n                symbols.push(last.to_string());\n            }\n        }\n    } else {\n        let last = clause.rsplit(\"::\").next().unwrap_or(clause).trim();\n        if !last.is_empty() && last != \"*\" && last != \"self\" && last != \"super\" {\n            symbols.push(last.to_string());\n        }\n    }\n\n    symbols\n}\n\nfn scan_crate_paths(line: &str) -> Vec<String> {\n    let mut symbols = Vec::new();\n    let mut idx = 0;\n    while let Some(found) = line[idx..].find(\"crate::\") {\n        let start = idx + found + \"crate::\".len();\n        let mut end = start;\n        for ch in line[start..].chars() {\n            if ch.is_ascii_alphanumeric() || ch == '_' || ch == ':' {\n                end += ch.len_utf8();\n            } else {\n                break;\n            }\n        }\n        if end > start {\n            let path = &line[start..end];\n            if let Some(last) = path.rsplit(\"::\").next() {\n                if !last.is_empty() {\n                    symbols.push(last.to_string());\n                }\n            }\n        }\n        idx = end;\n    }\n    symbols\n}\n\nfn collect_symbol_references(root_path: &Path) -> HashMap<String, HashSet<PathBuf>> {\n    let mut references: HashMap<String, HashSet<PathBuf>> = HashMap::new();\n    let src_dir = root_path.join(\"src\");\n    for entry in WalkDir::new(&src_dir).into_iter().filter_map(|e| e.ok()) {\n        let path = entry.path();\n        if !path.is_file() || path.extension().and_then(|e| e.to_str()) != Some(\"rs\") {\n            continue;\n        }\n        let Ok(contents) = fs::read_to_string(path) else {\n            continue;\n        };\n        for line in contents.lines() {\n            if line.contains(\"use crate::\") {\n                for symbol in parse_use_symbols(line) {\n                    references\n                        .entry(symbol)\n                        .or_insert_with(HashSet::new)\n                        .insert(path.to_path_buf());\n                }\n            }\n            if line.contains(\"crate::\") {\n                for symbol in scan_crate_paths(line) {\n                    references\n                        .entry(symbol)\n                        .or_insert_with(HashSet::new)\n                        .insert(path.to_path_buf());\n                }\n            }\n        }\n    }\n    references\n}\n\nfn is_public_function(file_path: &Path, name: &str) -> Option<bool> {\n    let Ok(contents) = fs::read_to_string(file_path) else {\n        return None;\n    };\n    let needle = format!(\"fn {}\", name);\n    for line in contents.lines() {\n        if let Some(pos) = line.find(&needle) {\n            let prefix = line[..pos].trim_start();\n            return Some(prefix.starts_with(\"pub\"));\n        }\n    }\n    None\n}\n\nfn path_matches(entry_path: &Path, candidate: &Path) -> bool {\n    entry_path == candidate || entry_path.ends_with(candidate) || candidate.ends_with(entry_path)\n}\n\nfn is_entrypoint_main(entry: &FunctionPlacement) -> bool {\n    entry.name == \"main\"\n        && entry\n            .current_file\n            .ends_with(Path::new(\"src/190_main.rs\"))\n}\n\nfn referenced_elsewhere(\n    entry: &FunctionPlacement,\n    references: &HashMap<String, HashSet<PathBuf>>,\n) -> bool {\n    let Some(files) = references.get(&entry.name) else {\n        return false;\n    };\n    files\n        .iter()\n        .any(|path| !path_matches(&entry.current_file, path))\n}\n\nfn is_dead_code_candidate(\n    entry: &FunctionPlacement,\n    dead_code: &HashMap<String, HashSet<PathBuf>>,\n) -> bool {\n    let Some(paths) = dead_code.get(&entry.name) else {\n        return false;\n    };\n    if paths.is_empty() {\n        return true;\n    }\n    paths.iter().any(|path| path_matches(&entry.current_file, path))\n}\n\nfn filter_orphaned<'a>(\n    placements: &'a [FunctionPlacement],\n    root_path: &Path,\n    output_dir: &str,\n) -> (Vec<&'a FunctionPlacement>, Vec<&'a FunctionPlacement>) {\n    let references = collect_symbol_references(root_path);\n    let dead_code = load_cargo_warnings(output_dir)\n        .as_deref()\n        .map(parse_dead_code_warnings)\n        .unwrap_or_default();\n\n    let mut orphaned = Vec::new();\n    let mut delete_candidates = Vec::new();\n    for entry in placements\n        .iter()\n        .filter(|p| matches!(p.placement_status, PlacementStatus::Orphaned { .. }))\n    {\n        if is_entrypoint_main(entry) {\n            continue;\n        }\n        if let Some(true) = is_public_function(&entry.current_file, &entry.name) {\n            if referenced_elsewhere(entry, &references) {\n                continue;\n            }\n        }\n        let is_delete_candidate = is_dead_code_candidate(entry, &dead_code);\n        if is_delete_candidate {\n            delete_candidates.push(entry);\n        }\n        orphaned.push(entry);\n    }\n    (orphaned, delete_candidates)\n}\n\n#[derive(Clone, Debug)]\npub struct ReportConfig {\n    pub file_line_warning: usize,\n    pub dir_file_warning: usize,\n    pub naming_score_warning: f64,\n    pub baseline_path: String,\n}\n\nimpl ReportConfig {\n    fn defaults() -> Self {\n        Self {\n            file_line_warning: 800,\n            dir_file_warning: 30,\n            naming_score_warning: 70.0,\n            baseline_path: \"metrics_baseline.txt\".to_string(),\n        }\n    }\n}\n\nfn load_report_config(output_dir: &str) -> ReportConfig {\n    let path = Path::new(output_dir).join(\"analyzer_config.toml\");\n    let mut config = ReportConfig::defaults();\n    let Ok(contents) = fs::read_to_string(path) else {\n        return config;\n    };\n    for line in contents.lines() {\n        let trimmed = line.trim();\n        if trimmed.is_empty() || trimmed.starts_with('#') {\n            continue;\n        }\n        let Some((key, value)) = trimmed.split_once('=') else {\n            continue;\n        };\n        let key = key.trim();\n        let value = value.trim().trim_matches('\"');\n        match key {\n            \"file_line_warning\" => {\n                if let Ok(parsed) = value.parse::<usize>() {\n                    config.file_line_warning = parsed;\n                }\n            }\n            \"dir_file_warning\" => {\n                if let Ok(parsed) = value.parse::<usize>() {\n                    config.dir_file_warning = parsed;\n                }\n            }\n            \"baseline_path\" => {\n                if !value.is_empty() {\n                    config.baseline_path = value.to_string();\n                }\n            }\n            \"naming_score_warning\" => {\n                if let Ok(parsed) = value.parse::<f64>() {\n                    config.naming_score_warning = parsed;\n                }\n            }\n            _ => {}\n        }\n    }\n    config\n}\n\nfn collect_size_warnings(\n    directory: &DirectoryAnalysis,\n    config: &ReportConfig,\n    warnings: &mut Vec<String>,\n) {\n    if directory.files.len() >= config.dir_file_warning {\n        warnings.push(format!(\n            \"Directory `{}` has {} files; consider splitting into submodules.\",\n            compress_path(directory.path.to_string_lossy().as_ref()),\n            directory.files.len()\n        ));\n    }\n\n    for file in &directory.files {\n        if let Ok(contents) = fs::read_to_string(file) {\n            let lines = contents.lines().count();\n            if lines >= config.file_line_warning {\n                warnings.push(format!(\n                    \"File `{}` has {} lines; consider extracting helpers.\",\n                    compress_path(file.to_string_lossy().as_ref()),\n                    lines\n                ));\n            }\n        }\n    }\n\n    for child in &directory.subdirectories {\n        collect_size_warnings(child, config, warnings);\n    }\n}\n\nfn load_baseline_metrics(config: &ReportConfig, output_dir: &str) -> Option<HashMap<String, f64>> {\n    let path = Path::new(output_dir).join(&config.baseline_path);\n    let Ok(contents) = fs::read_to_string(path) else {\n        return None;\n    };\n    let mut metrics = HashMap::new();\n    for line in contents.lines() {\n        let trimmed = line.trim();\n        if trimmed.is_empty() || trimmed.starts_with('#') {\n            continue;\n        }\n        let Some((key, value)) = trimmed.split_once('=') else {\n            continue;\n        };\n        let key = key.trim().to_string();\n        let value = value.trim();\n        if let Ok(parsed) = value.parse::<f64>() {\n            metrics.insert(key, parsed);\n        }\n    }\n    Some(metrics)\n}\n\nfn baseline_deltas(\n    baseline: &HashMap<String, f64>,\n    dir_cohesion: f64,\n    ordering_correctness: f64,\n    avg_cohesion: f64,\n    renames_len: usize,\n    relocations: usize,\n) -> Vec<String> {\n    let mut deltas = Vec::new();\n    if let Some(prev) = baseline.get(\"directory_cohesion\") {\n        deltas.push(format!(\n            \"directory_cohesion: {:.2} -> {:.2} (delta {:+.2})\",\n            prev,\n            dir_cohesion,\n            dir_cohesion - prev\n        ));\n    }\n    if let Some(prev) = baseline.get(\"ordering_correctness\") {\n        let current = ordering_correctness * 100.0;\n        deltas.push(format!(\n            \"ordering_correctness: {:.1}% -> {:.1}% (delta {:+.1}%)\",\n            prev,\n            current,\n            current - prev\n        ));\n    }\n    if let Some(prev) = baseline.get(\"avg_function_cohesion\") {\n        deltas.push(format!(\n            \"avg_function_cohesion: {:.2} -> {:.2} (delta {:+.2})\",\n            prev,\n            avg_cohesion,\n            avg_cohesion - prev\n        ));\n    }\n    if let Some(prev) = baseline.get(\"rename_ops_needed\") {\n        let current = renames_len as f64;\n        deltas.push(format!(\n            \"rename_ops_needed: {:.0} -> {} (delta {:+.0})\",\n            prev,\n            renames_len,\n            current - prev\n        ));\n    }\n    if let Some(prev) = baseline.get(\"function_relocations\") {\n        let current = relocations as f64;\n        deltas.push(format!(\n            \"function_relocations: {:.0} -> {} (delta {:+.0})\",\n            prev,\n            relocations,\n            current - prev\n        ));\n    }\n    deltas\n}\n\nfn write_baseline_metrics(\n    config: &ReportConfig,\n    output_dir: &str,\n    dir_cohesion: f64,\n    ordering_correctness: f64,\n    avg_cohesion: f64,\n    renames_len: usize,\n    relocations: usize,\n) {\n    let path = Path::new(output_dir).join(&config.baseline_path);\n    if path.exists() {\n        return;\n    }\n    let content = format!(\n        \"directory_cohesion={:.2}\\nordering_correctness={:.1}\\navg_function_cohesion={:.2}\\nrename_ops_needed={}\\nfunction_relocations={}\\n\",\n        dir_cohesion,\n        ordering_correctness * 100.0,\n        avg_cohesion,\n        renames_len,\n        relocations\n    );\n    let _ = fs::write(path, content);\n}\n\nfn collect_directories<'a>(node: &'a DirectoryAnalysis, acc: &mut Vec<&'a DirectoryAnalysis>) {\n    acc.push(node);\n    for child in &node.subdirectories {\n        collect_directories(child, acc);\n    }\n}\n\nfn slugify_path(path: &Path) -> String {\n    let mut slug = String::new();\n    for component in path.components() {\n        if !slug.is_empty() {\n            slug.push_str(\"__\");\n        }\n        slug.push_str(&component.as_os_str().to_string_lossy().replace('/', \"_\"));\n    }\n    if slug.is_empty() {\n        \"root\".to_string()\n    } else {\n        slug\n    }\n}\n\nfn render_mermaid_graph(graph: &petgraph::graph::DiGraph<PathBuf, ()>) -> String {\n    let mut output = String::from(\"```mermaid\\ngraph TD\\n\");\n    let mut node_ids: HashMap<usize, String> = HashMap::new();\n    let mut idx = 0usize;\n    for node in graph.node_indices() {\n        let node_name = graph[node]\n            .file_name()\n            .and_then(|n| n.to_str())\n            .unwrap_or(\"file\");\n        let safe_id = format!(\"F{}\", idx);\n        idx += 1;\n        node_ids.insert(node.index(), safe_id.clone());\n        output.push_str(&format!(\"    {}[\\\"{}\\\"]\\n\", safe_id, node_name));\n    }\n    for edge in graph.edge_indices() {\n        if let Some((src, dst)) = graph.edge_endpoints(edge) {\n            if let (Some(from), Some(to)) = (node_ids.get(&src.index()), node_ids.get(&dst.index()))\n            {\n                output.push_str(&format!(\"    {} --> {}\\n\", from, to));\n            }\n        }\n    }\n    output.push_str(\"```\\n\");\n    output\n}\n\nfn compute_ordering_correctness(\n    rust_ordering: &FileOrderingResult,\n    julia_ordering: &FileOrderingResult,\n) -> f64 {\n    let mut total = 0usize;\n    let mut correct = 0usize;\n    for ordering in [rust_ordering, julia_ordering] {\n        total += ordering.ordered_files.len();\n        correct += ordering.ordered_files.len().saturating_sub(ordering.violations.len());\n    }\n    if total == 0 {\n        1.0\n    } else {\n        correct as f64 / total as f64\n    }\n}\n\nfn compute_directory_cohesion(placements: &[FunctionPlacement]) -> f64 {\n    let mut intra = 0usize;\n    let mut inter = 0usize;\n    for placement in placements {\n        let current_dir = placement.current_file.parent().map(|p| p.to_path_buf());\n        intra += placement.call_analysis.intra_file_calls;\n        for (file, count) in &placement.call_analysis.inter_file_calls {\n            let same_dir = current_dir\n                .as_ref()\n                .and_then(|dir| file.parent().map(|p| p == dir))\n                .unwrap_or(false);\n            if same_dir {\n                intra += count;\n            } else {\n                inter += count;\n            }\n        }\n    }\n    let total = intra + inter;\n    if total == 0 {\n        1.0\n    } else {\n        intra as f64 / total as f64\n    }\n}\n\npub struct ReportGenerator {\n    output_dir: String,\n}\n\nimpl ReportGenerator {\n    pub fn new(output_dir: String) -> Self {\n        Self { output_dir }\n    }\n\n    pub fn generate_all(\n        &self,\n        result: &AnalysisResult,\n        cf_analyzer: &ControlFlowAnalyzer,\n        rust_layers: &LayerGraph,\n        julia_layers: &LayerGraph,\n        rust_ordering: &FileOrderingResult,\n        julia_ordering: &FileOrderingResult,\n        function_placements: &[FunctionPlacement],\n        function_clusters: &[FunctionCluster],\n        directory_structure: &DirectoryAnalysis,\n        root_path: &Path,\n        correction_intelligence: bool,\n        correction_json: Option<PathBuf>,\n        verification_policy_json: Option<PathBuf>,\n    ) -> Result<()> {\n        fs::create_dir_all(&self.output_dir)?;\n        self.cleanup_legacy_reports()?;\n\n        println!(\"  Report: structure\");\n        self.generate_structure_report(result)?;\n        println!(\"  Report: call_graph\");\n        self.generate_call_graph_report(cf_analyzer)?;\n        println!(\"  Report: cfg\");\n        self.generate_cfg_report(cf_analyzer)?;\n        println!(\"  Report: module_dependencies\");\n        self.generate_module_dependencies(result)?;\n        println!(\"  Report: function_analysis\");\n        self.generate_function_analysis(result)?;\n        println!(\"  Report: layer_dependencies\");\n        self.generate_layer_dependency_report(rust_layers, julia_layers, root_path)?;\n        println!(\"  Report: file_ordering\");\n        self.generate_file_ordering_report(rust_ordering, julia_ordering, root_path)?;\n        println!(\"  Report: cohesion_analysis\");\n        self.generate_cohesion_report(function_placements, function_clusters, root_path)?;\n        println!(\"  Report: refactoring_plan\");\n        let correction_report = if correction_intelligence {\n            let actions = collect_refactor_actions(\n                rust_ordering,\n                julia_ordering,\n                function_placements,\n                function_clusters,\n                directory_structure,\n                root_path,\n            );\n            let metrics = build_correction_metrics(function_placements, directory_structure);\n            let state = crate::correction_intelligence_report::build_state(root_path, result, metrics);\n            let report = crate::correction_intelligence_report::generate_intelligence_report(&actions, &state);\n            let output_dir = Path::new(&self.output_dir).join(\"97_correction_intelligence\");\n            crate::correction_intelligence_report::write_intelligence_outputs_at(\n                &report,\n                &output_dir,\n                correction_json.as_deref(),\n                verification_policy_json.as_deref(),\n            )?;\n            Some(report)\n        } else {\n            None\n        };\n        self.generate_refactoring_plan(\n            rust_ordering,\n            julia_ordering,\n            function_placements,\n            function_clusters,\n            directory_structure,\n            root_path,\n            correction_report.as_ref(),\n        )?;\n        println!(\"  Report: file_organization\");\n        self.generate_file_organization_report(\n            directory_structure,\n            rust_ordering,\n            julia_ordering,\n            root_path,\n        )?;\n\n        Ok(())\n    }\n\n    fn cleanup_legacy_reports(&self) -> Result<()> {\n        let legacy_files = [\n            \"structure.md\",\n            \"call_graph.md\",\n            \"cfg.md\",\n            \"module_dependencies.md\",\n            \"function_analysis.md\",\n            \"layer_dependencies.md\",\n            \"file_ordering.md\",\n            \"cohesion_analysis.md\",\n            \"refactoring_plan.md\",\n            \"file_organization.md\",\n        ];\n        for file in legacy_files {\n            let path = Path::new(&self.output_dir).join(file);\n            if path.exists() {\n                fs::remove_file(path)?;\n            }\n        }\n        let report_dirs = [\n            \"structure\",\n            \"call_graph\",\n            \"cfg\",\n            \"module_dependencies\",\n            \"function_analysis\",\n            \"layer_dependencies\",\n            \"file_ordering\",\n            \"cohesion_analysis\",\n            \"refactoring_plan\",\n            \"file_organization\",\n            \"00_refactoring_plan\",\n            \"10_structure\",\n            \"20_call_graph\",\n            \"30_cfg\",\n            \"40_module_dependencies\",\n            \"50_function_analysis\",\n            \"60_layer_dependencies\",\n            \"70_file_ordering\",\n            \"80_cohesion_analysis\",\n            \"90_file_organization\",\n        ];\n        for dir in report_dirs {\n            let path = Path::new(&self.output_dir).join(dir);\n            if !path.exists() {\n                continue;\n            }\n            for entry in fs::read_dir(&path)? {\n                let entry = entry?;\n                let entry_path = entry.path();\n                if entry_path.is_dir() {\n                    if dir == \"30_cfg\" && entry_path.file_name().map_or(false, |n| n == \"dots\") {\n                        continue;\n                    }\n                    fs::remove_dir_all(entry_path)?;\n                } else {\n                    fs::remove_file(entry_path)?;\n                }\n            }\n        }\n        Ok(())\n    }\n\n    fn prepare_report_dir(&self, name: &str) -> Result<PathBuf> {\n        let dir = Path::new(&self.output_dir).join(name);\n        fs::create_dir_all(&dir)?;\n        Ok(dir)\n    }\n\n    fn generate_structure_report(&self, result: &AnalysisResult) -> Result<()> {\n        let dir = self.prepare_report_dir(\"10_structure\")?;\n        let generated_at = chrono::Local::now().format(\"%Y-%m-%d %H:%M:%S\");\n\n        let mut files: BTreeMap<String, Vec<&CodeElement>> = BTreeMap::new();\n        for elem in &result.elements {\n            files\n                .entry(elem.file_path.clone())\n                .or_insert_with(Vec::new)\n                .push(elem);\n        }\n\n        let mut grouped: BTreeMap<String, Vec<(String, Vec<&CodeElement>)>> = BTreeMap::new();\n        for (file_path, elements) in files {\n            let compressed = compress_path(&file_path);\n            let key = prefix_key_from_path(&compressed);\n            grouped\n                .entry(key)\n                .or_insert_with(Vec::new)\n                .push((compressed, elements));\n        }\n\n        let mut grouped: Vec<_> = grouped.into_iter().collect();\n        grouped.sort_by(|a, b| group_key_cmp(&a.0, &b.0));\n\n        let mut index = String::from(\"# MMSB Code Structure Overview\\n\\n\");\n        index.push_str(&format!(\"Generated: {}\\n\\n\", generated_at));\n        index.push_str(\n            \"Each numbered file groups source files by MMSB prefix so a simple `ls 10_structure/` \\\nshows the traversal order.\\n\\n\",\n        );\n\n        if grouped.is_empty() {\n            index.push_str(\"No code elements were recorded.\\n\");\n        } else {\n            index.push_str(\"## Group Files\\n\\n\");\n            for (idx, (group_key, _)) in grouped.iter().enumerate() {\n                let slug = slugify_key(group_key);\n                let file_name = format!(\"{:03}-{}.md\", idx * 10, slug);\n                index.push_str(&format!(\"- `{}`  `{}`\\n\", group_key, file_name));\n            }\n        }\n\n        for (idx, (group_key, mut entries)) in grouped.into_iter().enumerate() {\n            entries.sort_by(|a, b| a.0.cmp(&b.0));\n            let slug = slugify_key(&group_key);\n            let file_name = format!(\"{:03}-{}.md\", idx * 10, slug);\n            let mut content = format!(\"# Structure Group: {}\\n\\n\", group_key);\n\n            for (file_path, mut elements) in entries {\n                content.push_str(&format!(\"## File: {}\\n\\n\", file_path));\n\n                let layers: BTreeSet<String> = elements.iter().map(|e| e.layer.clone()).collect();\n                let layer_summary = if layers.is_empty() {\n                    \"root\".to_string()\n                } else {\n                    layers.iter().cloned().collect::<Vec<_>>().join(\", \")\n                };\n\n                let mut language_counts: BTreeMap<String, usize> = BTreeMap::new();\n                let mut type_counts: BTreeMap<String, usize> = BTreeMap::new();\n                for elem in &elements {\n                    *language_counts\n                        .entry(language_label(&elem.language).to_string())\n                        .or_insert(0) += 1;\n                    *type_counts\n                        .entry(format!(\"{:?}\", elem.element_type))\n                        .or_insert(0) += 1;\n                }\n\n                let lang_summary = if language_counts.is_empty() {\n                    \"n/a\".to_string()\n                } else {\n                    language_counts\n                        .iter()\n                        .map(|(lang, count)| format!(\"{} ({})\", lang, count))\n                        .collect::<Vec<_>>()\n                        .join(\", \")\n                };\n\n                let type_summary = if type_counts.is_empty() {\n                    \"n/a\".to_string()\n                } else {\n                    type_counts\n                        .iter()\n                        .map(|(ty, count)| format!(\"{} ({})\", ty, count))\n                        .collect::<Vec<_>>()\n                        .join(\", \")\n                };\n\n                content.push_str(&format!(\"- Layer(s): {}\\n\", layer_summary));\n                content.push_str(&format!(\"- Language coverage: {}\\n\", lang_summary));\n                content.push_str(&format!(\"- Element types: {}\\n\", type_summary));\n                content.push_str(&format!(\"- Total elements: {}\\n\\n\", elements.len()));\n\n                content.push_str(\"### Elements\\n\\n\");\n                elements.sort_by(|a, b| {\n                    a.line_number\n                        .cmp(&b.line_number)\n                        .then_with(|| a.name.cmp(&b.name))\n                });\n                for elem in elements {\n                    content.push_str(&self.format_element_entry(elem));\n                }\n                content.push('\\n');\n            }\n\n            fs::write(dir.join(file_name), content)?;\n        }\n\n        // Summary statistics\n        index.push_str(\"\\n## Summary Statistics\\n\\n\");\n        index.push_str(&format!(\"- Total elements: {}\\n\", result.elements.len()));\n        index.push_str(&format!(\n            \"- Rust elements: {}\\n\",\n            result\n                .elements\n                .iter()\n                .filter(|e| matches!(e.language, Language::Rust))\n                .count()\n        ));\n        index.push_str(&format!(\n            \"- Julia elements: {}\\n\",\n            result\n                .elements\n                .iter()\n                .filter(|e| matches!(e.language, Language::Julia))\n                .count()\n        ));\n\n        let mut type_counts: HashMap<String, usize> = HashMap::new();\n        for elem in &result.elements {\n            let key = format!(\"{:?}_{:?}\", elem.language, elem.element_type);\n            *type_counts.entry(key).or_insert(0) += 1;\n        }\n\n        index.push_str(\"\\n### Elements by Type\\n\\n\");\n        let mut sorted_types: Vec<_> = type_counts.iter().collect();\n        sorted_types.sort_by_key(|(k, _)| k.as_str());\n        for (type_name, count) in sorted_types {\n            index.push_str(&format!(\"- {}: {}\\n\", type_name, count));\n        }\n\n        fs::write(dir.join(\"index.md\"), index)?;\n        Ok(())\n    }\n\n    fn format_element_entry(&self, elem: &CodeElement) -> String {\n        let mut entry = format!(\n            \"- [{} | {:?}] `{}` (line {}, {})\\n\",\n            language_label(&elem.language),\n            elem.element_type,\n            elem.name,\n            elem.line_number,\n            visibility_label(&elem.visibility),\n        );\n\n        if !elem.signature.is_empty()\n            && matches!(\n                elem.element_type,\n                ElementType::Function | ElementType::Struct\n            )\n        {\n            entry.push_str(&format!(\n                \"  - Signature: `{}`\\n\",\n                short_signature(&elem.signature)\n            ));\n        }\n\n        if !elem.generic_params.is_empty() {\n            entry.push_str(&format!(\n                \"  - Generics: {}\\n\",\n                elem.generic_params.join(\", \")\n            ));\n        }\n\n        if matches!(elem.element_type, ElementType::Function) && !elem.calls.is_empty() {\n            entry.push_str(&format!(\"  - Calls: {}\\n\", elem.calls.join(\", \")));\n        }\n\n        entry\n    }\n\n    fn generate_call_graph_report(&self, cf_analyzer: &ControlFlowAnalyzer) -> Result<()> {\n        let dir = self.prepare_report_dir(\"20_call_graph\")?;\n        let path = dir.join(\"index.md\");\n        let mut content = String::from(\"# Call Graph Analysis\\n\\n\");\n        content.push_str(\"This document shows the **interprocedural call graph** - which functions call which other functions.\\n\\n\");\n        content.push_str(\"> **Note:** This is NOT a control flow graph (CFG). CFG shows intraprocedural control flow (branches, loops) within individual functions.\\n\\n\");\n\n        let stats = cf_analyzer.get_statistics();\n\n        content.push_str(\"## Call Graph Statistics\\n\\n\");\n        content.push_str(&format!(\"- Total functions: {}\\n\", stats.total_functions));\n        content.push_str(&format!(\"- Total function calls: {}\\n\", stats.total_calls));\n        content.push_str(&format!(\"- Maximum call depth: {}\\n\", stats.max_depth));\n        content.push_str(&format!(\n            \"- Leaf functions (no outgoing calls): {}\\n\\n\",\n            stats.leaf_functions\n        ));\n\n        content.push_str(\"## Call Graph Visualization\\n\\n\");\n        content.push_str(&cf_analyzer.generate_mermaid());\n\n        fs::write(path, content)?;\n        Ok(())\n    }\n\n    fn generate_cfg_report(&self, cf_analyzer: &ControlFlowAnalyzer) -> Result<()> {\n        let dir = self.prepare_report_dir(\"30_cfg\")?;\n        let mut index = String::from(\"# Control Flow Graphs (CFG)\\n\\n\");\n        index.push_str(&format!(\n            \"Generated: {}\\n\\n\",\n            chrono::Local::now().format(\"%Y-%m-%d %H:%M:%S\")\n        ));\n\n        if cf_analyzer.cfgs().is_empty() {\n            index.push_str(\"No control flow graphs were captured.\\n\");\n            fs::write(dir.join(\"index.md\"), index)?;\n            return Ok(());\n        }\n\n        let mut grouped: BTreeMap<String, Vec<(String, &FunctionCfg)>> = BTreeMap::new();\n        for cfg in cf_analyzer.cfgs() {\n            let compressed = compress_path(&cfg.file_path);\n            let key = prefix_key_from_path(&compressed);\n            grouped\n                .entry(key)\n                .or_insert_with(Vec::new)\n                .push((compressed, cfg));\n        }\n\n        let mut grouped: Vec<_> = grouped.into_iter().collect();\n        grouped.sort_by(|a, b| group_key_cmp(&a.0, &b.0));\n\n        index.push_str(&format!(\"- Total CFGs: {}\\n\", cf_analyzer.cfgs().len()));\n        index.push_str(\n            \"- Files are grouped by MMSB directory prefix; numeric prefixes match lexical ordering.\\n\\n\",\n        );\n\n        index.push_str(\"## Group Files\\n\\n\");\n        for (idx, (group_key, _)) in grouped.iter().enumerate() {\n            let file_name = format!(\"{:03}-{}.md\", idx * 10, slugify_key(group_key));\n            index.push_str(&format!(\"- `{}`  `{}`\\n\", group_key, file_name));\n        }\n\n        for (idx, (group_key, mut entries)) in grouped.into_iter().enumerate() {\n            entries.sort_by(|a, b| a.1.function.cmp(&b.1.function));\n            let slug = slugify_key(&group_key);\n            let file_name = format!(\"{:03}-{}.md\", idx * 10, slug);\n            let mut content = format!(\"# CFG Group: {}\\n\\n\", group_key);\n\n            for (compressed, cfg) in entries {\n                content.push_str(&format!(\"## Function: `{}`\\n\\n\", cfg.function));\n                content.push_str(&format!(\n                    \"- File: {}\\n- Branches: {}\\n- Loops: {}\\n- Nodes: {}\\n- Edges: {}\\n\\n\",\n                    compressed,\n                    cfg.branch_count,\n                    cfg.loop_count,\n                    cfg.nodes.len(),\n                    cfg.edges.len(),\n                ));\n                if let Some(dot_rel) = self.dot_path_for(&compressed) {\n                    content.push_str(&format!(\"- DOT call graph: `{}`\\n\\n\", dot_rel));\n                }\n\n                content.push_str(\"```mermaid\\nflowchart TD\\n\");\n                let mut id_map = HashMap::new();\n                let prefix = sanitize_mermaid_id(&cfg.function);\n                for node in &cfg.nodes {\n                    let raw_id = format!(\"{}_{}\", prefix, node.id);\n                    let safe_id = sanitize_mermaid_id(&raw_id);\n                    id_map.insert(node.id, safe_id.clone());\n                    content.push_str(&format!(\n                        \"    {}[\\\"{}\\\"]\\n\",\n                        safe_id,\n                        sanitize_mermaid_label(&node.label)\n                    ));\n                }\n                for edge in &cfg.edges {\n                    if let (Some(src), Some(dst)) = (id_map.get(&edge.from), id_map.get(&edge.to)) {\n                        content.push_str(&format!(\"    {} --> {}\\n\", src, dst));\n                    }\n                }\n                content.push_str(\"```\\n\\n\");\n            }\n\n            fs::write(dir.join(file_name), content)?;\n        }\n\n        fs::write(dir.join(\"index.md\"), index)?;\n        Ok(())\n    }\n\n    fn generate_layer_dependency_report(\n        &self,\n        rust_layers: &LayerGraph,\n        julia_layers: &LayerGraph,\n        root_path: &Path,\n    ) -> Result<()> {\n        let dir = self.prepare_report_dir(\"60_layer_dependencies\")?;\n        let path = dir.join(\"index.md\");\n        let mut content = String::from(\"# Layer Dependency Report\\n\\n\");\n        content.push_str(&format!(\n            \"Generated: {}\\n\\n\",\n            chrono::Local::now().format(\"%Y-%m-%d %H:%M:%S\")\n        ));\n\n        self.write_layer_section(&mut content, \"Rust\", rust_layers, root_path);\n        self.write_layer_section(&mut content, \"Julia\", julia_layers, root_path);\n\n        fs::write(path, content)?;\n        Ok(())\n    }\n\n    fn generate_file_ordering_report(\n        &self,\n        rust_ordering: &FileOrderingResult,\n        julia_ordering: &FileOrderingResult,\n        root_path: &Path,\n    ) -> Result<()> {\n        let dir = self.prepare_report_dir(\"70_file_ordering\")?;\n        let path = dir.join(\"index.md\");\n        let mut content = String::from(\"# File Ordering Report\\n\\n\");\n        content.push_str(&format!(\n            \"Generated: {}\\n\\n\",\n            chrono::Local::now().format(\"%Y-%m-%d %H:%M:%S\")\n        ));\n\n        self.write_ordering_section(&mut content, \"Rust\", rust_ordering, root_path);\n        self.write_ordering_section(&mut content, \"Julia\", julia_ordering, root_path);\n\n        fs::write(path, content)?;\n        Ok(())\n    }\n\n    fn write_ordering_section(\n        &self,\n        content: &mut String,\n        label: &str,\n        ordering: &FileOrderingResult,\n        root_path: &Path,\n    ) {\n        content.push_str(&format!(\"## {} File Ordering\\n\\n\", label));\n        if ordering.ordered_files.is_empty() {\n            content.push_str(\"No files analyzed.\\n\\n\");\n            return;\n        }\n\n        let rename_count = ordering\n            .ordered_files\n            .iter()\n            .filter(|entry| entry.needs_rename)\n            .count();\n        content.push_str(\"### Metrics\\n\\n\");\n        content.push_str(&format!(\n            \"- Total files: {}\\n- Rename suggestions: {}\\n- Ordering violations: {}\\n- Layer violations: {}\\n- Directories: {}\\n\\n\",\n            ordering.ordered_files.len(),\n            rename_count,\n            ordering.violations.len(),\n            ordering.layer_violations.len(),\n            ordering.ordered_directories.len()\n        ));\n\n        if !ordering.cycles.is_empty() {\n            content.push_str(\"### Cycles Detected\\n\");\n            for cycle in &ordering.cycles {\n                let listing = cycle\n                    .iter()\n                    .map(|p| compress_path(p.to_string_lossy().as_ref()))\n                    .collect::<Vec<_>>()\n                    .join(\", \");\n                content.push_str(&format!(\"- {}\\n\", listing));\n            }\n            content.push('\\n');\n        }\n\n        content.push_str(\"### Canonical Order\\n\\n\");\n        content.push_str(\"| Order | Current | Suggested | Rename |\\n\");\n        content.push_str(\"| --- | --- | --- | --- |\\n\");\n        for entry in &ordering.ordered_files {\n            let current = display_path(&entry.current_path, root_path);\n            let rename = if entry.needs_rename { \"yes\" } else { \"no\" };\n            content.push_str(&format!(\n                \"| {} | `{}` | `{}` | {} |\\n\",\n                entry.canonical_order, current, entry.suggested_name, rename\n            ));\n        }\n        content.push('\\n');\n\n        content.push_str(\"### Ordering Violations\\n\");\n        if ordering.violations.is_empty() {\n            content.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for violation in &ordering.violations {\n                let file = display_path(&violation.file, root_path);\n                content.push_str(&format!(\n                    \"- `{}`: alphabetical position {}, required position {}\\n\",\n                    file, violation.current_position, violation.required_position\n                ));\n                if !violation.blocking_dependencies.is_empty() {\n                    for dep in &violation.blocking_dependencies {\n                        let dep_path = display_path(dep, root_path);\n                        content.push_str(&format!(\"  - depends on `{}`\\n\", dep_path));\n                    }\n                }\n            }\n            content.push('\\n');\n        }\n\n        content.push_str(\"### Layer Violations\\n\");\n        if ordering.layer_violations.is_empty() {\n            content.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for violation in &ordering.layer_violations {\n                let from = display_path(&violation.from, root_path);\n                let to = display_path(&violation.to, root_path);\n                content.push_str(&format!(\n                    \"- `{}` ({}) depends on `{}` ({})\\n\",\n                    to, violation.to_layer, from, violation.from_layer\n                ));\n            }\n            content.push('\\n');\n        }\n\n        content.push_str(\"### Directory Order\\n\");\n        if ordering.ordered_directories.is_empty() {\n            content.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for dir in &ordering.ordered_directories {\n                let path = display_path(dir, root_path);\n                content.push_str(&format!(\"- `{}`\\n\", path));\n            }\n            content.push('\\n');\n        }\n    }\n\n    fn generate_cohesion_report(\n        &self,\n        placements: &[FunctionPlacement],\n        clusters: &[FunctionCluster],\n        root_path: &Path,\n    ) -> Result<()> {\n        let dir = self.prepare_report_dir(\"80_cohesion_analysis\")?;\n        let path = dir.join(\"index.md\");\n        let mut content = String::from(\"# Function Cohesion Analysis\\n\\n\");\n        content.push_str(&format!(\n            \"Generated: {}\\n\\n\",\n            chrono::Local::now().format(\"%Y-%m-%d %H:%M:%S\")\n        ));\n\n        if placements.is_empty() {\n            content.push_str(\"No function placement data recorded.\\n\");\n            fs::write(path, content)?;\n            return Ok(());\n        }\n\n        let mut by_file: BTreeMap<String, Vec<&FunctionPlacement>> = BTreeMap::new();\n        let avg_cohesion = if placements.is_empty() {\n            0.0\n        } else {\n            placements\n                .iter()\n                .map(|p| p.cohesion_score)\n                .sum::<f64>()\n                / placements.len() as f64\n        };\n        let move_count = placements\n            .iter()\n            .filter(|p| matches!(p.placement_status, PlacementStatus::ShouldMove { .. }))\n            .count();\n        let (orphaned, delete_candidates) =\n            filter_orphaned(placements, root_path, &self.output_dir);\n        let orphaned_count = orphaned.len();\n        let layer_violation_count = placements\n            .iter()\n            .filter(|p| matches!(p.placement_status, PlacementStatus::LayerViolation { .. }))\n            .count();\n        content.push_str(\"## Metrics\\n\\n\");\n        content.push_str(&format!(\n            \"- Avg cohesion: {:.2}\\n- Move suggestions: {}\\n- Orphaned functions: {}\\n- Layer violations: {}\\n\\n\",\n            avg_cohesion, move_count, orphaned_count, layer_violation_count\n        ));\n        for placement in placements {\n            by_file\n                .entry(placement.current_file.to_string_lossy().to_string())\n                .or_default()\n                .push(placement);\n        }\n\n        for (file, mut entries) in by_file {\n            entries.sort_by(|a, b| {\n                a.cohesion_score\n                    .partial_cmp(&b.cohesion_score)\n                    .unwrap_or(Ordering::Equal)\n            });\n            let compressed = compress_path(&file);\n            content.push_str(&format!(\"## File: {}\\n\\n\", compressed));\n            content.push_str(\"| Function | Signature | Cohesion | Calls | Type refs | Status | Suggestion |\\n\");\n            content.push_str(\"| --- | --- | --- | --- | --- | --- | --- |\\n\");\n            for entry in entries {\n                let status = placement_status_label(&entry.placement_status);\n                let mut suggestion = entry\n                    .suggested_file\n                    .as_ref()\n                    .map(|p| compress_path(p.to_string_lossy().as_ref()))\n                    .unwrap_or_else(|| \"-\".to_string());\n                let notes = placement_status_notes(&entry.placement_status);\n                if !notes.is_empty() {\n                    suggestion = format!(\"{} ({})\", suggestion, notes);\n                }\n                let call_summary = format!(\n                    \"intra {}, inter {}\",\n                    entry.call_analysis.intra_file_calls,\n                    entry.call_analysis.inter_file_calls.len()\n                );\n                let type_summary = format!(\n                    \"same {}, other {}\",\n                    entry.call_analysis.same_file_type_refs,\n                    entry.call_analysis.other_file_type_refs\n                );\n                content.push_str(&format!(\n                    \"| `{}` | `{}` | {:.2} | {} | {} | {} | {} |\\n\",\n                    entry.name,\n                    entry.signature.replace('|', \"\\\\|\"),\n                    entry.cohesion_score,\n                    call_summary,\n                    type_summary,\n                    status,\n                    suggestion\n                ));\n            }\n            content.push('\\n');\n        }\n\n        content.push_str(\"## Orphaned Functions (Review Only)\\n\\n\");\n        content.push_str(\"Action: review each item for expected usage. Delete only if it also appears under \\\"Delete Candidates (Orphaned + Dead Code)\\\".\\n\");\n        content.push_str(\"Note: excludes public symbols referenced by other modules and entry points. Delete candidates require dead_code warnings.\\n\\n\");\n        if orphaned.is_empty() {\n            content.push_str(\"- None detected.\\n\");\n        } else {\n            for entry in orphaned {\n                let file = compress_path(entry.current_file.to_string_lossy().as_ref());\n                content.push_str(&format!(\"- `{}` in `{}`\\n\", entry.name, file));\n            }\n        }\n        content.push('\\n');\n        content.push_str(\"## Delete Candidates (Orphaned + Dead Code)\\n\\n\");\n        if delete_candidates.is_empty() {\n            content.push_str(\"- None detected.\\n\");\n        } else {\n            for entry in delete_candidates {\n                let file = compress_path(entry.current_file.to_string_lossy().as_ref());\n                content.push_str(&format!(\"- `{}` in `{}`\\n\", entry.name, file));\n            }\n        }\n        content.push('\\n');\n\n        let utility_candidates = collect_utility_candidates(placements);\n        content.push_str(\"## Utility Module Candidates\\n\\n\");\n        if utility_candidates.is_empty() {\n            content.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for candidate in utility_candidates {\n                content.push_str(&format!(\"- {}\\n\", candidate));\n            }\n            content.push('\\n');\n        }\n\n        content.push_str(\"## Function Clusters\\n\\n\");\n        if clusters.is_empty() {\n            content.push_str(\"- None detected.\\n\");\n        } else {\n            for cluster in clusters {\n                let suggested = cluster\n                    .suggested_file\n                    .as_ref()\n                    .map(|p| compress_path(p.to_string_lossy().as_ref()))\n                    .unwrap_or_else(|| \"-\".to_string());\n                let members = cluster\n                    .members\n                    .iter()\n                    .map(|m| compress_path(m))\n                    .collect::<Vec<_>>()\n                    .join(\", \");\n                content.push_str(&format!(\n                    \"- cohesion {:.2}, suggested `{}`\\n  - {}\\n\",\n                    cluster.cohesion, suggested, members\n                ));\n            }\n        }\n        content.push('\\n');\n\n        fs::write(path, content)?;\n        Ok(())\n    }\n\n    fn generate_refactoring_plan(\n        &self,\n        rust_ordering: &FileOrderingResult,\n        julia_ordering: &FileOrderingResult,\n        placements: &[FunctionPlacement],\n        clusters: &[FunctionCluster],\n        directory: &DirectoryAnalysis,\n        root_path: &Path,\n        correction_report: Option<&crate::correction_intelligence_report::CorrectionIntelligenceReport>,\n    ) -> Result<()> {\n        let dir = self.prepare_report_dir(\"00_refactoring_plan\")?;\n        let generated_at = chrono::Local::now().format(\"%Y-%m-%d %H:%M:%S\").to_string();\n\n        let mut renames = collect_rename_items(rust_ordering, \"Rust\")\n            .into_iter()\n            .chain(collect_rename_items(julia_ordering, \"Julia\"))\n            .collect::<Vec<_>>();\n        renames.extend(directory_moves_to_plan(\n            \"Rust\",\n            collect_directory_moves(rust_ordering, root_path),\n        ));\n        renames.extend(directory_moves_to_plan(\n            \"Julia\",\n            collect_directory_moves(julia_ordering, root_path),\n        ));\n        let cluster_plans = collect_cluster_plans(clusters, root_path);\n        let cluster_items = collect_cluster_items(&cluster_plans);\n        let mut utility_names = BTreeSet::new();\n        for placement in placements {\n            if placement.call_analysis.calls_from_other_files.len() >= 3 {\n                utility_names.insert(placement.name.clone());\n            }\n        }\n        let mut moves = collect_move_items(placements, &utility_names, directory, root_path);\n\n        // MECHANICAL CONSTRAINT ENFORCEMENT\n        // Filter moves by invariant constraints to prevent unsafe refactorings\n        if let Ok(constraints_json) = std::fs::read_to_string(\n            std::path::Path::new(&self.output_dir).join(\"96_constraints/refactor_constraints.json\")\n        ) {\n            use crate::refactor_constraints::RefactorConstraint;\n            if let Ok(constraints) = serde_json::from_str::<Vec<RefactorConstraint>>(&constraints_json) {\n                let mut blocked_count = 0;\n\n                moves.retain(|m| {\n                    // Check if this move violates any constraint\n                    use crate::action_validator::check_move_allowed;\n\n                    // Skip items without name or current/target file paths\n                    let name = match &m.name {\n                        Some(n) => n.clone(),\n                        None => return true, // Keep if no name to check\n                    };\n\n                    let from = match &m.current_file {\n                        Some(f) => f.clone(),\n                        None => return true,\n                    };\n\n                    let to = match &m.target_file {\n                        Some(t) => t.clone(),\n                        None => return true,\n                    };\n\n                    match check_move_allowed(&name, &from, &to, &constraints) {\n                        Ok(_) => true,  // Allowed\n                        Err(reason) => {\n                            // Log rejection (only in verbose mode to avoid clutter)\n                            if std::env::var(\"VERBOSE\").is_ok() {\n                                eprintln!(\"  BLOCKED: {} - {}\", name, reason);\n                            }\n                            blocked_count += 1;\n                            false  // Filtered out\n                        }\n                    }\n                });\n\n                if blocked_count > 0 {\n                    println!(\" Constraint enforcement: {} moves allowed, {} blocked by invariants\",\n                             moves.len(), blocked_count);\n                }\n            }\n        }\n\n        let (orphaned, delete_candidates) =\n            filter_orphaned(placements, root_path, &self.output_dir);\n\n        let mut all_items = Vec::new();\n        all_items.extend(cluster_items.iter().cloned());\n        all_items.extend(renames.iter().cloned());\n        all_items.extend(moves.iter().cloned());\n\n        let mut correctness: Vec<PlanItem> = Vec::new();\n        let mut clusters_phase = Vec::new();\n        let mut structural = Vec::new();\n        let mut cohesion = Vec::new();\n        let mut ordering = Vec::new();\n\n        for item in all_items {\n            match item.kind {\n                ActionKind::Cluster => clusters_phase.push(item),\n                ActionKind::Structural => structural.push(item),\n                ActionKind::Cohesion => cohesion.push(item),\n                ActionKind::Ordering => ordering.push(item),\n            }\n        }\n\n        sort_plan_items(&mut correctness);\n        sort_cluster_items(&mut clusters_phase);\n        sort_structural_items(&mut structural);\n        sort_plan_items(&mut cohesion);\n        sort_plan_items(&mut ordering);\n\n        let config = load_report_config(&self.output_dir);\n        let dir_cohesion = compute_directory_cohesion(placements);\n        let avg_cohesion = if placements.is_empty() {\n            0.0\n        } else {\n            placements\n                .iter()\n                .map(|p| p.cohesion_score)\n                .sum::<f64>()\n                / placements.len() as f64\n        };\n        let ordering_correctness = compute_ordering_correctness(rust_ordering, julia_ordering);\n        let relocations = placements\n            .iter()\n            .filter(|p| matches!(p.placement_status, PlacementStatus::ShouldMove { .. }\n                | PlacementStatus::LayerViolation { .. }))\n            .count();\n\n        write_baseline_metrics(\n            &config,\n            &self.output_dir,\n            dir_cohesion,\n            ordering_correctness,\n            avg_cohesion,\n            renames.len(),\n            relocations,\n        );\n\n        let mut summary = String::from(\"# Refactoring Plan\\n\\n\");\n        summary.push_str(&format!(\"Generated: {}\\n\\n\", generated_at));\n        summary.push_str(\"## Summary\\n\\n\");\n        summary.push_str(\"Action: use this as the quick status snapshot for planning work.\\n\");\n        summary.push_str(\"Note: counts are derived from current analysis output.\\n\\n\");\n        summary.push_str(&format!(\n            \"- File/dir renames: {}\\n- Function moves: {}\\n- Orphaned functions: {}\\n- Clusters: {}\\n\\n\",\n            renames.len(),\n            moves.len(),\n            orphaned.len(),\n            clusters.len()\n        ));\n\n        let mut metrics = String::new();\n        metrics.push_str(\"## Metrics\\n\\n\");\n        metrics.push_str(\"Action: monitor trends and regressions across runs.\\n\");\n        metrics.push_str(\"Note: compare against baseline metrics when available.\\n\\n\");\n        metrics.push_str(&format!(\n            \"- Directory cohesion: {:.2}\\n- Ordering correctness: {:.1}%\\n- Avg function cohesion: {:.2}\\n- Rename ops needed: {}\\n- Function relocations suggested: {}\\n\\n\",\n            dir_cohesion,\n            ordering_correctness * 100.0,\n            avg_cohesion,\n            renames.len(),\n            relocations\n        ));\n\n        let mut baseline_section = String::new();\n        if let Some(baseline) = load_baseline_metrics(&config, &self.output_dir) {\n            let mut regression_warnings = Vec::new();\n            let epsilon = 0.005;\n            if let Some(prev) = baseline.get(\"directory_cohesion\") {\n                if dir_cohesion + epsilon < *prev {\n                    regression_warnings.push(format!(\n                        \"Directory cohesion dropped from {:.2} to {:.2}.\",\n                        prev, dir_cohesion\n                    ));\n                }\n            }\n            if let Some(prev) = baseline.get(\"ordering_correctness\") {\n                let current = ordering_correctness * 100.0;\n                if current + epsilon < *prev {\n                    regression_warnings.push(format!(\n                        \"Ordering correctness dropped from {:.1}% to {:.1}%.\",\n                        prev, current\n                    ));\n                }\n            }\n            if let Some(prev) = baseline.get(\"avg_function_cohesion\") {\n                if avg_cohesion + epsilon < *prev {\n                    regression_warnings.push(format!(\n                        \"Avg function cohesion dropped from {:.2} to {:.2}.\",\n                        prev, avg_cohesion\n                    ));\n                }\n            }\n            if let Some(prev) = baseline.get(\"rename_ops_needed\") {\n                if (renames.len() as f64) > *prev + epsilon {\n                    regression_warnings.push(format!(\n                        \"Rename ops needed increased from {:.0} to {}.\",\n                        prev,\n                        renames.len()\n                    ));\n                }\n            }\n            if let Some(prev) = baseline.get(\"function_relocations\") {\n                if (relocations as f64) > *prev + epsilon {\n                    regression_warnings.push(format!(\n                        \"Function relocations suggested increased from {:.0} to {}.\",\n                        prev,\n                        relocations\n                    ));\n                }\n            }\n\n            let deltas = baseline_deltas(\n                &baseline,\n                dir_cohesion,\n                ordering_correctness,\n                avg_cohesion,\n                renames.len(),\n                relocations,\n            );\n            if !deltas.is_empty() {\n                println!(\"Baseline deltas:\");\n                for line in &deltas {\n                    println!(\"  {}\", line);\n                }\n            }\n\n            baseline_section.push_str(\"## Baseline Regression Warnings\\n\\n\");\n            baseline_section\n                .push_str(\"Action: investigate any regressions before proceeding with refactors.\\n\");\n            baseline_section.push_str(\"Note: derived from the last saved baseline metrics.\\n\\n\");\n            if regression_warnings.is_empty() {\n                baseline_section.push_str(\"- None.\\n\\n\");\n            } else {\n                for warning in regression_warnings {\n                    baseline_section.push_str(&format!(\"- {}\\n\", warning));\n                }\n                baseline_section.push('\\n');\n            }\n        } else {\n            baseline_section.push_str(\"## Baseline Regression Warnings\\n\\n\");\n            baseline_section\n                .push_str(\"Action: save a baseline to enable regression tracking.\\n\");\n            baseline_section\n                .push_str(\"Note: baseline metrics file not found for this output directory.\\n\\n\");\n            baseline_section.push_str(\"- None.\\n\\n\");\n        }\n\n        let mut phase1 = String::new();\n        write_priority_section(&mut phase1, \"Phase 1: Correctness Blockers\", &correctness);\n\n        let mut phase2 = String::new();\n        write_priority_section(&mut phase2, \"Phase 2: Cluster Extraction\", &clusters_phase);\n        write_cluster_tips(&mut phase2, &cluster_plans);\n        write_cluster_batches(&mut phase2, &cluster_plans, root_path);\n\n        let mut phase3 = String::new();\n        write_priority_section(&mut phase3, \"Phase 3: Structural Constraints\", &structural);\n        write_structural_tips(&mut phase3, &structural);\n        write_structural_batches(&mut phase3, &structural);\n\n        let mut phase4 = String::new();\n        write_priority_section(&mut phase4, \"Phase 4: Cohesion Improvements\", &cohesion);\n\n        let mut phase5 = String::new();\n        write_priority_section(&mut phase5, \"Phase 5: Ordering & Renames\", &ordering);\n\n        let mut orphaned_section = String::new();\n        orphaned_section.push_str(\"## Orphaned Functions (Review Only)\\n\\n\");\n        orphaned_section.push_str(\"Action: review each item for expected usage. Delete only if it also appears under \\\"Delete Candidates (Orphaned + Dead Code)\\\".\\n\");\n        orphaned_section.push_str(\"Note: excludes public symbols referenced by other modules and entry points. Delete candidates require dead_code warnings.\\n\\n\");\n        if orphaned.is_empty() {\n            orphaned_section.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for entry in &orphaned {\n                let file = compress_path(entry.current_file.to_string_lossy().as_ref());\n                orphaned_section.push_str(&format!(\"- `{}` in `{}`\\n\", entry.name, file));\n            }\n            orphaned_section.push('\\n');\n        }\n\n        let mut delete_section = String::new();\n        delete_section.push_str(\"## Delete Candidates (Orphaned + Dead Code)\\n\\n\");\n        delete_section.push_str(\"Action: consider removal after confirming behavior and running tests.\\n\");\n        delete_section.push_str(\"Note: derived from orphaned list plus compiler dead_code warnings.\\n\\n\");\n        if delete_candidates.is_empty() {\n            delete_section.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for entry in &delete_candidates {\n                let file = compress_path(entry.current_file.to_string_lossy().as_ref());\n                delete_section.push_str(&format!(\"- `{}` in `{}`\\n\", entry.name, file));\n            }\n            delete_section.push('\\n');\n        }\n\n        let mut cluster_suggestions = String::new();\n        cluster_suggestions.push_str(\"## Suggested New Files (Clusters)\\n\\n\");\n        cluster_suggestions.push_str(\"Action: consider creating these files to improve cohesion.\\n\");\n        cluster_suggestions.push_str(\"Note: suggestions are heuristic and should be validated.\\n\\n\");\n        if clusters.is_empty() {\n            cluster_suggestions.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for cluster in clusters {\n                let suggested = cluster\n                    .suggested_file\n                    .as_ref()\n                    .map(|p| compress_path(p.to_string_lossy().as_ref()))\n                    .unwrap_or_else(|| \"new module\".to_string());\n                let members = cluster\n                    .members\n                    .iter()\n                    .map(|m| compress_path(m))\n                    .collect::<Vec<_>>()\n                    .join(\", \");\n                cluster_suggestions.push_str(&format!(\n                    \"- cohesion {:.2}, suggested `{}`\\n  - {}\\n\",\n                    cluster.cohesion, suggested, members\n                ));\n            }\n            cluster_suggestions.push('\\n');\n        }\n\n        let utility_candidates = collect_utility_candidates(placements);\n        let mut utility_section = String::new();\n        utility_section.push_str(\"## Utility Module Candidates\\n\\n\");\n        utility_section.push_str(\"Action: consider consolidating these into a shared utilities module.\\n\");\n        utility_section.push_str(\"Note: candidates are based on cross-file usage frequency.\\n\\n\");\n        if utility_candidates.is_empty() {\n            utility_section.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for candidate in utility_candidates {\n                utility_section.push_str(&format!(\"- {}\\n\", candidate));\n            }\n            utility_section.push('\\n');\n        }\n\n        let mut naming_warnings = Vec::new();\n        let _ = collect_naming_warnings(directory, &config, &mut naming_warnings);\n        let mut naming_section = String::new();\n        naming_section.push_str(\"## Naming Warnings\\n\\n\");\n        naming_section.push_str(\"Action: rename files if the suggested name improves ordering clarity.\\n\");\n        naming_section.push_str(\"Note: `_old` paths are excluded from naming warnings.\\n\\n\");\n        if naming_warnings.is_empty() {\n            naming_section.push_str(\"- None.\\n\\n\");\n        } else {\n            for warning in naming_warnings {\n                naming_section.push_str(&format!(\"- {}\\n\", warning));\n            }\n            naming_section.push('\\n');\n        }\n\n        let mut size_warnings = Vec::new();\n        collect_size_warnings(directory, &config, &mut size_warnings);\n        let mut size_section = String::new();\n        size_section.push_str(\"## Size Warnings\\n\\n\");\n        size_section.push_str(\"Action: consider extracting helpers to reduce file size.\\n\");\n        size_section.push_str(\"Note: thresholds come from report configuration.\\n\\n\");\n        if size_warnings.is_empty() {\n            size_section.push_str(\"- None.\\n\\n\");\n        } else {\n            for warning in size_warnings {\n                size_section.push_str(&format!(\"- {}\\n\", warning));\n            }\n            size_section.push('\\n');\n        }\n\n        let mut cargo_section = String::new();\n        if let Some(warnings) = load_cargo_warnings(&self.output_dir) {\n            cargo_section.push_str(\"## Cargo Warnings\\n\\n\");\n            cargo_section.push_str(\"Action: address compiler warnings before major refactors.\\n\");\n            cargo_section.push_str(\"Note: captured from cargo check/test outputs.\\n\\n\");\n            if warnings.trim().is_empty() {\n                cargo_section.push_str(\"- None.\\n\\n\");\n            } else {\n                cargo_section.push_str(\"```text\\n\");\n                cargo_section.push_str(warnings.trim());\n                cargo_section.push_str(\"\\n```\\n\\n\");\n            }\n        } else {\n            cargo_section.push_str(\"## Cargo Warnings\\n\\n\");\n            cargo_section.push_str(\"Action: address compiler warnings before major refactors.\\n\");\n            cargo_section.push_str(\"Note: no cargo warnings captured in this run.\\n\\n\");\n            cargo_section.push_str(\"- None.\\n\\n\");\n        }\n\n        let mut correction_section = String::new();\n        if let Some(report) = correction_report {\n            correction_section.push_str(\"## Correction Intelligence\\n\\n\");\n            correction_section.push_str(\"Action: review correction plans and verification policies before execution.\\n\");\n            correction_section.push_str(\"Note: generated for mmsb-executor ingestion; analyzer does not mutate code.\\n\\n\");\n            correction_section.push_str(&format!(\n                \"- Actions analyzed: {}\\n- Trivial: {}\\n- Moderate: {}\\n- Complex: {}\\n- Avg confidence: {:.1}%\\n\\n\",\n                report.actions_analyzed,\n                report.summary.trivial_count,\n                report.summary.moderate_count,\n                report.summary.complex_count,\n                report.summary.average_confidence * 100.0\n            ));\n            correction_section.push_str(\n                \"- JSON: `97_correction_intelligence/correction_intelligence.json`\\n\",\n            );\n            correction_section.push_str(\n                \"- Verification policy: `97_correction_intelligence/verification_policy.json`\\n\\n\",\n            );\n        }\n\n        let mut files = Vec::new();\n        files.push((\"00_summary.md\", summary));\n        files.push((\"01_metrics.md\", metrics));\n        files.push((\"02_baseline_regressions.md\", baseline_section));\n        files.push((\"03_phase1_correctness.md\", phase1));\n        files.push((\"04_phase2_clusters.md\", phase2));\n        files.push((\"05_phase3_structural.md\", phase3));\n        files.push((\"06_phase4_cohesion.md\", phase4));\n        files.push((\"07_phase5_ordering_renames.md\", phase5));\n        files.push((\"08_orphaned_functions.md\", orphaned_section));\n        files.push((\"09_delete_candidates.md\", delete_section));\n        files.push((\"10_suggested_new_files.md\", cluster_suggestions));\n        files.push((\"11_utility_module_candidates.md\", utility_section));\n        files.push((\"12_naming_warnings.md\", naming_section));\n        files.push((\"13_size_warnings.md\", size_section));\n        files.push((\"14_cargo_warnings.md\", cargo_section));\n        if correction_report.is_some() {\n            files.push((\"15_correction_intelligence.md\", correction_section));\n        }\n\n        let mut index = String::from(\"# Refactoring Plan Index\\n\\n\");\n        index.push_str(&format!(\"Generated: {}\\n\\n\", generated_at));\n        for (name, _) in &files {\n            index.push_str(&format!(\"- `{}`\\n\", name));\n        }\n        fs::write(dir.join(\"index.md\"), index)?;\n        for (name, content) in files {\n            fs::write(dir.join(name), content)?;\n        }\n        Ok(())\n    }\n\n    fn generate_file_organization_report(\n        &self,\n        directory: &DirectoryAnalysis,\n        _rust_ordering: &FileOrderingResult,\n        _julia_ordering: &FileOrderingResult,\n        root_path: &Path,\n    ) -> Result<()> {\n        let dir = self.prepare_report_dir(\"90_file_organization\")?;\n        let mut index = String::from(\"# File Organization Report\\n\\n\");\n        index.push_str(&format!(\n            \"Generated: {}\\n\\n\",\n            chrono::Local::now().format(\"%Y-%m-%d %H:%M:%S\")\n        ));\n\n        let mut entries = Vec::new();\n        collect_directories(directory, &mut entries);\n        for entry in &entries {\n            if entry.files.is_empty() {\n                continue;\n            }\n            let file_map = build_directory_entry_map(&entry.files)?;\n            let relative = entry\n                .path\n                .strip_prefix(root_path)\n                .unwrap_or(&entry.path)\n                .to_path_buf();\n            let slug = slugify_path(&relative);\n            let file_name = format!(\"{}.md\", slug);\n            index.push_str(&format!(\n                \"- `{}`  `{}`\\n\",\n                compress_path(entry.path.to_string_lossy().as_ref()),\n                file_name\n            ));\n\n            let mut content = format!(\n                \"# Directory: {}\\n\\n\",\n                compress_path(entry.path.to_string_lossy().as_ref())\n            );\n            content.push_str(&format!(\"- Layer: `{}`\\n\\n\", entry.layer));\n            content.push_str(\"## Files\\n\\n\");\n            content.push_str(\"| File | Suggested | Rename |\\n\");\n            content.push_str(\"| --- | --- | --- |\\n\");\n            let mut files = entry.files.clone();\n            files.sort();\n            for file in files {\n                let entry_info = file_map.get(&file);\n                let suggested = entry_info\n                    .map(|info| info.suggested_name.as_str())\n                    .unwrap_or(\"-\");\n                let rename = entry_info\n                    .map(|info| if info.needs_rename { \"yes\" } else { \"no\" })\n                    .unwrap_or(\"no\");\n                content.push_str(&format!(\n                    \"| `{}` | `{}` | {} |\\n\",\n                    compress_path(file.to_string_lossy().as_ref()),\n                    suggested,\n                    rename\n                ));\n            }\n            content.push('\\n');\n\n            content.push_str(\"## Dependency Graph\\n\\n\");\n            if entry.files.is_empty() {\n                content.push_str(\"No source files.\\n\\n\");\n            } else {\n                let graph = build_file_dependency_graph(&entry.files)?;\n                content.push_str(&render_mermaid_graph(&graph));\n                content.push('\\n');\n            }\n\n            fs::write(dir.join(file_name), content)?;\n        }\n\n        fs::write(dir.join(\"index.md\"), index)?;\n        Ok(())\n    }\n\n    fn write_layer_section(\n        &self,\n        content: &mut String,\n        label: &str,\n        graph: &LayerGraph,\n        root_path: &Path,\n    ) {\n        content.push_str(&format!(\"## {} Layer Graph\\n\\n\", label));\n\n        if graph.ordered_layers.is_empty() {\n            content.push_str(\"No layers discovered.\\n\\n\");\n            return;\n        }\n\n        content.push_str(\"### Layer Order\\n\");\n        for (idx, layer) in graph.ordered_layers.iter().enumerate() {\n            let cycle_tag = if graph.cycles.contains(layer) {\n                \" (cycle)\"\n            } else {\n                \"\"\n            };\n            content.push_str(&format!(\"{}. `{}`{}\\n\", idx + 1, layer, cycle_tag));\n        }\n        content.push('\\n');\n\n        if !graph.cycles.is_empty() {\n            content.push_str(\"### Cycles Detected\\n\");\n            for cycle in &graph.cycles {\n                content.push_str(&format!(\"- `{}`\\n\", cycle));\n            }\n            content.push('\\n');\n        }\n\n        let violations: Vec<_> = graph.edges.iter().filter(|e| e.violation).collect();\n        content.push_str(\"### Layer Violations\\n\");\n        if violations.is_empty() {\n            content.push_str(\"- None detected.\\n\\n\");\n        } else {\n            for edge in violations {\n                content.push_str(&format!(\n                    \"- `{}` depends on `{}` ({} references)\\n\",\n                    edge.to,\n                    edge.from,\n                    edge.references.len()\n                ));\n                for reference in &edge.references {\n                    let compressed = display_path(&reference.file, root_path);\n                    content.push_str(&format!(\"  - {} :: {}\\n\", compressed, reference.reference));\n                }\n            }\n            content.push('\\n');\n        }\n\n        content.push_str(\"### Dependency Edges\\n\");\n        if graph.edges.is_empty() {\n            content.push_str(\"- No cross-layer dependencies recorded.\\n\\n\");\n        } else {\n            for edge in &graph.edges {\n                content.push_str(&format!(\n                    \"- `{}`  `{}` ({} references{})\\n\",\n                    edge.from,\n                    edge.to,\n                    edge.references.len(),\n                    if edge.violation { \", VIOLATION\" } else { \"\" }\n                ));\n                for reference in &edge.references {\n                    let compressed = display_path(&reference.file, root_path);\n                    content.push_str(&format!(\"  - {} :: {}\\n\", compressed, reference.reference));\n                }\n            }\n            content.push('\\n');\n        }\n\n        content.push_str(\"### Unresolved References\\n\");\n        if graph.unresolved.is_empty() {\n            content.push_str(\"- None.\\n\\n\");\n        } else {\n            for unresolved in &graph.unresolved {\n                let compressed = display_path(&unresolved.file, root_path);\n                content.push_str(&format!(\"- {}  `{}`\\n\", compressed, unresolved.reference));\n            }\n            content.push('\\n');\n        }\n    }\n\n    fn generate_module_dependencies(&self, result: &AnalysisResult) -> Result<()> {\n        let dir = self.prepare_report_dir(\"40_module_dependencies\")?;\n        let index_path = dir.join(\"index.md\");\n        let mut index = String::from(\"# Module Dependencies\\n\\n\");\n\n        if result.modules.is_empty() {\n            index.push_str(\"No module metadata captured yet.\\n\");\n            fs::write(index_path, index)?;\n            return Ok(());\n        }\n\n        let mut modules_by_file: BTreeMap<String, ModuleAggregate> = BTreeMap::new();\n        for module in &result.modules {\n            let layer = self.extract_layer_from_path(&module.file_path);\n            let entry = modules_by_file\n                .entry(module.file_path.clone())\n                .or_insert_with(|| ModuleAggregate::new(module.name.clone(), layer.clone()));\n\n            if entry.name == \"unknown\" && !module.name.is_empty() {\n                entry.name = module.name.clone();\n            }\n\n            entry.layer = layer;\n            for import in &module.imports {\n                entry.imports.insert(normalize_use_stmt(import));\n            }\n            for export in &module.exports {\n                entry.exports.insert(normalize_use_stmt(export));\n            }\n            for sub in &module.submodules {\n                entry.submodules.insert(sub.clone());\n            }\n        }\n\n        let total_imports: usize = modules_by_file.values().map(|m| m.imports.len()).sum();\n        let total_exports: usize = modules_by_file.values().map(|m| m.exports.len()).sum();\n        let total_submodules: usize = modules_by_file.values().map(|m| m.submodules.len()).sum();\n\n        let mut modules: Vec<_> = modules_by_file.into_iter().collect();\n        modules.sort_by(|a, b| a.0.cmp(&b.0));\n\n        index.push_str(&format!(\"- Module files analyzed: {}\\n\", modules.len()));\n        index.push_str(&format!(\"- Unique imports captured: {}\\n\", total_imports));\n        index.push_str(&format!(\"- Unique exports captured: {}\\n\", total_exports));\n        index.push_str(&format!(\n            \"- Submodule declarations captured: {}\\n\\n\",\n            total_submodules\n        ));\n        index.push_str(\"## Per-file Summary\\n\\n\");\n        for (file_path, module) in &modules {\n            let compressed = compress_path(file_path);\n            index.push_str(&format!(\n                \"- `{}`  module `{}` (layer {}, {} imports / {} exports / {} submodules)\\n\",\n                compressed,\n                module.name,\n                module.layer,\n                module.imports.len(),\n                module.exports.len(),\n                module.submodules.len()\n            ));\n        }\n        index.push_str(\n            \"\\n## Detailed Files\\n\\n- `010-imports.md`  expanded import lists\\n- `020-exports.md`  export statements\\n- `030-submodules.md`  nested module declarations\\n- `040-violations.md`  placeholder for future per-module violations\\n\",\n        );\n        fs::write(&index_path, index)?;\n\n        let mut imports_doc = String::from(\"# Module Imports\\n\\n\");\n        let mut has_imports = false;\n        for (file_path, module) in &modules {\n            if module.imports.is_empty() {\n                continue;\n            }\n            has_imports = true;\n            let compressed = compress_path(file_path);\n            imports_doc.push_str(&format!(\"## {} ({})\\n\\n\", compressed, module.layer));\n            imports_doc.push_str(&format!(\"Module `{}`\\n\\n\", module.name));\n            for import in &module.imports {\n                imports_doc.push_str(&format!(\"- `{}`\\n\", import));\n            }\n            imports_doc.push('\\n');\n        }\n        if !has_imports {\n            imports_doc.push_str(\"No imports captured across modules.\\n\");\n        }\n        fs::write(dir.join(\"010-imports.md\"), imports_doc)?;\n\n        let mut exports_doc = String::from(\"# Module Exports\\n\\n\");\n        let mut has_exports = false;\n        for (file_path, module) in &modules {\n            if module.exports.is_empty() {\n                continue;\n            }\n            has_exports = true;\n            let compressed = compress_path(file_path);\n            exports_doc.push_str(&format!(\"## {} ({})\\n\\n\", compressed, module.layer));\n            exports_doc.push_str(&format!(\"Module `{}`\\n\\n\", module.name));\n            for export in &module.exports {\n                exports_doc.push_str(&format!(\"- `{}`\\n\", export));\n            }\n            exports_doc.push('\\n');\n        }\n        if !has_exports {\n            exports_doc.push_str(\"No exports captured across modules.\\n\");\n        }\n        fs::write(dir.join(\"020-exports.md\"), exports_doc)?;\n\n        let mut subs_doc = String::from(\"# Submodules\\n\\n\");\n        let mut has_submodules = false;\n        for (file_path, module) in &modules {\n            if module.submodules.is_empty() {\n                continue;\n            }\n            has_submodules = true;\n            let compressed = compress_path(file_path);\n            subs_doc.push_str(&format!(\"## {} ({})\\n\\n\", compressed, module.layer));\n            subs_doc.push_str(&format!(\"Module `{}`\\n\\n\", module.name));\n            for sub in &module.submodules {\n                subs_doc.push_str(&format!(\"- `{}`\\n\", sub));\n            }\n            subs_doc.push('\\n');\n        }\n        if !has_submodules {\n            subs_doc.push_str(\"No nested modules recorded.\\n\");\n        }\n        fs::write(dir.join(\"030-submodules.md\"), subs_doc)?;\n\n        let mut violations_doc = String::from(\"# Module Violations\\n\\n\");\n        violations_doc.push_str(\n            \"Per-module import/export violations are not computed yet.\\n\\\nRefer to `60_layer_dependencies/index.md` for cross-layer problems.\\n\",\n        );\n        fs::write(dir.join(\"040-violations.md\"), violations_doc)?;\n\n        Ok(())\n    }\n\n    fn generate_function_analysis(&self, result: &AnalysisResult) -> Result<()> {\n        let dir = self.prepare_report_dir(\"50_function_analysis\")?;\n        let mut index = String::from(\"# Function Analysis\\n\\n\");\n\n        let functions: Vec<_> = result\n            .elements\n            .iter()\n            .filter(|e| matches!(e.element_type, ElementType::Function))\n            .collect();\n\n        index.push_str(&format!(\"## Total Functions: {}\\n\\n\", functions.len()));\n        index.push_str(\n            \"Functions are bucketed alphabetically so `ls 50_function_analysis/` advertises the range.\\n\\n\",\n        );\n\n        if functions.is_empty() {\n            fs::write(dir.join(\"index.md\"), index)?;\n            return Ok(());\n        }\n\n        let bucket_labels = [\"A-F\", \"G-M\", \"N-S\", \"T-Z\", \"Other\"];\n        let mut buckets: HashMap<&'static str, Vec<&CodeElement>> = HashMap::new();\n        for label in bucket_labels {\n            buckets.insert(label, Vec::new());\n        }\n\n        for func in &functions {\n            let label = function_bucket_label(&func.name);\n            buckets.entry(label).or_insert_with(Vec::new).push(func);\n        }\n\n        index.push_str(\"## Bucket Files\\n\\n\");\n        for (idx, label) in bucket_labels.iter().enumerate() {\n            let file_name = format!(\"{:03}-functions_{}.md\", (idx + 1) * 10, label);\n            let count = buckets.get(label).map(|v| v.len()).unwrap_or(0);\n            index.push_str(&format!(\n                \"- `{}`  `{}` ({} functions)\\n\",\n                label, file_name, count\n            ));\n        }\n        fs::write(dir.join(\"index.md\"), index)?;\n\n        for (idx, label) in bucket_labels.iter().enumerate() {\n            let mut funcs = buckets.remove(label).unwrap_or_default();\n            funcs.sort_by_key(|f| (&f.layer, &f.name));\n            let file_name = format!(\"{:03}-functions_{}.md\", (idx + 1) * 10, label);\n            let mut content = format!(\"# Functions {}\\n\\n\", label);\n\n            if funcs.is_empty() {\n                content.push_str(\"No functions fell into this range.\\n\");\n                fs::write(dir.join(file_name), content)?;\n                continue;\n            }\n\n            let mut layer_map: BTreeMap<String, Vec<&CodeElement>> = BTreeMap::new();\n            for func in funcs {\n                layer_map\n                    .entry(func.layer.clone())\n                    .or_insert_with(Vec::new)\n                    .push(func);\n            }\n\n            for (layer, entries) in layer_map {\n                content.push_str(&format!(\"## Layer: {}\\n\\n\", layer));\n\n                let mut rust_funcs: Vec<_> = entries\n                    .iter()\n                    .filter(|f| matches!(f.language, Language::Rust))\n                    .collect();\n                let mut julia_funcs: Vec<_> = entries\n                    .iter()\n                    .filter(|f| matches!(f.language, Language::Julia))\n                    .collect();\n\n                rust_funcs.sort_by_key(|f| &f.name);\n                julia_funcs.sort_by_key(|f| &f.name);\n\n                if !rust_funcs.is_empty() {\n                    content.push_str(\"### Rust Functions\\n\\n\");\n                    for func in rust_funcs {\n                        content.push_str(&format!(\"#### `{}`\\n\\n\", func.name));\n                        let compressed = compress_path(&func.file_path);\n                        content.push_str(&format!(\n                            \"- **File:** {}:{}\\n\",\n                            compressed, func.line_number\n                        ));\n                        content.push_str(&format!(\"- **Visibility:** {:?}\\n\", func.visibility));\n\n                        if !func.generic_params.is_empty() {\n                            content.push_str(&format!(\n                                \"- **Generics:** {}\\n\",\n                                func.generic_params.join(\", \")\n                            ));\n                        }\n\n                        if !func.calls.is_empty() {\n                            content.push_str(\"- **Calls:**\\n\");\n                            for call in &func.calls {\n                                content.push_str(&format!(\"  - `{}`\\n\", call));\n                            }\n                        }\n                        content.push_str(\"\\n\");\n                    }\n                }\n\n                if !julia_funcs.is_empty() {\n                    content.push_str(\"### Julia Functions\\n\\n\");\n                    for func in julia_funcs {\n                        content.push_str(&format!(\"#### `{}`\\n\\n\", func.name));\n                        let compressed = compress_path(&func.file_path);\n                        content.push_str(&format!(\n                            \"- **File:** {}:{}\\n\",\n                            compressed, func.line_number\n                        ));\n                        content.push_str(&format!(\"- **Signature:** `{}`\\n\", func.signature));\n\n                        if !func.calls.is_empty() {\n                            content.push_str(\"- **Calls:**\\n\");\n                            for call in &func.calls {\n                                content.push_str(&format!(\"  - `{}`\\n\", call));\n                            }\n                        }\n                        content.push_str(\"\\n\");\n                    }\n                }\n            }\n\n            fs::write(dir.join(file_name), content)?;\n        }\n\n        Ok(())\n    }\n\n    fn extract_layer_from_path(&self, path: &str) -> String {\n        for component in path.split('/') {\n            if component\n                .chars()\n                .next()\n                .map_or(false, |c| c.is_ascii_digit())\n            {\n                if let Some(pos) = component.find('_') {\n                    if component[..pos].chars().all(|c| c.is_ascii_digit()) {\n                        return component.to_string();\n                    }\n                }\n            }\n        }\n        \"root\".to_string()\n    }\n\n    fn dot_path_for(&self, compressed_path: &str) -> Option<String> {\n        let slug = slugify_file_path(compressed_path);\n        let rel = format!(\"30_cfg/dots/{}/call_graph.dot\", slug);\n        let absolute = Path::new(&self.output_dir).join(&rel);\n        if absolute.exists() {\n            Some(rel)\n        } else {\n            None\n        }\n    }\n}\n\nfn prefix_key_from_path(path: &str) -> String {\n    let relative = path.strip_prefix(\"MMSB/\").unwrap_or(path);\n    if relative.is_empty() {\n        return \"root\".to_string();\n    }\n    let parts: Vec<&str> = relative.split('/').collect();\n    if parts.len() == 1 {\n        return \"root\".to_string();\n    }\n    if parts[0] == \"src\" && parts.len() >= 2 {\n        return format!(\"{}/{}\", parts[0], parts[1]);\n    }\n    parts[0].to_string()\n}\n\nfn slugify_key(input: &str) -> String {\n    input\n        .chars()\n        .map(|c| match c {\n            '/' => '-',\n            ' ' => '_',\n            _ if c.is_ascii_alphanumeric() || c == '-' => c.to_ascii_lowercase(),\n            _ => '_',\n        })\n        .collect()\n}\n\nfn group_key_cmp(a: &str, b: &str) -> Ordering {\n    match (a == \"root\", b == \"root\") {\n        (true, true) => Ordering::Equal,\n        (true, false) => Ordering::Less,\n        (false, true) => Ordering::Greater,\n        _ => a.cmp(b),\n    }\n}\n\nfn function_bucket_label(name: &str) -> &'static str {\n    let first = name\n        .chars()\n        .find(|c| c.is_ascii_alphabetic())\n        .map(|c| c.to_ascii_uppercase())\n        .unwrap_or('#');\n\n    match first {\n        'A'..='F' => \"A-F\",\n        'G'..='M' => \"G-M\",\n        'N'..='S' => \"N-S\",\n        'T'..='Z' => \"T-Z\",\n        _ => \"Other\",\n    }\n}\n\nfn slugify_file_path(path: &str) -> String {\n    path.trim_start_matches(\"MMSB/\")\n        .replace('/', \"-\")\n        .replace('.', \"_\")\n        .to_lowercase()\n}\n\nfn language_label(language: &Language) -> &'static str {\n    match language {\n        Language::Rust => \"Rust\",\n        Language::Julia => \"Julia\",\n    }\n}\n\nfn visibility_label(vis: &Visibility) -> &'static str {\n    match vis {\n        Visibility::Public => \"pub\",\n        Visibility::Crate => \"pub(crate)\",\n        Visibility::Private => \"priv\",\n    }\n}\n\nfn short_signature(input: &str) -> String {\n    let collapsed = input.split_whitespace().collect::<Vec<_>>().join(\" \");\n    if collapsed.len() > 120 {\n        let mut truncated = collapsed.chars().take(117).collect::<String>();\n        truncated.push_str(\"...\");\n        truncated\n    } else {\n        collapsed\n    }\n}\n\nstruct ModuleAggregate {\n    name: String,\n    layer: String,\n    imports: BTreeSet<String>,\n    exports: BTreeSet<String>,\n    submodules: BTreeSet<String>,\n}\n\nimpl ModuleAggregate {\n    fn new(name: String, layer: String) -> Self {\n        Self {\n            name: if name.is_empty() {\n                \"unknown\".to_string()\n            } else {\n                name\n            },\n            layer,\n            imports: BTreeSet::new(),\n            exports: BTreeSet::new(),\n            submodules: BTreeSet::new(),\n        }\n    }\n}\n\nfn normalize_use_stmt(stmt: &str) -> String {\n    let collapsed = stmt.replace('\\n', \" \");\n    let mut cleaned = collapsed.split_whitespace().collect::<Vec<_>>().join(\" \");\n    if let Some(idx) = cleaned.find(';') {\n        cleaned.truncate(idx);\n    }\n    cleaned = cleaned.trim().to_string();\n    if cleaned.starts_with(\"pub\") {\n        if let Some(pos) = cleaned.find(' ') {\n            cleaned = cleaned[pos + 1..].trim().to_string();\n        }\n    }\n    if let Some(stripped) = cleaned.strip_prefix(\"use \") {\n        cleaned = stripped.trim().to_string();\n    }\n    cleaned\n}\n\nfn sanitize_mermaid_id(input: &str) -> String {\n    input\n        .chars()\n        .map(|c| if c.is_ascii_alphanumeric() { c } else { '_' })\n        .collect()\n}\n\nfn sanitize_mermaid_label(label: &str) -> String {\n    label.replace('\"', \"'\").replace('`', \"'\")\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/190_main.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/320_main.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/190_main.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/320_main.rs",
          "original_content": "#[path = \"000_invariant_types.rs\"]\nmod invariant_types;\n#[path = \"005_refactor_constraints.rs\"]\nmod refactor_constraints;\n#[path = \"010_scc_compressor.rs\"]\nmod scc_compressor;\n#[path = \"020_layer_inference.rs\"]\nmod layer_inference;\n#[path = \"030_fixpoint_solver.rs\"]\nmod fixpoint_solver;\n#[path = \"040_structural_detector.rs\"]\nmod structural_detector;\n#[path = \"050_semantic_detector.rs\"]\nmod semantic_detector;\n#[path = \"060_path_detector.rs\"]\nmod path_detector;\n#[path = \"070_invariant_integrator.rs\"]\nmod invariant_integrator;\n#[path = \"080_invariant_reporter.rs\"]\nmod invariant_reporter;\n#[path = \"082_conscience_graph.rs\"]\nmod conscience_graph;\n#[path = \"083_action_validator.rs\"]\nmod action_validator;\n#[path = \"085_agent_conscience.rs\"]\nmod agent_conscience;\n#[path = \"191_agent_cli.rs\"]\nmod agent_cli;\n#[path = \"000_cluster_001.rs\"]\nmod cluster_001;\n#[path = \"010_cluster_008.rs\"]\nmod cluster_008;\n#[path = \"020_cluster_010.rs\"]\nmod cluster_010;\n#[path = \"030_cluster_011.rs\"]\nmod cluster_011;\n#[path = \"050_cluster_006.rs\"]\nmod cluster_006;\n#[path = \"130_control_flow.rs\"]\nmod control_flow;\n#[path = \"120_directory_analyzer.rs\"]\nmod directory_analyzer;\n#[path = \"060_layer_core.rs\"]\nmod layer_core;\n#[path = \"070_layer_utilities.rs\"]\nmod layer_utilities;\n#[path = \"040_dependency.rs\"]\nmod dependency;\n#[path = \"090_utilities.rs\"]\nmod utilities;\n#[path = \"170_dot_exporter.rs\"]\nmod dot_exporter;\n#[path = \"110_cohesion_analyzer.rs\"]\nmod cohesion_analyzer;\n#[path = \"140_file_ordering.rs\"]\nmod file_ordering;\n#[path = \"150_julia_parser.rs\"]\nmod julia_parser;\n#[path = \"180_report.rs\"]\nmod report;\n#[path = \"210_dead_code_types.rs\"]\nmod dead_code_types;\n#[path = \"211_dead_code_attribute_parser.rs\"]\nmod dead_code_attribute_parser;\n#[path = \"212_dead_code_doc_comment_parser.rs\"]\nmod dead_code_doc_comment_parser;\n#[path = \"213_dead_code_call_graph.rs\"]\nmod dead_code_call_graph;\n#[path = \"214_dead_code_intent.rs\"]\nmod dead_code_intent;\n#[path = \"215_dead_code_test_boundaries.rs\"]\nmod dead_code_test_boundaries;\n#[path = \"216_dead_code_entrypoints.rs\"]\nmod dead_code_entrypoints;\n#[path = \"217_dead_code_classifier.rs\"]\nmod dead_code_classifier;\n#[path = \"218_dead_code_confidence.rs\"]\nmod dead_code_confidence;\n#[path = \"219_dead_code_actions.rs\"]\nmod dead_code_actions;\n#[path = \"220_dead_code_report.rs\"]\nmod dead_code_report;\n#[path = \"221_dead_code_filter.rs\"]\nmod dead_code_filter;\n#[path = \"222_dead_code_cli.rs\"]\nmod dead_code_cli;\n#[path = \"223_dead_code_policy.rs\"]\nmod dead_code_policy;\n#[path = \"224_dead_code_report_split.rs\"]\nmod dead_code_report_split;\n#[path = \"220_correction_plan_types.rs\"]\nmod correction_plan_types;\n#[path = \"221_verification_policy_types.rs\"]\nmod verification_policy_types;\n#[path = \"222_quality_delta_types.rs\"]\nmod quality_delta_types;\n#[path = \"223_violation_predictor.rs\"]\nmod violation_predictor;\n#[path = \"224_tier_classifier.rs\"]\nmod tier_classifier;\n#[path = \"225_confidence_scorer.rs\"]\nmod confidence_scorer;\n#[path = \"226_correction_plan_generator.rs\"]\nmod correction_plan_generator;\n#[path = \"227_verification_scope_planner.rs\"]\nmod verification_scope_planner;\n#[path = \"228_rollback_criteria_builder.rs\"]\nmod rollback_criteria_builder;\n#[path = \"229_quality_delta_calculator.rs\"]\nmod quality_delta_calculator;\n#[path = \"230_action_impact_estimator.rs\"]\nmod action_impact_estimator;\n#[path = \"231_correction_plan_serializer.rs\"]\nmod correction_plan_serializer;\n#[path = \"232_verification_policy_emitter.rs\"]\nmod verification_policy_emitter;\n#[path = \"233_correction_intelligence_report.rs\"]\nmod correction_intelligence_report;\n#[path = \"160_rust_parser.rs\"]\nmod rust_parser;\n#[path = \"100_types.rs\"]\nmod types;\n\nuse anyhow::Result;\n\nfn main() -> Result<()> {\n    // Check if running in agent mode\n    let args: Vec<String> = std::env::args().collect();\n    if args.len() > 1 && args[1] == \"agent\" {\n        // Remove \"agent\" from args and run agent CLI\n        return agent_cli::run_agent_cli();\n    }\n\n    // Normal analyzer mode\n    crate::layer_utilities::main()\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/191_agent_cli.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/330_agent_cli.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/191_agent_cli.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/330_agent_cli.rs",
          "original_content": "//! Agent CLI Interface\n//!\n//! CLI for agents to query conscience and validate actions.\n\nuse crate::action_validator::AgentAction;\nuse crate::agent_conscience::AgentConscience;\nuse crate::invariant_types::Invariant;\nuse anyhow::Result;\nuse clap::Parser;\nuse std::path::PathBuf;\n\n#[derive(Parser, Debug)]\n#[command(name = \"mmsb-agent\")]\n#[command(about = \"Agent interface to MMSB conscience\", long_about = None)]\npub struct AgentCli {\n    #[command(subcommand)]\n    pub command: AgentCommand,\n}\n\n#[derive(Parser, Debug)]\npub enum AgentCommand {\n    /// Check if action is allowed\n    Check {\n        /// JSON file with action proposal\n        #[arg(short, long)]\n        action: PathBuf,\n\n        /// Conscience file (invariants.json)\n        #[arg(short, long)]\n        conscience: PathBuf,\n    },\n\n    /// Query allowed actions for function\n    Query {\n        /// Function name\n        #[arg(short, long)]\n        function: String,\n\n        /// Conscience file\n        #[arg(short, long)]\n        conscience: PathBuf,\n    },\n\n    /// List all blocking invariants\n    Invariants {\n        /// Conscience file\n        #[arg(short, long)]\n        conscience: PathBuf,\n\n        /// Show only blocking invariants\n        #[arg(short, long)]\n        blocking_only: bool,\n    },\n\n    /// Show conscience statistics\n    Stats {\n        /// Conscience file\n        #[arg(short, long)]\n        conscience: PathBuf,\n    },\n}\n\npub fn run_agent_cli() -> Result<()> {\n    let cli = AgentCli::parse();\n\n    match cli.command {\n        AgentCommand::Check { action, conscience } => {\n            check_action(&action, &conscience)?;\n        }\n\n        AgentCommand::Query {\n            function,\n            conscience,\n        } => {\n            query_function(&function, &conscience)?;\n        }\n\n        AgentCommand::Invariants {\n            conscience,\n            blocking_only,\n        } => {\n            list_invariants(&conscience, blocking_only)?;\n        }\n\n        AgentCommand::Stats { conscience } => {\n            show_stats(&conscience)?;\n        }\n    }\n\n    Ok(())\n}\n\n/// Check if an action is allowed\nfn check_action(action_path: &PathBuf, conscience_path: &PathBuf) -> Result<()> {\n    // Load action\n    let action_json = std::fs::read_to_string(action_path)?;\n    let action: AgentAction = serde_json::from_str(&action_json)?;\n\n    // Load conscience\n    let invariants = load_invariants(conscience_path)?;\n    let conscience = AgentConscience::new(invariants);\n\n    // Check action\n    let result = conscience.check_action(&action);\n\n    // Output JSON result\n    let output = serde_json::to_string_pretty(&result)?;\n    println!(\"{}\", output);\n\n    // Exit code: 0=allowed, 1=blocked\n    std::process::exit(if result.allowed { 0 } else { 1 });\n}\n\n/// Query allowed actions for a function\nfn query_function(function: &str, conscience_path: &PathBuf) -> Result<()> {\n    let invariants = load_invariants(conscience_path)?;\n    let conscience = AgentConscience::new(invariants);\n\n    let allowed = conscience.query_allowed_actions(function);\n\n    let output = serde_json::to_string_pretty(&allowed)?;\n    println!(\"{}\", output);\n\n    Ok(())\n}\n\n/// List all invariants\nfn list_invariants(conscience_path: &PathBuf, blocking_only: bool) -> Result<()> {\n    let invariants = load_invariants(conscience_path)?;\n\n    let filtered: Vec<_> = if blocking_only {\n        invariants\n            .iter()\n            .filter(|i| i.is_blocking())\n            .cloned()\n            .collect()\n    } else {\n        invariants\n    };\n\n    println!(\"Total invariants: {}\", filtered.len());\n\n    if blocking_only {\n        println!(\"(Showing only blocking invariants)\\n\");\n    }\n\n    let output = serde_json::to_string_pretty(&filtered)?;\n    println!(\"{}\", output);\n\n    Ok(())\n}\n\n/// Show conscience statistics\nfn show_stats(conscience_path: &PathBuf) -> Result<()> {\n    let invariants = load_invariants(conscience_path)?;\n    let conscience = AgentConscience::new(invariants);\n\n    let stats = conscience.stats();\n\n    println!(\"Conscience Statistics\");\n    println!(\"====================\\n\");\n    println!(\"Total invariants:    {}\", stats.total_invariants);\n    println!(\"Blocking invariants: {}\", stats.blocking_invariants);\n    println!(\"Total constraints:   {}\", stats.total_constraints);\n    println!();\n    println!(\"By strength:\");\n    println!(\"  Proven:     {}\", stats.proven_count);\n    println!(\"  Empirical:  {}\", stats.empirical_count);\n    println!(\"  Heuristic:  {}\", stats.heuristic_count);\n\n    Ok(())\n}\n\n/// Load invariants from JSON file\nfn load_invariants(path: &PathBuf) -> Result<Vec<Invariant>> {\n    let json = std::fs::read_to_string(path)?;\n    Ok(serde_json::from_str(&json)?)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_load_invariants_empty() {\n        // Write test JSON to a temp location\n        let temp_path = std::env::temp_dir().join(\"test_invariants.json\");\n        std::fs::write(&temp_path, \"[]\").unwrap();\n\n        let result = load_invariants(&temp_path);\n        assert!(result.is_ok());\n        assert_eq!(result.unwrap().len(), 0);\n\n        // Cleanup\n        let _ = std::fs::remove_file(&temp_path);\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/200_lib.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/340_lib.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/200_lib.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/340_lib.rs",
          "original_content": "//! MMSB Analyzer Library\n//!\n//! Provides code analysis capabilities for Rust and Julia projects.\n\n#[path = \"000_invariant_types.rs\"]\npub mod invariant_types;\n#[path = \"005_refactor_constraints.rs\"]\npub mod refactor_constraints;\n#[path = \"010_scc_compressor.rs\"]\npub mod scc_compressor;\n#[path = \"020_layer_inference.rs\"]\npub mod layer_inference;\n#[path = \"030_fixpoint_solver.rs\"]\npub mod fixpoint_solver;\n#[path = \"040_structural_detector.rs\"]\npub mod structural_detector;\n#[path = \"050_semantic_detector.rs\"]\npub mod semantic_detector;\n#[path = \"060_path_detector.rs\"]\npub mod path_detector;\n#[path = \"070_invariant_integrator.rs\"]\npub mod invariant_integrator;\n#[path = \"080_invariant_reporter.rs\"]\npub mod invariant_reporter;\n#[path = \"082_conscience_graph.rs\"]\npub mod conscience_graph;\n#[path = \"083_action_validator.rs\"]\npub mod action_validator;\n#[path = \"085_agent_conscience.rs\"]\npub mod agent_conscience;\n#[path = \"000_cluster_001.rs\"]\npub mod cluster_001;\n#[path = \"010_cluster_008.rs\"]\npub mod cluster_008;\n#[path = \"020_cluster_010.rs\"]\npub mod cluster_010;\n#[path = \"030_cluster_011.rs\"]\npub mod cluster_011;\n#[path = \"050_cluster_006.rs\"]\npub mod cluster_006;\n#[path = \"040_dependency.rs\"]\npub mod dependency;\n#[path = \"060_layer_core.rs\"]\npub mod layer_core;\n#[path = \"070_layer_utilities.rs\"]\npub mod layer_utilities;\n#[path = \"090_utilities.rs\"]\npub mod utilities;\n#[path = \"100_types.rs\"]\npub mod types;\n#[path = \"110_cohesion_analyzer.rs\"]\npub mod cohesion_analyzer;\n#[path = \"120_directory_analyzer.rs\"]\npub mod directory_analyzer;\n#[path = \"130_control_flow.rs\"]\npub mod control_flow;\n#[path = \"140_file_ordering.rs\"]\npub mod file_ordering;\n#[path = \"150_julia_parser.rs\"]\npub mod julia_parser;\n#[path = \"160_rust_parser.rs\"]\npub mod rust_parser;\n#[path = \"170_dot_exporter.rs\"]\npub mod dot_exporter;\n#[path = \"180_report.rs\"]\npub mod report;\n#[path = \"210_dead_code_types.rs\"]\npub mod dead_code_types;\n#[path = \"211_dead_code_attribute_parser.rs\"]\npub mod dead_code_attribute_parser;\n#[path = \"212_dead_code_doc_comment_parser.rs\"]\npub mod dead_code_doc_comment_parser;\n#[path = \"213_dead_code_call_graph.rs\"]\npub mod dead_code_call_graph;\n#[path = \"214_dead_code_intent.rs\"]\npub mod dead_code_intent;\n#[path = \"215_dead_code_test_boundaries.rs\"]\npub mod dead_code_test_boundaries;\n#[path = \"216_dead_code_entrypoints.rs\"]\npub mod dead_code_entrypoints;\n#[path = \"217_dead_code_classifier.rs\"]\npub mod dead_code_classifier;\n#[path = \"218_dead_code_confidence.rs\"]\npub mod dead_code_confidence;\n#[path = \"219_dead_code_actions.rs\"]\npub mod dead_code_actions;\n#[path = \"220_dead_code_report.rs\"]\npub mod dead_code_report;\n#[path = \"221_dead_code_filter.rs\"]\npub mod dead_code_filter;\n#[path = \"222_dead_code_cli.rs\"]\npub mod dead_code_cli;\n#[path = \"223_dead_code_policy.rs\"]\npub mod dead_code_policy;\n#[path = \"224_dead_code_report_split.rs\"]\npub mod dead_code_report_split;\n#[path = \"220_correction_plan_types.rs\"]\npub mod correction_plan_types;\n#[path = \"221_verification_policy_types.rs\"]\npub mod verification_policy_types;\n#[path = \"222_quality_delta_types.rs\"]\npub mod quality_delta_types;\n#[path = \"223_violation_predictor.rs\"]\npub mod violation_predictor;\n#[path = \"224_tier_classifier.rs\"]\npub mod tier_classifier;\n#[path = \"225_confidence_scorer.rs\"]\npub mod confidence_scorer;\n#[path = \"226_correction_plan_generator.rs\"]\npub mod correction_plan_generator;\n#[path = \"227_verification_scope_planner.rs\"]\npub mod verification_scope_planner;\n#[path = \"228_rollback_criteria_builder.rs\"]\npub mod rollback_criteria_builder;\n#[path = \"229_quality_delta_calculator.rs\"]\npub mod quality_delta_calculator;\n#[path = \"230_action_impact_estimator.rs\"]\npub mod action_impact_estimator;\n#[path = \"231_correction_plan_serializer.rs\"]\npub mod correction_plan_serializer;\n#[path = \"232_verification_policy_emitter.rs\"]\npub mod verification_policy_emitter;\n#[path = \"233_correction_intelligence_report.rs\"]\npub mod correction_intelligence_report;\n\npub use invariant_types::*;\npub use refactor_constraints::*;\npub use action_validator::{AgentAction, ConstraintViolation, ViolationSeverity};\npub use agent_conscience::{ActionPermission, AgentConscience};\npub use conscience_graph::{generate_conscience_map, generate_conscience_stats};\npub use dependency::{\n    julia_entry_paths, order_julia_files_by_dependency, order_rust_files_by_dependency, LayerGraph,\n    build_file_dependency_graph, analyze_file_ordering,\n};\npub use types::AnalysisResult;\npub use cohesion_analyzer::FunctionCohesionAnalyzer;\npub use directory_analyzer::DirectoryAnalyzer;\npub use control_flow::ControlFlowAnalyzer;\npub use file_ordering::{DagCache, parallel_build_file_dag};\npub use julia_parser::JuliaAnalyzer;\npub use rust_parser::RustAnalyzer;\npub use dot_exporter::{export_complete_program_dot, export_program_cfg_to_path};\npub use report::ReportGenerator;\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/210_dead_code_types.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/350_dead_code_types.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/210_dead_code_types.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/350_dead_code_types.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code analysis types.\n\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::path::PathBuf;\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum DeadCodeCategory {\n    Unreachable,\n    ReachableUnused,\n    TestOnly,\n    LatentPlanned,\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum ConfidenceLevel {\n    CallGraph,\n    TestReference,\n    IntentTag,\n    Heuristic,\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum RecommendedAction {\n    DeleteSafe,\n    Quarantine,\n    RelocateTests,\n    Keep,\n    ManualReview,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadCodeItem {\n    pub symbol: String,\n    pub file: PathBuf,\n    pub line: usize,\n    pub category: DeadCodeCategory,\n    pub confidence: ConfidenceLevel,\n    pub action: RecommendedAction,\n    pub reason: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct DeadCodeSummary {\n    pub unreachable: usize,\n    pub reachable_unused: usize,\n    pub test_only: usize,\n    pub latent_planned: usize,\n    pub total_analyzed: usize,\n}\n\nimpl DeadCodeSummary {\n    pub fn from_items(items: &[DeadCodeItem]) -> Self {\n        let mut summary = DeadCodeSummary::default();\n        summary.total_analyzed = items.len();\n        for item in items {\n            match item.category {\n                DeadCodeCategory::Unreachable => summary.unreachable += 1,\n                DeadCodeCategory::ReachableUnused => summary.reachable_unused += 1,\n                DeadCodeCategory::TestOnly => summary.test_only += 1,\n                DeadCodeCategory::LatentPlanned => summary.latent_planned += 1,\n            }\n        }\n        summary\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadCodeReport {\n    pub timestamp: String,\n    pub summary: DeadCodeSummary,\n    pub items: Vec<DeadCodeItem>,\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum IntentSource {\n    Attribute,\n    DocComment,\n    Directory,\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub enum IntentMarker {\n    Latent,\n    Planned,\n    Future,\n    DeprecatedPlanned,\n}\n\nimpl IntentMarker {\n    pub fn from_comment(comment: &str) -> Option<Self> {\n        let lower = comment.to_ascii_lowercase();\n        if lower.contains(\"@deprecated-planned\") {\n            return Some(IntentMarker::DeprecatedPlanned);\n        }\n        if lower.contains(\"@planned\") {\n            return Some(IntentMarker::Planned);\n        }\n        if lower.contains(\"@future\") {\n            return Some(IntentMarker::Future);\n        }\n        if lower.contains(\"@latent\") {\n            return Some(IntentMarker::Latent);\n        }\n        None\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct IntentMetadata {\n    pub marker: IntentMarker,\n    pub source: IntentSource,\n    pub value: Option<String>,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct IntentTag {\n    pub symbol: String,\n    pub file: PathBuf,\n    pub line: Option<usize>,\n    pub marker: IntentMarker,\n    pub source: IntentSource,\n    pub value: Option<String>,\n}\n\npub type IntentMap = HashMap<String, Vec<IntentMetadata>>;\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/211_dead_code_attribute_parser.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/360_dead_code_attribute_parser.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/211_dead_code_attribute_parser.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/360_dead_code_attribute_parser.rs",
          "original_content": "#![allow(dead_code)]\n//! Attribute parsing for dead code intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMetadata, IntentSource, IntentTag};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse syn::{Attribute, Item};\n\npub fn parse_mmsb_latent_attr(path: &Path) -> HashMap<String, Vec<IntentMetadata>> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMetadata>> = HashMap::new();\n    for item in &file.items {\n        let Some(name) = item_name(item) else {\n            continue;\n        };\n        let tags = collect_latent_attrs(item_attrs(item));\n        if tags.is_empty() {\n            continue;\n        }\n        map.entry(name).or_default().extend(tags);\n    }\n    map\n}\n\npub fn scan_file_attributes(path: &Path) -> Vec<IntentTag> {\n    let contents = std::fs::read_to_string(path).unwrap_or_default();\n    let file = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    let mut tags = Vec::new();\n    for item in &file.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        for meta in collect_latent_attrs(item_attrs(item)) {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: path.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n    tags\n}\n\npub fn extract_attribute_value(attr: &Attribute, key: &str) -> Option<String> {\n    let mut found = None;\n    let _ = attr.parse_nested_meta(|meta| {\n        if meta.path.is_ident(key) {\n            let value = meta.value()?;\n            let lit: syn::LitStr = value.parse()?;\n            found = Some(lit.value());\n        }\n        Ok(())\n    });\n    found\n}\n\nfn collect_latent_attrs(attrs: &[Attribute]) -> Vec<IntentMetadata> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"mmsb_latent\") {\n            continue;\n        }\n        let mut marker = IntentMarker::Latent;\n        let mut value = None;\n        let mut saw_nested = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            saw_nested = true;\n            if meta.path.is_ident(\"planned\") {\n                marker = IntentMarker::Planned;\n            } else if meta.path.is_ident(\"future\") {\n                marker = IntentMarker::Future;\n            } else if meta.path.is_ident(\"deprecated_planned\")\n                || meta.path.is_ident(\"deprecated-planned\")\n            {\n                marker = IntentMarker::DeprecatedPlanned;\n            } else if meta.path.is_ident(\"reason\") || meta.path.is_ident(\"note\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                value = Some(lit.value());\n            } else if meta.path.is_ident(\"marker\") {\n                let value_meta = meta.value()?;\n                let lit: syn::LitStr = value_meta.parse()?;\n                marker = marker_from_str(&lit.value());\n            }\n            Ok(())\n        });\n        if !saw_nested {\n            if let Ok(lit) = attr.parse_args::<syn::LitStr>() {\n                value = Some(lit.value());\n            }\n        }\n\n        markers.push(IntentMetadata {\n            marker,\n            source: IntentSource::Attribute,\n            value,\n        });\n    }\n    markers\n}\n\nfn marker_from_str(raw: &str) -> IntentMarker {\n    match raw.to_ascii_lowercase().as_str() {\n        \"planned\" => IntentMarker::Planned,\n        \"future\" => IntentMarker::Future,\n        \"deprecated_planned\" | \"deprecated-planned\" => IntentMarker::DeprecatedPlanned,\n        _ => IntentMarker::Latent,\n    }\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/212_dead_code_doc_comment_parser.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/370_dead_code_doc_comment_parser.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/212_dead_code_doc_comment_parser.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/370_dead_code_doc_comment_parser.rs",
          "original_content": "#![allow(dead_code)]\n//! Doc comment parsing for intent markers.\n\nuse crate::dead_code_types::{IntentMarker, IntentMap};\nuse std::collections::{HashMap, HashSet};\nuse std::path::Path;\nuse syn::{Attribute, Item, Meta, MetaNameValue};\n\npub fn scan_doc_comments(file: &Path) -> HashMap<String, Vec<IntentMarker>> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashMap::new(),\n    };\n    let mut map: HashMap<String, Vec<IntentMarker>> = HashMap::new();\n    for item in &parsed.items {\n        let Some(symbol) = item_name(item) else {\n            continue;\n        };\n        let markers = extract_doc_markers(item_attrs(item));\n        if markers.is_empty() {\n            continue;\n        }\n        map.entry(symbol).or_default().extend(markers);\n    }\n    map\n}\n\npub fn detect_latent_markers(comment: &str) -> Option<IntentMarker> {\n    IntentMarker::from_comment(comment)\n}\n\npub fn merge_doc_intent(map: HashMap<String, Vec<IntentMarker>>) -> IntentMap {\n    let mut merged = IntentMap::new();\n    for (symbol, markers) in map {\n        let mut uniques = HashSet::new();\n        for marker in markers {\n            if !uniques.insert(marker) {\n                continue;\n            }\n            merged\n                .entry(symbol.clone())\n                .or_default()\n                .push(crate::dead_code_types::IntentMetadata {\n                    marker,\n                    source: crate::dead_code_types::IntentSource::DocComment,\n                    value: None,\n                });\n        }\n    }\n    merged\n}\n\nfn extract_doc_markers(attrs: &[Attribute]) -> Vec<IntentMarker> {\n    let mut markers = Vec::new();\n    for attr in attrs {\n        if !attr.path().is_ident(\"doc\") {\n            continue;\n        }\n        let Meta::NameValue(MetaNameValue { value, .. }) = &attr.meta else {\n            continue;\n        };\n        let syn::Expr::Lit(expr_lit) = value else {\n            continue;\n        };\n        let syn::Lit::Str(lit) = &expr_lit.lit else {\n            continue;\n        };\n        if let Some(marker) = detect_latent_markers(&lit.value()) {\n            markers.push(marker);\n        }\n    }\n    markers\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/213_dead_code_call_graph.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/380_dead_code_call_graph.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/213_dead_code_call_graph.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/380_dead_code_call_graph.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code call graph utilities.\n\nuse crate::types::{CodeElement, ElementType, Language};\nuse std::collections::{HashMap, HashSet, VecDeque};\n\npub type CallGraph = HashMap<String, Vec<String>>;\n\npub fn build_call_graph(elements: &[CodeElement]) -> CallGraph {\n    let mut graph: CallGraph = HashMap::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let entry = graph.entry(element.name.clone()).or_default();\n        entry.extend(element.calls.iter().cloned());\n    }\n    graph\n}\n\npub fn build_reverse_call_graph(graph: &CallGraph) -> CallGraph {\n    let mut reverse: CallGraph = HashMap::new();\n    for (caller, callees) in graph {\n        for callee in callees {\n            reverse.entry(callee.clone()).or_default().push(caller.clone());\n        }\n    }\n    reverse\n}\n\npub fn compute_reachability(graph: &CallGraph, entrypoints: &HashSet<String>) -> HashSet<String> {\n    let mut reachable = HashSet::new();\n    let mut queue: VecDeque<String> = entrypoints.iter().cloned().collect();\n\n    while let Some(node) = queue.pop_front() {\n        if !reachable.insert(node.clone()) {\n            continue;\n        }\n        if let Some(callees) = graph.get(&node) {\n            for callee in callees {\n                if !reachable.contains(callee) {\n                    queue.push_back(callee.clone());\n                }\n            }\n        }\n    }\n\n    reachable\n}\n\npub fn is_reachable(\n    symbol: &str,\n    graph: &CallGraph,\n    entrypoints: &HashSet<String>,\n) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    compute_reachability(graph, entrypoints).contains(symbol)\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/214_dead_code_intent.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/390_dead_code_intent.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/214_dead_code_intent.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/390_dead_code_intent.rs",
          "original_content": "#![allow(dead_code)]\n//! Intent detection aggregation for dead code classification.\n\nuse crate::dead_code_attribute_parser::parse_mmsb_latent_attr;\nuse crate::dead_code_doc_comment_parser::{merge_doc_intent, scan_doc_comments};\nuse crate::dead_code_types::{\n    IntentMap, IntentMarker, IntentMetadata, IntentSource, IntentTag,\n};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\nuse syn::Item;\n\n#[derive(Debug, Clone, Default)]\npub struct DeadCodePolicy {\n    pub planned_directories: Vec<PathBuf>,\n    pub public_api_roots: Vec<PathBuf>,\n    pub entrypoint_symbols: Vec<String>,\n    pub treat_public_as_entrypoint: bool,\n}\n\npub fn detect_intent_signals(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    let attrs = parse_mmsb_latent_attr(file);\n    let doc_map = scan_doc_comments(file);\n    let docs = merge_doc_intent(doc_map);\n    let dir_map = planned_directory_intent(file, policy);\n    merge_intent_sources(attrs, docs, dir_map)\n}\n\npub fn check_planned_directory(path: &Path, policy: Option<&DeadCodePolicy>) -> bool {\n    let Some(policy) = policy else {\n        return false;\n    };\n    for dir in &policy.planned_directories {\n        if path.starts_with(dir) {\n            return true;\n        }\n    }\n    false\n}\n\npub fn merge_intent_sources(\n    attrs: IntentMap,\n    docs: IntentMap,\n    dir: IntentMap,\n) -> IntentMap {\n    let mut merged = IntentMap::new();\n    for (symbol, items) in attrs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in docs {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    for (symbol, items) in dir {\n        merged.entry(symbol).or_default().extend(items);\n    }\n    merged\n}\n\npub fn scan_intent_tags(file: &Path, policy: Option<&DeadCodePolicy>) -> Vec<IntentTag> {\n    let mut tags = Vec::new();\n    let attrs = parse_mmsb_latent_attr(file);\n    for (symbol, items) in attrs {\n        for meta in items {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker: meta.marker,\n                source: meta.source,\n                value: meta.value.clone(),\n            });\n        }\n    }\n\n    let doc_map = scan_doc_comments(file);\n    for (symbol, markers) in doc_map {\n        for marker in markers {\n            tags.push(IntentTag {\n                symbol: symbol.clone(),\n                file: file.to_path_buf(),\n                line: None,\n                marker,\n                source: IntentSource::DocComment,\n                value: None,\n            });\n        }\n    }\n\n    if check_planned_directory(file, policy) {\n        for symbol in collect_symbols(file) {\n            tags.push(IntentTag {\n                symbol,\n                file: file.to_path_buf(),\n                line: None,\n                marker: IntentMarker::Planned,\n                source: IntentSource::Directory,\n                value: None,\n            });\n        }\n    }\n\n    tags\n}\n\nfn planned_directory_intent(file: &Path, policy: Option<&DeadCodePolicy>) -> IntentMap {\n    if !check_planned_directory(file, policy) {\n        return IntentMap::new();\n    }\n    let mut map: IntentMap = HashMap::new();\n    for symbol in collect_symbols(file) {\n        map.entry(symbol).or_default().push(IntentMetadata {\n            marker: IntentMarker::Planned,\n            source: IntentSource::Directory,\n            value: None,\n        });\n    }\n    map\n}\n\nfn collect_symbols(file: &Path) -> Vec<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return Vec::new(),\n    };\n    parsed\n        .items\n        .iter()\n        .filter_map(item_name)\n        .collect()\n}\n\nfn item_name(item: &Item) -> Option<String> {\n    match item {\n        Item::Fn(item_fn) => Some(item_fn.sig.ident.to_string()),\n        Item::Struct(item_struct) => Some(item_struct.ident.to_string()),\n        Item::Enum(item_enum) => Some(item_enum.ident.to_string()),\n        Item::Mod(item_mod) => Some(item_mod.ident.to_string()),\n        Item::Trait(item_trait) => Some(item_trait.ident.to_string()),\n        _ => None,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/215_dead_code_test_boundaries.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/400_dead_code_test_boundaries.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/215_dead_code_test_boundaries.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/400_dead_code_test_boundaries.rs",
          "original_content": "#![allow(dead_code)]\n//! Test boundary detection for dead code classification.\n\nuse crate::dead_code_call_graph::{build_reverse_call_graph, CallGraph};\nuse std::collections::{HashSet, VecDeque};\nuse std::path::{Path, PathBuf};\nuse syn::{Attribute, Item};\n\n#[derive(Debug, Clone, Default)]\npub struct TestBoundaries {\n    pub test_modules: HashSet<String>,\n    pub test_symbols: HashSet<String>,\n    pub test_files: HashSet<PathBuf>,\n}\n\npub fn detect_test_modules(file: &Path) -> HashSet<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashSet::new(),\n    };\n    let mut modules = HashSet::new();\n    for item in &parsed.items {\n        if let Item::Mod(item_mod) = item {\n            if is_cfg_test_item(item) {\n                modules.insert(item_mod.ident.to_string());\n            }\n        }\n    }\n    modules\n}\n\npub fn detect_test_symbols(file: &Path) -> HashSet<String> {\n    let contents = std::fs::read_to_string(file).unwrap_or_default();\n    let parsed = match syn::parse_file(&contents) {\n        Ok(file) => file,\n        Err(_) => return HashSet::new(),\n    };\n    let mut symbols = HashSet::new();\n    for item in &parsed.items {\n        if let Item::Fn(item_fn) = item {\n            if has_test_attr(&item_fn.attrs) {\n                symbols.insert(item_fn.sig.ident.to_string());\n            }\n        }\n        if let Item::Mod(item_mod) = item {\n            if is_cfg_test_item(item) {\n                symbols.insert(item_mod.ident.to_string());\n                if let Some((_, items)) = &item_mod.content {\n                    for nested in items {\n                        if let Item::Fn(nested_fn) = nested {\n                            symbols.insert(nested_fn.sig.ident.to_string());\n                        }\n                    }\n                }\n            }\n        }\n    }\n    symbols\n}\n\npub fn is_cfg_test_item(item: &Item) -> bool {\n    item_attrs(item).iter().any(|attr| {\n        if !attr.path().is_ident(\"cfg\") {\n            return false;\n        }\n        let mut found = false;\n        let _ = attr.parse_nested_meta(|meta| {\n            if meta.path.is_ident(\"test\") {\n                found = true;\n                return Ok(());\n            }\n            if meta.path.is_ident(\"any\") {\n                meta.parse_nested_meta(|nested| {\n                    if nested.path.is_ident(\"test\") {\n                        found = true;\n                    }\n                    Ok(())\n                })?;\n            }\n            Ok(())\n        });\n        found\n    })\n}\n\npub fn find_test_callers(\n    symbol: &str,\n    call_graph: &CallGraph,\n    test_symbols: &HashSet<String>,\n) -> Vec<String> {\n    if test_symbols.is_empty() {\n        return Vec::new();\n    }\n    let reverse = build_reverse_call_graph(call_graph);\n    let mut callers = Vec::new();\n    let mut visited = HashSet::new();\n    let mut queue: VecDeque<String> = reverse\n        .get(symbol)\n        .cloned()\n        .unwrap_or_default()\n        .into_iter()\n        .collect();\n\n    while let Some(caller) = queue.pop_front() {\n        if !visited.insert(caller.clone()) {\n            continue;\n        }\n        if test_symbols.contains(&caller) {\n            callers.push(caller.clone());\n        }\n        if let Some(next) = reverse.get(&caller) {\n            for parent in next {\n                if !visited.contains(parent) {\n                    queue.push_back(parent.clone());\n                }\n            }\n        }\n    }\n\n    callers\n}\n\nfn has_test_attr(attrs: &[Attribute]) -> bool {\n    attrs.iter().any(|attr| {\n        let path = attr.path();\n        if path.is_ident(\"test\") {\n            return true;\n        }\n        let last = path.segments.last().map(|seg| seg.ident.to_string());\n        matches!(last.as_deref(), Some(\"test\"))\n    })\n}\n\nfn item_attrs(item: &Item) -> &[Attribute] {\n    match item {\n        Item::Fn(item_fn) => &item_fn.attrs,\n        Item::Struct(item_struct) => &item_struct.attrs,\n        Item::Enum(item_enum) => &item_enum.attrs,\n        Item::Mod(item_mod) => &item_mod.attrs,\n        Item::Trait(item_trait) => &item_trait.attrs,\n        _ => &[],\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/216_dead_code_entrypoints.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/410_dead_code_entrypoints.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/216_dead_code_entrypoints.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/410_dead_code_entrypoints.rs",
          "original_content": "#![allow(dead_code)]\n//! Entrypoint and export resolution for dead code analysis.\n\nuse crate::dead_code_intent::DeadCodePolicy;\nuse crate::types::{CodeElement, ElementType, Visibility};\nuse std::collections::HashSet;\nuse std::path::Path;\nuse walkdir::WalkDir;\n\npub fn collect_entrypoints(\n    elements: &[CodeElement],\n    policy: Option<&DeadCodePolicy>,\n) -> HashSet<String> {\n    let mut entrypoints = HashSet::new();\n    if let Some(policy) = policy {\n        for symbol in &policy.entrypoint_symbols {\n            entrypoints.insert(symbol.clone());\n        }\n    }\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.name == \"main\" {\n            entrypoints.insert(element.name.clone());\n            continue;\n        }\n        if matches!(element.visibility, Visibility::Public) && treat_public_as_entrypoint(policy) {\n            entrypoints.insert(element.name.clone());\n        }\n    }\n    entrypoints\n}\n\npub fn collect_exports(root: &Path) -> HashSet<String> {\n    let mut exports = HashSet::new();\n    let src_dir = root.join(\"src\");\n    for entry in WalkDir::new(src_dir).into_iter().filter_map(Result::ok) {\n        let path = entry.path();\n        if !path.is_file() {\n            continue;\n        }\n        if path.extension().and_then(|e| e.to_str()) != Some(\"rs\") {\n            continue;\n        }\n        let contents = std::fs::read_to_string(path).unwrap_or_default();\n        let parsed = match syn::parse_file(&contents) {\n            Ok(file) => file,\n            Err(_) => continue,\n        };\n        for item in parsed.items {\n            match item {\n                syn::Item::Use(item_use) => {\n                    if matches!(item_use.vis, syn::Visibility::Public(_)) {\n                        collect_use_tree_idents(&item_use.tree, &mut exports);\n                    }\n                }\n                syn::Item::Mod(item_mod) => {\n                    if matches!(item_mod.vis, syn::Visibility::Public(_)) {\n                        exports.insert(item_mod.ident.to_string());\n                    }\n                }\n                _ => {}\n            }\n        }\n    }\n    exports\n}\n\npub fn is_public_api(symbol: &str, exports: &HashSet<String>) -> bool {\n    exports.contains(symbol)\n}\n\nfn collect_use_tree_idents(tree: &syn::UseTree, exports: &mut HashSet<String>) {\n    match tree {\n        syn::UseTree::Name(name) => {\n            exports.insert(name.ident.to_string());\n        }\n        syn::UseTree::Rename(rename) => {\n            exports.insert(rename.rename.to_string());\n        }\n        syn::UseTree::Path(path) => {\n            collect_use_tree_idents(&path.tree, exports);\n        }\n        syn::UseTree::Group(group) => {\n            for item in &group.items {\n                collect_use_tree_idents(item, exports);\n            }\n        }\n        syn::UseTree::Glob(_) => {\n            exports.insert(\"*\".to_string());\n        }\n    }\n}\n\nfn treat_public_as_entrypoint(policy: Option<&DeadCodePolicy>) -> bool {\n    policy.map(|p| p.treat_public_as_entrypoint).unwrap_or(true)\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/217_dead_code_classifier.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/420_dead_code_classifier.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/217_dead_code_classifier.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/420_dead_code_classifier.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code category classification.\n\nuse crate::dead_code_call_graph::{build_reverse_call_graph, CallGraph};\nuse crate::dead_code_intent::DeadCodePolicy;\nuse crate::dead_code_test_boundaries::TestBoundaries;\nuse crate::dead_code_types::{DeadCodeCategory, IntentMap};\nuse std::collections::HashSet;\n\npub fn classify_symbol(\n    symbol: &str,\n    call_graph: &CallGraph,\n    intent_map: &IntentMap,\n    test_boundaries: &TestBoundaries,\n    entrypoints: &HashSet<String>,\n    _policy: Option<&DeadCodePolicy>,\n) -> DeadCodeCategory {\n    if intent_map.contains_key(symbol) {\n        return DeadCodeCategory::LatentPlanned;\n    }\n\n    if is_test_only(symbol, call_graph, test_boundaries) {\n        return DeadCodeCategory::TestOnly;\n    }\n\n    if !is_reachable(symbol, call_graph, entrypoints) {\n        return DeadCodeCategory::Unreachable;\n    }\n\n    DeadCodeCategory::ReachableUnused\n}\n\npub fn is_reachable(symbol: &str, call_graph: &CallGraph, entrypoints: &HashSet<String>) -> bool {\n    if entrypoints.is_empty() {\n        return false;\n    }\n    let reachable = crate::dead_code_call_graph::compute_reachability(call_graph, entrypoints);\n    reachable.contains(symbol)\n}\n\npub fn is_test_only(\n    symbol: &str,\n    call_graph: &CallGraph,\n    test_boundaries: &TestBoundaries,\n) -> bool {\n    if test_boundaries.test_symbols.contains(symbol) {\n        return true;\n    }\n    let reverse = build_reverse_call_graph(call_graph);\n    let callers = reverse.get(symbol);\n    let Some(callers) = callers else {\n        return false;\n    };\n    if callers.is_empty() {\n        return false;\n    }\n    callers\n        .iter()\n        .all(|caller| test_boundaries.test_symbols.contains(caller))\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/218_dead_code_confidence.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/430_dead_code_confidence.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/218_dead_code_confidence.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/430_dead_code_confidence.rs",
          "original_content": "#![allow(dead_code)]\n//! Confidence scoring for dead code analysis.\n\nuse crate::dead_code_types::{ConfidenceLevel, DeadCodeCategory, DeadCodeItem};\n\n#[derive(Debug, Clone, Default)]\npub struct Evidence {\n    pub intent_tag: bool,\n    pub test_reference: bool,\n    pub call_graph_proven: bool,\n}\n\npub fn assign_confidence(item: &DeadCodeItem, evidence: &Evidence) -> ConfidenceLevel {\n    if evidence.intent_tag || matches!(item.category, DeadCodeCategory::LatentPlanned) {\n        return ConfidenceLevel::IntentTag;\n    }\n    if evidence.test_reference || matches!(item.category, DeadCodeCategory::TestOnly) {\n        return ConfidenceLevel::TestReference;\n    }\n    if evidence.call_graph_proven || matches!(item.category, DeadCodeCategory::Unreachable) {\n        return ConfidenceLevel::CallGraph;\n    }\n    ConfidenceLevel::Heuristic\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/219_dead_code_actions.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/440_dead_code_actions.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/219_dead_code_actions.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/440_dead_code_actions.rs",
          "original_content": "#![allow(dead_code)]\n//! Action recommendations for dead code analysis.\n\nuse crate::dead_code_types::{ConfidenceLevel, DeadCodeCategory, RecommendedAction};\n\npub fn recommend_action(\n    category: DeadCodeCategory,\n    confidence: ConfidenceLevel,\n    is_public_api: bool,\n) -> RecommendedAction {\n    match (category, confidence) {\n        (DeadCodeCategory::Unreachable, ConfidenceLevel::CallGraph) if !is_public_api => {\n            RecommendedAction::DeleteSafe\n        }\n        (DeadCodeCategory::Unreachable, _) => RecommendedAction::ManualReview,\n        (DeadCodeCategory::ReachableUnused, _) => RecommendedAction::Quarantine,\n        (DeadCodeCategory::TestOnly, _) => RecommendedAction::RelocateTests,\n        (DeadCodeCategory::LatentPlanned, _) => RecommendedAction::Keep,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/220_correction_plan_types.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/450_correction_plan_types.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/220_correction_plan_types.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/450_correction_plan_types.rs",
          "original_content": "#![allow(dead_code)]\n//! Correction intelligence core types.\n\nuse serde::{Deserialize, Serialize};\nuse std::path::PathBuf;\n\n#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord)]\npub enum ErrorTier {\n    Trivial,\n    Moderate,\n    Complex,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub enum CorrectionStrategy {\n    AddImport { module_path: String, symbol: String },\n    UpdatePath { old_path: String, new_path: String },\n    ReExport { from_module: String, symbol: String },\n    RenameWithSuffix { original: String, suffix: String },\n    MoveToLayer { function: String, target_layer: String },\n    UpdateCaller { caller_file: PathBuf, old_ref: String, new_ref: String },\n    ManualReview { reason: String, context: String },\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionPlan {\n    pub action_id: String,\n    pub tier: ErrorTier,\n    pub predicted_violations: Vec<ViolationPrediction>,\n    pub strategies: Vec<CorrectionStrategy>,\n    pub confidence: f64,\n    pub estimated_fix_time_seconds: u32,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct ViolationPrediction {\n    pub violation_type: ViolationType,\n    pub affected_files: Vec<PathBuf>,\n    pub severity: Severity,\n    pub confidence: f64,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)]\npub enum ViolationType {\n    UnresolvedImport,\n    BrokenReference,\n    NameCollision,\n    LayerViolation,\n    TypeMismatch,\n    OwnershipIssue,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)]\npub enum Severity {\n    Critical,\n    High,\n    Medium,\n    Low,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub enum RefactorAction {\n    MoveFunction {\n        function: String,\n        from: PathBuf,\n        to: PathBuf,\n        required_layer: Option<String>,\n    },\n    RenameFunction {\n        old_name: String,\n        new_name: String,\n        file: PathBuf,\n    },\n    RenameFile {\n        from: PathBuf,\n        to: PathBuf,\n    },\n    CreateFile {\n        path: PathBuf,\n    },\n}\n\nimpl RefactorAction {\n    pub fn action_id(&self) -> String {\n        match self {\n            RefactorAction::MoveFunction { function, to, .. } => {\n                format!(\"move_{}_to_{}\", function, to.display())\n            }\n            RefactorAction::RenameFunction { old_name, new_name, .. } => {\n                format!(\"rename_{}_to_{}\", old_name, new_name)\n            }\n            RefactorAction::RenameFile { from, to } => {\n                format!(\"rename_file_{}_to_{}\", from.display(), to.display())\n            }\n            RefactorAction::CreateFile { path } => format!(\"create_file_{}\", path.display()),\n        }\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/220_dead_code_report.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/460_dead_code_report.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/220_dead_code_report.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/460_dead_code_report.rs",
          "original_content": "#![allow(dead_code)]\n//! JSON report generator for dead code analysis.\n\nuse crate::dead_code_types::{DeadCodeItem, DeadCodeReport, DeadCodeSummary};\nuse serde::{Deserialize, Serialize};\nuse std::path::Path;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadCodeReportMetadata {\n    pub analyzer_version: String,\n    pub project_root: String,\n    pub entrypoints_found: usize,\n}\n\npub fn build_report(\n    timestamp: String,\n    items: Vec<DeadCodeItem>,\n    metadata: DeadCodeReportMetadata,\n) -> DeadCodeReportWithMeta {\n    let summary = DeadCodeSummary::from_items(&items);\n    DeadCodeReportWithMeta {\n        timestamp,\n        summary,\n        items,\n        metadata,\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadCodeReportWithMeta {\n    pub timestamp: String,\n    pub summary: DeadCodeSummary,\n    pub items: Vec<DeadCodeItem>,\n    pub metadata: DeadCodeReportMetadata,\n}\n\npub fn write_report(path: &Path, report: &DeadCodeReportWithMeta) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(report)?;\n    std::fs::write(path, json)\n}\n\npub fn build_basic_report(timestamp: String, items: Vec<DeadCodeItem>) -> DeadCodeReport {\n    let summary = DeadCodeSummary::from_items(&items);\n    DeadCodeReport {\n        timestamp,\n        summary,\n        items,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/221_dead_code_filter.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/470_dead_code_filter.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/221_dead_code_filter.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/470_dead_code_filter.rs",
          "original_content": "#![allow(dead_code)]\n//! Filter dead code from downstream analysis inputs.\n\nuse crate::dead_code_types::{DeadCodeCategory};\nuse crate::dead_code_report::DeadCodeReportWithMeta;\nuse crate::types::CodeElement;\nuse std::collections::HashSet;\n\npub fn filter_dead_code_elements(\n    elements: &[CodeElement],\n    report: &DeadCodeReportWithMeta,\n) -> Vec<CodeElement> {\n    let excluded = collect_excluded_symbols(report);\n    elements\n        .iter()\n        .filter(|el| !excluded.contains(&el.name))\n        .cloned()\n        .collect()\n}\n\npub fn should_exclude_from_analysis(category: DeadCodeCategory) -> bool {\n    matches!(\n        category,\n        DeadCodeCategory::Unreachable | DeadCodeCategory::TestOnly\n    )\n}\n\nfn collect_excluded_symbols(report: &DeadCodeReportWithMeta) -> HashSet<String> {\n    report\n        .items\n        .iter()\n        .filter(|item| should_exclude_from_analysis(item.category))\n        .map(|item| item.symbol.clone())\n        .collect()\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/221_verification_policy_types.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/480_verification_policy_types.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/221_verification_policy_types.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/480_verification_policy_types.rs",
          "original_content": "#![allow(dead_code)]\n//! Verification policy types for correction intelligence.\n\nuse serde::{Deserialize, Serialize};\nuse std::path::PathBuf;\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct VerificationPolicy {\n    pub action_id: String,\n    pub scope: VerificationScope,\n    pub required_checks: Vec<VerificationCheck>,\n    pub incremental_eligible: bool,\n    pub estimated_time_seconds: u32,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub enum VerificationScope {\n    SyntaxOnly { files: Vec<PathBuf> },\n    ModuleLocal { module: String, transitive_depth: u32 },\n    CallerChain { root_function: String },\n    FullWorkspace,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub enum VerificationCheck {\n    CargoCheck,\n    CargoTest { filter: Option<String> },\n    InvariantValidation { invariant_ids: Vec<String> },\n    QualityMetrics { thresholds: QualityThresholds },\n    ManualInspection { reason: String },\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct QualityThresholds {\n    pub min_cohesion_delta: f64,\n    pub max_violation_delta: i32,\n    pub max_complexity_delta: f64,\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/222_dead_code_cli.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/490_dead_code_cli.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/222_dead_code_cli.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/490_dead_code_cli.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code analysis pipeline runner.\n\nuse anyhow::Result;\nuse crate::dead_code_actions::recommend_action;\nuse crate::dead_code_call_graph::build_call_graph;\nuse crate::dead_code_classifier::{classify_symbol, is_reachable};\nuse crate::dead_code_confidence::{assign_confidence, Evidence};\nuse crate::dead_code_entrypoints::{collect_entrypoints, collect_exports, is_public_api};\nuse crate::dead_code_intent::{detect_intent_signals, DeadCodePolicy};\nuse crate::dead_code_report::{build_report, write_report, DeadCodeReportMetadata, DeadCodeReportWithMeta};\nuse crate::dead_code_report_split::write_summary_markdown;\nuse crate::dead_code_test_boundaries::{detect_test_modules, detect_test_symbols, TestBoundaries};\nuse crate::dead_code_types::{DeadCodeCategory, DeadCodeItem};\nuse crate::layer_utilities::gather_rust_files;\nuse crate::types::{CodeElement, ElementType, Language, Visibility};\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\n#[derive(Debug, Clone)]\npub struct DeadCodeRunConfig {\n    pub root: PathBuf,\n    pub output_dir: PathBuf,\n    pub policy: Option<DeadCodePolicy>,\n    pub write_json: Option<PathBuf>,\n    pub write_summary: Option<PathBuf>,\n    pub summary_limit: usize,\n}\n\npub fn run_dead_code_pipeline(\n    elements: &[CodeElement],\n    config: &DeadCodeRunConfig,\n) -> Result<DeadCodeReportWithMeta> {\n    let rust_files = gather_rust_files(&config.root);\n    let mut intent_map = HashMap::new();\n    let mut test_boundaries = TestBoundaries::default();\n\n    for file in &rust_files {\n        let intents = detect_intent_signals(file, config.policy.as_ref());\n        merge_intent_map(&mut intent_map, intents);\n        let test_modules = detect_test_modules(file);\n        test_boundaries.test_modules.extend(test_modules);\n        let test_symbols = detect_test_symbols(file);\n        test_boundaries.test_symbols.extend(test_symbols);\n        if is_test_path(file) {\n            test_boundaries.test_files.insert(file.clone());\n        }\n    }\n\n    let call_graph = build_call_graph(elements);\n    let entrypoints = collect_entrypoints(elements, config.policy.as_ref());\n    let exports = collect_exports(&config.root);\n\n    let mut items = Vec::new();\n    for element in elements {\n        if element.element_type != ElementType::Function {\n            continue;\n        }\n        if element.language != Language::Rust {\n            continue;\n        }\n        let category = classify_symbol(\n            &element.name,\n            &call_graph,\n            &intent_map,\n            &test_boundaries,\n            &entrypoints,\n            config.policy.as_ref(),\n        );\n        let intent_tag = intent_map.contains_key(&element.name);\n        let test_reference = test_boundaries.test_symbols.contains(&element.name);\n        let call_graph_proven =\n            category == DeadCodeCategory::Unreachable && is_reachable(&element.name, &call_graph, &entrypoints) == false;\n\n        let mut item = DeadCodeItem {\n            symbol: element.name.clone(),\n            file: PathBuf::from(&element.file_path),\n            line: element.line_number,\n            category,\n            confidence: crate::dead_code_types::ConfidenceLevel::Heuristic,\n            action: crate::dead_code_types::RecommendedAction::ManualReview,\n            reason: reason_for_category(category, intent_tag, test_reference),\n        };\n\n        let confidence = assign_confidence(\n            &item,\n            &Evidence {\n                intent_tag,\n                test_reference,\n                call_graph_proven,\n            },\n        );\n        item.confidence = confidence;\n\n        let public_api = is_public_api(&element.name, &exports)\n            || matches!(element.visibility, Visibility::Public);\n        item.action = recommend_action(category, confidence, public_api);\n\n        items.push(item);\n    }\n\n    let metadata = DeadCodeReportMetadata {\n        analyzer_version: env!(\"CARGO_PKG_VERSION\").to_string(),\n        project_root: config.root.display().to_string(),\n        entrypoints_found: entrypoints.len(),\n    };\n    let report = build_report(\n        chrono::Local::now().to_rfc3339(),\n        items,\n        metadata,\n    );\n\n    write_outputs(&report, config)?;\n    Ok(report)\n}\n\nfn write_outputs(report: &DeadCodeReportWithMeta, config: &DeadCodeRunConfig) -> Result<()> {\n    let json_path = config\n        .write_json\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_full.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_report(&json_path, report)?;\n\n    let summary_path = config\n        .write_summary\n        .clone()\n        .unwrap_or_else(|| config.output_dir.join(\"dead_code_summary.md\"));\n    if let Some(parent) = summary_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    write_summary_markdown(&summary_path, report, config.summary_limit)?;\n    Ok(())\n}\n\nfn merge_intent_map(base: &mut HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>, next: HashMap<String, Vec<crate::dead_code_types::IntentMetadata>>) {\n    for (symbol, items) in next {\n        base.entry(symbol).or_default().extend(items);\n    }\n}\n\nfn reason_for_category(category: DeadCodeCategory, intent_tag: bool, test_reference: bool) -> String {\n    match category {\n        DeadCodeCategory::LatentPlanned => {\n            if intent_tag {\n                \"Intent tag present\".to_string()\n            } else {\n                \"Intent directory policy\".to_string()\n            }\n        }\n        DeadCodeCategory::TestOnly => {\n            if test_reference {\n                \"Called only by test symbols\".to_string()\n            } else {\n                \"Defined in test-only module\".to_string()\n            }\n        }\n        DeadCodeCategory::Unreachable => \"No callers reachable from entrypoints\".to_string(),\n        DeadCodeCategory::ReachableUnused => \"Reachable but unused in execution\".to_string(),\n    }\n}\n\nfn is_test_path(path: &Path) -> bool {\n    path.components().any(|c| {\n        let name = c.as_os_str().to_str().unwrap_or(\"\");\n        name == \"tests\" || name == \"test\" || name == \"benches\"\n    })\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/222_quality_delta_types.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/500_quality_delta_types.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/222_quality_delta_types.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/500_quality_delta_types.rs",
          "original_content": "#![allow(dead_code)]\n//! Quality delta types and rollback criteria.\n\nuse serde::{Deserialize, Serialize};\n\nuse crate::correction_plan_types::ViolationType;\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct QualityDelta {\n    pub action_id: String,\n    pub cohesion_delta: f64,\n    pub violation_delta: i32,\n    pub complexity_delta: f64,\n    pub overall_score_delta: f64,\n    pub acceptable: bool,\n    pub reason: String,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct RollbackCriteria {\n    pub action_id: String,\n    pub mandatory_rollback_if: Vec<RollbackCondition>,\n    pub suggested_rollback_if: Vec<RollbackCondition>,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub enum RollbackCondition {\n    QualityDecreased { threshold: f64 },\n    BuildFailed,\n    TestsFailed { critical_tests: Vec<String> },\n    InvariantViolated { invariant_ids: Vec<String> },\n    Tier3Error { error_type: ViolationType },\n    ManualReviewRequired,\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/223_dead_code_policy.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/510_dead_code_policy.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/223_dead_code_policy.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/510_dead_code_policy.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code policy file support.\n\nuse crate::dead_code_intent::DeadCodePolicy;\nuse std::path::Path;\n\npub fn load_policy(path: &Path) -> std::io::Result<DeadCodePolicy> {\n    let contents = std::fs::read_to_string(path)?;\n    Ok(parse_policy(&contents, path.parent().unwrap_or(path)))\n}\n\npub fn parse_policy(contents: &str, base: &Path) -> DeadCodePolicy {\n    let mut planned_directories = Vec::new();\n    let mut public_api_roots = Vec::new();\n    let mut entrypoint_symbols = Vec::new();\n    let mut treat_public_as_entrypoint = true;\n\n    for line in contents.lines() {\n        let trimmed = line.trim();\n        if trimmed.is_empty() || trimmed.starts_with('#') || trimmed.starts_with(\"//\") {\n            continue;\n        }\n        let Some((key, value)) = trimmed.split_once('=') else {\n            continue;\n        };\n        let key = key.trim();\n        let value = value.trim();\n        match key {\n            \"planned_directories\" => {\n                planned_directories = parse_list(value)\n                    .into_iter()\n                    .map(|p| base.join(p))\n                    .collect();\n            }\n            \"public_api_roots\" => {\n                public_api_roots = parse_list(value)\n                    .into_iter()\n                    .map(|p| base.join(p))\n                    .collect();\n            }\n            \"entrypoint_symbols\" => {\n                entrypoint_symbols = parse_list(value);\n            }\n            \"treat_public_as_entrypoint\" => {\n                treat_public_as_entrypoint = parse_bool(value).unwrap_or(true);\n            }\n            _ => {}\n        }\n    }\n\n    DeadCodePolicy {\n        planned_directories,\n        public_api_roots,\n        entrypoint_symbols,\n        treat_public_as_entrypoint,\n    }\n}\n\nfn parse_list(value: &str) -> Vec<String> {\n    let mut trimmed = value.trim().to_string();\n    if let Some(stripped) = trimmed.strip_prefix('[') {\n        trimmed = stripped.to_string();\n    }\n    if let Some(stripped) = trimmed.strip_suffix(']') {\n        trimmed = stripped.to_string();\n    }\n    trimmed\n        .split(',')\n        .map(|s| s.trim().trim_matches('\"').trim_matches('\\'').to_string())\n        .filter(|s| !s.is_empty())\n        .collect()\n}\n\nfn parse_bool(value: &str) -> Option<bool> {\n    match value.trim().to_ascii_lowercase().as_str() {\n        \"true\" => Some(true),\n        \"false\" => Some(false),\n        _ => None,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/223_violation_predictor.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/520_violation_predictor.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/223_violation_predictor.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/520_violation_predictor.rs",
          "original_content": "#![allow(dead_code)]\n//! Predict violations for refactor actions.\n\nuse crate::correction_plan_types::{\n    RefactorAction, Severity, ViolationPrediction, ViolationType,\n};\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{CallGraphNode, CodeElement};\nuse std::collections::{HashMap, HashSet};\nuse std::path::PathBuf;\n\npub fn predict_violations(\n    action: &RefactorAction,\n    invariants: &InvariantAnalysisResult,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<ViolationPrediction> {\n    let mut predictions = Vec::new();\n    match action {\n        RefactorAction::MoveFunction { function, from, to, required_layer } => {\n            let callers = find_callers(function, call_graph, elements);\n            if !callers.is_empty() {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::UnresolvedImport,\n                    affected_files: callers,\n                    severity: Severity::Critical,\n                    confidence: 0.95,\n                });\n            }\n            if let Some(layer) = required_layer {\n                if !layer.is_empty() {\n                    predictions.push(ViolationPrediction {\n                        violation_type: ViolationType::LayerViolation,\n                        affected_files: vec![to.clone()],\n                        severity: Severity::High,\n                        confidence: 1.0,\n                    });\n                }\n            } else if move_violates_invariant(function, from, to, invariants) {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::LayerViolation,\n                    affected_files: vec![to.clone()],\n                    severity: Severity::High,\n                    confidence: 0.9,\n                });\n            }\n        }\n        RefactorAction::RenameFunction { old_name, new_name, file } => {\n            if symbol_exists(new_name, elements) {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::NameCollision,\n                    affected_files: vec![file.clone()],\n                    severity: Severity::Critical,\n                    confidence: 1.0,\n                });\n            }\n            let references = find_reference_files(old_name, call_graph, elements);\n            if !references.is_empty() {\n                predictions.push(ViolationPrediction {\n                    violation_type: ViolationType::BrokenReference,\n                    affected_files: references,\n                    severity: Severity::Critical,\n                    confidence: 0.85,\n                });\n            }\n        }\n        RefactorAction::RenameFile { from, to } => {\n            predictions.push(ViolationPrediction {\n                violation_type: ViolationType::BrokenReference,\n                affected_files: vec![from.clone(), to.clone()],\n                severity: Severity::High,\n                confidence: 0.7,\n            });\n        }\n        RefactorAction::CreateFile { path } => {\n            predictions.push(ViolationPrediction {\n                violation_type: ViolationType::UnresolvedImport,\n                affected_files: vec![path.clone()],\n                severity: Severity::Low,\n                confidence: 0.5,\n            });\n        }\n    }\n    predictions\n}\n\nfn find_callers(\n    function: &str,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<PathBuf> {\n    let mut files = HashSet::new();\n    if let Some(node) = call_graph.get(function) {\n        for caller in &node.called_by {\n            if let Some(file) = find_element_file(caller, elements) {\n                files.insert(file);\n            }\n        }\n    }\n    files.into_iter().collect()\n}\n\nfn find_reference_files(\n    function: &str,\n    call_graph: &HashMap<String, CallGraphNode>,\n    elements: &[CodeElement],\n) -> Vec<PathBuf> {\n    let mut files = HashSet::new();\n    for (caller, node) in call_graph {\n        if node.calls.iter().any(|c| c == function) {\n            if let Some(file) = find_element_file(caller, elements) {\n                files.insert(file);\n            }\n        }\n    }\n    files.into_iter().collect()\n}\n\nfn find_element_file(function: &str, elements: &[CodeElement]) -> Option<PathBuf> {\n    elements\n        .iter()\n        .find(|el| el.name == function)\n        .map(|el| PathBuf::from(&el.file_path))\n}\n\nfn symbol_exists(symbol: &str, elements: &[CodeElement]) -> bool {\n    elements.iter().any(|el| el.name == symbol)\n}\n\nfn move_violates_invariant(\n    _function: &str,\n    _from: &PathBuf,\n    _to: &PathBuf,\n    _invariants: &InvariantAnalysisResult,\n) -> bool {\n    false\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/224_dead_code_report_split.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/530_dead_code_report_split.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/224_dead_code_report_split.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/530_dead_code_report_split.rs",
          "original_content": "#![allow(dead_code)]\n//! Dead code report split utilities.\n\nuse crate::dead_code_report::DeadCodeReportWithMeta;\nuse crate::dead_code_types::DeadCodeItem;\nuse std::path::Path;\n\npub fn write_summary_markdown(\n    path: &Path,\n    report: &DeadCodeReportWithMeta,\n    limit: usize,\n) -> std::io::Result<()> {\n    let mut content = String::new();\n    content.push_str(\"# Dead Code Summary\\n\\n\");\n    content.push_str(&format!(\n        \"Generated: {}\\n\\n\",\n        report.timestamp\n    ));\n    content.push_str(\"## Summary Counts\\n\\n\");\n    content.push_str(&format!(\n        \"- Unreachable: {}\\n- Reachable-unused: {}\\n- Test-only: {}\\n- Latent/planned: {}\\n- Total analyzed: {}\\n\\n\",\n        report.summary.unreachable,\n        report.summary.reachable_unused,\n        report.summary.test_only,\n        report.summary.latent_planned,\n        report.summary.total_analyzed\n    ));\n\n    let items = top_items(&report.items, limit);\n    content.push_str(\"## Top Findings\\n\\n\");\n    if items.is_empty() {\n        content.push_str(\"- None.\\n\");\n    } else {\n        for item in items {\n            content.push_str(&format!(\n                \"- `{}` in `{}`  {:?} / {:?} / {:?}\\n\",\n                item.symbol,\n                item.file.display(),\n                item.category,\n                item.confidence,\n                item.action\n            ));\n        }\n    }\n    content.push('\\n');\n\n    std::fs::write(path, content)\n}\n\nfn top_items(items: &[DeadCodeItem], limit: usize) -> Vec<DeadCodeItem> {\n    let mut items = items.to_vec();\n    items.sort_by_key(|item| item.action as u8);\n    if items.len() > limit {\n        items.truncate(limit);\n    }\n    items\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/224_tier_classifier.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/540_tier_classifier.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/224_tier_classifier.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/540_tier_classifier.rs",
          "original_content": "#![allow(dead_code)]\n//! Tier classification for predicted violations.\n\nuse crate::correction_plan_types::{ErrorTier, Severity, ViolationPrediction, ViolationType};\n\npub fn classify_tier(violation: &ViolationPrediction) -> ErrorTier {\n    match (&violation.violation_type, &violation.severity) {\n        (ViolationType::UnresolvedImport, _) => ErrorTier::Trivial,\n        (ViolationType::BrokenReference, Severity::Low | Severity::Medium) => ErrorTier::Trivial,\n        (ViolationType::NameCollision, _) => ErrorTier::Moderate,\n        (ViolationType::LayerViolation, _) => ErrorTier::Moderate,\n        (ViolationType::TypeMismatch, _) => ErrorTier::Complex,\n        (ViolationType::OwnershipIssue, _) => ErrorTier::Complex,\n        (ViolationType::BrokenReference, Severity::Critical | Severity::High) => ErrorTier::Complex,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/225_confidence_scorer.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/550_confidence_scorer.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/225_confidence_scorer.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/550_confidence_scorer.rs",
          "original_content": "#![allow(dead_code)]\n//! Confidence scoring for predicted violations.\n\nuse crate::correction_plan_types::{ViolationPrediction, ViolationType};\n\n#[derive(Debug, Clone, Default)]\npub struct PredictionContext {\n    pub has_test_coverage: bool,\n}\n\npub fn compute_confidence(\n    prediction: &ViolationPrediction,\n    context: &PredictionContext,\n) -> f64 {\n    let base: f64 = match prediction.violation_type {\n        ViolationType::UnresolvedImport => 0.95,\n        ViolationType::NameCollision => 1.0,\n        ViolationType::LayerViolation => 1.0,\n        ViolationType::BrokenReference => 0.85,\n        ViolationType::TypeMismatch => 0.6,\n        ViolationType::OwnershipIssue => 0.5,\n    };\n    let multiplier: f64 = if context.has_test_coverage { 1.1 } else { 0.9 };\n    (base * multiplier).min(1.0)\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/226_correction_plan_generator.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/560_correction_plan_generator.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/226_correction_plan_generator.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/560_correction_plan_generator.rs",
          "original_content": "#![allow(dead_code)]\n//! Generate correction plans from predictions.\n\nuse crate::correction_plan_types::{\n    CorrectionPlan, CorrectionStrategy, ErrorTier, RefactorAction, ViolationPrediction,\n    ViolationType,\n};\nuse crate::tier_classifier::classify_tier;\n\npub fn generate_correction_plan(\n    action: &RefactorAction,\n    predictions: &[ViolationPrediction],\n) -> CorrectionPlan {\n    let mut strategies = Vec::new();\n    for prediction in predictions {\n        match prediction.violation_type {\n            ViolationType::UnresolvedImport => {\n                if let Some(symbol) = action_symbol(action) {\n                    strategies.push(CorrectionStrategy::AddImport {\n                        module_path: action_module_path(action),\n                        symbol,\n                    });\n                }\n            }\n            ViolationType::BrokenReference => {\n                match action {\n                    RefactorAction::RenameFile { .. } => {\n                        if let Some((old_ref, new_ref)) = action_refs(action) {\n                            strategies.push(CorrectionStrategy::UpdatePath {\n                                old_path: old_ref,\n                                new_path: new_ref,\n                            });\n                        }\n                    }\n                    _ => {\n                        if let Some((old_ref, new_ref)) = action_refs(action) {\n                            for file in &prediction.affected_files {\n                                strategies.push(CorrectionStrategy::UpdateCaller {\n                                    caller_file: file.clone(),\n                                    old_ref: old_ref.clone(),\n                                    new_ref: new_ref.clone(),\n                                });\n                            }\n                        }\n                    }\n                }\n            }\n            ViolationType::NameCollision => {\n                if let Some(symbol) = action_symbol(action) {\n                    strategies.push(CorrectionStrategy::RenameWithSuffix {\n                        original: symbol,\n                        suffix: \"_v2\".to_string(),\n                    });\n                }\n            }\n            ViolationType::LayerViolation => {\n                if let Some(layer) = action_target_layer(action) {\n                    if let Some(function) = action_function(action) {\n                        strategies.push(CorrectionStrategy::MoveToLayer {\n                            function,\n                            target_layer: layer,\n                        });\n                    }\n                }\n            }\n            ViolationType::TypeMismatch | ViolationType::OwnershipIssue => {\n                strategies.push(CorrectionStrategy::ManualReview {\n                    reason: format!(\"{:?} requires semantic analysis\", prediction.violation_type),\n                    context: format!(\"{:?}\", action),\n                });\n            }\n        }\n    }\n\n    let tier = predictions\n        .iter()\n        .map(classify_tier)\n        .max()\n        .unwrap_or(ErrorTier::Trivial);\n\n    CorrectionPlan {\n        action_id: action.action_id(),\n        tier,\n        predicted_violations: predictions.to_vec(),\n        strategies,\n        confidence: average_confidence(predictions),\n        estimated_fix_time_seconds: estimate_fix_time(predictions.len()),\n    }\n}\n\nfn average_confidence(predictions: &[ViolationPrediction]) -> f64 {\n    if predictions.is_empty() {\n        return 1.0;\n    }\n    let total: f64 = predictions.iter().map(|p| p.confidence).sum();\n    total / predictions.len() as f64\n}\n\nfn estimate_fix_time(count: usize) -> u32 {\n    10 + (count as u32 * 5)\n}\n\nfn action_symbol(action: &RefactorAction) -> Option<String> {\n    match action {\n        RefactorAction::MoveFunction { function, .. } => Some(function.clone()),\n        RefactorAction::RenameFunction { new_name, .. } => Some(new_name.clone()),\n        _ => None,\n    }\n}\n\nfn action_function(action: &RefactorAction) -> Option<String> {\n    match action {\n        RefactorAction::MoveFunction { function, .. } => Some(function.clone()),\n        _ => None,\n    }\n}\n\nfn action_module_path(action: &RefactorAction) -> String {\n    match action {\n        RefactorAction::MoveFunction { to, .. } => to.display().to_string(),\n        RefactorAction::RenameFile { to, .. } => to.display().to_string(),\n        RefactorAction::CreateFile { path } => path.display().to_string(),\n        _ => \"crate\".to_string(),\n    }\n}\n\nfn action_refs(action: &RefactorAction) -> Option<(String, String)> {\n    match action {\n        RefactorAction::RenameFunction { old_name, new_name, .. } => {\n            Some((old_name.clone(), new_name.clone()))\n        }\n        RefactorAction::RenameFile { from, to } => {\n            Some((from.display().to_string(), to.display().to_string()))\n        }\n        _ => None,\n    }\n}\n\nfn action_target_layer(action: &RefactorAction) -> Option<String> {\n    match action {\n        RefactorAction::MoveFunction { required_layer, .. } => required_layer.clone(),\n        _ => None,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/227_verification_scope_planner.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/570_verification_scope_planner.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/227_verification_scope_planner.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/570_verification_scope_planner.rs",
          "original_content": "#![allow(dead_code)]\n//! Verification scope planner.\n\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction};\nuse crate::verification_policy_types::{\n    VerificationCheck, VerificationPolicy, VerificationScope,\n};\n\npub fn plan_verification_scope(\n    action: &RefactorAction,\n    correction_plan: &CorrectionPlan,\n) -> VerificationPolicy {\n    let scope = match correction_plan.tier {\n        ErrorTier::Trivial if correction_plan.predicted_violations.len() <= 3 => {\n            VerificationScope::SyntaxOnly {\n                files: affected_files(action),\n            }\n        }\n        ErrorTier::Trivial | ErrorTier::Moderate => VerificationScope::ModuleLocal {\n            module: action_module(action),\n            transitive_depth: 2,\n        },\n        ErrorTier::Complex => VerificationScope::FullWorkspace,\n    };\n\n    let mut required_checks = vec![VerificationCheck::CargoCheck];\n    if matches!(correction_plan.tier, ErrorTier::Moderate | ErrorTier::Complex) {\n        required_checks.push(VerificationCheck::CargoTest { filter: None });\n    }\n\n    VerificationPolicy {\n        action_id: correction_plan.action_id.clone(),\n        scope,\n        required_checks,\n        incremental_eligible: matches!(correction_plan.tier, ErrorTier::Trivial),\n        estimated_time_seconds: estimate_verification_time(&correction_plan.tier),\n    }\n}\n\nfn affected_files(action: &RefactorAction) -> Vec<std::path::PathBuf> {\n    match action {\n        RefactorAction::MoveFunction { from, to, .. } => vec![from.clone(), to.clone()],\n        RefactorAction::RenameFunction { file, .. } => vec![file.clone()],\n        RefactorAction::RenameFile { from, to } => vec![from.clone(), to.clone()],\n        RefactorAction::CreateFile { path } => vec![path.clone()],\n    }\n}\n\nfn action_module(action: &RefactorAction) -> String {\n    match action {\n        RefactorAction::MoveFunction { to, .. } => to.display().to_string(),\n        RefactorAction::RenameFunction { file, .. } => file.display().to_string(),\n        RefactorAction::RenameFile { to, .. } => to.display().to_string(),\n        RefactorAction::CreateFile { path } => path.display().to_string(),\n    }\n}\n\nfn estimate_verification_time(tier: &ErrorTier) -> u32 {\n    match tier {\n        ErrorTier::Trivial => 10,\n        ErrorTier::Moderate => 60,\n        ErrorTier::Complex => 180,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/228_rollback_criteria_builder.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/580_rollback_criteria_builder.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/228_rollback_criteria_builder.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/580_rollback_criteria_builder.rs",
          "original_content": "#![allow(dead_code)]\n//! Rollback criteria builder.\n\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction, ViolationType};\nuse crate::quality_delta_types::{RollbackCondition, RollbackCriteria};\n\npub fn build_rollback_criteria(\n    action: &RefactorAction,\n    correction_plan: &CorrectionPlan,\n) -> RollbackCriteria {\n    let mut mandatory = vec![RollbackCondition::BuildFailed];\n    let mut suggested = vec![RollbackCondition::QualityDecreased { threshold: 0.05 }];\n\n    match correction_plan.tier {\n        ErrorTier::Complex => {\n            mandatory.push(RollbackCondition::Tier3Error {\n                error_type: ViolationType::TypeMismatch,\n            });\n            mandatory.push(RollbackCondition::ManualReviewRequired);\n        }\n        ErrorTier::Moderate => {\n            suggested.push(RollbackCondition::TestsFailed {\n                critical_tests: extract_critical_tests(action),\n            });\n        }\n        ErrorTier::Trivial => {}\n    }\n\n    for prediction in &correction_plan.predicted_violations {\n        if prediction.violation_type == ViolationType::LayerViolation {\n            mandatory.push(RollbackCondition::InvariantViolated {\n                invariant_ids: vec![\"layer_ordering\".to_string()],\n            });\n        }\n    }\n\n    RollbackCriteria {\n        action_id: correction_plan.action_id.clone(),\n        mandatory_rollback_if: mandatory,\n        suggested_rollback_if: suggested,\n    }\n}\n\nfn extract_critical_tests(_action: &RefactorAction) -> Vec<String> {\n    Vec::new()\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/229_quality_delta_calculator.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/590_quality_delta_calculator.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/229_quality_delta_calculator.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/590_quality_delta_calculator.rs",
          "original_content": "#![allow(dead_code)]\n//! Quality delta calculator.\n\nuse crate::correction_plan_types::RefactorAction;\nuse crate::quality_delta_types::QualityDelta;\n\n#[derive(Clone, Debug, Default)]\npub struct Metrics {\n    pub cohesion: f64,\n    pub violations: usize,\n    pub complexity: f64,\n}\n\npub fn calculate_quality_delta(\n    action: &RefactorAction,\n    current: &Metrics,\n    simulated: &Metrics,\n) -> QualityDelta {\n    let cohesion_delta = simulated.cohesion - current.cohesion;\n    let violation_delta = simulated.violations as i32 - current.violations as i32;\n    let complexity_delta = simulated.complexity - current.complexity;\n    let overall = 0.5 * cohesion_delta - 0.3 * violation_delta as f64 - 0.2 * complexity_delta;\n    let acceptable = overall > -0.05 && violation_delta <= 0;\n    let reason = if acceptable {\n        \"Quality improved or maintained\".to_string()\n    } else if overall < -0.1 {\n        \"Quality degradation exceeds threshold\".to_string()\n    } else if violation_delta > 0 {\n        format!(\"Introduced {} new violations\", violation_delta)\n    } else {\n        \"Quality barely acceptable\".to_string()\n    };\n    QualityDelta {\n        action_id: action.action_id(),\n        cohesion_delta,\n        violation_delta,\n        complexity_delta,\n        overall_score_delta: overall,\n        acceptable,\n        reason,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/230_action_impact_estimator.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/600_action_impact_estimator.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/230_action_impact_estimator.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/600_action_impact_estimator.rs",
          "original_content": "#![allow(dead_code)]\n//! Action impact estimator.\n\nuse crate::correction_plan_types::RefactorAction;\nuse crate::quality_delta_calculator::{calculate_quality_delta, Metrics};\nuse crate::quality_delta_types::QualityDelta;\n\n#[derive(Clone, Debug)]\npub struct AnalysisState {\n    pub metrics: Metrics,\n}\n\npub fn estimate_impact(action: &RefactorAction, current_state: &AnalysisState) -> QualityDelta {\n    let simulated = simulate_action(action, current_state);\n    calculate_quality_delta(action, &current_state.metrics, &simulated.metrics)\n}\n\nfn simulate_action(_action: &RefactorAction, state: &AnalysisState) -> AnalysisState {\n    state.clone()\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/231_correction_plan_serializer.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/610_correction_plan_serializer.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/231_correction_plan_serializer.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/610_correction_plan_serializer.rs",
          "original_content": "#![allow(dead_code)]\n//! Serialize correction plans to JSON values.\n\nuse crate::correction_plan_types::{CorrectionPlan, CorrectionStrategy};\nuse crate::quality_delta_types::RollbackCriteria;\nuse crate::verification_policy_types::{QualityThresholds, VerificationCheck, VerificationPolicy, VerificationScope};\nuse serde_json::{json, Value};\n\npub fn serialize_correction_plan(\n    plan: &CorrectionPlan,\n    verification: &VerificationPolicy,\n    rollback: &RollbackCriteria,\n) -> Value {\n    json!({\n        \"action_id\": plan.action_id,\n        \"tier\": format!(\"{:?}\", plan.tier),\n        \"confidence\": plan.confidence,\n        \"estimated_fix_time_seconds\": plan.estimated_fix_time_seconds,\n        \"predicted_violations\": plan.predicted_violations.iter().map(|v| json!({\n            \"type\": format!(\"{:?}\", v.violation_type),\n            \"severity\": format!(\"{:?}\", v.severity),\n            \"affected_files\": v.affected_files,\n            \"confidence\": v.confidence,\n        })).collect::<Vec<_>>(),\n        \"correction_strategies\": plan.strategies.iter().map(serialize_strategy).collect::<Vec<_>>(),\n        \"verification_policy\": {\n            \"scope\": serialize_scope(&verification.scope),\n            \"required_checks\": verification.required_checks.iter()\n                .map(serialize_check)\n                .collect::<Vec<_>>(),\n            \"incremental_eligible\": verification.incremental_eligible,\n            \"estimated_time_seconds\": verification.estimated_time_seconds,\n        },\n        \"rollback_criteria\": {\n            \"mandatory\": rollback.mandatory_rollback_if.iter()\n                .map(|c| format!(\"{:?}\", c))\n                .collect::<Vec<_>>(),\n            \"suggested\": rollback.suggested_rollback_if.iter()\n                .map(|c| format!(\"{:?}\", c))\n                .collect::<Vec<_>>(),\n        }\n    })\n}\n\nfn serialize_scope(scope: &VerificationScope) -> Value {\n    match scope {\n        VerificationScope::SyntaxOnly { files } => json!({\n            \"type\": \"SyntaxOnly\",\n            \"files\": files,\n        }),\n        VerificationScope::ModuleLocal { module, transitive_depth } => json!({\n            \"type\": \"ModuleLocal\",\n            \"module\": module,\n            \"transitive_depth\": transitive_depth,\n        }),\n        VerificationScope::CallerChain { root_function } => json!({\n            \"type\": \"CallerChain\",\n            \"root_function\": root_function,\n        }),\n        VerificationScope::FullWorkspace => json!({\n            \"type\": \"FullWorkspace\",\n        }),\n    }\n}\n\nfn serialize_check(check: &VerificationCheck) -> Value {\n    match check {\n        VerificationCheck::CargoCheck => json!({ \"type\": \"CargoCheck\" }),\n        VerificationCheck::CargoTest { filter } => json!({\n            \"type\": \"CargoTest\",\n            \"filter\": filter,\n        }),\n        VerificationCheck::InvariantValidation { invariant_ids } => json!({\n            \"type\": \"InvariantValidation\",\n            \"invariant_ids\": invariant_ids,\n        }),\n        VerificationCheck::QualityMetrics { thresholds } => json!({\n            \"type\": \"QualityMetrics\",\n            \"thresholds\": serialize_thresholds(thresholds),\n        }),\n        VerificationCheck::ManualInspection { reason } => json!({\n            \"type\": \"ManualInspection\",\n            \"reason\": reason,\n        }),\n    }\n}\n\nfn serialize_thresholds(thresholds: &QualityThresholds) -> Value {\n    json!({\n        \"min_cohesion_delta\": thresholds.min_cohesion_delta,\n        \"max_violation_delta\": thresholds.max_violation_delta,\n        \"max_complexity_delta\": thresholds.max_complexity_delta,\n    })\n}\n\nfn serialize_strategy(strategy: &CorrectionStrategy) -> Value {\n    match strategy {\n        CorrectionStrategy::AddImport { module_path, symbol } => json!({\n            \"type\": \"AddImport\",\n            \"module_path\": module_path,\n            \"symbol\": symbol,\n        }),\n        CorrectionStrategy::UpdatePath { old_path, new_path } => json!({\n            \"type\": \"UpdatePath\",\n            \"old_path\": old_path,\n            \"new_path\": new_path,\n        }),\n        CorrectionStrategy::ReExport { from_module, symbol } => json!({\n            \"type\": \"ReExport\",\n            \"from_module\": from_module,\n            \"symbol\": symbol,\n        }),\n        CorrectionStrategy::RenameWithSuffix { original, suffix } => json!({\n            \"type\": \"RenameWithSuffix\",\n            \"original\": original,\n            \"suffix\": suffix,\n        }),\n        CorrectionStrategy::MoveToLayer { function, target_layer } => json!({\n            \"type\": \"MoveToLayer\",\n            \"function\": function,\n            \"target_layer\": target_layer,\n        }),\n        CorrectionStrategy::UpdateCaller { caller_file, old_ref, new_ref } => json!({\n            \"type\": \"UpdateCaller\",\n            \"caller_file\": caller_file,\n            \"old_ref\": old_ref,\n            \"new_ref\": new_ref,\n        }),\n        CorrectionStrategy::ManualReview { reason, context } => json!({\n            \"type\": \"ManualReview\",\n            \"reason\": reason,\n            \"context\": context,\n        }),\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/232_verification_policy_emitter.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/620_verification_policy_emitter.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/232_verification_policy_emitter.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/620_verification_policy_emitter.rs",
          "original_content": "#![allow(dead_code)]\n//! Emit verification policies to JSON.\n\nuse crate::verification_policy_types::{QualityThresholds, VerificationCheck, VerificationPolicy, VerificationScope};\nuse serde_json::json;\nuse std::path::Path;\n\npub fn emit_verification_policy(\n    policies: &[VerificationPolicy],\n    output_path: &Path,\n) -> std::io::Result<()> {\n    let policy_file = json!({\n        \"version\": \"1.0\",\n        \"policies\": policies.iter().map(|p| json!({\n            \"action_id\": p.action_id,\n            \"scope\": serialize_scope(&p.scope),\n            \"checks\": p.required_checks.iter()\n                .map(serialize_check)\n                .collect::<Vec<_>>(),\n            \"incremental\": p.incremental_eligible,\n            \"estimated_time_seconds\": p.estimated_time_seconds,\n        })).collect::<Vec<_>>()\n    });\n    std::fs::write(output_path, serde_json::to_string_pretty(&policy_file)?)?;\n    Ok(())\n}\n\nfn serialize_scope(scope: &VerificationScope) -> serde_json::Value {\n    match scope {\n        VerificationScope::SyntaxOnly { files } => json!({\n            \"type\": \"SyntaxOnly\",\n            \"files\": files,\n        }),\n        VerificationScope::ModuleLocal { module, transitive_depth } => json!({\n            \"type\": \"ModuleLocal\",\n            \"module\": module,\n            \"transitive_depth\": transitive_depth,\n        }),\n        VerificationScope::CallerChain { root_function } => json!({\n            \"type\": \"CallerChain\",\n            \"root_function\": root_function,\n        }),\n        VerificationScope::FullWorkspace => json!({\n            \"type\": \"FullWorkspace\",\n        }),\n    }\n}\n\nfn serialize_check(check: &VerificationCheck) -> serde_json::Value {\n    match check {\n        VerificationCheck::CargoCheck => json!({ \"type\": \"CargoCheck\" }),\n        VerificationCheck::CargoTest { filter } => json!({\n            \"type\": \"CargoTest\",\n            \"filter\": filter,\n        }),\n        VerificationCheck::InvariantValidation { invariant_ids } => json!({\n            \"type\": \"InvariantValidation\",\n            \"invariant_ids\": invariant_ids,\n        }),\n        VerificationCheck::QualityMetrics { thresholds } => json!({\n            \"type\": \"QualityMetrics\",\n            \"thresholds\": serialize_thresholds(thresholds),\n        }),\n        VerificationCheck::ManualInspection { reason } => json!({\n            \"type\": \"ManualInspection\",\n            \"reason\": reason,\n        }),\n    }\n}\n\nfn serialize_thresholds(thresholds: &QualityThresholds) -> serde_json::Value {\n    json!({\n        \"min_cohesion_delta\": thresholds.min_cohesion_delta,\n        \"max_violation_delta\": thresholds.max_violation_delta,\n        \"max_complexity_delta\": thresholds.max_complexity_delta,\n    })\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    },
    {
      "action_id": "rename_file_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/233_correction_intelligence_report.rs_to_/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/630_correction_intelligence_report.rs",
      "mutations": [
        {
          "type": "Rename",
          "from": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/233_correction_intelligence_report.rs",
          "to": "/home/cicero-arch-omen/ai_sandbox/codex-agent/codex_sse/server-tools/MMSB/tools/mmsb-analyzer/src/630_correction_intelligence_report.rs",
          "original_content": "#![allow(dead_code)]\n//! Correction intelligence report generator.\n\nuse crate::action_impact_estimator::{estimate_impact, AnalysisState as ImpactState};\nuse crate::correction_plan_generator::generate_correction_plan;\nuse crate::correction_plan_serializer::serialize_correction_plan;\nuse crate::correction_plan_types::{CorrectionPlan, ErrorTier, RefactorAction, ViolationPrediction};\nuse crate::quality_delta_calculator::Metrics;\nuse crate::quality_delta_types::{QualityDelta, RollbackCriteria};\nuse crate::rollback_criteria_builder::build_rollback_criteria;\nuse crate::verification_policy_emitter::emit_verification_policy;\nuse crate::verification_scope_planner::plan_verification_scope;\nuse crate::violation_predictor::predict_violations;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse std::collections::HashMap;\nuse std::path::{Path, PathBuf};\n\nuse crate::invariant_types::InvariantAnalysisResult;\nuse crate::types::{AnalysisResult, CallGraphNode, CodeElement};\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionIntelligenceReport {\n    pub version: String,\n    pub timestamp: String,\n    pub project_root: PathBuf,\n    pub actions_analyzed: usize,\n    pub correction_plans: Vec<CorrectionPlan>,\n    pub verification_policies: Vec<crate::verification_policy_types::VerificationPolicy>,\n    pub rollback_criteria: Vec<RollbackCriteria>,\n    pub quality_deltas: Vec<QualityDelta>,\n    pub summary: CorrectionSummary,\n}\n\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct CorrectionSummary {\n    pub trivial_count: usize,\n    pub moderate_count: usize,\n    pub complex_count: usize,\n    pub total_predicted_violations: usize,\n    pub average_confidence: f64,\n    pub estimated_total_fix_time_seconds: u32,\n}\n\n#[derive(Clone, Debug)]\npub struct IntelligenceState<'a> {\n    pub root: PathBuf,\n    pub invariants: &'a InvariantAnalysisResult,\n    pub call_graph: &'a HashMap<String, CallGraphNode>,\n    pub elements: &'a [CodeElement],\n    pub metrics: Metrics,\n}\n\npub fn build_state<'a>(\n    root: &'a Path,\n    analysis: &'a AnalysisResult,\n    metrics: Metrics,\n) -> IntelligenceState<'a> {\n    IntelligenceState {\n        root: root.to_path_buf(),\n        invariants: &analysis.invariants,\n        call_graph: &analysis.call_graph,\n        elements: &analysis.elements,\n        metrics,\n    }\n}\n\npub fn generate_intelligence_report(\n    actions: &[RefactorAction],\n    state: &IntelligenceState<'_>,\n) -> CorrectionIntelligenceReport {\n    let mut plans = Vec::new();\n    let mut policies = Vec::new();\n    let mut criteria = Vec::new();\n    let mut deltas = Vec::new();\n\n    for action in actions {\n        let mut predictions =\n            predict_violations(action, state.invariants, state.call_graph, state.elements);\n        fill_prediction_confidence(&mut predictions);\n        let plan = generate_correction_plan(action, &predictions);\n        let policy = plan_verification_scope(action, &plan);\n        let rollback = build_rollback_criteria(action, &plan);\n        let delta = estimate_impact(action, &ImpactState {\n            metrics: state.metrics.clone(),\n        });\n\n        plans.push(plan);\n        policies.push(policy);\n        criteria.push(rollback);\n        deltas.push(delta);\n    }\n\n    let summary = compute_summary(&plans, &deltas);\n\n    CorrectionIntelligenceReport {\n        version: \"1.0\".to_string(),\n        timestamp: chrono::Utc::now().to_rfc3339(),\n        project_root: state.root.clone(),\n        actions_analyzed: actions.len(),\n        correction_plans: plans,\n        verification_policies: policies,\n        rollback_criteria: criteria,\n        quality_deltas: deltas,\n        summary,\n    }\n}\n\npub fn write_intelligence_outputs(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n) -> std::io::Result<()> {\n    write_intelligence_outputs_at(report, output_dir, None, None)\n}\n\npub fn write_intelligence_outputs_at(\n    report: &CorrectionIntelligenceReport,\n    output_dir: &Path,\n    correction_json: Option<&Path>,\n    verification_policy_json: Option<&Path>,\n) -> std::io::Result<()> {\n    std::fs::create_dir_all(output_dir)?;\n    let json_path = correction_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"correction_intelligence.json\"));\n    if let Some(parent) = json_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    let contract = serialize_correction_plans(report);\n    std::fs::write(&json_path, serde_json::to_string_pretty(&contract)?)?;\n\n    let policy_path = verification_policy_json\n        .map(|p| p.to_path_buf())\n        .unwrap_or_else(|| output_dir.join(\"verification_policy.json\"));\n    if let Some(parent) = policy_path.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    emit_verification_policy(&report.verification_policies, &policy_path)?;\n    Ok(())\n}\n\npub fn serialize_correction_plans(\n    report: &CorrectionIntelligenceReport,\n) -> serde_json::Value {\n    let items = report\n        .correction_plans\n        .iter()\n        .zip(report.verification_policies.iter())\n        .zip(report.rollback_criteria.iter())\n        .map(|((plan, policy), rollback)| serialize_correction_plan(plan, policy, rollback))\n        .collect::<Vec<_>>();\n    json!({\n        \"version\": report.version,\n        \"timestamp\": report.timestamp,\n        \"project_root\": report.project_root,\n        \"actions_analyzed\": report.actions_analyzed,\n        \"correction_plans\": items,\n        \"quality_deltas\": report.quality_deltas,\n        \"summary\": report.summary,\n    })\n}\n\nfn compute_summary(plans: &[CorrectionPlan], deltas: &[QualityDelta]) -> CorrectionSummary {\n    let mut trivial = 0;\n    let mut moderate = 0;\n    let mut complex = 0;\n    let mut total_violations = 0;\n    let mut total_confidence = 0.0;\n    let mut total_time = 0;\n\n    for plan in plans {\n        match plan.tier {\n            ErrorTier::Trivial => trivial += 1,\n            ErrorTier::Moderate => moderate += 1,\n            ErrorTier::Complex => complex += 1,\n        }\n        total_violations += plan.predicted_violations.len();\n        total_confidence += plan.confidence;\n        total_time += plan.estimated_fix_time_seconds;\n    }\n\n    let avg_conf = if plans.is_empty() {\n        0.0\n    } else {\n        total_confidence / plans.len() as f64\n    };\n\n    let _ = deltas;\n\n    CorrectionSummary {\n        trivial_count: trivial,\n        moderate_count: moderate,\n        complex_count: complex,\n        total_predicted_violations: total_violations,\n        average_confidence: avg_conf,\n        estimated_total_fix_time_seconds: total_time,\n    }\n}\n\nfn fill_prediction_confidence(predictions: &mut [ViolationPrediction]) {\n    for prediction in predictions {\n        if prediction.confidence <= 0.0 {\n            prediction.confidence = default_confidence(&prediction.violation_type);\n        }\n    }\n}\n\nfn default_confidence(violation_type: &crate::correction_plan_types::ViolationType) -> f64 {\n    match violation_type {\n        crate::correction_plan_types::ViolationType::UnresolvedImport => 0.95,\n        crate::correction_plan_types::ViolationType::NameCollision => 1.0,\n        crate::correction_plan_types::ViolationType::LayerViolation => 0.9,\n        crate::correction_plan_types::ViolationType::BrokenReference => 0.85,\n        crate::correction_plan_types::ViolationType::TypeMismatch => 0.6,\n        crate::correction_plan_types::ViolationType::OwnershipIssue => 0.5,\n    }\n}\n",
          "updated_content": null
        }
      ],
      "applied": false,
      "errors": []
    }
  ]
}